<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js data-theme-init><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=color-scheme content="light dark"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#000000"><style>html{background:Canvas;color:CanvasText}@media(prefers-color-scheme:dark){html{background:#0b0d12;color:#e6e6e6}}html[data-theme-init] *{transition:none!important}</style><script>(function(){const t="td-color-theme",n=localStorage.getItem(t);let e=n||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");e==="auto"&&(e=window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"),document.documentElement.setAttribute("data-bs-theme",e)})()</script><link rel=canonical type=text/html href=https://pigsty.io/docs/node/><link rel=alternate type=application/rss+xml href=https://pigsty.io/docs/node/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Module: NODE | PIGSTY</title><meta name=description content="Tune nodes into the desired state and monitor it, manage node, VIP, HAProxy, and exporters."><meta property="og:url" content="https://pigsty.io/docs/node/"><meta property="og:site_name" content="PIGSTY"><meta property="og:title" content="Module: NODE"><meta property="og:description" content="Tune nodes into the desired state and monitor it, manage node, VIP, HAProxy, and exporters."><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta itemprop=name content="Module: NODE"><meta itemprop=description content="Tune nodes into the desired state and monitor it, manage node, VIP, HAProxy, and exporters."><meta itemprop=dateModified content="2026-01-08T18:48:49+08:00"><meta itemprop=wordCount content="15"><meta itemprop=keywords content="Reference"><meta name=twitter:card content="summary"><meta name=twitter:title content="Module: NODE"><meta name=twitter:description content="Tune nodes into the desired state and monitor it, manage node, VIP, HAProxy, and exporters."><link rel=preload href=/scss/main.min.f6da858eb22dc48110abb983f095554930a5a614c30190b4e32ee7b08a496331.css as=style integrity="sha256-9tqFjrItxIEQq7mD8JVVSTClphTDAZC04y7nsIpJYzE=" crossorigin=anonymous><link href=/scss/main.min.f6da858eb22dc48110abb983f095554930a5a614c30190b4e32ee7b08a496331.css rel=stylesheet integrity="sha256-9tqFjrItxIEQq7mD8JVVSTClphTDAZC04y7nsIpJYzE=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-LG1V9WTKGE"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-LG1V9WTKGE")}</script></head><body class=td-section><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="td-navbar-container container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"><svg viewBox="0 0 24 24" width="24" height="24"><defs/><g id="32" fill="none" stroke-dasharray="none" fill-opacity="1" stroke-opacity="1" stroke="none"><title>32</title><g id="32_图层_2"><title>图层 2</title><g id="Group_17"><g id="Graphic_16"/><g id="Graphic_15"><path d="M7.666187 11.971335l2.165064-3.75H14.16138l2.165065 3.75-2.165065 3.75H9.831251z" fill="#bbb" fill-opacity=".9526367"/><path d="M7.666187 11.971335l2.165064-3.75H14.16138l2.165065 3.75-2.165065 3.75H9.831251z" stroke="#fff" stroke-linecap="round" stroke-linejoin="round" stroke-width=".6730434"/></g><g id="Graphic_14"><path d="M7.666187 19.474806l2.165064-3.75H14.16138l2.165065 3.75-2.165065 3.75H9.831251z" fill="#de372c" fill-opacity=".8545852"/><path d="M7.666187 19.474806l2.165064-3.75H14.16138l2.165065 3.75-2.165065 3.75H9.831251z" stroke="#fff" stroke-linecap="round" stroke-linejoin="round" stroke-width=".6730434"/></g><g id="Graphic_13"><path d="M14.161476 15.751202l2.165064-3.75h4.33013l2.165064 3.75-2.165064 3.75H16.32654z" fill="#424242" fill-opacity=".9016462"/><path d="M14.161476 15.751202l2.165064-3.75h4.33013l2.165064 3.75-2.165064 3.75H16.32654z" stroke="#fff" stroke-linecap="round" stroke-linejoin="round" stroke-width=".6730434"/></g><g id="Graphic_12"><path d="M14.161476 8.226008 16.32654 4.4760076h4.33013L22.821734 8.226008l-2.165064 3.75H16.32654z" fill="#ffa269" fill-opacity=".8975772"/><path d="M14.161476 8.226008 16.32654 4.4760076h4.33013L22.821734 8.226008l-2.165064 3.75H16.32654z" stroke="#fff" stroke-linecap="round" stroke-linejoin="round" stroke-width=".6730434"/></g><g id="Graphic_11"><path d="M7.666187 4.5 9.831251.75H14.16138L16.326445 4.5 14.16138 8.25H9.831251z" fill="#419edb" fill-opacity=".8979957"/><path d="M7.666187 4.5 9.831251.75H14.16138L16.326445 4.5 14.16138 8.25H9.831251z" stroke="#fff" stroke-linecap="round" stroke-linejoin="round" stroke-width=".6730434"/></g><g id="Graphic_10"><path d="M1.1491742 8.226008 3.3142388 4.4760076H7.644368L9.809432 8.226008l-2.165064 3.75H3.3142388z" fill="#2f6793" fill-opacity=".9002511"/><path d="M1.1491742 8.226008 3.3142388 4.4760076H7.644368L9.809432 8.226008l-2.165064 3.75H3.3142388z" stroke="#fff" stroke-linecap="round" stroke-linejoin="round" stroke-width=".6730434"/></g><g id="Graphic_9"><path d="M1.182071 15.741036l2.1650645-3.75h4.330129l2.1650645 3.75-2.1650645 3.75H3.3471355z" fill="#53ac79" fill-opacity=".9"/><path d="M1.182071 15.741036l2.1650645-3.75h4.330129l2.1650645 3.75-2.1650645 3.75H3.3471355z" stroke="#fff" stroke-linecap="round" stroke-linejoin="round" stroke-width=".6730434"/></g></g></g></g></svg></span><span class=navbar-brand__name>PIGSTY</span></a><div class="td-navbar-nav-scroll td-navbar-nav-scroll--indicator" id=main_navbar><div class="scroll-indicator scroll-left"></div><ul class=navbar-nav><li class=nav-item><a class=nav-link href=/docs/><i class='fa-solid fa-book'></i><span>Docs</span></a></li><li class=nav-item><a class=nav-link href=/blog/><i class="fas fa-blog"></i><span>Blog</span></a></li><li class="nav-item dropdown d-none d-lg-block td-navbar__version-menu"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Ver</a><ul class=dropdown-menu><li><a class=dropdown-item href=https://pigsty.cc>Pigsty 中文网站</a></li><li><a class=dropdown-item href=https://doc.pgsty.com>Pigsty v3.7 Doc</a></li><li><a class=dropdown-item href=https://v34.pigsty.cc>Pigsty v3.4 Doc</a></li><li><a class=dropdown-item href=https://v27.pigsty.cc>Pigsty v2.7 Doc</a></li><li><a class=dropdown-item href=https://v15.pigsty.cc/docs>Pigsty v1.5 Doc</a></li></ul></div></li><li class="nav-item td-navbar__light-dark-menu"><div class="td-light-dark-menu dropdown"><svg class="d-none"><symbol id="check2" viewBox="0 0 16 16"><path d="M13.854 3.646a.5.5.0 010 .708l-7 7a.5.5.0 01-.708.0l-3.5-3.5a.5.5.0 11.708-.708L6.5 10.293l6.646-6.647a.5.5.0 01.708.0z"/></symbol><symbol id="circle-half" viewBox="0 0 16 16"><path d="M8 15A7 7 0 108 1v14zm0 1A8 8 0 118 0a8 8 0 010 16z"/></symbol><symbol id="moon-stars-fill" viewBox="0 0 16 16"><path d="M6 .278a.768.768.0 01.08.858 7.208 7.208.0 00-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527.0 1.04-.055 1.533-.16a.787.787.0 01.81.316.733.733.0 01-.031.893A8.349 8.349.0 018.344 16C3.734 16 0 12.286.0 7.71.0 4.266 2.114 1.312 5.124.06A.752.752.0 016 .278z"/><path d="M10.794 3.148a.217.217.0 01.412.0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217.0 010 .412l-1.162.387A1.734 1.734.0 0011.593 7.69l-.387 1.162a.217.217.0 01-.412.0l-.387-1.162A1.734 1.734.0 009.31 6.593l-1.162-.387a.217.217.0 010-.412l1.162-.387a1.734 1.734.0 001.097-1.097l.387-1.162zM13.863.099a.145.145.0 01.274.0l.258.774c.115.346.386.617.732.732l.774.258a.145.145.0 010 .274l-.774.258a1.156 1.156.0 00-.732.732l-.258.774a.145.145.0 01-.274.0l-.258-.774a1.156 1.156.0 00-.732-.732l-.774-.258a.145.145.0 010-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"/></symbol><symbol id="sun-fill" viewBox="0 0 16 16"><path d="M8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.5.5.0 01.5.5v2a.5.5.0 01-1 0v-2A.5.5.0 018 0zm0 13a.5.5.0 01.5.5v2a.5.5.0 01-1 0v-2A.5.5.0 018 13zm8-5a.5.5.0 01-.5.5h-2a.5.5.0 010-1h2a.5.5.0 01.5.5zM3 8a.5.5.0 01-.5.5h-2a.5.5.0 010-1h2A.5.5.0 013 8zm10.657-5.657a.5.5.0 010 .707l-1.414 1.415a.5.5.0 11-.707-.708l1.414-1.414a.5.5.0 01.707.0zm-9.193 9.193a.5.5.0 010 .707L3.05 13.657a.5.5.0 01-.707-.707l1.414-1.414a.5.5.0 01.707.0zm9.193 2.121a.5.5.0 01-.707.0l-1.414-1.414a.5.5.0 01.707-.707l1.414 1.414a.5.5.0 010 .707zM4.464 4.465a.5.5.0 01-.707.0L2.343 3.05a.5.5.0 11.707-.707l1.414 1.414a.5.5.0 010 .708z"/></symbol></svg>
<button class="btn btn-link nav-link dropdown-toggle d-flex align-items-center" id=bd-theme type=button aria-expanded=false data-bs-toggle=dropdown aria-label="Toggle theme (auto)">
<svg class="bi my-1 theme-icon-active"><use href="#circle-half"/></svg></button><ul class=dropdown-menu aria-labelledby=bd-theme><li><button type=button class="dropdown-item d-flex align-items-center" data-bs-theme-value=light aria-pressed=false>
<svg class="bi me-2 opacity-50"><use href="#sun-fill"/></svg>
Light
<svg class="bi ms-auto d-none"><use href="#check2"/></svg></button></li><li><button type=button class="dropdown-item d-flex align-items-center" data-bs-theme-value=dark aria-pressed=false>
<svg class="bi me-2 opacity-50"><use href="#moon-stars-fill"/></svg>
Dark
<svg class="bi ms-auto d-none"><use href="#check2"/></svg></button></li><li><button type=button class="dropdown-item d-flex align-items-center active" data-bs-theme-value=auto aria-pressed=true>
<svg class="bi me-2 opacity-50"><use href="#circle-half"/></svg>
Auto
<svg class="bi ms-auto d-none"><use href="#check2"/></svg></button></li></ul></div></li><li class=nav-item><a class=nav-link href=# onclick="return switchLang(),!1" title="Switch Language / 切换语言"><i class="fas fa-language"></i></a></li></ul><div class="scroll-indicator scroll-right"></div></div><div class="d-none d-lg-block td-navbar__search"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.d757e151050fd40088751a74721d74f6.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav><script>function switchLang(){var t=window.location.host,e=window.location.pathname+window.location.search+window.location.hash;t.indexOf("pigsty.io")!==-1?window.location.href="https://pigsty.cc"+e:t.indexOf("pigsty.cc")!==-1?window.location.href="https://pigsty.io"+e:window.location.href="https://pigsty.cc"+e}</script></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/node/>Return to the regular view of this page</a>.</p></div><h1 class=title>Module: NODE</h1><div class=lead>Tune nodes into the desired state and monitor it, manage node, VIP, HAProxy, and exporters.</div><ul><li>1: <a href=#pg-7d9d56f5e4ad9aceb9988138f1b29161>Configuration</a></li><li>2: <a href=#pg-9bce3100e169915a4d53169df43c90f4>Parameters</a></li><li>3: <a href=#pg-82bbb07e4b385ccd2f29e3417328b1c8>Playbook</a></li><li>4: <a href=#pg-f85610345b10eb3ebcd5b3f4d040bbb5>Administration</a></li><li>5: <a href=#pg-09468f551281e08aa947b92b9aa4f1a7>Monitoring</a></li><li>6: <a href=#pg-21159d62f30ba4efa931b151d5790100>Metrics</a></li><li>7: <a href=#pg-239230a82e0955081c7a4e1b76e4c445>FAQ</a></li></ul><div class=content><p>Tune nodes into the desired state and monitor it, manage node, VIP, HAProxy, and exporters.</p></div></div><div class=td-content><h1 id=pg-7d9d56f5e4ad9aceb9988138f1b29161>1 - Configuration</h1><div class=lead>Configure node identity, cluster, and identity borrowing from PostgreSQL</div><p>Pigsty uses <strong>IP address</strong> as the unique identifier for <strong>nodes</strong>. <strong>This IP should be the internal IP address on which the database instance listens and provides external services</strong>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>node-test</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span><span style=color:#204a87;font-weight:700>hosts</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>10.10.10.11</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span>{<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>nodename</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>node-test-1 }</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>10.10.10.12</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span>{<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>nodename</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>node-test-2 }</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>10.10.10.13</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span>{<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>nodename</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>node-test-3 }</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span><span style=color:#204a87;font-weight:700>vars</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>node_cluster</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>node-test</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><p>This IP address must be the address on which the database instance listens and provides external services, but should not be a public IP address. That said, you don&rsquo;t necessarily have to connect to the database via this IP. For example, managing target nodes indirectly through SSH tunnels or jump hosts is also feasible. However, when identifying database nodes, the primary IPv4 address remains the node&rsquo;s core identifier. <strong>This is critical, and you should ensure this during configuration</strong>.</p><p>The IP address is the <code>inventory_hostname</code> in the inventory, represented as the <code>key</code> in the <code>&lt;cluster>.hosts</code> object. In addition, each node has two optional identity parameters:</p><table><thead><tr><th style=text-align:center>Name</th><th style=text-align:center>Type</th><th style=text-align:center>Level</th><th>Necessity</th><th>Comment</th></tr></thead><tbody><tr><td style=text-align:center><code>inventory_hostname</code></td><td style=text-align:center><code>ip</code></td><td style=text-align:center><strong>-</strong></td><td><strong>Required</strong></td><td><strong>Node IP</strong></td></tr><tr><td style=text-align:center><a href=/docs/node/param/#nodename><code>nodename</code></a></td><td style=text-align:center><code>string</code></td><td style=text-align:center><strong>I</strong></td><td>Optional</td><td><strong>Node Name</strong></td></tr><tr><td style=text-align:center><a href=/docs/node/param/#node_cluster><code>node_cluster</code></a></td><td style=text-align:center><code>string</code></td><td style=text-align:center><strong>C</strong></td><td>Optional</td><td><strong>Node cluster name</strong></td></tr></tbody></table><p>The parameters <a href=/docs/node/param/#nodename><code>nodename</code></a> and <a href=/docs/node/param/#node_cluster><code>node_cluster</code></a> are optional. If not provided, the node&rsquo;s existing hostname and the fixed value <code>nodes</code> will be used as defaults. In Pigsty&rsquo;s monitoring system, these two will be used as the node&rsquo;s <strong>cluster identifier</strong> (<code>cls</code>) and <strong>instance identifier</strong> (<code>ins</code>).</p><p>For <a href=/docs/concept/arch/node#pgsql-node><strong>PGSQL nodes</strong></a>, because Pigsty defaults to a 1:1 exclusive deployment of PG to node, you can use the <a href=/docs/node/param/#node_id_from_pg><strong><code>node_id_from_pg</code></strong></a> parameter to borrow the PostgreSQL instance&rsquo;s identity parameters (<a href=/docs/pgsql/param/#pg_cluster><code>pg_cluster</code></a> and <a href=/docs/pgsql/param/#pg_seq><code>pg_seq</code></a>) for the node&rsquo;s <code>ins</code> and <code>cls</code> labels. This allows database and node monitoring metrics to share the same labels for cross-analysis.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#nodename:                # [instance] # node instance identity, uses existing hostname if missing, optional</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_cluster</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>nodes       # [cluster]</span><span style=color:#f8f8f8> </span><span style=color:#8f5902;font-style:italic># node cluster identity, uses &#39;nodes&#39; if missing, optional</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>nodename_overwrite</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8>          </span><span style=color:#8f5902;font-style:italic># overwrite node&#39;s hostname with nodename?</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>nodename_exchange</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>false</span><span style=color:#f8f8f8>          </span><span style=color:#8f5902;font-style:italic># exchange nodename among play hosts?</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_id_from_pg</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8>             </span><span style=color:#8f5902;font-style:italic># borrow postgres identity as node identity if applicable?</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><p>You can also configure rich functionality for host clusters. For example, use HAProxy on the node cluster for load balancing and service exposure, or bind an L2 VIP to the cluster.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9bce3100e169915a4d53169df43c90f4>2 - Parameters</h1><div class=lead>NODE module provides 11 sections with 83 parameters</div><p>The <a href=/docs/node>NODE</a> module tunes target nodes into the desired state and integrates them into the Pigsty monitoring system.</p><hr><table><thead><tr><th style=text-align:left>Parameter Section</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><a href=#node_id><code>NODE_ID</code></a></td><td style=text-align:left>Node identity parameters</td></tr><tr><td style=text-align:left><a href=#node_dns><code>NODE_DNS</code></a></td><td style=text-align:left>Node DNS resolution</td></tr><tr><td style=text-align:left><a href=#node_package><code>NODE_PACKAGE</code></a></td><td style=text-align:left>Upstream repo & package install</td></tr><tr><td style=text-align:left><a href=#node_tune><code>NODE_TUNE</code></a></td><td style=text-align:left>Node tuning & kernel features</td></tr><tr><td style=text-align:left><a href=#node_sec><code>NODE_SEC</code></a></td><td style=text-align:left>Node security configurations</td></tr><tr><td style=text-align:left><a href=#node_admin><code>NODE_ADMIN</code></a></td><td style=text-align:left>Admin user & SSH keys</td></tr><tr><td style=text-align:left><a href=#node_time><code>NODE_TIME</code></a></td><td style=text-align:left>Timezone, NTP, crontab</td></tr><tr><td style=text-align:left><a href=#node_vip><code>NODE_VIP</code></a></td><td style=text-align:left>Optional L2 VIP for cluster</td></tr><tr><td style=text-align:left><a href=#haproxy><code>HAPROXY</code></a></td><td style=text-align:left>HAProxy load balancer</td></tr><tr><td style=text-align:left><a href=#node_exporter><code>NODE_EXPORTER</code></a></td><td style=text-align:left>Node monitoring exporter</td></tr><tr><td style=text-align:left><a href=#vector><code>VECTOR</code></a></td><td style=text-align:left>Vector log collector</td></tr></tbody></table><hr><h2 id=parameters-overview>Parameters Overview</h2><p><a href=#node_id><code>NODE_ID</code></a> section defines node identity parameters, including node name, cluster name, and whether to borrow identity from PostgreSQL.</p><table><thead><tr><th style=text-align:left>Parameter</th><th style=text-align:center>Type</th><th style=text-align:center>Level</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><a href=#nodename><code>nodename</code></a></td><td style=text-align:center><code>string</code></td><td style=text-align:center><code>I</code></td><td style=text-align:left>node instance identity, use hostname if missing</td></tr><tr><td style=text-align:left><a href=#node_cluster><code>node_cluster</code></a></td><td style=text-align:center><code>string</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>node cluster identity, use &rsquo;nodes&rsquo; if missing</td></tr><tr><td style=text-align:left><a href=#nodename_overwrite><code>nodename_overwrite</code></a></td><td style=text-align:center><code>bool</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>overwrite node&rsquo;s hostname with nodename?</td></tr><tr><td style=text-align:left><a href=#nodename_exchange><code>nodename_exchange</code></a></td><td style=text-align:center><code>bool</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>exchange nodename among play hosts?</td></tr><tr><td style=text-align:left><a href=#node_id_from_pg><code>node_id_from_pg</code></a></td><td style=text-align:center><code>bool</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>use postgres identity as node identity if applicable?</td></tr></tbody></table><p><a href=#node_dns><code>NODE_DNS</code></a> section configures node DNS resolution, including static hosts records and dynamic DNS servers.</p><table><thead><tr><th style=text-align:left>Parameter</th><th style=text-align:center>Type</th><th style=text-align:center>Level</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><a href=#node_write_etc_hosts><code>node_write_etc_hosts</code></a></td><td style=text-align:center><code>bool</code></td><td style=text-align:center><code>G/C/I</code></td><td style=text-align:left>modify <code>/etc/hosts</code> on target node?</td></tr><tr><td style=text-align:left><a href=#node_default_etc_hosts><code>node_default_etc_hosts</code></a></td><td style=text-align:center><code>string[]</code></td><td style=text-align:center><code>G</code></td><td style=text-align:left>static dns records in <code>/etc/hosts</code></td></tr><tr><td style=text-align:left><a href=#node_etc_hosts><code>node_etc_hosts</code></a></td><td style=text-align:center><code>string[]</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>extra static dns records in <code>/etc/hosts</code></td></tr><tr><td style=text-align:left><a href=#node_dns_method><code>node_dns_method</code></a></td><td style=text-align:center><code>enum</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>how to handle dns servers: add,none,overwrite</td></tr><tr><td style=text-align:left><a href=#node_dns_servers><code>node_dns_servers</code></a></td><td style=text-align:center><code>string[]</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>dynamic nameserver in <code>/etc/resolv.conf</code></td></tr><tr><td style=text-align:left><a href=#node_dns_options><code>node_dns_options</code></a></td><td style=text-align:center><code>string[]</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>dns resolv options in <code>/etc/resolv.conf</code></td></tr></tbody></table><p><a href=#node_package><code>NODE_PACKAGE</code></a> section configures node software repositories and package installation.</p><table><thead><tr><th style=text-align:left>Parameter</th><th style=text-align:center>Type</th><th style=text-align:center>Level</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><a href=#node_repo_modules><code>node_repo_modules</code></a></td><td style=text-align:center><code>enum</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>which repo modules to enable on node? local default</td></tr><tr><td style=text-align:left><a href=#node_repo_remove><code>node_repo_remove</code></a></td><td style=text-align:center><code>bool</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>remove existing repo on node when configuring?</td></tr><tr><td style=text-align:left><a href=#node_packages><code>node_packages</code></a></td><td style=text-align:center><code>string[]</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>packages to be installed on current nodes</td></tr><tr><td style=text-align:left><a href=#node_default_packages><code>node_default_packages</code></a></td><td style=text-align:center><code>string[]</code></td><td style=text-align:center><code>G</code></td><td style=text-align:left>default packages to be installed on all nodes</td></tr></tbody></table><p><a href=#node_tune><code>NODE_TUNE</code></a> section configures node kernel parameters, feature toggles, and tuning templates.</p><table><thead><tr><th style=text-align:left>Parameter</th><th style=text-align:center>Type</th><th style=text-align:center>Level</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><a href=#node_disable_numa><code>node_disable_numa</code></a></td><td style=text-align:center><code>bool</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>disable node numa, reboot required</td></tr><tr><td style=text-align:left><a href=#node_disable_swap><code>node_disable_swap</code></a></td><td style=text-align:center><code>bool</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>disable node swap, use with caution</td></tr><tr><td style=text-align:left><a href=#node_static_network><code>node_static_network</code></a></td><td style=text-align:center><code>bool</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>preserve dns resolver settings after reboot</td></tr><tr><td style=text-align:left><a href=#node_disk_prefetch><code>node_disk_prefetch</code></a></td><td style=text-align:center><code>bool</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>setup disk prefetch on HDD to increase performance</td></tr><tr><td style=text-align:left><a href=#node_kernel_modules><code>node_kernel_modules</code></a></td><td style=text-align:center><code>string[]</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>kernel modules to be enabled on this node</td></tr><tr><td style=text-align:left><a href=#node_hugepage_count><code>node_hugepage_count</code></a></td><td style=text-align:center><code>int</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>number of 2MB hugepage, take precedence over ratio</td></tr><tr><td style=text-align:left><a href=#node_hugepage_ratio><code>node_hugepage_ratio</code></a></td><td style=text-align:center><code>float</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>node mem hugepage ratio, 0 disable it by default</td></tr><tr><td style=text-align:left><a href=#node_overcommit_ratio><code>node_overcommit_ratio</code></a></td><td style=text-align:center><code>float</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>node mem overcommit ratio (50-100), 0 disable it</td></tr><tr><td style=text-align:left><a href=#node_tune><code>node_tune</code></a></td><td style=text-align:center><code>enum</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>node tuned profile: none,oltp,olap,crit,tiny</td></tr><tr><td style=text-align:left><a href=#node_sysctl_params><code>node_sysctl_params</code></a></td><td style=text-align:center><code>dict</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>extra sysctl parameters in k:v format</td></tr></tbody></table><p><a href=#node_sec><code>NODE_SEC</code></a> section configures node security options, including SELinux and firewall.</p><table><thead><tr><th style=text-align:left>Parameter</th><th style=text-align:center>Type</th><th style=text-align:center>Level</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><a href=#node_selinux_mode><code>node_selinux_mode</code></a></td><td style=text-align:center><code>enum</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>SELinux mode: disabled, permissive, enforcing</td></tr><tr><td style=text-align:left><a href=#node_firewall_mode><code>node_firewall_mode</code></a></td><td style=text-align:center><code>enum</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>firewall mode: off, none, zone</td></tr><tr><td style=text-align:left><a href=#node_firewall_intranet><code>node_firewall_intranet</code></a></td><td style=text-align:center><code>cidr[]</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>intranet CIDR list for firewall rules</td></tr><tr><td style=text-align:left><a href=#node_firewall_public_port><code>node_firewall_public_port</code></a></td><td style=text-align:center><code>port[]</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>public exposed port list, default [22, 80, 443, 5432]</td></tr></tbody></table><p><a href=#node_admin><code>NODE_ADMIN</code></a> section configures admin user, data directory, and shell aliases.</p><table><thead><tr><th style=text-align:left>Parameter</th><th style=text-align:center>Type</th><th style=text-align:center>Level</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><a href=#node_data><code>node_data</code></a></td><td style=text-align:center><code>path</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>node main data directory, <code>/data</code> by default</td></tr><tr><td style=text-align:left><a href=#node_admin_enabled><code>node_admin_enabled</code></a></td><td style=text-align:center><code>bool</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>create a admin user on target node?</td></tr><tr><td style=text-align:left><a href=#node_admin_uid><code>node_admin_uid</code></a></td><td style=text-align:center><code>int</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>uid and gid for node admin user</td></tr><tr><td style=text-align:left><a href=#node_admin_username><code>node_admin_username</code></a></td><td style=text-align:center><code>username</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>name of node admin user, <code>dba</code> by default</td></tr><tr><td style=text-align:left><a href=#node_admin_sudo><code>node_admin_sudo</code></a></td><td style=text-align:center><code>enum</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>admin sudo privilege: limited, nopass, all, none</td></tr><tr><td style=text-align:left><a href=#node_admin_ssh_exchange><code>node_admin_ssh_exchange</code></a></td><td style=text-align:center><code>bool</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>exchange admin ssh key among node cluster</td></tr><tr><td style=text-align:left><a href=#node_admin_pk_current><code>node_admin_pk_current</code></a></td><td style=text-align:center><code>bool</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>add current user&rsquo;s ssh pk to admin authorized_keys</td></tr><tr><td style=text-align:left><a href=#node_admin_pk_list><code>node_admin_pk_list</code></a></td><td style=text-align:center><code>string[]</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>ssh public keys to be added to admin user</td></tr><tr><td style=text-align:left><a href=#node_aliases><code>node_aliases</code></a></td><td style=text-align:center><code>dict</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>shell aliases in K:V dict format</td></tr></tbody></table><p><a href=#node_time><code>NODE_TIME</code></a> section configures timezone, NTP time sync, and crontab.</p><table><thead><tr><th style=text-align:left>Parameter</th><th style=text-align:center>Type</th><th style=text-align:center>Level</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><a href=#node_timezone><code>node_timezone</code></a></td><td style=text-align:center><code>string</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>setup node timezone, empty string to skip</td></tr><tr><td style=text-align:left><a href=#node_ntp_enabled><code>node_ntp_enabled</code></a></td><td style=text-align:center><code>bool</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>enable chronyd time sync service?</td></tr><tr><td style=text-align:left><a href=#node_ntp_servers><code>node_ntp_servers</code></a></td><td style=text-align:center><code>string[]</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>ntp servers in <code>/etc/chrony.conf</code></td></tr><tr><td style=text-align:left><a href=#node_crontab_overwrite><code>node_crontab_overwrite</code></a></td><td style=text-align:center><code>bool</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>overwrite or append to <code>/etc/crontab</code>?</td></tr><tr><td style=text-align:left><a href=#node_crontab><code>node_crontab</code></a></td><td style=text-align:center><code>string[]</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>crontab entries in <code>/etc/crontab</code></td></tr></tbody></table><p><a href=#node_vip><code>NODE_VIP</code></a> section configures L2 VIP for node cluster, implemented by keepalived.</p><table><thead><tr><th style=text-align:left>Parameter</th><th style=text-align:center>Type</th><th style=text-align:center>Level</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><a href=#vip_enabled><code>vip_enabled</code></a></td><td style=text-align:center><code>bool</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>enable L2 vip on this node cluster?</td></tr><tr><td style=text-align:left><a href=#vip_address><code>vip_address</code></a></td><td style=text-align:center><code>ip</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>node vip address in ipv4 format, required if enabled</td></tr><tr><td style=text-align:left><a href=#vip_vrid><code>vip_vrid</code></a></td><td style=text-align:center><code>int</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>integer 1-254, should be unique in same VLAN</td></tr><tr><td style=text-align:left><a href=#vip_role><code>vip_role</code></a></td><td style=text-align:center><code>enum</code></td><td style=text-align:center><code>I</code></td><td style=text-align:left>optional, master/backup, backup by default</td></tr><tr><td style=text-align:left><a href=#vip_preempt><code>vip_preempt</code></a></td><td style=text-align:center><code>bool</code></td><td style=text-align:center><code>C/I</code></td><td style=text-align:left>optional, true/false, enable vip preemption</td></tr><tr><td style=text-align:left><a href=#vip_interface><code>vip_interface</code></a></td><td style=text-align:center><code>string</code></td><td style=text-align:center><code>C/I</code></td><td style=text-align:left>node vip network interface, <code>eth0</code> by default</td></tr><tr><td style=text-align:left><a href=#vip_dns_suffix><code>vip_dns_suffix</code></a></td><td style=text-align:center><code>string</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>node vip dns name suffix, empty string by default</td></tr><tr><td style=text-align:left><a href=#vip_auth_pass><code>vip_auth_pass</code></a></td><td style=text-align:center><code>password</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>vrrp authentication password, auto-generated if empty</td></tr><tr><td style=text-align:left><a href=#vip_exporter_port><code>vip_exporter_port</code></a></td><td style=text-align:center><code>port</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>keepalived exporter listen port, 9650 by default</td></tr></tbody></table><p><a href=#haproxy><code>HAPROXY</code></a> section configures HAProxy load balancer and service exposure.</p><table><thead><tr><th style=text-align:left>Parameter</th><th style=text-align:center>Type</th><th style=text-align:center>Level</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><a href=#haproxy_enabled><code>haproxy_enabled</code></a></td><td style=text-align:center><code>bool</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>enable haproxy on this node?</td></tr><tr><td style=text-align:left><a href=#haproxy_clean><code>haproxy_clean</code></a></td><td style=text-align:center><code>bool</code></td><td style=text-align:center><code>G/C/A</code></td><td style=text-align:left>cleanup all existing haproxy config?</td></tr><tr><td style=text-align:left><a href=#haproxy_reload><code>haproxy_reload</code></a></td><td style=text-align:center><code>bool</code></td><td style=text-align:center><code>A</code></td><td style=text-align:left>reload haproxy after config?</td></tr><tr><td style=text-align:left><a href=#haproxy_auth_enabled><code>haproxy_auth_enabled</code></a></td><td style=text-align:center><code>bool</code></td><td style=text-align:center><code>G</code></td><td style=text-align:left>enable authentication for admin page</td></tr><tr><td style=text-align:left><a href=#haproxy_admin_username><code>haproxy_admin_username</code></a></td><td style=text-align:center><code>username</code></td><td style=text-align:center><code>G</code></td><td style=text-align:left>haproxy admin username, <code>admin</code> default</td></tr><tr><td style=text-align:left><a href=#haproxy_admin_password><code>haproxy_admin_password</code></a></td><td style=text-align:center><code>password</code></td><td style=text-align:center><code>G</code></td><td style=text-align:left>haproxy admin password, <code>pigsty</code> default</td></tr><tr><td style=text-align:left><a href=#haproxy_exporter_port><code>haproxy_exporter_port</code></a></td><td style=text-align:center><code>port</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>haproxy exporter port, 9101 by default</td></tr><tr><td style=text-align:left><a href=#haproxy_client_timeout><code>haproxy_client_timeout</code></a></td><td style=text-align:center><code>interval</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>client connection timeout, 24h default</td></tr><tr><td style=text-align:left><a href=#haproxy_server_timeout><code>haproxy_server_timeout</code></a></td><td style=text-align:center><code>interval</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>server connection timeout, 24h default</td></tr><tr><td style=text-align:left><a href=#haproxy_services><code>haproxy_services</code></a></td><td style=text-align:center><code>service[]</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>list of haproxy services to expose</td></tr></tbody></table><p><a href=#node_exporter><code>NODE_EXPORTER</code></a> section configures node monitoring exporter.</p><table><thead><tr><th style=text-align:left>Parameter</th><th style=text-align:center>Type</th><th style=text-align:center>Level</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><a href=#node_exporter_enabled><code>node_exporter_enabled</code></a></td><td style=text-align:center><code>bool</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>setup node_exporter on this node?</td></tr><tr><td style=text-align:left><a href=#node_exporter_port><code>node_exporter_port</code></a></td><td style=text-align:center><code>port</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>node exporter listen port, 9100 default</td></tr><tr><td style=text-align:left><a href=#node_exporter_options><code>node_exporter_options</code></a></td><td style=text-align:center><code>arg</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>extra server options for node_exporter</td></tr></tbody></table><p><a href=#vector><code>VECTOR</code></a> section configures Vector log collector.</p><table><thead><tr><th style=text-align:left>Parameter</th><th style=text-align:center>Type</th><th style=text-align:center>Level</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><a href=#vector_enabled><code>vector_enabled</code></a></td><td style=text-align:center><code>bool</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>enable vector log collector?</td></tr><tr><td style=text-align:left><a href=#vector_clean><code>vector_clean</code></a></td><td style=text-align:center><code>bool</code></td><td style=text-align:center><code>G/A</code></td><td style=text-align:left>purge vector data dir during init?</td></tr><tr><td style=text-align:left><a href=#vector_data><code>vector_data</code></a></td><td style=text-align:center><code>path</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>vector data directory, /data/vector default</td></tr><tr><td style=text-align:left><a href=#vector_port><code>vector_port</code></a></td><td style=text-align:center><code>port</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>vector metrics listen port, 9598 default</td></tr><tr><td style=text-align:left><a href=#vector_read_from><code>vector_read_from</code></a></td><td style=text-align:center><code>enum</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>read log from beginning or end</td></tr><tr><td style=text-align:left><a href=#vector_log_endpoint><code>vector_log_endpoint</code></a></td><td style=text-align:center><code>string[]</code></td><td style=text-align:center><code>C</code></td><td style=text-align:left>log endpoint, default send to infra group</td></tr></tbody></table><hr><h2 id=node_id><code>NODE_ID</code></h2><p>Each node has <strong>identity parameters</strong> that are configured through the parameters in <code>&lt;cluster>.hosts</code> and <code>&lt;cluster>.vars</code>.</p><p>Pigsty uses <strong>IP address</strong> as the unique identifier for <strong>database nodes</strong>. <strong>This IP address must be the one that the database instance listens on and provides services</strong>, but should not be a public IP address.
However, users don&rsquo;t have to connect to the database via this IP address. For example, managing target nodes indirectly through SSH tunnels or jump servers is feasible.
When identifying database nodes, the primary IPv4 address remains the core identifier. <strong>This is very important, and users should ensure this when configuring</strong>.
The IP address is the <code>inventory_hostname</code> in the inventory, which is the <code>key</code> of the <code>&lt;cluster>.hosts</code> object.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>node-test</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span><span style=color:#204a87;font-weight:700>hosts</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>10.10.10.11</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span>{<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>nodename</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>node-test-1 }</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>10.10.10.12</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span>{<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>nodename</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>node-test-2 }</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>10.10.10.13</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span>{<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>nodename</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>node-test-3 }</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span><span style=color:#204a87;font-weight:700>vars</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>node_cluster</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>node-test</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><p>In addition, nodes have two important identity parameters in the Pigsty monitoring system: <a href=#nodename><code>nodename</code></a> and <a href=#node_cluster><code>node_cluster</code></a>, which are used as the <strong>instance identity</strong> (<code>ins</code>) and <strong>cluster identity</strong> (<code>cls</code>) in the monitoring system.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#000>node_load1{cls=&#34;pg-meta&#34;, ins=&#34;pg-meta-1&#34;, ip=&#34;10.10.10.10&#34;, job=&#34;nodes&#34;}</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#000>node_load1{cls=&#34;pg-test&#34;, ins=&#34;pg-test-1&#34;, ip=&#34;10.10.10.11&#34;, job=&#34;nodes&#34;}</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#000>node_load1{cls=&#34;pg-test&#34;, ins=&#34;pg-test-2&#34;, ip=&#34;10.10.10.12&#34;, job=&#34;nodes&#34;}</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#000>node_load1{cls=&#34;pg-test&#34;, ins=&#34;pg-test-3&#34;, ip=&#34;10.10.10.13&#34;, job=&#34;nodes&#34;}</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><p>When executing the default PostgreSQL deployment, since Pigsty uses exclusive 1:1 deployment by default, you can borrow the database instance&rsquo;s identity parameters (<a href=#pg_cluster><code>pg_cluster</code></a>) to the node&rsquo;s <code>ins</code> and <code>cls</code> labels through the <a href=#node_id_from_pg><code>node_id_from_pg</code></a> parameter.</p><table><thead><tr><th style=text-align:center>Name</th><th style=text-align:center>Type</th><th style=text-align:center>Level</th><th style=text-align:left>Required</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>inventory_hostname</code></td><td style=text-align:center><code>ip</code></td><td style=text-align:center><strong>-</strong></td><td style=text-align:left><strong>Required</strong></td><td style=text-align:left><strong>Node IP Address</strong></td></tr><tr><td style=text-align:center><a href=#nodename><code>nodename</code></a></td><td style=text-align:center><code>string</code></td><td style=text-align:center><strong>I</strong></td><td style=text-align:left>Optional</td><td style=text-align:left><strong>Node Name</strong></td></tr><tr><td style=text-align:center><a href=#node_cluster><code>node_cluster</code></a></td><td style=text-align:center><code>string</code></td><td style=text-align:center><strong>C</strong></td><td style=text-align:left>Optional</td><td style=text-align:left><strong>Node Cluster Name</strong></td></tr></tbody></table><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#nodename:                # [instance] # node instance identity, use hostname if missing, optional</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_cluster</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>nodes       # [cluster]</span><span style=color:#f8f8f8> </span><span style=color:#8f5902;font-style:italic># node cluster identity, use &#39;nodes&#39; if missing, optional</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>nodename_overwrite</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8>          </span><span style=color:#8f5902;font-style:italic># overwrite node&#39;s hostname with nodename?</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>nodename_exchange</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>false</span><span style=color:#f8f8f8>          </span><span style=color:#8f5902;font-style:italic># exchange nodename among play hosts?</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_id_from_pg</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8>             </span><span style=color:#8f5902;font-style:italic># use postgres identity as node identity if applicable?</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><h3 id=nodename><code>nodename</code></h3><p>name: <code>nodename</code>, type: <code>string</code>, level: <code>I</code></p><p>Node instance identity parameter. If not explicitly set, the existing hostname will be used as the node name. This parameter is optional since it has a reasonable default value.</p><p>If <a href=#node_id_from_pg><code>node_id_from_pg</code></a> is enabled (default), and <code>nodename</code> is not explicitly specified, <a href=#nodename><code>nodename</code></a> will try to use <code>${pg_cluster}-${pg_seq}</code> as the instance identity. If the PGSQL module is not defined on this cluster, it will fall back to the default, which is the node&rsquo;s HOSTNAME.</p><h3 id=node_cluster><code>node_cluster</code></h3><p>name: <code>node_cluster</code>, type: <code>string</code>, level: <code>C</code></p><p>This option allows explicitly specifying a cluster name for the node, which is only meaningful when defined at the node cluster level. Using the default empty value will use the fixed value <code>nodes</code> as the node cluster identity.</p><p>If <a href=#node_id_from_pg><code>node_id_from_pg</code></a> is enabled (default), and <code>node_cluster</code> is not explicitly specified, <a href=#node_cluster><code>node_cluster</code></a> will try to use <code>${pg_cluster}</code> as the cluster identity. If the PGSQL module is not defined on this cluster, it will fall back to the default <code>nodes</code>.</p><h3 id=nodename_overwrite><code>nodename_overwrite</code></h3><p>name: <code>nodename_overwrite</code>, type: <code>bool</code>, level: <code>C</code></p><p>Overwrite node&rsquo;s hostname with nodename? Default is <code>true</code>. In this case, if you set a non-empty <a href=#nodename><code>nodename</code></a>, it will be used as the current host&rsquo;s HOSTNAME.</p><p>When <code>nodename</code> is empty, if <a href=#node_id_from_pg><code>node_id_from_pg</code></a> is <code>true</code> (default), Pigsty will try to borrow the identity parameters of the PostgreSQL instance defined 1:1 on the node as the node name, i.e., <code>{{ pg_cluster }}-{{ pg_seq }}</code>. If the PGSQL module is not installed on this node, it will fall back to not doing anything.</p><p>Therefore, if you leave <a href=#nodename><code>nodename</code></a> empty and don&rsquo;t enable <a href=#node_id_from_pg><code>node_id_from_pg</code></a>, Pigsty will not make any changes to the existing hostname.</p><h3 id=nodename_exchange><code>nodename_exchange</code></h3><p>name: <code>nodename_exchange</code>, type: <code>bool</code>, level: <code>C</code></p><p>Exchange nodename among play hosts? Default is <code>false</code>.</p><p>When enabled, nodes executing the <a href=/docs/node#nodeyml><code>node.yml</code></a> playbook in the same batch will exchange node names with each other, writing them to <code>/etc/hosts</code>.</p><h3 id=node_id_from_pg><code>node_id_from_pg</code></h3><p>name: <code>node_id_from_pg</code>, type: <code>bool</code>, level: <code>C</code></p><p>Borrow identity parameters from the PostgreSQL instance/cluster deployed 1:1 on the node? Default is <code>true</code>.</p><p>PostgreSQL instances and nodes in Pigsty use 1:1 deployment by default, so you can &ldquo;borrow&rdquo; identity parameters from the database instance.
This parameter is enabled by default, meaning that if a PostgreSQL cluster has no special configuration, the host node cluster and instance identity parameters will default to matching the database identity parameters. This provides extra convenience for problem analysis and monitoring data processing.</p><hr><h2 id=node_dns><code>NODE_DNS</code></h2><p>Pigsty configures static DNS records and dynamic DNS servers for nodes.</p><p>If your node provider has already configured DNS servers for you, you can set <a href=#node_dns_method><code>node_dns_method</code></a> to <code>none</code> to skip DNS setup.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_write_etc_hosts</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8>        </span><span style=color:#8f5902;font-style:italic># modify `/etc/hosts` on target node?</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_default_etc_hosts</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>           </span><span style=color:#8f5902;font-style:italic># static dns records in `/etc/hosts`</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span>- <span style=color:#4e9a06>&#34;${admin_ip} i.pigsty&#34;</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_etc_hosts</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000;font-weight:700>[]</span><span style=color:#f8f8f8>                </span><span style=color:#8f5902;font-style:italic># extra static dns records in `/etc/hosts`</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_dns_method: add              # how to handle dns servers</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>add,none,overwrite</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_dns_servers</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000;font-weight:700>[</span><span style=color:#4e9a06>&#39;${admin_ip}&#39;</span><span style=color:#000;font-weight:700>]</span><span style=color:#f8f8f8> </span><span style=color:#8f5902;font-style:italic># dynamic nameserver in `/etc/resolv.conf`</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_dns_options</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>                 </span><span style=color:#8f5902;font-style:italic># dns resolv options in `/etc/resolv.conf`</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span>- <span style=color:#000>options single-request-reopen timeout:1</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><h3 id=node_write_etc_hosts>node_write_etc_hosts</h3><p>name: <code>node_write_etc_hosts</code>, type: <code>bool</code>, level: <code>G|C|I</code></p><p>Modify <code>/etc/hosts</code> on target node? For example, in container environments, this file usually cannot be modified.</p><h3 id=node_default_etc_hosts><code>node_default_etc_hosts</code></h3><p>name: <code>node_default_etc_hosts</code>, type: <code>string[]</code>, level: <code>G</code></p><p>Static DNS records to be written to all nodes&rsquo; <code>/etc/hosts</code>. Default value:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#000;font-weight:700>[</span><span style=color:#4e9a06>&#34;${admin_ip} i.pigsty&#34;</span><span style=color:#000;font-weight:700>]</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><p><a href=#node_default_etc_hosts><code>node_default_etc_hosts</code></a> is an array. Each element is a DNS record with format <code>&lt;ip> &lt;name></code>. You can specify multiple domain names separated by spaces.</p><p>This parameter is used to configure global static DNS records. If you want to configure specific static DNS records for individual clusters and instances, use the <a href=#node_etc_hosts><code>node_etc_hosts</code></a> parameter.</p><h3 id=node_etc_hosts><code>node_etc_hosts</code></h3><p>name: <code>node_etc_hosts</code>, type: <code>string[]</code>, level: <code>C</code></p><p>Extra static DNS records to write to node&rsquo;s <code>/etc/hosts</code>. Default is <code>[]</code> (empty array).</p><p>Same format as <a href=#node_default_etc_hosts><code>node_default_etc_hosts</code></a>, but suitable for configuration at the cluster/instance level.</p><h3 id=node_dns_method><code>node_dns_method</code></h3><p>name: <code>node_dns_method</code>, type: <code>enum</code>, level: <code>C</code></p><p>How to configure DNS servers? Three options: <code>add</code>, <code>none</code>, <code>overwrite</code>. Default is <code>add</code>.</p><ul><li><code>add</code>: <strong>Append</strong> the records in <a href=#node_dns_servers><code>node_dns_servers</code></a> to <code>/etc/resolv.conf</code> and keep existing DNS servers. (default)</li><li><code>overwrite</code>: Overwrite <code>/etc/resolv.conf</code> with the records in <a href=#node_dns_servers><code>node_dns_servers</code></a></li><li><code>none</code>: Skip DNS server configuration. If your environment already has DNS servers configured, you can skip DNS configuration directly.</li></ul><h3 id=node_dns_servers><code>node_dns_servers</code></h3><p>name: <code>node_dns_servers</code>, type: <code>string[]</code>, level: <code>C</code></p><p>Configure the dynamic DNS server list in <code>/etc/resolv.conf</code>. Default is <code>["${admin_ip}"]</code>, using the admin node as the primary DNS server.</p><h3 id=node_dns_options><code>node_dns_options</code></h3><p>name: <code>node_dns_options</code>, type: <code>string[]</code>, level: <code>C</code></p><p>DNS resolution options in <code>/etc/resolv.conf</code>. Default value:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>- <span style=color:#4e9a06>&#34;options single-request-reopen timeout:1&#34;</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><p>If <a href=#node_dns_method><code>node_dns_method</code></a> is configured as <code>add</code> or <code>overwrite</code>, the records in this configuration will be written to <code>/etc/resolv.conf</code> first. Refer to Linux documentation for <code>/etc/resolv.conf</code> format details.</p><hr><h2 id=node_package><code>NODE_PACKAGE</code></h2><p>Pigsty configures software repositories and installs packages on managed nodes.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_repo_modules</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>local         </span><span style=color:#f8f8f8> </span><span style=color:#8f5902;font-style:italic># upstream repo to be added on node, local by default.</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_repo_remove</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8>            </span><span style=color:#8f5902;font-style:italic># remove existing repo on node?</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_packages</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000;font-weight:700>[</span><span style=color:#000>openssh-server]  </span><span style=color:#f8f8f8> </span><span style=color:#8f5902;font-style:italic># packages to be installed current nodes with latest version</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#node_default_packages:           # default packages to be installed on all nodes</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><h3 id=node_repo_modules><code>node_repo_modules</code></h3><p>name: <code>node_repo_modules</code>, type: <code>string</code>, level: <code>C/A</code></p><p>List of software repository modules to be added on the node, same format as <a href=#repo_modules><code>repo_modules</code></a>. Default is <code>local</code>, using the local software repository specified in <a href=#repo_upstream><code>repo_upstream</code></a>.</p><p>When Pigsty manages nodes, it filters entries in <a href=#repo_upstream><code>repo_upstream</code></a> based on this parameter value. Only entries whose <code>module</code> field matches this parameter value will be added to the node&rsquo;s software sources.</p><h3 id=node_repo_remove><code>node_repo_remove</code></h3><p>name: <code>node_repo_remove</code>, type: <code>bool</code>, level: <code>C/A</code></p><p>Remove existing software repository definitions on the node? Default is <code>true</code>.</p><p>When enabled, Pigsty will <strong>remove</strong> existing configuration files in <code>/etc/yum.repos.d</code> on the node and back them up to <code>/etc/yum.repos.d/backup</code>.
On Debian/Ubuntu systems, it backs up <code>/etc/apt/sources.list(.d)</code> to <code>/etc/apt/backup</code>.</p><h3 id=node_packages><code>node_packages</code></h3><p>name: <code>node_packages</code>, type: <code>string[]</code>, level: <code>C</code></p><p>List of software packages to install and upgrade on the current node. Default is <code>[openssh-server]</code>, which upgrades sshd to the latest version during installation (to avoid security vulnerabilities).</p><p>Each array element is a string of comma-separated package names. Same format as <a href=#node_default_packages><code>node_default_packages</code></a>. This parameter is usually used to specify additional packages to install at the node/cluster level.</p><p>Packages specified in this parameter will be <strong>upgraded to the latest available version</strong>. If you need to keep existing node software versions unchanged (just ensure they exist), use the <a href=#node_default_packages><code>node_default_packages</code></a> parameter.</p><h3 id=node_default_packages><code>node_default_packages</code></h3><p>name: <code>node_default_packages</code>, type: <code>string[]</code>, level: <code>G</code></p><p>Default packages to be installed on all nodes. Default value is a common RPM package list for EL 7/8/9. Array where each element is a <strong>space-separated</strong> package list string.</p><p>Packages specified in this variable only require <strong>existence</strong>, not <strong>latest</strong>. If you need to install the latest version, use the <a href=#node_packages><code>node_packages</code></a> parameter.</p><p>This parameter has no default value (undefined state). If users don&rsquo;t explicitly specify this parameter in the configuration file, Pigsty will load default values from the <code>node_packages_default</code> variable defined in <a href=https://github.com/pgsty/pigsty/blob/main/roles/node_id/vars/><code>roles/node_id/vars</code></a> based on the current node&rsquo;s OS family.</p><p>Default value (EL-based systems):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>- <span style=color:#000>lz4,unzip,bzip2,pv,jq,git,ncdu,make,patch,bash,lsof,wget,uuid,tuned,nvme-cli,numactl,sysstat,iotop,htop,rsync,tcpdump</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span>- <span style=color:#000>python3,python3-pip,socat,lrzsz,net-tools,ipvsadm,telnet,ca-certificates,openssl,keepalived,etcd,haproxy,chrony,pig</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span>- <span style=color:#000>zlib,yum,audit,bind-utils,readline,vim-minimal,node_exporter,grubby,openssh-server,openssh-clients,chkconfig,vector</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><p>Default value (Debian/Ubuntu):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>- <span style=color:#000>lz4,unzip,bzip2,pv,jq,git,ncdu,make,patch,bash,lsof,wget,uuid,tuned,nvme-cli,numactl,sysstat,iotop,htop,rsync</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span>- <span style=color:#000>python3,python3-pip,socat,lrzsz,net-tools,ipvsadm,telnet,ca-certificates,openssl,keepalived,etcd,haproxy,chrony,pig</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span>- <span style=color:#000>zlib1g,acl,dnsutils,libreadline-dev,vim-tiny,node-exporter,openssh-server,openssh-client,vector</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><p>Same format as <a href=#node_packages><code>node_packages</code></a>, but this parameter is usually used to specify default packages that must be installed on all nodes at the global level.</p><hr><h2 id=node_tune><code>NODE_TUNE</code></h2><p>Host node features, kernel modules, and tuning templates.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_disable_numa</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>false</span><span style=color:#f8f8f8>          </span><span style=color:#8f5902;font-style:italic># disable node numa, reboot required</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_disable_swap</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>false</span><span style=color:#f8f8f8>          </span><span style=color:#8f5902;font-style:italic># disable node swap, use with caution</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_static_network</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8>         </span><span style=color:#8f5902;font-style:italic># preserve dns resolver settings after reboot</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_disk_prefetch</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>false</span><span style=color:#f8f8f8>         </span><span style=color:#8f5902;font-style:italic># setup disk prefetch on HDD to increase performance</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_kernel_modules</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000;font-weight:700>[</span><span style=color:#f8f8f8> </span><span style=color:#000>softdog, ip_vs, ip_vs_rr, ip_vs_wrr, ip_vs_sh ]</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_hugepage_count</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#0000cf;font-weight:700>0</span><span style=color:#f8f8f8>            </span><span style=color:#8f5902;font-style:italic># number of 2MB hugepage, take precedence over ratio</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_hugepage_ratio</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#0000cf;font-weight:700>0</span><span style=color:#f8f8f8>            </span><span style=color:#8f5902;font-style:italic># node mem hugepage ratio, 0 disable it by default</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_overcommit_ratio</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#0000cf;font-weight:700>0</span><span style=color:#f8f8f8>          </span><span style=color:#8f5902;font-style:italic># node mem overcommit ratio, 0 disable it by default</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_tune: oltp                   # node tuned profile</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>none,oltp,olap,crit,tiny</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_sysctl_params</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span>{<span style=color:#f8f8f8> </span>}<span style=color:#f8f8f8>           </span><span style=color:#8f5902;font-style:italic># sysctl parameters in k:v format in addition to tuned</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><h3 id=node_disable_numa><code>node_disable_numa</code></h3><p>name: <code>node_disable_numa</code>, type: <code>bool</code>, level: <code>C</code></p><p>Disable NUMA? Default is <code>false</code> (NUMA not disabled).</p><p>Note that disabling NUMA requires a machine reboot to take effect! If you don&rsquo;t know how to set CPU affinity, it&rsquo;s recommended to disable NUMA when using databases in production environments.</p><h3 id=node_disable_swap><code>node_disable_swap</code></h3><p>name: <code>node_disable_swap</code>, type: <code>bool</code>, level: <code>C</code></p><p>Disable SWAP? Default is <code>false</code> (SWAP not disabled).</p><p>Disabling SWAP is generally not recommended. The exception is if you have enough memory for exclusive PostgreSQL deployment, you can disable SWAP to improve performance.</p><p>Exception: SWAP should be disabled when your node is used for Kubernetes deployments.</p><h3 id=node_static_network><code>node_static_network</code></h3><p>name: <code>node_static_network</code>, type: <code>bool</code>, level: <code>C</code></p><p>Use static DNS servers? Default is <code>true</code> (enabled).</p><p>Enabling static networking means your DNS Resolv configuration won&rsquo;t be overwritten by machine reboots or NIC changes. Recommended to enable, or have network engineers handle the configuration.</p><h3 id=node_disk_prefetch><code>node_disk_prefetch</code></h3><p>name: <code>node_disk_prefetch</code>, type: <code>bool</code>, level: <code>C</code></p><p>Enable disk prefetch? Default is <code>false</code> (not enabled).</p><p>Can optimize performance for HDD-deployed instances. Recommended to enable when using mechanical hard drives.</p><h3 id=node_kernel_modules><code>node_kernel_modules</code></h3><p>name: <code>node_kernel_modules</code>, type: <code>string[]</code>, level: <code>C</code></p><p>Which kernel modules to enable? Default enables the following kernel modules:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_kernel_modules</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000;font-weight:700>[</span><span style=color:#f8f8f8> </span><span style=color:#000>softdog, ip_vs, ip_vs_rr, ip_vs_wrr, ip_vs_sh ]</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><p>An array of kernel module names declaring the kernel modules that need to be installed on the node.</p><h3 id=node_hugepage_count><code>node_hugepage_count</code></h3><p>name: <code>node_hugepage_count</code>, type: <code>int</code>, level: <code>C</code></p><p>Number of 2MB hugepages to allocate on the node. Default is <code>0</code>. Related parameter is <a href=#node_hugepage_ratio><code>node_hugepage_ratio</code></a>.</p><p>If both <code>node_hugepage_count</code> and <code>node_hugepage_ratio</code> are <code>0</code> (default), hugepages will be completely disabled. This parameter has higher priority than <a href=#node_hugepage_ratio><code>node_hugepage_ratio</code></a> because it&rsquo;s more precise.</p><p>If a non-zero value is set, it will be written to <code>/etc/sysctl.d/hugepage.conf</code> to take effect. Negative values won&rsquo;t work, and numbers higher than 90% of node memory will be capped at 90% of node memory.</p><p>If not zero, it should be slightly larger than the corresponding <a href=#pg_shared_buffer_ratio><code>pg_shared_buffer_ratio</code></a> value so PostgreSQL can use hugepages.</p><h3 id=node_hugepage_ratio><code>node_hugepage_ratio</code></h3><p>name: <code>node_hugepage_ratio</code>, type: <code>float</code>, level: <code>C</code></p><p>Ratio of node memory for hugepages. Default is <code>0</code>. Valid range: <code>0</code> ~ <code>0.40</code>.</p><p>This memory ratio will be allocated as hugepages and reserved for PostgreSQL. <a href=#node_hugepage_count><code>node_hugepage_count</code></a> is the higher priority and more precise version of this parameter.</p><p>Default: <code>0</code>, which sets <code>vm.nr_hugepages=0</code> and completely disables hugepages.</p><p>This parameter should equal or be slightly larger than <a href=#pg_shared_buffer_ratio><code>pg_shared_buffer_ratio</code></a> if not zero.</p><p>For example, if you allocate 25% of memory for Postgres shared buffers by default, you can set this value to 0.27 ~ 0.30, and use <code>/pg/bin/pg-tune-hugepage</code> after initialization to precisely reclaim wasted hugepages.</p><h3 id=node_overcommit_ratio><code>node_overcommit_ratio</code></h3><p>name: <code>node_overcommit_ratio</code>, type: <code>int</code>, level: <code>C</code></p><p>Node memory overcommit ratio. Default is <code>0</code>. This is an integer from <code>0</code> to <code>100+</code>.</p><p>Default: <code>0</code>, which sets <code>vm.overcommit_memory=0</code>. Otherwise, <code>vm.overcommit_memory=2</code> will be used with this value as <code>vm.overcommit_ratio</code>.</p><p>Recommended to set <code>vm.overcommit_ratio</code> on dedicated pgsql nodes to avoid memory overcommit.</p><h3 id=node_tune-1><code>node_tune</code></h3><p>name: <code>node_tune</code>, type: <code>enum</code>, level: <code>C</code></p><p>Preset tuning profiles for machines, provided through <code>tuned</code>. Four preset modes:</p><ul><li><code>tiny</code>: Micro virtual machine</li><li><code>oltp</code>: Regular OLTP template, optimizes latency (default)</li><li><code>olap</code>: Regular OLAP template, optimizes throughput</li><li><code>crit</code>: Core financial business template, optimizes dirty page count</li></ul><p>Typically, the database tuning template <a href=/docs/pgsql/param#pg_conf><code>pg_conf</code></a> should match the machine tuning template.</p><h3 id=node_sysctl_params><code>node_sysctl_params</code></h3><p>name: <code>node_sysctl_params</code>, type: <code>dict</code>, level: <code>C</code></p><p>Sysctl kernel parameters in K:V format, added to the <code>tuned</code> profile. Default is <code>{}</code> (empty object).</p><p>This is a KV dictionary parameter where Key is the kernel <code>sysctl</code> parameter name and Value is the parameter value. You can also consider defining extra sysctl parameters directly in the tuned templates in <code>roles/node/templates</code>.</p><hr><h2 id=node_sec><code>NODE_SEC</code></h2><p>Node security related parameters, including SELinux and firewall configuration.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_selinux_mode: permissive             # selinux mode</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>disabled, permissive, enforcing</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_firewall_mode: zone                  # firewall mode</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>disabled, zone, rules</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_firewall_intranet</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>           </span><span style=color:#8f5902;font-style:italic># which intranet cidr considered as internal network</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span>- <span style=color:#0000cf;font-weight:700>10.0.0.0</span><span style=color:#000>/8</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span>- <span style=color:#0000cf;font-weight:700>192.168.0.0</span><span style=color:#000>/16</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span>- <span style=color:#0000cf;font-weight:700>172.16.0.0</span><span style=color:#000>/12</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_firewall_public_port</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>        </span><span style=color:#8f5902;font-style:italic># expose these ports to public network in (zone, strict) mode</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span>- <span style=color:#0000cf;font-weight:700>22</span><span style=color:#f8f8f8>                            </span><span style=color:#8f5902;font-style:italic># enable ssh access</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span>- <span style=color:#0000cf;font-weight:700>80</span><span style=color:#f8f8f8>                            </span><span style=color:#8f5902;font-style:italic># enable http access</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span>- <span style=color:#0000cf;font-weight:700>443</span><span style=color:#f8f8f8>                           </span><span style=color:#8f5902;font-style:italic># enable https access</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span>- <span style=color:#0000cf;font-weight:700>5432</span><span style=color:#f8f8f8>                          </span><span style=color:#8f5902;font-style:italic># enable postgresql access (think twice before exposing it!)</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><h3 id=node_selinux_mode><code>node_selinux_mode</code></h3><p>name: <code>node_selinux_mode</code>, type: <code>enum</code>, level: <code>C</code></p><p>SELinux running mode. Default is <code>permissive</code>.</p><p>Options:</p><ul><li><code>disabled</code>: Completely disable SELinux (equivalent to old version&rsquo;s <code>node_disable_selinux: true</code>)</li><li><code>permissive</code>: Permissive mode, logs violations but doesn&rsquo;t block (recommended, default)</li><li><code>enforcing</code>: Enforcing mode, strictly enforces SELinux policies</li></ul><p>If you don&rsquo;t have professional OS/security experts, it&rsquo;s recommended to use <code>permissive</code> or <code>disabled</code> mode.</p><p>Note that SELinux is only enabled by default on EL-based systems. If you want to enable SELinux on Debian/Ubuntu systems, you need to install and enable SELinux configuration yourself.
Also, SELinux mode changes may require a system reboot to fully take effect.</p><h3 id=node_firewall_mode><code>node_firewall_mode</code></h3><p>name: <code>node_firewall_mode</code>, type: <code>enum</code>, level: <code>C</code></p><p>Firewall running mode. Default is <code>zone</code>.</p><p>Options:</p><ul><li><code>off</code>: Turn off and disable firewall (equivalent to old version&rsquo;s <code>node_disable_firewall: true</code>)</li><li><code>none</code>: Do nothing, maintain existing firewall rules unchanged</li><li><code>zone</code>: Use firewalld / ufw to configure firewall rules: trust intranet, only open specified ports to public</li></ul><p>Uses <code>firewalld</code> service on EL systems, <code>ufw</code> service on Debian/Ubuntu systems.</p><p>If you&rsquo;re deploying in a completely trusted intranet environment, or using cloud provider security groups for access control, you can choose <code>none</code> mode to keep existing firewall configuration, or set to <code>off</code> to completely disable the firewall.</p><p>Production environments recommend using <code>zone</code> mode with <a href=#node_firewall_intranet><code>node_firewall_intranet</code></a> and <a href=#node_firewall_public_port><code>node_firewall_public_port</code></a> for fine-grained access control.</p><p>Note that <code>zone</code> mode won&rsquo;t automatically enable the firewall for you.</p><h3 id=node_firewall_intranet><code>node_firewall_intranet</code></h3><p>name: <code>node_firewall_intranet</code>, type: <code>cidr[]</code>, level: <code>C</code></p><p>Intranet CIDR address list. Introduced in v4.0. Default value:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_firewall_intranet</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span>- <span style=color:#0000cf;font-weight:700>10.0.0.0</span><span style=color:#000>/8</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span>- <span style=color:#0000cf;font-weight:700>172.16.0.0</span><span style=color:#000>/12</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span>- <span style=color:#0000cf;font-weight:700>192.168.0.0</span><span style=color:#000>/16</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><p>This parameter defines IP address ranges considered as &ldquo;internal network&rdquo;. Traffic from these networks will be allowed to access all service ports without separate open rules.</p><p>Hosts within these CIDR ranges will be treated as trusted intranet hosts with more relaxed firewall rules. Also, in PG/PGB HBA rules, the intranet ranges defined here will be treated as &ldquo;intranet&rdquo;.</p><h3 id=node_firewall_public_port><code>node_firewall_public_port</code></h3><p>name: <code>node_firewall_public_port</code>, type: <code>port[]</code>, level: <code>C</code></p><p>Public exposed port list. Default is <code>[22, 80, 443, 5432]</code>.</p><p>This parameter defines ports exposed to public network (non-intranet CIDR). Default exposed ports include:</p><ul><li><code>22</code>: SSH service port</li><li><code>80</code>: HTTP service port</li><li><code>443</code>: HTTPS service port</li><li><code>5432</code>: PostgreSQL database port</li></ul><p>You can adjust this list according to actual needs. For example, if you don&rsquo;t need to expose the database port externally, remove <code>5432</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_firewall_public_port</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000;font-weight:700>[</span><span style=color:#0000cf;font-weight:700>22</span><span style=color:#000;font-weight:700>,</span><span style=color:#f8f8f8> </span><span style=color:#0000cf;font-weight:700>80</span><span style=color:#000;font-weight:700>,</span><span style=color:#f8f8f8> </span><span style=color:#0000cf;font-weight:700>443</span><span style=color:#000;font-weight:700>]</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><p>PostgreSQL default security policy in Pigsty only allows administrators to access the database port from public networks.
If you want other users to access the database from public networks, make sure to correctly configure corresponding access permissions in PG/PGB HBA rules.</p><p>If you want to expose other service ports to public networks, you can also add them to this list.
If you want to tighten firewall rules, you can remove the 5432 database port to ensure only truly needed service ports are exposed.</p><p>Note that this parameter only takes effect when <a href=#node_firewall_mode><code>node_firewall_mode</code></a> is set to <code>zone</code>.</p><hr><h2 id=node_admin><code>NODE_ADMIN</code></h2><p>This section is about administrators on host nodes - who can log in and how.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_data</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>/data                 </span><span style=color:#f8f8f8> </span><span style=color:#8f5902;font-style:italic># node main data directory, `/data` by default</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_admin_enabled</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8>          </span><span style=color:#8f5902;font-style:italic># create a admin user on target node?</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_admin_uid</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#0000cf;font-weight:700>88</span><span style=color:#f8f8f8>                </span><span style=color:#8f5902;font-style:italic># uid and gid for node admin user</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_admin_username</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>dba         </span><span style=color:#f8f8f8> </span><span style=color:#8f5902;font-style:italic># name of node admin user, `dba` by default</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_admin_sudo: nopass           # admin user&#39;s sudo privilege</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>limited, nopass, all, none</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_admin_ssh_exchange</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8>     </span><span style=color:#8f5902;font-style:italic># exchange admin ssh key among node cluster</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_admin_pk_current</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8>       </span><span style=color:#8f5902;font-style:italic># add current user&#39;s ssh pk to admin authorized_keys</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_admin_pk_list</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000;font-weight:700>[]</span><span style=color:#f8f8f8>            </span><span style=color:#8f5902;font-style:italic># ssh public keys to be added to admin user</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_aliases</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span>{}<span style=color:#f8f8f8>                  </span><span style=color:#8f5902;font-style:italic># alias name -&gt; IP address dict for `/etc/hosts`</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><h3 id=node_data><code>node_data</code></h3><p>name: <code>node_data</code>, type: <code>path</code>, level: <code>C</code></p><p>Node&rsquo;s main data directory. Default is <code>/data</code>.</p><p>If this directory doesn&rsquo;t exist, it will be created. This directory should be owned by <code>root</code> with <code>777</code> permissions.</p><h3 id=node_admin_enabled><code>node_admin_enabled</code></h3><p>name: <code>node_admin_enabled</code>, type: <code>bool</code>, level: <code>C</code></p><p>Create a dedicated admin user on this node? Default is <code>true</code>.</p><p>Pigsty creates an admin user on each node by default (with password-free sudo and ssh). The default admin is named <code>dba (uid=88)</code>, which can access other nodes in the environment from the admin node via password-free SSH and execute password-free sudo.</p><h3 id=node_admin_uid><code>node_admin_uid</code></h3><p>name: <code>node_admin_uid</code>, type: <code>int</code>, level: <code>C</code></p><p>Admin user UID. Default is <code>88</code>.</p><p>Please ensure the UID is the same across all nodes whenever possible to avoid unnecessary permission issues.</p><p>If the default UID 88 is already taken, you can choose another UID. Be careful about UID namespace conflicts when manually assigning.</p><h3 id=node_admin_username><code>node_admin_username</code></h3><p>name: <code>node_admin_username</code>, type: <code>username</code>, level: <code>C</code></p><p>Admin username. Default is <code>dba</code>.</p><h3 id=node_admin_sudo><code>node_admin_sudo</code></h3><p>name: <code>node_admin_sudo</code>, type: <code>enum</code>, level: <code>C</code></p><p>Admin user&rsquo;s sudo privilege level. Default is <code>nopass</code> (password-free sudo).</p><p>Options:</p><ul><li><code>none</code>: No sudo privileges</li><li><code>limited</code>: Limited sudo privileges (only allowed to execute specific commands)</li><li><code>nopass</code>: Password-free sudo privileges (default, allows all commands without password)</li><li><code>all</code>: Full sudo privileges (requires password)</li></ul><p>Pigsty uses <code>nopass</code> mode by default, allowing admin users to execute any sudo command without password, which is very convenient for automated operations.</p><p>In production environments with high security requirements, you may need to adjust this parameter to <code>limited</code> or <code>all</code> to restrict admin privileges.</p><h3 id=node_admin_ssh_exchange><code>node_admin_ssh_exchange</code></h3><p>name: <code>node_admin_ssh_exchange</code>, type: <code>bool</code>, level: <code>C</code></p><p>Exchange node admin SSH keys between node clusters. Default is <code>true</code>.</p><p>When enabled, Pigsty will exchange SSH public keys between members during playbook execution, allowing admin <a href=#node_admin_username><code>node_admin_username</code></a> to access each other from different nodes.</p><h3 id=node_admin_pk_current><code>node_admin_pk_current</code></h3><p>name: <code>node_admin_pk_current</code>, type: <code>bool</code>, level: <code>C</code></p><p>Add current node & user&rsquo;s public key to admin account? Default is <code>true</code>.</p><p>When enabled, the SSH public key (<code>~/.ssh/id_rsa.pub</code>) of the admin user executing this playbook on the current node will be copied to the target node admin user&rsquo;s <code>authorized_keys</code>.</p><p>When deploying in production environments, please pay attention to this parameter, as it will install the default public key of the user currently executing the command to the admin user on all machines.</p><h3 id=node_admin_pk_list><code>node_admin_pk_list</code></h3><p>name: <code>node_admin_pk_list</code>, type: <code>string[]</code>, level: <code>C</code></p><p>List of public keys for admins who can log in. Default is <code>[]</code> (empty array).</p><p>Each array element is a string containing the public key to be written to the admin user&rsquo;s <code>~/.ssh/authorized_keys</code>. Users with the corresponding private key can log in as admin.</p><p>When deploying in production environments, please pay attention to this parameter and only add trusted keys to this list.</p><h3 id=node_aliases><code>node_aliases</code></h3><p>name: <code>node_aliases</code>, type: <code>dict</code>, level: <code>C</code></p><p>Shell aliases to be written to host&rsquo;s <code>/etc/profile.d/node.alias.sh</code>. Default is <code>{}</code> (empty dict).</p><p>This parameter allows you to configure convenient shell aliases for the host&rsquo;s shell environment. The K:V dict defined here will be written to the target node&rsquo;s <code>profile.d</code> file in the format <code>alias k=v</code>.</p><p>For example, the following declares an alias named <code>dp</code> for quickly executing <code>docker compose pull</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_alias</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span><span style=color:#204a87;font-weight:700>dp</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#4e9a06>&#39;docker compose pull&#39;</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><hr><h2 id=node_time><code>NODE_TIME</code></h2><p>Configuration related to host time/timezone/NTP/scheduled tasks.</p><p>Time synchronization is very important for database services. Please ensure the system <code>chronyd</code> time service is running properly.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_timezone</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#4e9a06>&#39;&#39;</span><span style=color:#f8f8f8>                 </span><span style=color:#8f5902;font-style:italic># setup node timezone, empty string to skip</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_ntp_enabled</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8>            </span><span style=color:#8f5902;font-style:italic># enable chronyd time sync service?</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_ntp_servers</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>                 </span><span style=color:#8f5902;font-style:italic># ntp servers in `/etc/chrony.conf`</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span>- <span style=color:#000>pool pool.ntp.org iburst</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_crontab_overwrite</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8>      </span><span style=color:#8f5902;font-style:italic># overwrite or append to `/etc/crontab`?</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_crontab</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000;font-weight:700>[</span><span style=color:#f8f8f8> </span><span style=color:#000;font-weight:700>]</span><span style=color:#f8f8f8>                 </span><span style=color:#8f5902;font-style:italic># crontab entries in `/etc/crontab`</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><h3 id=node_timezone><code>node_timezone</code></h3><p>name: <code>node_timezone</code>, type: <code>string</code>, level: <code>C</code></p><p>Set node timezone. Empty string means skip. Default is empty string, which won&rsquo;t modify the default timezone (usually UTC).</p><p>When using in China region, it&rsquo;s recommended to set to <code>Asia/Hong_Kong</code> / <code>Asia/Shanghai</code>.</p><h3 id=node_ntp_enabled><code>node_ntp_enabled</code></h3><p>name: <code>node_ntp_enabled</code>, type: <code>bool</code>, level: <code>C</code></p><p>Enable chronyd time sync service? Default is <code>true</code>.</p><p>Pigsty will override the node&rsquo;s <code>/etc/chrony.conf</code> with the NTP server list specified in <a href=#node_ntp_servers><code>node_ntp_servers</code></a>.</p><p>If your node already has NTP servers configured, you can set this parameter to <code>false</code> to skip time sync configuration.</p><h3 id=node_ntp_servers><code>node_ntp_servers</code></h3><p>name: <code>node_ntp_servers</code>, type: <code>string[]</code>, level: <code>C</code></p><p>NTP server list used in <code>/etc/chrony.conf</code>. Default: <code>["pool pool.ntp.org iburst"]</code></p><p>This parameter is an array where each element is a string representing one line of NTP server configuration. Only takes effect when <a href=#node_ntp_enabled><code>node_ntp_enabled</code></a> is enabled.</p><p>Pigsty uses the global NTP server <code>pool.ntp.org</code> by default. You can modify this parameter according to your network environment, e.g., <code>cn.pool.ntp.org iburst</code>, or internal time services.</p><p>You can also use the <code>${admin_ip}</code> placeholder in the configuration to use the time server on the admin node.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_ntp_servers</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000;font-weight:700>[</span><span style=color:#f8f8f8> </span><span style=color:#4e9a06>&#39;pool ${admin_ip} iburst&#39;</span><span style=color:#f8f8f8> </span><span style=color:#000;font-weight:700>]</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><h3 id=node_crontab_overwrite><code>node_crontab_overwrite</code></h3><p>name: <code>node_crontab_overwrite</code>, type: <code>bool</code>, level: <code>C</code></p><p>When handling scheduled tasks in <a href=#node_crontab><code>node_crontab</code></a>, append or overwrite? Default is <code>true</code> (overwrite).</p><p>If you want to append scheduled tasks on the node, set this parameter to <code>false</code>, and Pigsty will <strong>append</strong> rather than <strong>overwrite all</strong> scheduled tasks on the node&rsquo;s crontab.</p><h3 id=node_crontab><code>node_crontab</code></h3><p>name: <code>node_crontab</code>, type: <code>string[]</code>, level: <code>C</code></p><p>Scheduled tasks defined in node&rsquo;s <code>/etc/crontab</code>. Default is <code>[]</code> (empty array).</p><p>Each array element is a string representing one scheduled task line. Use standard cron format for definition.</p><p>For example, the following configuration will execute a system task as root at 3am every day:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_crontab</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span>- <span style=color:#4e9a06>&#39;00 03 * * * root /usr/bin/some-system-task&#39;</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><blockquote><p><strong>Note</strong>: For PostgreSQL backup tasks and other postgres user cron jobs, use the <a href=/docs/pgsql/param#pg_crontab><code>pg_crontab</code></a> parameter
instead of <code>node_crontab</code>. Because <code>node_crontab</code> is written to <code>/etc/crontab</code> during NODE initialization, the <code>postgres</code> user may not exist yet,
which will cause cron to report <code>bad username</code> and ignore the entire crontab file.</p></blockquote><p>When <a href=#node_crontab_overwrite><code>node_crontab_overwrite</code></a> is <code>true</code> (default), the default <code>/etc/crontab</code> will be restored when removing the node.</p><hr><h2 id=node_vip><code>NODE_VIP</code></h2><p>You can bind an optional L2 VIP to a node cluster. This feature is disabled by default. L2 VIP only makes sense for a group of node clusters. The VIP will switch between nodes in the cluster according to configured priorities, ensuring high availability of node services.</p><p>Note that L2 VIP can <strong>only</strong> be used within the same L2 network segment, which may impose additional restrictions on your network topology. If you don&rsquo;t want this restriction, you can consider using DNS LB or HAProxy for similar functionality.</p><p>When enabling this feature, you need to explicitly assign available <a href=#vip_address><code>vip_address</code></a> and <a href=#vip_vrid><code>vip_vrid</code></a> for this L2 VIP. Users should ensure both are unique within the same network segment.</p><p>Note that NODE VIP is different from PG VIP. PG VIP is a VIP serving PostgreSQL instances, managed by vip-manager and bound to the PG cluster primary.
NODE VIP is managed by Keepalived and bound to node clusters. It can be in master-backup mode or load-balanced mode, and both can coexist.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>vip_enabled</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>false</span><span style=color:#f8f8f8>                </span><span style=color:#8f5902;font-style:italic># enable vip on this node cluster?</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># vip_address:         [IDENTITY] # node vip address in ipv4 format, required if vip is enabled</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># vip_vrid:            [IDENTITY] # required, integer, 1-254, should be unique among same VLAN</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>vip_role</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>backup                 </span><span style=color:#f8f8f8> </span><span style=color:#8f5902;font-style:italic># optional, `master/backup`, backup by default, use as init role</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>vip_preempt</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>false</span><span style=color:#f8f8f8>                </span><span style=color:#8f5902;font-style:italic># optional, `true/false`, false by default, enable vip preemption</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>vip_interface</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>eth0              </span><span style=color:#f8f8f8> </span><span style=color:#8f5902;font-style:italic># node vip network interface to listen, `eth0` by default</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>vip_dns_suffix</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#4e9a06>&#39;&#39;</span><span style=color:#f8f8f8>                </span><span style=color:#8f5902;font-style:italic># node vip dns name suffix, empty string by default</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>vip_auth_pass</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#4e9a06>&#39;&#39;</span><span style=color:#f8f8f8>                 </span><span style=color:#8f5902;font-style:italic># vrrp auth password, empty to use `&lt;cls&gt;-&lt;vrid&gt;` as default</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>vip_exporter_port</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#0000cf;font-weight:700>9650</span><span style=color:#f8f8f8>           </span><span style=color:#8f5902;font-style:italic># keepalived exporter listen port, 9650 by default</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><h3 id=vip_enabled><code>vip_enabled</code></h3><p>name: <code>vip_enabled</code>, type: <code>bool</code>, level: <code>C</code></p><p>Enable an L2 VIP managed by Keepalived on this node cluster? Default is <code>false</code>.</p><h3 id=vip_address><code>vip_address</code></h3><p>name: <code>vip_address</code>, type: <code>ip</code>, level: <code>C</code></p><p>Node VIP address in IPv4 format (without CIDR suffix). This is a <strong>required</strong> parameter when <a href=#vip_enabled><code>vip_enabled</code></a> is enabled.</p><p>This parameter has no default value, meaning you must explicitly assign a unique VIP address for the node cluster.</p><h3 id=vip_vrid><code>vip_vrid</code></h3><p>name: <code>vip_vrid</code>, type: <code>int</code>, level: <code>C</code></p><p>VRID is a positive integer from <code>1</code> to <code>254</code> used to identify a VIP in the network. This is a <strong>required</strong> parameter when <a href=#vip_enabled><code>vip_enabled</code></a> is enabled.</p><p>This parameter has no default value, meaning you must explicitly assign a unique ID within the network segment for the node cluster.</p><h3 id=vip_role><code>vip_role</code></h3><p>name: <code>vip_role</code>, type: <code>enum</code>, level: <code>I</code></p><p>Node VIP role. Options are <code>master</code> or <code>backup</code>. Default is <code>backup</code>.</p><p>This parameter value will be set as keepalived&rsquo;s initial state.</p><h3 id=vip_preempt><code>vip_preempt</code></h3><p>name: <code>vip_preempt</code>, type: <code>bool</code>, level: <code>C/I</code></p><p>Enable VIP preemption? Optional parameter. Default is <code>false</code> (no preemption).</p><p>Preemption means when a <code>backup</code> node has higher priority than the currently alive and working <code>master</code> node, should it preempt the VIP?</p><h3 id=vip_interface><code>vip_interface</code></h3><p>name: <code>vip_interface</code>, type: <code>string</code>, level: <code>C/I</code></p><p>Network interface for node VIP to listen on. Default is <code>eth0</code>.</p><p>You should use the same interface name as the node&rsquo;s primary IP address (the IP address you put in the inventory).</p><p>If your nodes have different interface names, you can override it at the instance/node level.</p><h3 id=vip_dns_suffix><code>vip_dns_suffix</code></h3><p>name: <code>vip_dns_suffix</code>, type: <code>string</code>, level: <code>C/I</code></p><p>DNS name for node cluster L2 VIP. Default is empty string, meaning the cluster name itself is used as the DNS name.</p><h3 id=vip_auth_pass><code>vip_auth_pass</code></h3><p>name: <code>vip_auth_pass</code>, type: <code>password</code>, level: <code>C</code></p><p>VRRP authentication password for keepalived. Default is empty string.</p><p>When empty, Pigsty will auto-generate a password using the pattern <code>&lt;cluster_name>-&lt;vrid></code>.
For production environments with security requirements, set an explicit strong password.</p><h3 id=vip_exporter_port><code>vip_exporter_port</code></h3><p>name: <code>vip_exporter_port</code>, type: <code>port</code>, level: <code>C/I</code></p><p>Keepalived exporter listen port. Default is <code>9650</code>.</p><hr><h2 id=haproxy><code>HAPROXY</code></h2><p>HAProxy is installed and enabled on all nodes by default, exposing services in a manner similar to Kubernetes NodePort.</p><p>The <a href=/docs/pgsql><code>PGSQL</code></a> module uses HAProxy for <a href=/docs/pgsql/service>services</a>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>haproxy_enabled</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8>             </span><span style=color:#8f5902;font-style:italic># enable haproxy on this node?</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>haproxy_clean</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>false</span><span style=color:#f8f8f8>              </span><span style=color:#8f5902;font-style:italic># cleanup all existing haproxy config?</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>haproxy_reload</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8>              </span><span style=color:#8f5902;font-style:italic># reload haproxy after config?</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>haproxy_auth_enabled</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8>        </span><span style=color:#8f5902;font-style:italic># enable authentication for haproxy admin page</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>haproxy_admin_username</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>admin    </span><span style=color:#f8f8f8> </span><span style=color:#8f5902;font-style:italic># haproxy admin username, `admin` by default</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>haproxy_admin_password</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>pigsty   </span><span style=color:#f8f8f8> </span><span style=color:#8f5902;font-style:italic># haproxy admin password, `pigsty` by default</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>haproxy_exporter_port</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#0000cf;font-weight:700>9101</span><span style=color:#f8f8f8>       </span><span style=color:#8f5902;font-style:italic># haproxy admin/exporter port, 9101 by default</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>haproxy_client_timeout</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>24h      </span><span style=color:#f8f8f8> </span><span style=color:#8f5902;font-style:italic># client connection timeout, 24h by default</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>haproxy_server_timeout</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>24h      </span><span style=color:#f8f8f8> </span><span style=color:#8f5902;font-style:italic># server connection timeout, 24h by default</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>haproxy_services</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000;font-weight:700>[]</span><span style=color:#f8f8f8>              </span><span style=color:#8f5902;font-style:italic># list of haproxy services to be exposed on node</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><h3 id=haproxy_enabled><code>haproxy_enabled</code></h3><p>name: <code>haproxy_enabled</code>, type: <code>bool</code>, level: <code>C</code></p><p>Enable haproxy on this node? Default is <code>true</code>.</p><h3 id=haproxy_clean><code>haproxy_clean</code></h3><p>name: <code>haproxy_clean</code>, type: <code>bool</code>, level: <code>G/C/A</code></p><p>Cleanup all existing haproxy config? Default is <code>false</code>.</p><h3 id=haproxy_reload><code>haproxy_reload</code></h3><p>name: <code>haproxy_reload</code>, type: <code>bool</code>, level: <code>A</code></p><p>Reload haproxy after config? Default is <code>true</code>, will reload haproxy after config changes.</p><p>If you want to check before applying, you can disable this option with command arguments, check, then apply.</p><h3 id=haproxy_auth_enabled><code>haproxy_auth_enabled</code></h3><p>name: <code>haproxy_auth_enabled</code>, type: <code>bool</code>, level: <code>G</code></p><p>Enable authentication for haproxy admin page. Default is <code>true</code>, which requires HTTP basic auth for the admin page.</p><p>Not recommended to disable authentication, as your traffic control page will be exposed, which is risky.</p><h3 id=haproxy_admin_username><code>haproxy_admin_username</code></h3><p>name: <code>haproxy_admin_username</code>, type: <code>username</code>, level: <code>G</code></p><p>HAProxy admin username. Default is <code>admin</code>.</p><h3 id=haproxy_admin_password><code>haproxy_admin_password</code></h3><p>name: <code>haproxy_admin_password</code>, type: <code>password</code>, level: <code>G</code></p><p>HAProxy admin password. Default is <code>pigsty</code>.</p><blockquote><p>PLEASE CHANGE THIS PASSWORD IN YOUR PRODUCTION ENVIRONMENT!</p></blockquote><h3 id=haproxy_exporter_port><code>haproxy_exporter_port</code></h3><p>name: <code>haproxy_exporter_port</code>, type: <code>port</code>, level: <code>C</code></p><p>HAProxy traffic management/metrics exposed port. Default is <code>9101</code>.</p><h3 id=haproxy_client_timeout><code>haproxy_client_timeout</code></h3><p>name: <code>haproxy_client_timeout</code>, type: <code>interval</code>, level: <code>C</code></p><p>Client connection timeout. Default is <code>24h</code>.</p><p>Setting a timeout can avoid long-lived connections that are difficult to clean up. If you really need long connections, you can set it to a longer time.</p><h3 id=haproxy_server_timeout><code>haproxy_server_timeout</code></h3><p>name: <code>haproxy_server_timeout</code>, type: <code>interval</code>, level: <code>C</code></p><p>Server connection timeout. Default is <code>24h</code>.</p><p>Setting a timeout can avoid long-lived connections that are difficult to clean up. If you really need long connections, you can set it to a longer time.</p><h3 id=haproxy_services><code>haproxy_services</code></h3><p>name: <code>haproxy_services</code>, type: <code>service[]</code>, level: <code>C</code></p><p>List of services to expose via HAProxy on this node. Default is <code>[]</code> (empty array).</p><p>Each array element is a service definition. Here&rsquo;s an example service definition:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>haproxy_services</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>                   </span><span style=color:#8f5902;font-style:italic># list of haproxy service</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span><span style=color:#8f5902;font-style:italic># expose pg-test read only replicas</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span>- <span style=color:#204a87;font-weight:700>name</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>pg-test-ro               </span><span style=color:#f8f8f8> </span><span style=color:#8f5902;font-style:italic># [REQUIRED] service name, unique</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>port</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#0000cf;font-weight:700>5440</span><span style=color:#f8f8f8>                      </span><span style=color:#8f5902;font-style:italic># [REQUIRED] service port, unique</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>ip</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#4e9a06>&#34;*&#34;</span><span style=color:#f8f8f8>                         </span><span style=color:#8f5902;font-style:italic># [OPTIONAL] service listen addr, &#34;*&#34; by default</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>protocol</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>tcp                  </span><span style=color:#f8f8f8> </span><span style=color:#8f5902;font-style:italic># [OPTIONAL] service protocol, &#39;tcp&#39; by default</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>balance</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>leastconn             </span><span style=color:#f8f8f8> </span><span style=color:#8f5902;font-style:italic># [OPTIONAL] load balance algorithm, roundrobin by default (or leastconn)</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>maxconn</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#0000cf;font-weight:700>20000</span><span style=color:#f8f8f8>                  </span><span style=color:#8f5902;font-style:italic># [OPTIONAL] max allowed front-end connection, 20000 by default</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>default</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#4e9a06>&#39;inter 3s fastinter 1s downinter 5s rise 3 fall 3 on-marked-down shutdown-sessions slowstart 30s maxconn 3000 maxqueue 128 weight 100&#39;</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>options</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>      </span>- <span style=color:#000>option httpchk</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>      </span>- <span style=color:#000>option http-keep-alive</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>      </span>- <span style=color:#000>http-check send meth OPTIONS uri /read-only</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>      </span>- <span style=color:#000>http-check expect status 200</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>servers</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>      </span>- {<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>name: pg-test-1 ,ip: 10.10.10.11 , port: 5432 , options: check port 8008 , backup</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8> </span>}<span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>      </span>- {<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>name: pg-test-2 ,ip: 10.10.10.12 , port: 5432 , options</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>check port 8008 }</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>      </span>- {<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>name: pg-test-3 ,ip: 10.10.10.13 , port: 5432 , options</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>check port 8008 }</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><p>Each service definition will be rendered to <code>/etc/haproxy/&lt;service.name>.cfg</code> configuration file and take effect after HAProxy reload.</p><hr><h2 id=node_exporter><code>NODE_EXPORTER</code></h2><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_exporter_enabled</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8>       </span><span style=color:#8f5902;font-style:italic># setup node_exporter on this node?</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_exporter_port</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#0000cf;font-weight:700>9100</span><span style=color:#f8f8f8>          </span><span style=color:#8f5902;font-style:italic># node exporter listen port, 9100 by default</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_exporter_options</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#4e9a06>&#39;--no-collector.softnet --no-collector.nvme --collector.tcpstat --collector.processes&#39;</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><h3 id=node_exporter_enabled><code>node_exporter_enabled</code></h3><p>name: <code>node_exporter_enabled</code>, type: <code>bool</code>, level: <code>C</code></p><p>Enable node metrics collector on current node? Default is <code>true</code>.</p><h3 id=node_exporter_port><code>node_exporter_port</code></h3><p>name: <code>node_exporter_port</code>, type: <code>port</code>, level: <code>C</code></p><p>Port used to expose node metrics. Default is <code>9100</code>.</p><h3 id=node_exporter_options><code>node_exporter_options</code></h3><p>name: <code>node_exporter_options</code>, type: <code>arg</code>, level: <code>C</code></p><p>Command line arguments for node metrics collector. Default value:</p><p><code>--no-collector.softnet --no-collector.nvme --collector.tcpstat --collector.processes</code></p><p>This option enables/disables some metrics collectors. Please adjust according to your needs.</p><hr><h2 id=vector><code>VECTOR</code></h2><p>Vector is the log collection component used in Pigsty v4.0. It collects logs from various modules and sends them to VictoriaLogs service on infrastructure nodes.</p><ul><li><p><code>INFRA</code>: Infrastructure component logs, collected only on Infra nodes.</p><ul><li><code>nginx-access</code>: <code>/var/log/nginx/access.log</code></li><li><code>nginx-error</code>: <code>/var/log/nginx/error.log</code></li><li><code>grafana</code>: <code>/var/log/grafana/grafana.log</code></li></ul></li><li><p><code>NODES</code>: Host-related logs, collection enabled on all nodes.</p><ul><li><code>syslog</code>: <code>/var/log/messages</code> (<code>/var/log/syslog</code> on Debian)</li><li><code>dmesg</code>: <code>/var/log/dmesg</code></li><li><code>cron</code>: <code>/var/log/cron</code></li></ul></li><li><p><code>PGSQL</code>: PostgreSQL-related logs, collection enabled only when node has <a href=/docs/pgsql>PGSQL</a> module configured.</p><ul><li><code>postgres</code>: <code>/pg/log/postgres/*</code></li><li><code>patroni</code>: <code>/pg/log/patroni.log</code></li><li><code>pgbouncer</code>: <code>/pg/log/pgbouncer/pgbouncer.log</code></li><li><code>pgbackrest</code>: <code>/pg/log/pgbackrest/*.log</code></li></ul></li><li><p><code>REDIS</code>: Redis-related logs, collection enabled only when node has <a href=/docs/redis>REDIS</a> module configured.</p><ul><li><code>redis</code>: <code>/var/log/redis/*.log</code></li></ul></li></ul><blockquote><p>Log directories are automatically adjusted according to these parameter configurations: <a href=/docs/pgsql/param#pg_log_dir><code>pg_log_dir</code></a>, <a href=/docs/pgsql/param#patroni_log_dir><code>patroni_log_dir</code></a>, <a href=/docs/pgsql/param#pgbouncer_log_dir><code>pgbouncer_log_dir</code></a>, <a href=/docs/pgsql/param#pgbackrest_log_dir><code>pgbackrest_log_dir</code></a></p></blockquote><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>vector_enabled</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8>              </span><span style=color:#8f5902;font-style:italic># enable vector log collector?</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>vector_clean</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>false</span><span style=color:#f8f8f8>               </span><span style=color:#8f5902;font-style:italic># purge vector data dir during init?</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>vector_data</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>/data/vector        </span><span style=color:#f8f8f8> </span><span style=color:#8f5902;font-style:italic># vector data directory, /data/vector by default</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>vector_port</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#0000cf;font-weight:700>9598</span><span style=color:#f8f8f8>                 </span><span style=color:#8f5902;font-style:italic># vector metrics port, 9598 by default</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>vector_read_from</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>beginning      </span><span style=color:#f8f8f8> </span><span style=color:#8f5902;font-style:italic># read log from beginning or end</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>vector_log_endpoint</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000;font-weight:700>[</span><span style=color:#f8f8f8> </span><span style=color:#000>infra ]   </span><span style=color:#f8f8f8> </span><span style=color:#8f5902;font-style:italic># log endpoint, default send to infra group</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><h3 id=vector_enabled><code>vector_enabled</code></h3><p>name: <code>vector_enabled</code>, type: <code>bool</code>, level: <code>C</code></p><p>Enable Vector log collection service? Default is <code>true</code>.</p><p>Vector is the log collection agent used in Pigsty v4.0, replacing Promtail from previous versions. It collects node and service logs and sends them to VictoriaLogs.</p><h3 id=vector_clean><code>vector_clean</code></h3><p>name: <code>vector_clean</code>, type: <code>bool</code>, level: <code>G/A</code></p><p>Clean existing data directory when installing Vector? Default is <code>false</code>.</p><p>By default, it won&rsquo;t clean. When you choose to clean, Pigsty will remove the existing data directory <a href=#vector_data><code>vector_data</code></a> when deploying Vector. This means Vector will re-collect all logs on the current node and send them to VictoriaLogs.</p><h3 id=vector_data><code>vector_data</code></h3><p>name: <code>vector_data</code>, type: <code>path</code>, level: <code>C</code></p><p>Vector data directory path. Default is <code>/data/vector</code>.</p><p>Vector stores log read offsets and buffered data in this directory.</p><h3 id=vector_port><code>vector_port</code></h3><p>name: <code>vector_port</code>, type: <code>port</code>, level: <code>C</code></p><p>Vector metrics listen port. Default is <code>9598</code>.</p><p>This port is used to expose Vector&rsquo;s own monitoring metrics, which can be scraped by VictoriaMetrics.</p><h3 id=vector_read_from><code>vector_read_from</code></h3><p>name: <code>vector_read_from</code>, type: <code>enum</code>, level: <code>C</code></p><p>Vector log reading start position. Default is <code>beginning</code>.</p><p>Options are <code>beginning</code> (start from beginning) or <code>end</code> (start from end). <code>beginning</code> reads the entire content of existing log files, <code>end</code> only reads newly generated logs.</p><h3 id=vector_log_endpoint><code>vector_log_endpoint</code></h3><p>name: <code>vector_log_endpoint</code>, type: <code>string[]</code>, level: <code>C</code></p><p>Log destination endpoint list. Default is <code>[ infra ]</code>.</p><p>Specifies which node group&rsquo;s VictoriaLogs service to send logs to. Default sends to nodes in the <code>infra</code> group.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-82bbb07e4b385ccd2f29e3417328b1c8>3 - Playbook</h1><div class=lead>How to use built-in Ansible playbooks to manage NODE clusters, with a quick reference for common commands.</div><p>Pigsty provides two playbooks related to the NODE module:</p><ul><li><a href=#nodeyml><code>node.yml</code></a>: Add nodes to Pigsty and configure them to the desired state</li><li><a href=#node-rmyml><code>node-rm.yml</code></a>: Remove managed nodes from Pigsty</li></ul><p>Two wrapper scripts are also provided: <code>bin/node-add</code> and <code>bin/node-rm</code>, for quickly invoking these playbooks.</p><hr><h2 id=nodeyml><code>node.yml</code></h2><p>The <a href=https://github.com/pgsty/pigsty/blob/main/node.yml><code>node.yml</code></a> playbook for adding nodes to Pigsty contains the following subtasks:</p><pre tabindex=0><code>node-id       : generate node identity
node_name     : setup hostname
node_hosts    : setup /etc/hosts records
node_resolv   : setup DNS resolver /etc/resolv.conf
node_firewall : setup firewall &amp; selinux
node_ca       : add &amp; trust CA certificate
node_repo     : add upstream software repository
node_pkg      : install rpm/deb packages
node_feature  : setup numa, grub, static network
node_kernel   : enable kernel modules
node_tune     : setup tuned profile
node_sysctl   : setup additional sysctl parameters
node_profile  : write /etc/profile.d/node.sh
node_ulimit   : setup resource limits
node_data     : setup data directory
node_admin    : setup admin user and ssh key
node_timezone : setup timezone
node_ntp      : setup NTP server/client
node_crontab  : add/overwrite crontab tasks
node_vip      : setup optional L2 VIP for node cluster
haproxy       : setup haproxy on node to expose services
monitor       : setup node monitoring: node_exporter &amp; vector
</code></pre><hr><h2 id=node-rmyml><code>node-rm.yml</code></h2><p>The <a href=https://github.com/pgsty/pigsty/blob/main/node-rm.yml><code>node-rm.yml</code></a> playbook for removing nodes from Pigsty contains the following subtasks:</p><pre tabindex=0><code>register       : remove registration from prometheus &amp; nginx
  - prometheus : remove registered prometheus monitoring target
  - nginx      : remove nginx proxy record for haproxy admin
vip            : remove keepalived &amp; L2 VIP (if VIP enabled)
haproxy        : remove haproxy load balancer
node_exporter  : remove node monitoring: Node Exporter
vip_exporter   : remove keepalived_exporter (if VIP enabled)
vector         : remove log collection agent vector
profile        : remove /etc/profile.d/node.sh
</code></pre><hr><h2 id=quick-reference>Quick Reference</h2><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Basic node management</span>
</span></span><span style=display:flex><span>./node.yml -l &lt;cls<span style=color:#000;font-weight:700>|</span>ip<span style=color:#000;font-weight:700>|</span>group&gt;          <span style=color:#8f5902;font-style:italic># Add node to Pigsty</span>
</span></span><span style=display:flex><span>./node-rm.yml -l &lt;cls<span style=color:#000;font-weight:700>|</span>ip<span style=color:#000;font-weight:700>|</span>group&gt;       <span style=color:#8f5902;font-style:italic># Remove node from Pigsty</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Node management shortcuts</span>
</span></span><span style=display:flex><span>bin/node-add node-test                 <span style=color:#8f5902;font-style:italic># Initialize node cluster &#39;node-test&#39;</span>
</span></span><span style=display:flex><span>bin/node-add 10.10.10.10               <span style=color:#8f5902;font-style:italic># Initialize node &#39;10.10.10.10&#39;</span>
</span></span><span style=display:flex><span>bin/node-rm node-test                  <span style=color:#8f5902;font-style:italic># Remove node cluster &#39;node-test&#39;</span>
</span></span><span style=display:flex><span>bin/node-rm 10.10.10.10                <span style=color:#8f5902;font-style:italic># Remove node &#39;10.10.10.10&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Node main initialization</span>
</span></span><span style=display:flex><span>./node.yml -t node                     <span style=color:#8f5902;font-style:italic># Complete node main init (excludes haproxy, monitor)</span>
</span></span><span style=display:flex><span>./node.yml -t haproxy                  <span style=color:#8f5902;font-style:italic># Setup haproxy on node</span>
</span></span><span style=display:flex><span>./node.yml -t monitor                  <span style=color:#8f5902;font-style:italic># Setup node monitoring: node_exporter &amp; vector</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># VIP management</span>
</span></span><span style=display:flex><span>./node.yml -t node_vip                 <span style=color:#8f5902;font-style:italic># Setup optional L2 VIP for node cluster</span>
</span></span><span style=display:flex><span>./node.yml -t vip_config,vip_reload    <span style=color:#8f5902;font-style:italic># Refresh node L2 VIP configuration</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># HAProxy management</span>
</span></span><span style=display:flex><span>./node.yml -t haproxy_config,haproxy_reload   <span style=color:#8f5902;font-style:italic># Refresh service definitions on node</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Registration management</span>
</span></span><span style=display:flex><span>./node.yml -t register_prometheus      <span style=color:#8f5902;font-style:italic># Re-register node to Prometheus</span>
</span></span><span style=display:flex><span>./node.yml -t register_nginx           <span style=color:#8f5902;font-style:italic># Re-register node haproxy admin to Nginx</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Specific tasks</span>
</span></span><span style=display:flex><span>./node.yml -t node-id                  <span style=color:#8f5902;font-style:italic># Generate node identity</span>
</span></span><span style=display:flex><span>./node.yml -t node_name                <span style=color:#8f5902;font-style:italic># Setup hostname</span>
</span></span><span style=display:flex><span>./node.yml -t node_hosts               <span style=color:#8f5902;font-style:italic># Setup node /etc/hosts records</span>
</span></span><span style=display:flex><span>./node.yml -t node_resolv              <span style=color:#8f5902;font-style:italic># Setup node DNS resolver /etc/resolv.conf</span>
</span></span><span style=display:flex><span>./node.yml -t node_firewall            <span style=color:#8f5902;font-style:italic># Setup firewall &amp; selinux</span>
</span></span><span style=display:flex><span>./node.yml -t node_ca                  <span style=color:#8f5902;font-style:italic># Setup node CA certificate</span>
</span></span><span style=display:flex><span>./node.yml -t node_repo                <span style=color:#8f5902;font-style:italic># Setup node upstream software repository</span>
</span></span><span style=display:flex><span>./node.yml -t node_pkg                 <span style=color:#8f5902;font-style:italic># Install yum packages on node</span>
</span></span><span style=display:flex><span>./node.yml -t node_feature             <span style=color:#8f5902;font-style:italic># Setup numa, grub, static network</span>
</span></span><span style=display:flex><span>./node.yml -t node_kernel              <span style=color:#8f5902;font-style:italic># Enable kernel modules</span>
</span></span><span style=display:flex><span>./node.yml -t node_tune                <span style=color:#8f5902;font-style:italic># Setup tuned profile</span>
</span></span><span style=display:flex><span>./node.yml -t node_sysctl              <span style=color:#8f5902;font-style:italic># Setup additional sysctl parameters</span>
</span></span><span style=display:flex><span>./node.yml -t node_profile             <span style=color:#8f5902;font-style:italic># Setup node environment: /etc/profile.d/node.sh</span>
</span></span><span style=display:flex><span>./node.yml -t node_ulimit              <span style=color:#8f5902;font-style:italic># Setup node resource limits</span>
</span></span><span style=display:flex><span>./node.yml -t node_data                <span style=color:#8f5902;font-style:italic># Setup node primary data directory</span>
</span></span><span style=display:flex><span>./node.yml -t node_admin               <span style=color:#8f5902;font-style:italic># Setup admin user and ssh key</span>
</span></span><span style=display:flex><span>./node.yml -t node_timezone            <span style=color:#8f5902;font-style:italic># Setup node timezone</span>
</span></span><span style=display:flex><span>./node.yml -t node_ntp                 <span style=color:#8f5902;font-style:italic># Setup node NTP server/client</span>
</span></span><span style=display:flex><span>./node.yml -t node_crontab             <span style=color:#8f5902;font-style:italic># Add/overwrite crontab tasks</span>
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-f85610345b10eb3ebcd5b3f4d040bbb5>4 - Administration</h1><div class=lead>Node cluster management SOP - create, destroy, expand, shrink, and handle node/disk failures</div><p>Here are common administration operations for the NODE module:</p><ul><li><a href=#add-node>Add Node</a></li><li><a href=#remove-node>Remove Node</a></li><li><a href=#create-admin>Create Admin</a></li><li><a href=#bind-vip>Bind VIP</a></li><li><a href=#add-node-monitoring>Add Node Monitoring</a></li><li><a href=#other-tasks>Other Tasks</a></li></ul><p>For more questions, see <a href=faq/>FAQ: NODE</a></p><hr><h2 id=add-node>Add Node</h2><p>To add a node to Pigsty, you need passwordless ssh/sudo access to that node.</p><p>You can also add an entire cluster at once, or use wildcards to match nodes in the inventory to add to Pigsty.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># ./node.yml -l &lt;cls|ip|group&gt;        # actual playbook to add nodes to Pigsty</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># bin/node-add &lt;selector|ip...&gt;       # add node to Pigsty</span>
</span></span><span style=display:flex><span>bin/node-add node-test                <span style=color:#8f5902;font-style:italic># init node cluster &#39;node-test&#39;</span>
</span></span><span style=display:flex><span>bin/node-add 10.10.10.10              <span style=color:#8f5902;font-style:italic># init node &#39;10.10.10.10&#39;</span>
</span></span></code></pre></div><p><strong>Example: Add three nodes of PG cluster <code>pg-test</code> to Pigsty management</strong></p><link rel=stylesheet href=/css/asciinema-player.css><script src=/js/asciinema-player.min.js></script><div id=asciinema-7069a298b7596d78fe1fbba39686127a class="asciinema-container td-max-width-on-larger-screens" style="margin:1em 0"></div><script>(function(){var n,s="asciinema-7069a298b7596d78fe1fbba39686127a",o="/demo/node-add.cast",e="auto",i=null;function a(){var e=document.documentElement.getAttribute("data-bs-theme");return e==="dark"?"solarized-dark":e==="light"?"solarized-light":window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"solarized-dark":"solarized-light"}function r(){return e==="auto"?a():e}function t(){var e,t=document.getElementById(s);t.innerHTML="",e={theme:r(),speed:"1.2",startAt:0,fit:"width"},e.autoPlay=!0,e.loop=!0,e.markers=[[4,"Execute"]],i=AsciinemaPlayer.create(o,t,e)}t(),e==="auto"&&(n=new MutationObserver(function(e){e.forEach(function(e){e.attributeName==="data-bs-theme"&&t()})}),n.observe(document.documentElement,{attributes:!0}),window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",function(){var e=document.documentElement.getAttribute("data-bs-theme");(!e||e==="auto")&&t()}))})()</script><hr><h2 id=remove-node>Remove Node</h2><p>To remove a node from Pigsty, you can use the following commands:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># ./node-rm.yml -l &lt;cls|ip|group&gt;    # actual playbook to remove node from Pigsty</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># bin/node-rm &lt;cls|ip|selector&gt; ...  # remove node from Pigsty</span>
</span></span><span style=display:flex><span>bin/node-rm node-test                <span style=color:#8f5902;font-style:italic># remove node cluster &#39;node-test&#39;</span>
</span></span><span style=display:flex><span>bin/node-rm 10.10.10.10              <span style=color:#8f5902;font-style:italic># remove node &#39;10.10.10.10&#39;</span>
</span></span></code></pre></div><p>You can also remove an entire cluster at once, or use wildcards to match nodes in the inventory to remove from Pigsty.</p><div id=asciinema-1a9591d6a68c0eefe3529d84981541a6 class="asciinema-container td-max-width-on-larger-screens" style="margin:1em 0"></div><script>(function(){var n,s="asciinema-1a9591d6a68c0eefe3529d84981541a6",o="/demo/node-rm.cast",e="auto",i=null;function a(){var e=document.documentElement.getAttribute("data-bs-theme");return e==="dark"?"solarized-dark":e==="light"?"solarized-light":window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"solarized-dark":"solarized-light"}function r(){return e==="auto"?a():e}function t(){var e,t=document.getElementById(s);t.innerHTML="",e={theme:r(),speed:"1.2",startAt:0,fit:"width"},e.autoPlay=!0,e.loop=!0,i=AsciinemaPlayer.create(o,t,e)}t(),e==="auto"&&(n=new MutationObserver(function(e){e.forEach(function(e){e.attributeName==="data-bs-theme"&&t()})}),n.observe(document.documentElement,{attributes:!0}),window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",function(){var e=document.documentElement.getAttribute("data-bs-theme");(!e||e==="auto")&&t()}))})()</script><hr><h2 id=create-admin>Create Admin</h2><p>If the current user doesn&rsquo;t have passwordless ssh/sudo access to the node, you can use another admin user to bootstrap it:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>node.yml -t node_admin -k -K -e <span style=color:#000>ansible_user</span><span style=color:#ce5c00;font-weight:700>=</span>&lt;another admin&gt;   <span style=color:#8f5902;font-style:italic># enter ssh/sudo password for another admin to complete this task</span>
</span></span></code></pre></div><hr><h2 id=bind-vip>Bind VIP</h2><p>You can bind an optional L2 VIP on a node cluster using the <a href=param/#vip_enabled><code>vip_enabled</code></a> parameter.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>proxy</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span><span style=color:#204a87;font-weight:700>hosts</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>10.10.10.29</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span>{<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>nodename: proxy-1 }   # you can explicitly specify initial VIP role</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>MASTER / BACKUP</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>10.10.10.30</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span>{<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>nodename: proxy-2 }   # , vip_role</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>master }</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span><span style=color:#204a87;font-weight:700>vars</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>node_cluster</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>proxy</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>vip_enabled</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>vip_vrid</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#0000cf;font-weight:700>128</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>vip_address</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#0000cf;font-weight:700>10.10.10.99</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>vip_interface</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>eth1</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./node.yml -l proxy -t node_vip     <span style=color:#8f5902;font-style:italic># enable VIP for the first time</span>
</span></span><span style=display:flex><span>./node.yml -l proxy -t vip_refresh  <span style=color:#8f5902;font-style:italic># refresh VIP config (e.g., designate master)</span>
</span></span></code></pre></div><hr><h2 id=add-node-monitoring>Add Node Monitoring</h2><p>If you want to add or reconfigure monitoring on existing nodes, use the following commands:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./node.yml -t node_exporter,node_register  <span style=color:#8f5902;font-style:italic># configure monitoring and register</span>
</span></span><span style=display:flex><span>./node.yml -t vector                        <span style=color:#8f5902;font-style:italic># configure log collection</span>
</span></span></code></pre></div><hr><h2 id=other-tasks>Other Tasks</h2><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Play</span>
</span></span><span style=display:flex><span>./node.yml -t node                            <span style=color:#8f5902;font-style:italic># complete node initialization (excludes haproxy, monitoring)</span>
</span></span><span style=display:flex><span>./node.yml -t haproxy                         <span style=color:#8f5902;font-style:italic># setup haproxy on node</span>
</span></span><span style=display:flex><span>./node.yml -t monitor                         <span style=color:#8f5902;font-style:italic># configure node monitoring: node_exporter &amp; vector</span>
</span></span><span style=display:flex><span>./node.yml -t node_vip                        <span style=color:#8f5902;font-style:italic># install, configure, enable L2 VIP for clusters without VIP</span>
</span></span><span style=display:flex><span>./node.yml -t vip_config,vip_reload           <span style=color:#8f5902;font-style:italic># refresh node L2 VIP configuration</span>
</span></span><span style=display:flex><span>./node.yml -t haproxy_config,haproxy_reload   <span style=color:#8f5902;font-style:italic># refresh service definitions on node</span>
</span></span><span style=display:flex><span>./node.yml -t register_prometheus             <span style=color:#8f5902;font-style:italic># re-register node with Prometheus</span>
</span></span><span style=display:flex><span>./node.yml -t register_nginx                  <span style=color:#8f5902;font-style:italic># re-register node haproxy admin page with Nginx</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Task</span>
</span></span><span style=display:flex><span>./node.yml -t node-id        <span style=color:#8f5902;font-style:italic># generate node identity</span>
</span></span><span style=display:flex><span>./node.yml -t node_name      <span style=color:#8f5902;font-style:italic># setup hostname</span>
</span></span><span style=display:flex><span>./node.yml -t node_hosts     <span style=color:#8f5902;font-style:italic># configure node /etc/hosts records</span>
</span></span><span style=display:flex><span>./node.yml -t node_resolv    <span style=color:#8f5902;font-style:italic># configure node DNS resolver /etc/resolv.conf</span>
</span></span><span style=display:flex><span>./node.yml -t node_firewall  <span style=color:#8f5902;font-style:italic># configure firewall &amp; selinux</span>
</span></span><span style=display:flex><span>./node.yml -t node_ca        <span style=color:#8f5902;font-style:italic># configure node CA certificate</span>
</span></span><span style=display:flex><span>./node.yml -t node_repo      <span style=color:#8f5902;font-style:italic># configure node upstream software repository</span>
</span></span><span style=display:flex><span>./node.yml -t node_pkg       <span style=color:#8f5902;font-style:italic># install yum packages on node</span>
</span></span><span style=display:flex><span>./node.yml -t node_feature   <span style=color:#8f5902;font-style:italic># configure numa, grub, static network, etc.</span>
</span></span><span style=display:flex><span>./node.yml -t node_kernel    <span style=color:#8f5902;font-style:italic># configure OS kernel modules</span>
</span></span><span style=display:flex><span>./node.yml -t node_tune      <span style=color:#8f5902;font-style:italic># configure tuned profile</span>
</span></span><span style=display:flex><span>./node.yml -t node_sysctl    <span style=color:#8f5902;font-style:italic># set additional sysctl parameters</span>
</span></span><span style=display:flex><span>./node.yml -t node_profile   <span style=color:#8f5902;font-style:italic># configure node environment variables: /etc/profile.d/node.sh</span>
</span></span><span style=display:flex><span>./node.yml -t node_ulimit    <span style=color:#8f5902;font-style:italic># configure node resource limits</span>
</span></span><span style=display:flex><span>./node.yml -t node_data      <span style=color:#8f5902;font-style:italic># configure node primary data directory</span>
</span></span><span style=display:flex><span>./node.yml -t node_admin     <span style=color:#8f5902;font-style:italic># configure admin user and ssh keys</span>
</span></span><span style=display:flex><span>./node.yml -t node_timezone  <span style=color:#8f5902;font-style:italic># configure node timezone</span>
</span></span><span style=display:flex><span>./node.yml -t node_ntp       <span style=color:#8f5902;font-style:italic># configure node NTP server/client</span>
</span></span><span style=display:flex><span>./node.yml -t node_crontab   <span style=color:#8f5902;font-style:italic># add/overwrite crontab entries</span>
</span></span><span style=display:flex><span>./node.yml -t node_vip       <span style=color:#8f5902;font-style:italic># setup optional L2 VIP for node cluster</span>
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-09468f551281e08aa947b92b9aa4f1a7>5 - Monitoring</h1><div class=lead>Monitor NODE in Pigsty with dashboards and alerting rules</div><p>The NODE module in Pigsty provides 6 monitoring dashboards and comprehensive alerting rules.</p><hr><h2 id=dashboards>Dashboards</h2><p>The NODE module provides 6 monitoring dashboards:</p><h3 id=node-overview>NODE Overview</h3><p>Displays an overall overview of all host nodes in the current environment.</p><p><a href=https://demo.pigsty.io/d/node-overview><img src=/img/dashboard/node-overview.jpg alt=node-overview.jpg></a></p><h3 id=node-cluster>NODE Cluster</h3><p>Shows detailed monitoring data for a specific host cluster.</p><p><a href=https://demo.pigsty.io/d/node-cluster><img src=/img/dashboard/node-cluster.jpg alt=node-cluster.jpg></a></p><h3 id=node-instance>Node Instance</h3><p>Presents detailed monitoring information for a single host node.</p><p><a href=https://demo.pigsty.io/d/node-instance><img src=/img/dashboard/node-instance.jpg alt=node-instance.jpg></a></p><h3 id=node-alert>NODE Alert</h3><p>Centrally displays alert information for all hosts in the environment.</p><p><a href=https://demo.pigsty.io/d/node-alert><img src=/img/dashboard/node-alert.jpg alt=node-alert.jpg></a></p><h3 id=node-vip>NODE VIP</h3><p>Monitors detailed status of L2 virtual IPs.</p><p><a href=https://demo.pigsty.io/d/node-vip><img src=/img/dashboard/node-vip.jpg alt=node-vip.jpg></a></p><h3 id=node-haproxy>Node Haproxy</h3><p>Tracks the operational status of HAProxy load balancers.</p><p><a href=https://demo.pigsty.io/d/node-haproxy><img src=/img/dashboard/node-haproxy.jpg alt=node-haproxy.jpg></a></p><hr><h2 id=alert-rules>Alert Rules</h2><p>Pigsty implements the following alerting rules for NODE:</p><h3 id=availability-alerts>Availability Alerts</h3><table><thead><tr><th>Rule</th><th style=text-align:center>Level</th><th>Description</th></tr></thead><tbody><tr><td><code>NodeDown</code></td><td style=text-align:center>CRIT</td><td>Node is offline</td></tr><tr><td><code>HaproxyDown</code></td><td style=text-align:center>CRIT</td><td>HAProxy service is offline</td></tr><tr><td><code>VectorDown</code></td><td style=text-align:center>WARN</td><td>Log collecting agent offline (Vector)</td></tr><tr><td><code>DockerDown</code></td><td style=text-align:center>WARN</td><td>Container engine offline</td></tr><tr><td><code>KeepalivedDown</code></td><td style=text-align:center>WARN</td><td>Keepalived daemon offline</td></tr></tbody></table><h3 id=cpu-alerts>CPU Alerts</h3><table><thead><tr><th>Rule</th><th style=text-align:center>Level</th><th>Description</th></tr></thead><tbody><tr><td><code>NodeCpuHigh</code></td><td style=text-align:center>WARN</td><td>CPU usage exceeds 70%</td></tr></tbody></table><h3 id=scheduling-alerts>Scheduling Alerts</h3><table><thead><tr><th>Rule</th><th style=text-align:center>Level</th><th>Description</th></tr></thead><tbody><tr><td><code>NodeLoadHigh</code></td><td style=text-align:center>WARN</td><td>Normalized load exceeds 100%</td></tr></tbody></table><h3 id=memory-alerts>Memory Alerts</h3><table><thead><tr><th>Rule</th><th style=text-align:center>Level</th><th>Description</th></tr></thead><tbody><tr><td><code>NodeOutOfMem</code></td><td style=text-align:center>WARN</td><td>Available memory less than 10%</td></tr><tr><td><code>NodeMemSwapped</code></td><td style=text-align:center>WARN</td><td>Swap usage exceeds 1%</td></tr></tbody></table><h3 id=filesystem-alerts>Filesystem Alerts</h3><table><thead><tr><th>Rule</th><th style=text-align:center>Level</th><th>Description</th></tr></thead><tbody><tr><td><code>NodeFsSpaceFull</code></td><td style=text-align:center>WARN</td><td>Disk usage exceeds 90%</td></tr><tr><td><code>NodeFsFilesFull</code></td><td style=text-align:center>WARN</td><td>Inode usage exceeds 90%</td></tr><tr><td><code>NodeFdFull</code></td><td style=text-align:center>WARN</td><td>File descriptor usage exceeds 90%</td></tr></tbody></table><h3 id=disk-alerts>Disk Alerts</h3><table><thead><tr><th>Rule</th><th style=text-align:center>Level</th><th>Description</th></tr></thead><tbody><tr><td><code>NodeDiskSlow</code></td><td style=text-align:center>WARN</td><td>Read/write latency exceeds 32ms</td></tr></tbody></table><h3 id=network-protocol-alerts>Network Protocol Alerts</h3><table><thead><tr><th>Rule</th><th style=text-align:center>Level</th><th>Description</th></tr></thead><tbody><tr><td><code>NodeTcpErrHigh</code></td><td style=text-align:center>WARN</td><td>TCP error rate exceeds 1/min</td></tr><tr><td><code>NodeTcpRetransHigh</code></td><td style=text-align:center>WARN</td><td>TCP retransmission rate exceeds 1%</td></tr></tbody></table><h3 id=time-synchronization-alerts>Time Synchronization Alerts</h3><table><thead><tr><th>Rule</th><th style=text-align:center>Level</th><th>Description</th></tr></thead><tbody><tr><td><code>NodeTimeDrift</code></td><td style=text-align:center>WARN</td><td>System time not synchronized</td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-21159d62f30ba4efa931b151d5790100>6 - Metrics</h1><div class=lead>Complete list of monitoring metrics provided by Pigsty NODE module</div><p>The <a href=/docs/node><strong><code>NODE</code></strong></a> module has 747 available metrics.</p><table><thead><tr><th>Metric Name</th><th>Type</th><th>Labels</th><th>Description</th></tr></thead><tbody><tr><td>ALERTS</td><td>Unknown</td><td><code>alertname</code>, <code>ip</code>, <code>level</code>, <code>severity</code>, <code>ins</code>, <code>job</code>, <code>alertstate</code>, <code>category</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>ALERTS_FOR_STATE</td><td>Unknown</td><td><code>alertname</code>, <code>ip</code>, <code>level</code>, <code>severity</code>, <code>ins</code>, <code>job</code>, <code>category</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>deprecated_flags_inuse_total</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>go_gc_duration_seconds</td><td>summary</td><td><code>quantile</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>A summary of the pause duration of garbage collection cycles.</td></tr><tr><td>go_gc_duration_seconds_count</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>go_gc_duration_seconds_sum</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>go_goroutines</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of goroutines that currently exist.</td></tr><tr><td>go_info</td><td>gauge</td><td><code>version</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Information about the Go environment.</td></tr><tr><td>go_memstats_alloc_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of bytes allocated and still in use.</td></tr><tr><td>go_memstats_alloc_bytes_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of bytes allocated, even if freed.</td></tr><tr><td>go_memstats_buck_hash_sys_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of bytes used by the profiling bucket hash table.</td></tr><tr><td>go_memstats_frees_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of frees.</td></tr><tr><td>go_memstats_gc_sys_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of bytes used for garbage collection system metadata.</td></tr><tr><td>go_memstats_heap_alloc_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of heap bytes allocated and still in use.</td></tr><tr><td>go_memstats_heap_idle_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of heap bytes waiting to be used.</td></tr><tr><td>go_memstats_heap_inuse_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of heap bytes that are in use.</td></tr><tr><td>go_memstats_heap_objects</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of allocated objects.</td></tr><tr><td>go_memstats_heap_released_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of heap bytes released to OS.</td></tr><tr><td>go_memstats_heap_sys_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of heap bytes obtained from system.</td></tr><tr><td>go_memstats_last_gc_time_seconds</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of seconds since 1970 of last garbage collection.</td></tr><tr><td>go_memstats_lookups_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of pointer lookups.</td></tr><tr><td>go_memstats_mallocs_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of mallocs.</td></tr><tr><td>go_memstats_mcache_inuse_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of bytes in use by mcache structures.</td></tr><tr><td>go_memstats_mcache_sys_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of bytes used for mcache structures obtained from system.</td></tr><tr><td>go_memstats_mspan_inuse_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of bytes in use by mspan structures.</td></tr><tr><td>go_memstats_mspan_sys_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of bytes used for mspan structures obtained from system.</td></tr><tr><td>go_memstats_next_gc_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of heap bytes when next garbage collection will take place.</td></tr><tr><td>go_memstats_other_sys_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of bytes used for other system allocations.</td></tr><tr><td>go_memstats_stack_inuse_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of bytes in use by the stack allocator.</td></tr><tr><td>go_memstats_stack_sys_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of bytes obtained from system for stack allocator.</td></tr><tr><td>go_memstats_sys_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of bytes obtained from system.</td></tr><tr><td>go_threads</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of OS threads created.</td></tr><tr><td>haproxy:cls:usage</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>haproxy:ins:uptime</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>haproxy:ins:usage</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>haproxy_backend_active_servers</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of active UP servers with a non-zero weight</td></tr><tr><td>haproxy_backend_agg_check_status</td><td>gauge</td><td><code>state</code>, <code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Backend&rsquo;s aggregated gauge of servers&rsquo; state check status</td></tr><tr><td>haproxy_backend_agg_server_check_status</td><td>gauge</td><td><code>state</code>, <code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>[DEPRECATED] Backend&rsquo;s aggregated gauge of servers&rsquo; status</td></tr><tr><td>haproxy_backend_agg_server_status</td><td>gauge</td><td><code>state</code>, <code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Backend&rsquo;s aggregated gauge of servers&rsquo; status</td></tr><tr><td>haproxy_backend_backup_servers</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of backup UP servers with a non-zero weight</td></tr><tr><td>haproxy_backend_bytes_in_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of request bytes since process started</td></tr><tr><td>haproxy_backend_bytes_out_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of response bytes since process started</td></tr><tr><td>haproxy_backend_check_last_change_seconds</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>How long ago the last server state changed, in seconds</td></tr><tr><td>haproxy_backend_check_up_down_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of failed checks causing UP to DOWN server transitions, per server/backend, since the worker process started</td></tr><tr><td>haproxy_backend_client_aborts_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of requests or connections aborted by the client since the worker process started</td></tr><tr><td>haproxy_backend_connect_time_average_seconds</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Avg. connect time for last 1024 successful connections.</td></tr><tr><td>haproxy_backend_connection_attempts_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of outgoing connection attempts on this backend/server since the worker process started</td></tr><tr><td>haproxy_backend_connection_errors_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of failed connections to server since the worker process started</td></tr><tr><td>haproxy_backend_connection_reuses_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of reused connection on this backend/server since the worker process started</td></tr><tr><td>haproxy_backend_current_queue</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of current queued connections</td></tr><tr><td>haproxy_backend_current_sessions</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of current sessions on the frontend, backend or server</td></tr><tr><td>haproxy_backend_downtime_seconds_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total time spent in DOWN state, for server or backend</td></tr><tr><td>haproxy_backend_failed_header_rewriting_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of failed HTTP header rewrites since the worker process started</td></tr><tr><td>haproxy_backend_http_cache_hits_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of HTTP requests not found in the cache on this frontend/backend since the worker process started</td></tr><tr><td>haproxy_backend_http_cache_lookups_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of HTTP requests looked up in the cache on this frontend/backend since the worker process started</td></tr><tr><td>haproxy_backend_http_comp_bytes_bypassed_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of bytes that bypassed HTTP compression for this object since the worker process started (CPU/memory/bandwidth limitation)</td></tr><tr><td>haproxy_backend_http_comp_bytes_in_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of bytes submitted to the HTTP compressor for this object since the worker process started</td></tr><tr><td>haproxy_backend_http_comp_bytes_out_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of bytes emitted by the HTTP compressor for this object since the worker process started</td></tr><tr><td>haproxy_backend_http_comp_responses_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of HTTP responses that were compressed for this object since the worker process started</td></tr><tr><td>haproxy_backend_http_requests_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of HTTP requests processed by this object since the worker process started</td></tr><tr><td>haproxy_backend_http_responses_total</td><td>counter</td><td><code>ip</code>, <code>proxy</code>, <code>ins</code>, <code>code</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Total number of HTTP responses with status 100-199 returned by this object since the worker process started</td></tr><tr><td>haproxy_backend_internal_errors_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of internal errors since process started</td></tr><tr><td>haproxy_backend_last_session_seconds</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>How long ago some traffic was seen on this object on this worker process, in seconds</td></tr><tr><td>haproxy_backend_limit_sessions</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Frontend/listener/server&rsquo;s maxconn, backend&rsquo;s fullconn</td></tr><tr><td>haproxy_backend_loadbalanced_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of requests routed by load balancing since the worker process started (ignores queue pop and stickiness)</td></tr><tr><td>haproxy_backend_max_connect_time_seconds</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Maximum observed time spent waiting for a connection to complete</td></tr><tr><td>haproxy_backend_max_queue</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Highest value of queued connections encountered since process started</td></tr><tr><td>haproxy_backend_max_queue_time_seconds</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Maximum observed time spent in the queue</td></tr><tr><td>haproxy_backend_max_response_time_seconds</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Maximum observed time spent waiting for a server response</td></tr><tr><td>haproxy_backend_max_session_rate</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Highest value of sessions per second observed since the worker process started</td></tr><tr><td>haproxy_backend_max_sessions</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Highest value of current sessions encountered since process started</td></tr><tr><td>haproxy_backend_max_total_time_seconds</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Maximum observed total request+response time (request+queue+connect+response+processing)</td></tr><tr><td>haproxy_backend_queue_time_average_seconds</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Avg. queue time for last 1024 successful connections.</td></tr><tr><td>haproxy_backend_redispatch_warnings_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of server redispatches due to connection failures since the worker process started</td></tr><tr><td>haproxy_backend_requests_denied_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of denied requests since process started</td></tr><tr><td>haproxy_backend_response_errors_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of invalid responses since the worker process started</td></tr><tr><td>haproxy_backend_response_time_average_seconds</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Avg. response time for last 1024 successful connections.</td></tr><tr><td>haproxy_backend_responses_denied_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of denied responses since process started</td></tr><tr><td>haproxy_backend_retry_warnings_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of server connection retries since the worker process started</td></tr><tr><td>haproxy_backend_server_aborts_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of requests or connections aborted by the server since the worker process started</td></tr><tr><td>haproxy_backend_sessions_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of sessions since process started</td></tr><tr><td>haproxy_backend_status</td><td>gauge</td><td><code>state</code>, <code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Current status of the service, per state label value.</td></tr><tr><td>haproxy_backend_total_time_average_seconds</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Avg. total time for last 1024 successful connections.</td></tr><tr><td>haproxy_backend_uweight</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Server&rsquo;s user weight, or sum of active servers&rsquo; user weights for a backend</td></tr><tr><td>haproxy_backend_weight</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Server&rsquo;s effective weight, or sum of active servers&rsquo; effective weights for a backend</td></tr><tr><td>haproxy_frontend_bytes_in_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of request bytes since process started</td></tr><tr><td>haproxy_frontend_bytes_out_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of response bytes since process started</td></tr><tr><td>haproxy_frontend_connections_rate_max</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Highest value of connections per second observed since the worker process started</td></tr><tr><td>haproxy_frontend_connections_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of new connections accepted on this frontend since the worker process started</td></tr><tr><td>haproxy_frontend_current_sessions</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of current sessions on the frontend, backend or server</td></tr><tr><td>haproxy_frontend_denied_connections_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of incoming connections blocked on a listener/frontend by a tcp-request connection rule since the worker process started</td></tr><tr><td>haproxy_frontend_denied_sessions_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of incoming sessions blocked on a listener/frontend by a tcp-request connection rule since the worker process started</td></tr><tr><td>haproxy_frontend_failed_header_rewriting_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of failed HTTP header rewrites since the worker process started</td></tr><tr><td>haproxy_frontend_http_cache_hits_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of HTTP requests not found in the cache on this frontend/backend since the worker process started</td></tr><tr><td>haproxy_frontend_http_cache_lookups_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of HTTP requests looked up in the cache on this frontend/backend since the worker process started</td></tr><tr><td>haproxy_frontend_http_comp_bytes_bypassed_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of bytes that bypassed HTTP compression for this object since the worker process started (CPU/memory/bandwidth limitation)</td></tr><tr><td>haproxy_frontend_http_comp_bytes_in_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of bytes submitted to the HTTP compressor for this object since the worker process started</td></tr><tr><td>haproxy_frontend_http_comp_bytes_out_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of bytes emitted by the HTTP compressor for this object since the worker process started</td></tr><tr><td>haproxy_frontend_http_comp_responses_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of HTTP responses that were compressed for this object since the worker process started</td></tr><tr><td>haproxy_frontend_http_requests_rate_max</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Highest value of http requests observed since the worker process started</td></tr><tr><td>haproxy_frontend_http_requests_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of HTTP requests processed by this object since the worker process started</td></tr><tr><td>haproxy_frontend_http_responses_total</td><td>counter</td><td><code>ip</code>, <code>proxy</code>, <code>ins</code>, <code>code</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Total number of HTTP responses with status 100-199 returned by this object since the worker process started</td></tr><tr><td>haproxy_frontend_intercepted_requests_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of HTTP requests intercepted on the frontend (redirects/stats/services) since the worker process started</td></tr><tr><td>haproxy_frontend_internal_errors_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of internal errors since process started</td></tr><tr><td>haproxy_frontend_limit_session_rate</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Limit on the number of sessions accepted in a second (frontend only, &lsquo;rate-limit sessions&rsquo; setting)</td></tr><tr><td>haproxy_frontend_limit_sessions</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Frontend/listener/server&rsquo;s maxconn, backend&rsquo;s fullconn</td></tr><tr><td>haproxy_frontend_max_session_rate</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Highest value of sessions per second observed since the worker process started</td></tr><tr><td>haproxy_frontend_max_sessions</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Highest value of current sessions encountered since process started</td></tr><tr><td>haproxy_frontend_request_errors_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of invalid requests since process started</td></tr><tr><td>haproxy_frontend_requests_denied_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of denied requests since process started</td></tr><tr><td>haproxy_frontend_responses_denied_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of denied responses since process started</td></tr><tr><td>haproxy_frontend_sessions_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of sessions since process started</td></tr><tr><td>haproxy_frontend_status</td><td>gauge</td><td><code>state</code>, <code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Current status of the service, per state label value.</td></tr><tr><td>haproxy_process_active_peers</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Current number of verified active peers connections on the current worker process</td></tr><tr><td>haproxy_process_build_info</td><td>gauge</td><td><code>version</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Build info</td></tr><tr><td>haproxy_process_busy_polling_enabled</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>1 if busy-polling is currently in use on the worker process, otherwise zero (config.busy-polling)</td></tr><tr><td>haproxy_process_bytes_out_rate</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of bytes emitted by current worker process over the last second</td></tr><tr><td>haproxy_process_bytes_out_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of bytes emitted by current worker process since started</td></tr><tr><td>haproxy_process_connected_peers</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Current number of peers having passed the connection step on the current worker process</td></tr><tr><td>haproxy_process_connections_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of connections on this worker process since started</td></tr><tr><td>haproxy_process_current_backend_ssl_key_rate</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of SSL keys created on backends in this worker process over the last second</td></tr><tr><td>haproxy_process_current_connection_rate</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of front connections created on this worker process over the last second</td></tr><tr><td>haproxy_process_current_connections</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Current number of connections on this worker process</td></tr><tr><td>haproxy_process_current_frontend_ssl_key_rate</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of SSL keys created on frontends in this worker process over the last second</td></tr><tr><td>haproxy_process_current_run_queue</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of active tasks+tasklets in the current worker process</td></tr><tr><td>haproxy_process_current_session_rate</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of sessions created on this worker process over the last second</td></tr><tr><td>haproxy_process_current_ssl_connections</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Current number of SSL endpoints on this worker process (front+back)</td></tr><tr><td>haproxy_process_current_ssl_rate</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of SSL connections created on this worker process over the last second</td></tr><tr><td>haproxy_process_current_tasks</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of tasks in the current worker process (active + sleeping)</td></tr><tr><td>haproxy_process_current_zlib_memory</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Amount of memory currently used by HTTP compression on the current worker process (in bytes)</td></tr><tr><td>haproxy_process_dropped_logs_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of dropped logs for current worker process since started</td></tr><tr><td>haproxy_process_failed_resolutions</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of failed DNS resolutions in current worker process since started</td></tr><tr><td>haproxy_process_frontend_ssl_reuse</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Percent of frontend SSL connections which did not require a new key</td></tr><tr><td>haproxy_process_hard_max_connections</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Hard limit on the number of per-process connections (imposed by Memmax_MB or Ulimit-n)</td></tr><tr><td>haproxy_process_http_comp_bytes_in_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of bytes submitted to the HTTP compressor in this worker process over the last second</td></tr><tr><td>haproxy_process_http_comp_bytes_out_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of bytes emitted by the HTTP compressor in this worker process over the last second</td></tr><tr><td>haproxy_process_idle_time_percent</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Percentage of last second spent waiting in the current worker thread</td></tr><tr><td>haproxy_process_jobs</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Current number of active jobs on the current worker process (frontend connections, master connections, listeners)</td></tr><tr><td>haproxy_process_limit_connection_rate</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Hard limit for ConnRate (global.maxconnrate)</td></tr><tr><td>haproxy_process_limit_http_comp</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Limit of CompressBpsOut beyond which HTTP compression is automatically disabled</td></tr><tr><td>haproxy_process_limit_session_rate</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Hard limit for SessRate (global.maxsessrate)</td></tr><tr><td>haproxy_process_limit_ssl_rate</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Hard limit for SslRate (global.maxsslrate)</td></tr><tr><td>haproxy_process_listeners</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Current number of active listeners on the current worker process</td></tr><tr><td>haproxy_process_max_backend_ssl_key_rate</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Highest SslBackendKeyRate reached on this worker process since started (in SSL keys per second)</td></tr><tr><td>haproxy_process_max_connection_rate</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Highest ConnRate reached on this worker process since started (in connections per second)</td></tr><tr><td>haproxy_process_max_connections</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Hard limit on the number of per-process connections (configured or imposed by Ulimit-n)</td></tr><tr><td>haproxy_process_max_fds</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Hard limit on the number of per-process file descriptors</td></tr><tr><td>haproxy_process_max_frontend_ssl_key_rate</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Highest SslFrontendKeyRate reached on this worker process since started (in SSL keys per second)</td></tr><tr><td>haproxy_process_max_memory_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Worker process&rsquo;s hard limit on memory usage in byes (-m on command line)</td></tr><tr><td>haproxy_process_max_pipes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Hard limit on the number of pipes for splicing, 0=unlimited</td></tr><tr><td>haproxy_process_max_session_rate</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Highest SessRate reached on this worker process since started (in sessions per second)</td></tr><tr><td>haproxy_process_max_sockets</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Hard limit on the number of per-process sockets</td></tr><tr><td>haproxy_process_max_ssl_connections</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Hard limit on the number of per-process SSL endpoints (front+back), 0=unlimited</td></tr><tr><td>haproxy_process_max_ssl_rate</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Highest SslRate reached on this worker process since started (in connections per second)</td></tr><tr><td>haproxy_process_max_zlib_memory</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Limit on the amount of memory used by HTTP compression above which it is automatically disabled (in bytes, see global.maxzlibmem)</td></tr><tr><td>haproxy_process_nbproc</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of started worker processes (historical, always 1)</td></tr><tr><td>haproxy_process_nbthread</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of started threads (global.nbthread)</td></tr><tr><td>haproxy_process_pipes_free_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Current number of allocated and available pipes in this worker process</td></tr><tr><td>haproxy_process_pipes_used_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Current number of pipes in use in this worker process</td></tr><tr><td>haproxy_process_pool_allocated_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Amount of memory allocated in pools (in bytes)</td></tr><tr><td>haproxy_process_pool_failures_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of failed pool allocations since this worker was started</td></tr><tr><td>haproxy_process_pool_used_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Amount of pool memory currently used (in bytes)</td></tr><tr><td>haproxy_process_recv_logs_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of log messages received by log-forwarding listeners on this worker process since started</td></tr><tr><td>haproxy_process_relative_process_id</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Relative worker process number (1)</td></tr><tr><td>haproxy_process_requests_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of requests on this worker process since started</td></tr><tr><td>haproxy_process_spliced_bytes_out_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of bytes emitted by current worker process through a kernel pipe since started</td></tr><tr><td>haproxy_process_ssl_cache_lookups_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of SSL session ID lookups in the SSL session cache on this worker since started</td></tr><tr><td>haproxy_process_ssl_cache_misses_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of SSL session ID lookups that didn&rsquo;t find a session in the SSL session cache on this worker since started</td></tr><tr><td>haproxy_process_ssl_connections_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of SSL endpoints on this worker process since started (front+back)</td></tr><tr><td>haproxy_process_start_time_seconds</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Start time in seconds</td></tr><tr><td>haproxy_process_stopping</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>1 if the worker process is currently stopping, otherwise zero</td></tr><tr><td>haproxy_process_unstoppable_jobs</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Current number of unstoppable jobs on the current worker process (master connections)</td></tr><tr><td>haproxy_process_uptime_seconds</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>How long ago this worker process was started (seconds)</td></tr><tr><td>haproxy_server_bytes_in_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Total number of request bytes since process started</td></tr><tr><td>haproxy_server_bytes_out_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Total number of response bytes since process started</td></tr><tr><td>haproxy_server_check_code</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>layer5-7 code, if available of the last health check.</td></tr><tr><td>haproxy_server_check_duration_seconds</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Total duration of the latest server health check, in seconds.</td></tr><tr><td>haproxy_server_check_failures_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Total number of failed individual health checks per server/backend, since the worker process started</td></tr><tr><td>haproxy_server_check_last_change_seconds</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>How long ago the last server state changed, in seconds</td></tr><tr><td>haproxy_server_check_status</td><td>gauge</td><td><code>state</code>, <code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Status of last health check, per state label value.</td></tr><tr><td>haproxy_server_check_up_down_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Total number of failed checks causing UP to DOWN server transitions, per server/backend, since the worker process started</td></tr><tr><td>haproxy_server_client_aborts_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Total number of requests or connections aborted by the client since the worker process started</td></tr><tr><td>haproxy_server_connect_time_average_seconds</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Avg. connect time for last 1024 successful connections.</td></tr><tr><td>haproxy_server_connection_attempts_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Total number of outgoing connection attempts on this backend/server since the worker process started</td></tr><tr><td>haproxy_server_connection_errors_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Total number of failed connections to server since the worker process started</td></tr><tr><td>haproxy_server_connection_reuses_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Total number of reused connection on this backend/server since the worker process started</td></tr><tr><td>haproxy_server_current_queue</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Number of current queued connections</td></tr><tr><td>haproxy_server_current_sessions</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Number of current sessions on the frontend, backend or server</td></tr><tr><td>haproxy_server_current_throttle</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Throttling ratio applied to a server&rsquo;s maxconn and weight during the slowstart period (0 to 100%)</td></tr><tr><td>haproxy_server_downtime_seconds_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Total time spent in DOWN state, for server or backend</td></tr><tr><td>haproxy_server_failed_header_rewriting_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Total number of failed HTTP header rewrites since the worker process started</td></tr><tr><td>haproxy_server_idle_connections_current</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Current number of idle connections available for reuse on this server</td></tr><tr><td>haproxy_server_idle_connections_limit</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Limit on the number of available idle connections on this server (server &lsquo;pool_max_conn&rsquo; directive)</td></tr><tr><td>haproxy_server_internal_errors_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Total number of internal errors since process started</td></tr><tr><td>haproxy_server_last_session_seconds</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>How long ago some traffic was seen on this object on this worker process, in seconds</td></tr><tr><td>haproxy_server_limit_sessions</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Frontend/listener/server&rsquo;s maxconn, backend&rsquo;s fullconn</td></tr><tr><td>haproxy_server_loadbalanced_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Total number of requests routed by load balancing since the worker process started (ignores queue pop and stickiness)</td></tr><tr><td>haproxy_server_max_connect_time_seconds</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Maximum observed time spent waiting for a connection to complete</td></tr><tr><td>haproxy_server_max_queue</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Highest value of queued connections encountered since process started</td></tr><tr><td>haproxy_server_max_queue_time_seconds</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Maximum observed time spent in the queue</td></tr><tr><td>haproxy_server_max_response_time_seconds</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Maximum observed time spent waiting for a server response</td></tr><tr><td>haproxy_server_max_session_rate</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Highest value of sessions per second observed since the worker process started</td></tr><tr><td>haproxy_server_max_sessions</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Highest value of current sessions encountered since process started</td></tr><tr><td>haproxy_server_max_total_time_seconds</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Maximum observed total request+response time (request+queue+connect+response+processing)</td></tr><tr><td>haproxy_server_need_connections_current</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Estimated needed number of connections</td></tr><tr><td>haproxy_server_queue_limit</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Limit on the number of connections in queue, for servers only (maxqueue argument)</td></tr><tr><td>haproxy_server_queue_time_average_seconds</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Avg. queue time for last 1024 successful connections.</td></tr><tr><td>haproxy_server_redispatch_warnings_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Total number of server redispatches due to connection failures since the worker process started</td></tr><tr><td>haproxy_server_response_errors_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Total number of invalid responses since the worker process started</td></tr><tr><td>haproxy_server_response_time_average_seconds</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Avg. response time for last 1024 successful connections.</td></tr><tr><td>haproxy_server_responses_denied_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Total number of denied responses since process started</td></tr><tr><td>haproxy_server_retry_warnings_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Total number of server connection retries since the worker process started</td></tr><tr><td>haproxy_server_safe_idle_connections_current</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Current number of safe idle connections</td></tr><tr><td>haproxy_server_server_aborts_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Total number of requests or connections aborted by the server since the worker process started</td></tr><tr><td>haproxy_server_sessions_total</td><td>counter</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Total number of sessions since process started</td></tr><tr><td>haproxy_server_status</td><td>gauge</td><td><code>state</code>, <code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Current status of the service, per state label value.</td></tr><tr><td>haproxy_server_total_time_average_seconds</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Avg. total time for last 1024 successful connections.</td></tr><tr><td>haproxy_server_unsafe_idle_connections_current</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Current number of unsafe idle connections</td></tr><tr><td>haproxy_server_used_connections_current</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Current number of connections in use</td></tr><tr><td>haproxy_server_uweight</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Server&rsquo;s user weight, or sum of active servers&rsquo; user weights for a backend</td></tr><tr><td>haproxy_server_weight</td><td>gauge</td><td><code>proxy</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>server</code>, <code>ip</code>, <code>cls</code></td><td>Server&rsquo;s effective weight, or sum of active servers&rsquo; effective weights for a backend</td></tr><tr><td>haproxy_up</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>inflight_requests</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>route</code>, <code>ip</code>, <code>cls</code>, <code>method</code></td><td>Current number of inflight requests.</td></tr><tr><td>jaeger_tracer_baggage_restrictions_updates_total</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>result</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>jaeger_tracer_baggage_truncations_total</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>jaeger_tracer_baggage_updates_total</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>result</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>jaeger_tracer_finished_spans_total</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>sampled</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>jaeger_tracer_reporter_queue_length</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Current number of spans in the reporter queue</td></tr><tr><td>jaeger_tracer_reporter_spans_total</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>result</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>jaeger_tracer_sampler_queries_total</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>result</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>jaeger_tracer_sampler_updates_total</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>result</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>jaeger_tracer_span_context_decoding_errors_total</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>jaeger_tracer_started_spans_total</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>sampled</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>jaeger_tracer_throttled_debug_spans_total</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>jaeger_tracer_throttler_updates_total</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>result</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>jaeger_tracer_traces_total</td><td>Unknown</td><td><code>state</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>sampled</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>loki_experimental_features_in_use_total</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>loki_internal_log_messages_total</td><td>Unknown</td><td><code>level</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>loki_log_flushes_bucket</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>le</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>loki_log_flushes_count</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>loki_log_flushes_sum</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>loki_log_messages_total</td><td>Unknown</td><td><code>level</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>loki_logql_querystats_duplicates_total</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>loki_logql_querystats_ingester_sent_lines_total</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>loki_querier_index_cache_corruptions_total</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>loki_querier_index_cache_encode_errors_total</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>loki_querier_index_cache_gets_total</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>loki_querier_index_cache_hits_total</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>loki_querier_index_cache_puts_total</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>net_conntrack_dialer_conn_attempted_total</td><td>counter</td><td><code>ip</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code>, <code>dialer_name</code></td><td>Total number of connections attempted by the given dialer a given name.</td></tr><tr><td>net_conntrack_dialer_conn_closed_total</td><td>counter</td><td><code>ip</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code>, <code>dialer_name</code></td><td>Total number of connections closed which originated from the dialer of a given name.</td></tr><tr><td>net_conntrack_dialer_conn_established_total</td><td>counter</td><td><code>ip</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code>, <code>dialer_name</code></td><td>Total number of connections successfully established by the given dialer a given name.</td></tr><tr><td>net_conntrack_dialer_conn_failed_total</td><td>counter</td><td><code>ip</code>, <code>ins</code>, <code>job</code>, <code>reason</code>, <code>instance</code>, <code>cls</code>, <code>dialer_name</code></td><td>Total number of connections failed to dial by the dialer a given name.</td></tr><tr><td>node:cls:avail_bytes</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:cpu_count</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:cpu_usage</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:cpu_usage_15m</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:cpu_usage_1m</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:cpu_usage_5m</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:disk_io_bytes_rate1m</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:disk_iops_1m</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:disk_mreads_rate1m</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:disk_mreads_ratio1m</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:disk_mwrites_rate1m</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:disk_mwrites_ratio1m</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:disk_read_bytes_rate1m</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:disk_reads_rate1m</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:disk_write_bytes_rate1m</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:disk_writes_rate1m</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:free_bytes</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:mem_usage</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:network_io_bytes_rate1m</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:network_rx_bytes_rate1m</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:network_rx_pps1m</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:network_tx_bytes_rate1m</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:network_tx_pps1m</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:size_bytes</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:space_usage</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:space_usage_max</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:stdload1</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:stdload15</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:stdload5</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cls:time_drift_max</td><td>Unknown</td><td><code>job</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cpu:idle_time_irate1m</td><td>Unknown</td><td><code>ip</code>, <code>ins</code>, <code>job</code>, <code>cpu</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cpu:sched_timeslices_rate1m</td><td>Unknown</td><td><code>ip</code>, <code>ins</code>, <code>job</code>, <code>cpu</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cpu:sched_wait_rate1m</td><td>Unknown</td><td><code>ip</code>, <code>ins</code>, <code>job</code>, <code>cpu</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cpu:time_irate1m</td><td>Unknown</td><td><code>ip</code>, <code>mode</code>, <code>ins</code>, <code>job</code>, <code>cpu</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cpu:total_time_irate1m</td><td>Unknown</td><td><code>ip</code>, <code>ins</code>, <code>job</code>, <code>cpu</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cpu:usage</td><td>Unknown</td><td><code>ip</code>, <code>ins</code>, <code>job</code>, <code>cpu</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cpu:usage_avg15m</td><td>Unknown</td><td><code>ip</code>, <code>ins</code>, <code>job</code>, <code>cpu</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cpu:usage_avg1m</td><td>Unknown</td><td><code>ip</code>, <code>ins</code>, <code>job</code>, <code>cpu</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:cpu:usage_avg5m</td><td>Unknown</td><td><code>ip</code>, <code>ins</code>, <code>job</code>, <code>cpu</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:dev:disk_avg_queue_size</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:dev:disk_io_batch_1m</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:dev:disk_io_bytes_rate1m</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:dev:disk_io_rt_1m</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:dev:disk_io_time_rate1m</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:dev:disk_iops_1m</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:dev:disk_mreads_rate1m</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:dev:disk_mreads_ratio1m</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:dev:disk_mwrites_rate1m</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:dev:disk_mwrites_ratio1m</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:dev:disk_read_batch_1m</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:dev:disk_read_bytes_rate1m</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:dev:disk_read_rt_1m</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:dev:disk_read_time_rate1m</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:dev:disk_reads_rate1m</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:dev:disk_util_1m</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:dev:disk_write_batch_1m</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:dev:disk_write_bytes_rate1m</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:dev:disk_write_rt_1m</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:dev:disk_write_time_rate1m</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:dev:disk_writes_rate1m</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:dev:network_io_bytes_rate1m</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:dev:network_rx_bytes_rate1m</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:dev:network_rx_pps1m</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:dev:network_tx_bytes_rate1m</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:dev:network_tx_pps1m</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:env:avail_bytes</td><td>Unknown</td><td><code>job</code></td><td>N/A</td></tr><tr><td>node:env:cpu_count</td><td>Unknown</td><td><code>job</code></td><td>N/A</td></tr><tr><td>node:env:cpu_usage</td><td>Unknown</td><td><code>job</code></td><td>N/A</td></tr><tr><td>node:env:cpu_usage_15m</td><td>Unknown</td><td><code>job</code></td><td>N/A</td></tr><tr><td>node:env:cpu_usage_1m</td><td>Unknown</td><td><code>job</code></td><td>N/A</td></tr><tr><td>node:env:cpu_usage_5m</td><td>Unknown</td><td><code>job</code></td><td>N/A</td></tr><tr><td>node:env:device_space_usage_max</td><td>Unknown</td><td><code>device</code>, <code>mountpoint</code>, <code>job</code>, <code>fstype</code></td><td>N/A</td></tr><tr><td>node:env:free_bytes</td><td>Unknown</td><td><code>job</code></td><td>N/A</td></tr><tr><td>node:env:mem_avail</td><td>Unknown</td><td><code>job</code></td><td>N/A</td></tr><tr><td>node:env:mem_total</td><td>Unknown</td><td><code>job</code></td><td>N/A</td></tr><tr><td>node:env:mem_usage</td><td>Unknown</td><td><code>job</code></td><td>N/A</td></tr><tr><td>node:env:size_bytes</td><td>Unknown</td><td><code>job</code></td><td>N/A</td></tr><tr><td>node:env:space_usage</td><td>Unknown</td><td><code>job</code></td><td>N/A</td></tr><tr><td>node:env:stdload1</td><td>Unknown</td><td><code>job</code></td><td>N/A</td></tr><tr><td>node:env:stdload15</td><td>Unknown</td><td><code>job</code></td><td>N/A</td></tr><tr><td>node:env:stdload5</td><td>Unknown</td><td><code>job</code></td><td>N/A</td></tr><tr><td>node:fs:avail_bytes</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>mountpoint</code>, <code>ins</code>, <code>cls</code>, <code>job</code>, <code>instance</code>, <code>fstype</code></td><td>N/A</td></tr><tr><td>node:fs:free_bytes</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>mountpoint</code>, <code>ins</code>, <code>cls</code>, <code>job</code>, <code>instance</code>, <code>fstype</code></td><td>N/A</td></tr><tr><td>node:fs:inode_free</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>mountpoint</code>, <code>ins</code>, <code>cls</code>, <code>job</code>, <code>instance</code>, <code>fstype</code></td><td>N/A</td></tr><tr><td>node:fs:inode_total</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>mountpoint</code>, <code>ins</code>, <code>cls</code>, <code>job</code>, <code>instance</code>, <code>fstype</code></td><td>N/A</td></tr><tr><td>node:fs:inode_usage</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>mountpoint</code>, <code>ins</code>, <code>cls</code>, <code>job</code>, <code>instance</code>, <code>fstype</code></td><td>N/A</td></tr><tr><td>node:fs:inode_used</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>mountpoint</code>, <code>ins</code>, <code>cls</code>, <code>job</code>, <code>instance</code>, <code>fstype</code></td><td>N/A</td></tr><tr><td>node:fs:size_bytes</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>mountpoint</code>, <code>ins</code>, <code>cls</code>, <code>job</code>, <code>instance</code>, <code>fstype</code></td><td>N/A</td></tr><tr><td>node:fs:space_deriv1h</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>mountpoint</code>, <code>ins</code>, <code>cls</code>, <code>job</code>, <code>instance</code>, <code>fstype</code></td><td>N/A</td></tr><tr><td>node:fs:space_exhaust</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>mountpoint</code>, <code>ins</code>, <code>cls</code>, <code>job</code>, <code>instance</code>, <code>fstype</code></td><td>N/A</td></tr><tr><td>node:fs:space_predict_1d</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>mountpoint</code>, <code>ins</code>, <code>cls</code>, <code>job</code>, <code>instance</code>, <code>fstype</code></td><td>N/A</td></tr><tr><td>node:fs:space_usage</td><td>Unknown</td><td><code>ip</code>, <code>device</code>, <code>mountpoint</code>, <code>ins</code>, <code>cls</code>, <code>job</code>, <code>instance</code>, <code>fstype</code></td><td>N/A</td></tr><tr><td>node:ins</td><td>Unknown</td><td><code>id</code>, <code>ip</code>, <code>ins</code>, <code>job</code>, <code>nodename</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:avail_bytes</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:cpu_count</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:cpu_usage</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:cpu_usage_15m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:cpu_usage_1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:cpu_usage_5m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:ctx_switch_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:disk_io_bytes_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:disk_iops_1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:disk_mreads_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:disk_mreads_ratio1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:disk_mwrites_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:disk_mwrites_ratio1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:disk_read_bytes_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:disk_reads_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:disk_write_bytes_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:disk_writes_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:fd_alloc_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:fd_usage</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:forks_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:free_bytes</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:inode_usage</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:interrupt_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:mem_avail</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:mem_commit_ratio</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:mem_kernel</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:mem_rss</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:mem_usage</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:network_io_bytes_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:network_rx_bytes_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:network_rx_pps1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:network_tx_bytes_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:network_tx_pps1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:pagefault_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:pagein_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:pageout_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:pgmajfault_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:sched_wait_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:size_bytes</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:space_usage_max</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:stdload1</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:stdload15</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:stdload5</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:swap_usage</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:swapin_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:swapout_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:tcp_active_opens_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:tcp_dropped_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:tcp_error</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:tcp_error_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:tcp_insegs_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:tcp_outsegs_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:tcp_overflow_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:tcp_passive_opens_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:tcp_retrans_ratio1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:tcp_retranssegs_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:tcp_segs_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:time_drift</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:udp_in_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:udp_out_rate1m</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node:ins:uptime</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node_arp_entries</td><td>gauge</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>ARP entries by device</td></tr><tr><td>node_boot_time_seconds</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Node boot time, in unixtime.</td></tr><tr><td>node_context_switches_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of context switches.</td></tr><tr><td>node_cooling_device_cur_state</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>type</code>, <code>ip</code>, <code>cls</code></td><td>Current throttle state of the cooling device</td></tr><tr><td>node_cooling_device_max_state</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>type</code>, <code>ip</code>, <code>cls</code></td><td>Maximum throttle state of the cooling device</td></tr><tr><td>node_cpu_guest_seconds_total</td><td>counter</td><td><code>ip</code>, <code>mode</code>, <code>ins</code>, <code>job</code>, <code>cpu</code>, <code>instance</code>, <code>cls</code></td><td>Seconds the CPUs spent in guests (VMs) for each mode.</td></tr><tr><td>node_cpu_seconds_total</td><td>counter</td><td><code>ip</code>, <code>mode</code>, <code>ins</code>, <code>job</code>, <code>cpu</code>, <code>instance</code>, <code>cls</code></td><td>Seconds the CPUs spent in each mode.</td></tr><tr><td>node_disk_discard_time_seconds_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>This is the total number of seconds spent by all discards.</td></tr><tr><td>node_disk_discarded_sectors_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>The total number of sectors discarded successfully.</td></tr><tr><td>node_disk_discards_completed_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>The total number of discards completed successfully.</td></tr><tr><td>node_disk_discards_merged_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>The total number of discards merged.</td></tr><tr><td>node_disk_filesystem_info</td><td>gauge</td><td><code>ip</code>, <code>usage</code>, <code>version</code>, <code>device</code>, <code>uuid</code>, <code>ins</code>, <code>type</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Info about disk filesystem.</td></tr><tr><td>node_disk_info</td><td>gauge</td><td><code>minor</code>, <code>ip</code>, <code>major</code>, <code>revision</code>, <code>device</code>, <code>model</code>, <code>serial</code>, <code>path</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Info of /sys/block/&lt;block_device>.</td></tr><tr><td>node_disk_io_now</td><td>gauge</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>The number of I/Os currently in progress.</td></tr><tr><td>node_disk_io_time_seconds_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Total seconds spent doing I/Os.</td></tr><tr><td>node_disk_io_time_weighted_seconds_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>The weighted # of seconds spent doing I/Os.</td></tr><tr><td>node_disk_read_bytes_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>The total number of bytes read successfully.</td></tr><tr><td>node_disk_read_time_seconds_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>The total number of seconds spent by all reads.</td></tr><tr><td>node_disk_reads_completed_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>The total number of reads completed successfully.</td></tr><tr><td>node_disk_reads_merged_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>The total number of reads merged.</td></tr><tr><td>node_disk_write_time_seconds_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>This is the total number of seconds spent by all writes.</td></tr><tr><td>node_disk_writes_completed_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>The total number of writes completed successfully.</td></tr><tr><td>node_disk_writes_merged_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>The number of writes merged.</td></tr><tr><td>node_disk_written_bytes_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>The total number of bytes written successfully.</td></tr><tr><td>node_dmi_info</td><td>gauge</td><td><code>bios_vendor</code>, <code>ip</code>, <code>product_family</code>, <code>product_version</code>, <code>product_uuid</code>, <code>system_vendor</code>, <code>bios_version</code>, <code>ins</code>, <code>bios_date</code>, <code>cls</code>, <code>job</code>, <code>product_name</code>, <code>instance</code>, <code>chassis_version</code>, <code>chassis_vendor</code>, <code>product_serial</code></td><td>A metric with a constant &lsquo;1&rsquo; value labeled by bios_date, bios_release, bios_vendor, bios_version, board_asset_tag, board_name, board_serial, board_vendor, board_version, chassis_asset_tag, chassis_serial, chassis_vendor, chassis_version, product_family, product_name, product_serial, product_sku, product_uuid, product_version, system_vendor if provided by DMI.</td></tr><tr><td>node_entropy_available_bits</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Bits of available entropy.</td></tr><tr><td>node_entropy_pool_size_bits</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Bits of entropy pool.</td></tr><tr><td>node_exporter_build_info</td><td>gauge</td><td><code>ip</code>, <code>version</code>, <code>revision</code>, <code>goversion</code>, <code>branch</code>, <code>ins</code>, <code>goarch</code>, <code>job</code>, <code>tags</code>, <code>instance</code>, <code>cls</code>, <code>goos</code></td><td>A metric with a constant &lsquo;1&rsquo; value labeled by version, revision, branch, goversion from which node_exporter was built, and the goos and goarch for the build.</td></tr><tr><td>node_filefd_allocated</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>File descriptor statistics: allocated.</td></tr><tr><td>node_filefd_maximum</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>File descriptor statistics: maximum.</td></tr><tr><td>node_filesystem_avail_bytes</td><td>gauge</td><td><code>ip</code>, <code>device</code>, <code>mountpoint</code>, <code>ins</code>, <code>cls</code>, <code>job</code>, <code>instance</code>, <code>fstype</code></td><td>Filesystem space available to non-root users in bytes.</td></tr><tr><td>node_filesystem_device_error</td><td>gauge</td><td><code>ip</code>, <code>device</code>, <code>mountpoint</code>, <code>ins</code>, <code>cls</code>, <code>job</code>, <code>instance</code>, <code>fstype</code></td><td>Whether an error occurred while getting statistics for the given device.</td></tr><tr><td>node_filesystem_files</td><td>gauge</td><td><code>ip</code>, <code>device</code>, <code>mountpoint</code>, <code>ins</code>, <code>cls</code>, <code>job</code>, <code>instance</code>, <code>fstype</code></td><td>Filesystem total file nodes.</td></tr><tr><td>node_filesystem_files_free</td><td>gauge</td><td><code>ip</code>, <code>device</code>, <code>mountpoint</code>, <code>ins</code>, <code>cls</code>, <code>job</code>, <code>instance</code>, <code>fstype</code></td><td>Filesystem total free file nodes.</td></tr><tr><td>node_filesystem_free_bytes</td><td>gauge</td><td><code>ip</code>, <code>device</code>, <code>mountpoint</code>, <code>ins</code>, <code>cls</code>, <code>job</code>, <code>instance</code>, <code>fstype</code></td><td>Filesystem free space in bytes.</td></tr><tr><td>node_filesystem_readonly</td><td>gauge</td><td><code>ip</code>, <code>device</code>, <code>mountpoint</code>, <code>ins</code>, <code>cls</code>, <code>job</code>, <code>instance</code>, <code>fstype</code></td><td>Filesystem read-only status.</td></tr><tr><td>node_filesystem_size_bytes</td><td>gauge</td><td><code>ip</code>, <code>device</code>, <code>mountpoint</code>, <code>ins</code>, <code>cls</code>, <code>job</code>, <code>instance</code>, <code>fstype</code></td><td>Filesystem size in bytes.</td></tr><tr><td>node_forks_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of forks.</td></tr><tr><td>node_hwmon_chip_names</td><td>gauge</td><td><code>chip_name</code>, <code>ip</code>, <code>ins</code>, <code>chip</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Annotation metric for human-readable chip names</td></tr><tr><td>node_hwmon_energy_joule_total</td><td>counter</td><td><code>sensor</code>, <code>ip</code>, <code>ins</code>, <code>chip</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Hardware monitor for joules used so far (input)</td></tr><tr><td>node_hwmon_sensor_label</td><td>gauge</td><td><code>sensor</code>, <code>ip</code>, <code>ins</code>, <code>chip</code>, <code>job</code>, <code>label</code>, <code>instance</code>, <code>cls</code></td><td>Label for given chip and sensor</td></tr><tr><td>node_intr_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of interrupts serviced.</td></tr><tr><td>node_ipvs_connections_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>The total number of connections made.</td></tr><tr><td>node_ipvs_incoming_bytes_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>The total amount of incoming data.</td></tr><tr><td>node_ipvs_incoming_packets_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>The total number of incoming packets.</td></tr><tr><td>node_ipvs_outgoing_bytes_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>The total amount of outgoing data.</td></tr><tr><td>node_ipvs_outgoing_packets_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>The total number of outgoing packets.</td></tr><tr><td>node_load1</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>1m load average.</td></tr><tr><td>node_load15</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>15m load average.</td></tr><tr><td>node_load5</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>5m load average.</td></tr><tr><td>node_memory_Active_anon_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field Active_anon_bytes.</td></tr><tr><td>node_memory_Active_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field Active_bytes.</td></tr><tr><td>node_memory_Active_file_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field Active_file_bytes.</td></tr><tr><td>node_memory_AnonHugePages_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field AnonHugePages_bytes.</td></tr><tr><td>node_memory_AnonPages_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field AnonPages_bytes.</td></tr><tr><td>node_memory_Bounce_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field Bounce_bytes.</td></tr><tr><td>node_memory_Buffers_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field Buffers_bytes.</td></tr><tr><td>node_memory_Cached_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field Cached_bytes.</td></tr><tr><td>node_memory_CommitLimit_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field CommitLimit_bytes.</td></tr><tr><td>node_memory_Committed_AS_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field Committed_AS_bytes.</td></tr><tr><td>node_memory_DirectMap1G_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field DirectMap1G_bytes.</td></tr><tr><td>node_memory_DirectMap2M_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field DirectMap2M_bytes.</td></tr><tr><td>node_memory_DirectMap4k_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field DirectMap4k_bytes.</td></tr><tr><td>node_memory_Dirty_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field Dirty_bytes.</td></tr><tr><td>node_memory_FileHugePages_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field FileHugePages_bytes.</td></tr><tr><td>node_memory_FilePmdMapped_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field FilePmdMapped_bytes.</td></tr><tr><td>node_memory_HardwareCorrupted_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field HardwareCorrupted_bytes.</td></tr><tr><td>node_memory_HugePages_Free</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field HugePages_Free.</td></tr><tr><td>node_memory_HugePages_Rsvd</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field HugePages_Rsvd.</td></tr><tr><td>node_memory_HugePages_Surp</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field HugePages_Surp.</td></tr><tr><td>node_memory_HugePages_Total</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field HugePages_Total.</td></tr><tr><td>node_memory_Hugepagesize_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field Hugepagesize_bytes.</td></tr><tr><td>node_memory_Hugetlb_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field Hugetlb_bytes.</td></tr><tr><td>node_memory_Inactive_anon_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field Inactive_anon_bytes.</td></tr><tr><td>node_memory_Inactive_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field Inactive_bytes.</td></tr><tr><td>node_memory_Inactive_file_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field Inactive_file_bytes.</td></tr><tr><td>node_memory_KReclaimable_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field KReclaimable_bytes.</td></tr><tr><td>node_memory_KernelStack_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field KernelStack_bytes.</td></tr><tr><td>node_memory_Mapped_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field Mapped_bytes.</td></tr><tr><td>node_memory_MemAvailable_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field MemAvailable_bytes.</td></tr><tr><td>node_memory_MemFree_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field MemFree_bytes.</td></tr><tr><td>node_memory_MemTotal_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field MemTotal_bytes.</td></tr><tr><td>node_memory_Mlocked_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field Mlocked_bytes.</td></tr><tr><td>node_memory_NFS_Unstable_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field NFS_Unstable_bytes.</td></tr><tr><td>node_memory_PageTables_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field PageTables_bytes.</td></tr><tr><td>node_memory_Percpu_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field Percpu_bytes.</td></tr><tr><td>node_memory_SReclaimable_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field SReclaimable_bytes.</td></tr><tr><td>node_memory_SUnreclaim_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field SUnreclaim_bytes.</td></tr><tr><td>node_memory_ShmemHugePages_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field ShmemHugePages_bytes.</td></tr><tr><td>node_memory_ShmemPmdMapped_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field ShmemPmdMapped_bytes.</td></tr><tr><td>node_memory_Shmem_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field Shmem_bytes.</td></tr><tr><td>node_memory_Slab_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field Slab_bytes.</td></tr><tr><td>node_memory_SwapCached_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field SwapCached_bytes.</td></tr><tr><td>node_memory_SwapFree_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field SwapFree_bytes.</td></tr><tr><td>node_memory_SwapTotal_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field SwapTotal_bytes.</td></tr><tr><td>node_memory_Unevictable_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field Unevictable_bytes.</td></tr><tr><td>node_memory_VmallocChunk_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field VmallocChunk_bytes.</td></tr><tr><td>node_memory_VmallocTotal_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field VmallocTotal_bytes.</td></tr><tr><td>node_memory_VmallocUsed_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field VmallocUsed_bytes.</td></tr><tr><td>node_memory_WritebackTmp_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field WritebackTmp_bytes.</td></tr><tr><td>node_memory_Writeback_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Memory information field Writeback_bytes.</td></tr><tr><td>node_netstat_Icmp6_InErrors</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic Icmp6InErrors.</td></tr><tr><td>node_netstat_Icmp6_InMsgs</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic Icmp6InMsgs.</td></tr><tr><td>node_netstat_Icmp6_OutMsgs</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic Icmp6OutMsgs.</td></tr><tr><td>node_netstat_Icmp_InErrors</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic IcmpInErrors.</td></tr><tr><td>node_netstat_Icmp_InMsgs</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic IcmpInMsgs.</td></tr><tr><td>node_netstat_Icmp_OutMsgs</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic IcmpOutMsgs.</td></tr><tr><td>node_netstat_Ip6_InOctets</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic Ip6InOctets.</td></tr><tr><td>node_netstat_Ip6_OutOctets</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic Ip6OutOctets.</td></tr><tr><td>node_netstat_IpExt_InOctets</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic IpExtInOctets.</td></tr><tr><td>node_netstat_IpExt_OutOctets</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic IpExtOutOctets.</td></tr><tr><td>node_netstat_Ip_Forwarding</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic IpForwarding.</td></tr><tr><td>node_netstat_TcpExt_ListenDrops</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic TcpExtListenDrops.</td></tr><tr><td>node_netstat_TcpExt_ListenOverflows</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic TcpExtListenOverflows.</td></tr><tr><td>node_netstat_TcpExt_SyncookiesFailed</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic TcpExtSyncookiesFailed.</td></tr><tr><td>node_netstat_TcpExt_SyncookiesRecv</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic TcpExtSyncookiesRecv.</td></tr><tr><td>node_netstat_TcpExt_SyncookiesSent</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic TcpExtSyncookiesSent.</td></tr><tr><td>node_netstat_TcpExt_TCPSynRetrans</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic TcpExtTCPSynRetrans.</td></tr><tr><td>node_netstat_TcpExt_TCPTimeouts</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic TcpExtTCPTimeouts.</td></tr><tr><td>node_netstat_Tcp_ActiveOpens</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic TcpActiveOpens.</td></tr><tr><td>node_netstat_Tcp_CurrEstab</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic TcpCurrEstab.</td></tr><tr><td>node_netstat_Tcp_InErrs</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic TcpInErrs.</td></tr><tr><td>node_netstat_Tcp_InSegs</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic TcpInSegs.</td></tr><tr><td>node_netstat_Tcp_OutRsts</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic TcpOutRsts.</td></tr><tr><td>node_netstat_Tcp_OutSegs</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic TcpOutSegs.</td></tr><tr><td>node_netstat_Tcp_PassiveOpens</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic TcpPassiveOpens.</td></tr><tr><td>node_netstat_Tcp_RetransSegs</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic TcpRetransSegs.</td></tr><tr><td>node_netstat_Udp6_InDatagrams</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic Udp6InDatagrams.</td></tr><tr><td>node_netstat_Udp6_InErrors</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic Udp6InErrors.</td></tr><tr><td>node_netstat_Udp6_NoPorts</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic Udp6NoPorts.</td></tr><tr><td>node_netstat_Udp6_OutDatagrams</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic Udp6OutDatagrams.</td></tr><tr><td>node_netstat_Udp6_RcvbufErrors</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic Udp6RcvbufErrors.</td></tr><tr><td>node_netstat_Udp6_SndbufErrors</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic Udp6SndbufErrors.</td></tr><tr><td>node_netstat_UdpLite6_InErrors</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic UdpLite6InErrors.</td></tr><tr><td>node_netstat_UdpLite_InErrors</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic UdpLiteInErrors.</td></tr><tr><td>node_netstat_Udp_InDatagrams</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic UdpInDatagrams.</td></tr><tr><td>node_netstat_Udp_InErrors</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic UdpInErrors.</td></tr><tr><td>node_netstat_Udp_NoPorts</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic UdpNoPorts.</td></tr><tr><td>node_netstat_Udp_OutDatagrams</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic UdpOutDatagrams.</td></tr><tr><td>node_netstat_Udp_RcvbufErrors</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic UdpRcvbufErrors.</td></tr><tr><td>node_netstat_Udp_SndbufErrors</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Statistic UdpSndbufErrors.</td></tr><tr><td>node_network_address_assign_type</td><td>gauge</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device property: address_assign_type</td></tr><tr><td>node_network_carrier</td><td>gauge</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device property: carrier</td></tr><tr><td>node_network_carrier_changes_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device property: carrier_changes_total</td></tr><tr><td>node_network_carrier_down_changes_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device property: carrier_down_changes_total</td></tr><tr><td>node_network_carrier_up_changes_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device property: carrier_up_changes_total</td></tr><tr><td>node_network_device_id</td><td>gauge</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device property: device_id</td></tr><tr><td>node_network_dormant</td><td>gauge</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device property: dormant</td></tr><tr><td>node_network_flags</td><td>gauge</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device property: flags</td></tr><tr><td>node_network_iface_id</td><td>gauge</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device property: iface_id</td></tr><tr><td>node_network_iface_link</td><td>gauge</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device property: iface_link</td></tr><tr><td>node_network_iface_link_mode</td><td>gauge</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device property: iface_link_mode</td></tr><tr><td>node_network_info</td><td>gauge</td><td><code>broadcast</code>, <code>ip</code>, <code>device</code>, <code>operstate</code>, <code>ins</code>, <code>job</code>, <code>adminstate</code>, <code>duplex</code>, <code>address</code>, <code>instance</code>, <code>cls</code></td><td>Non-numeric data from /sys/class/net/<iface>, value is always 1.</td></tr><tr><td>node_network_mtu_bytes</td><td>gauge</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device property: mtu_bytes</td></tr><tr><td>node_network_name_assign_type</td><td>gauge</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device property: name_assign_type</td></tr><tr><td>node_network_net_dev_group</td><td>gauge</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device property: net_dev_group</td></tr><tr><td>node_network_protocol_type</td><td>gauge</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device property: protocol_type</td></tr><tr><td>node_network_receive_bytes_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device statistic receive_bytes.</td></tr><tr><td>node_network_receive_compressed_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device statistic receive_compressed.</td></tr><tr><td>node_network_receive_drop_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device statistic receive_drop.</td></tr><tr><td>node_network_receive_errs_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device statistic receive_errs.</td></tr><tr><td>node_network_receive_fifo_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device statistic receive_fifo.</td></tr><tr><td>node_network_receive_frame_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device statistic receive_frame.</td></tr><tr><td>node_network_receive_multicast_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device statistic receive_multicast.</td></tr><tr><td>node_network_receive_nohandler_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device statistic receive_nohandler.</td></tr><tr><td>node_network_receive_packets_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device statistic receive_packets.</td></tr><tr><td>node_network_speed_bytes</td><td>gauge</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device property: speed_bytes</td></tr><tr><td>node_network_transmit_bytes_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device statistic transmit_bytes.</td></tr><tr><td>node_network_transmit_carrier_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device statistic transmit_carrier.</td></tr><tr><td>node_network_transmit_colls_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device statistic transmit_colls.</td></tr><tr><td>node_network_transmit_compressed_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device statistic transmit_compressed.</td></tr><tr><td>node_network_transmit_drop_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device statistic transmit_drop.</td></tr><tr><td>node_network_transmit_errs_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device statistic transmit_errs.</td></tr><tr><td>node_network_transmit_fifo_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device statistic transmit_fifo.</td></tr><tr><td>node_network_transmit_packets_total</td><td>counter</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device statistic transmit_packets.</td></tr><tr><td>node_network_transmit_queue_length</td><td>gauge</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Network device property: transmit_queue_length</td></tr><tr><td>node_network_up</td><td>gauge</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Value is 1 if operstate is &lsquo;up&rsquo;, 0 otherwise.</td></tr><tr><td>node_nf_conntrack_entries</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of currently allocated flow entries for connection tracking.</td></tr><tr><td>node_nf_conntrack_entries_limit</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Maximum size of connection tracking table.</td></tr><tr><td>node_nf_conntrack_stat_drop</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of packets dropped due to conntrack failure.</td></tr><tr><td>node_nf_conntrack_stat_early_drop</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of dropped conntrack entries to make room for new ones, if maximum table size was reached.</td></tr><tr><td>node_nf_conntrack_stat_found</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of searched entries which were successful.</td></tr><tr><td>node_nf_conntrack_stat_ignore</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of packets seen which are already connected to a conntrack entry.</td></tr><tr><td>node_nf_conntrack_stat_insert</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of entries inserted into the list.</td></tr><tr><td>node_nf_conntrack_stat_insert_failed</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of entries for which list insertion was attempted but failed.</td></tr><tr><td>node_nf_conntrack_stat_invalid</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of packets seen which can not be tracked.</td></tr><tr><td>node_nf_conntrack_stat_search_restart</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of conntrack table lookups which had to be restarted due to hashtable resizes.</td></tr><tr><td>node_os_info</td><td>gauge</td><td><code>id</code>, <code>ip</code>, <code>version</code>, <code>version_id</code>, <code>ins</code>, <code>instance</code>, <code>job</code>, <code>pretty_name</code>, <code>id_like</code>, <code>cls</code></td><td>A metric with a constant &lsquo;1&rsquo; value labeled by build_id, id, id_like, image_id, image_version, name, pretty_name, variant, variant_id, version, version_codename, version_id.</td></tr><tr><td>node_os_version</td><td>gauge</td><td><code>id</code>, <code>ip</code>, <code>ins</code>, <code>instance</code>, <code>job</code>, <code>id_like</code>, <code>cls</code></td><td>Metric containing the major.minor part of the OS version.</td></tr><tr><td>node_processes_max_processes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of max PIDs limit</td></tr><tr><td>node_processes_max_threads</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Limit of threads in the system</td></tr><tr><td>node_processes_pids</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of PIDs</td></tr><tr><td>node_processes_state</td><td>gauge</td><td><code>state</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of processes in each state.</td></tr><tr><td>node_processes_threads</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Allocated threads in system</td></tr><tr><td>node_processes_threads_state</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>thread_state</code>, <code>ip</code>, <code>cls</code></td><td>Number of threads in each state.</td></tr><tr><td>node_procs_blocked</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of processes blocked waiting for I/O to complete.</td></tr><tr><td>node_procs_running</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of processes in runnable state.</td></tr><tr><td>node_schedstat_running_seconds_total</td><td>counter</td><td><code>ip</code>, <code>ins</code>, <code>job</code>, <code>cpu</code>, <code>instance</code>, <code>cls</code></td><td>Number of seconds CPU spent running a process.</td></tr><tr><td>node_schedstat_timeslices_total</td><td>counter</td><td><code>ip</code>, <code>ins</code>, <code>job</code>, <code>cpu</code>, <code>instance</code>, <code>cls</code></td><td>Number of timeslices executed by CPU.</td></tr><tr><td>node_schedstat_waiting_seconds_total</td><td>counter</td><td><code>ip</code>, <code>ins</code>, <code>job</code>, <code>cpu</code>, <code>instance</code>, <code>cls</code></td><td>Number of seconds spent by processing waiting for this CPU.</td></tr><tr><td>node_scrape_collector_duration_seconds</td><td>gauge</td><td><code>ip</code>, <code>collector</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>node_exporter: Duration of a collector scrape.</td></tr><tr><td>node_scrape_collector_success</td><td>gauge</td><td><code>ip</code>, <code>collector</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>node_exporter: Whether a collector succeeded.</td></tr><tr><td>node_selinux_enabled</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>SELinux is enabled, 1 is true, 0 is false</td></tr><tr><td>node_sockstat_FRAG6_inuse</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of FRAG6 sockets in state inuse.</td></tr><tr><td>node_sockstat_FRAG6_memory</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of FRAG6 sockets in state memory.</td></tr><tr><td>node_sockstat_FRAG_inuse</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of FRAG sockets in state inuse.</td></tr><tr><td>node_sockstat_FRAG_memory</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of FRAG sockets in state memory.</td></tr><tr><td>node_sockstat_RAW6_inuse</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of RAW6 sockets in state inuse.</td></tr><tr><td>node_sockstat_RAW_inuse</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of RAW sockets in state inuse.</td></tr><tr><td>node_sockstat_TCP6_inuse</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of TCP6 sockets in state inuse.</td></tr><tr><td>node_sockstat_TCP_alloc</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of TCP sockets in state alloc.</td></tr><tr><td>node_sockstat_TCP_inuse</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of TCP sockets in state inuse.</td></tr><tr><td>node_sockstat_TCP_mem</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of TCP sockets in state mem.</td></tr><tr><td>node_sockstat_TCP_mem_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of TCP sockets in state mem_bytes.</td></tr><tr><td>node_sockstat_TCP_orphan</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of TCP sockets in state orphan.</td></tr><tr><td>node_sockstat_TCP_tw</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of TCP sockets in state tw.</td></tr><tr><td>node_sockstat_UDP6_inuse</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of UDP6 sockets in state inuse.</td></tr><tr><td>node_sockstat_UDPLITE6_inuse</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of UDPLITE6 sockets in state inuse.</td></tr><tr><td>node_sockstat_UDPLITE_inuse</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of UDPLITE sockets in state inuse.</td></tr><tr><td>node_sockstat_UDP_inuse</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of UDP sockets in state inuse.</td></tr><tr><td>node_sockstat_UDP_mem</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of UDP sockets in state mem.</td></tr><tr><td>node_sockstat_UDP_mem_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of UDP sockets in state mem_bytes.</td></tr><tr><td>node_sockstat_sockets_used</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of IPv4 sockets in use.</td></tr><tr><td>node_tcp_connection_states</td><td>gauge</td><td><code>state</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of connection states.</td></tr><tr><td>node_textfile_scrape_error</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>1 if there was an error opening or reading a file, 0 otherwise</td></tr><tr><td>node_time_clocksource_available_info</td><td>gauge</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>clocksource</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Available clocksources read from &lsquo;/sys/devices/system/clocksource&rsquo;.</td></tr><tr><td>node_time_clocksource_current_info</td><td>gauge</td><td><code>ip</code>, <code>device</code>, <code>ins</code>, <code>clocksource</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Current clocksource read from &lsquo;/sys/devices/system/clocksource&rsquo;.</td></tr><tr><td>node_time_seconds</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>System time in seconds since epoch (1970).</td></tr><tr><td>node_time_zone_offset_seconds</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>time_zone</code>, <code>ip</code>, <code>cls</code></td><td>System time zone offset in seconds.</td></tr><tr><td>node_timex_estimated_error_seconds</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Estimated error in seconds.</td></tr><tr><td>node_timex_frequency_adjustment_ratio</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Local clock frequency adjustment.</td></tr><tr><td>node_timex_loop_time_constant</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Phase-locked loop time constant.</td></tr><tr><td>node_timex_maxerror_seconds</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Maximum error in seconds.</td></tr><tr><td>node_timex_offset_seconds</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Time offset in between local system and reference clock.</td></tr><tr><td>node_timex_pps_calibration_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Pulse per second count of calibration intervals.</td></tr><tr><td>node_timex_pps_error_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Pulse per second count of calibration errors.</td></tr><tr><td>node_timex_pps_frequency_hertz</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Pulse per second frequency.</td></tr><tr><td>node_timex_pps_jitter_seconds</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Pulse per second jitter.</td></tr><tr><td>node_timex_pps_jitter_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Pulse per second count of jitter limit exceeded events.</td></tr><tr><td>node_timex_pps_shift_seconds</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Pulse per second interval duration.</td></tr><tr><td>node_timex_pps_stability_exceeded_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Pulse per second count of stability limit exceeded events.</td></tr><tr><td>node_timex_pps_stability_hertz</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Pulse per second stability, average of recent frequency changes.</td></tr><tr><td>node_timex_status</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Value of the status array bits.</td></tr><tr><td>node_timex_sync_status</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Is clock synchronized to a reliable server (1 = yes, 0 = no).</td></tr><tr><td>node_timex_tai_offset_seconds</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>International Atomic Time (TAI) offset.</td></tr><tr><td>node_timex_tick_seconds</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Seconds between clock ticks.</td></tr><tr><td>node_udp_queues</td><td>gauge</td><td><code>ip</code>, <code>queue</code>, <code>ins</code>, <code>job</code>, <code>exported_ip</code>, <code>instance</code>, <code>cls</code></td><td>Number of allocated memory in the kernel for UDP datagrams in bytes.</td></tr><tr><td>node_uname_info</td><td>gauge</td><td><code>ip</code>, <code>sysname</code>, <code>version</code>, <code>domainname</code>, <code>release</code>, <code>ins</code>, <code>job</code>, <code>nodename</code>, <code>instance</code>, <code>cls</code>, <code>machine</code></td><td>Labeled system information as provided by the uname system call.</td></tr><tr><td>node_up</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>node_vmstat_oom_kill</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>/proc/vmstat information field oom_kill.</td></tr><tr><td>node_vmstat_pgfault</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>/proc/vmstat information field pgfault.</td></tr><tr><td>node_vmstat_pgmajfault</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>/proc/vmstat information field pgmajfault.</td></tr><tr><td>node_vmstat_pgpgin</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>/proc/vmstat information field pgpgin.</td></tr><tr><td>node_vmstat_pgpgout</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>/proc/vmstat information field pgpgout.</td></tr><tr><td>node_vmstat_pswpin</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>/proc/vmstat information field pswpin.</td></tr><tr><td>node_vmstat_pswpout</td><td>unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>/proc/vmstat information field pswpout.</td></tr><tr><td>process_cpu_seconds_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total user and system CPU time spent in seconds.</td></tr><tr><td>process_max_fds</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Maximum number of open file descriptors.</td></tr><tr><td>process_open_fds</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of open file descriptors.</td></tr><tr><td>process_resident_memory_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Resident memory size in bytes.</td></tr><tr><td>process_start_time_seconds</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Start time of the process since unix epoch in seconds.</td></tr><tr><td>process_virtual_memory_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Virtual memory size in bytes.</td></tr><tr><td>process_virtual_memory_max_bytes</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Maximum amount of virtual memory available in bytes.</td></tr><tr><td>prometheus_remote_storage_exemplars_in_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Exemplars in to remote storage, compare to exemplars out for queue managers.</td></tr><tr><td>prometheus_remote_storage_histograms_in_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>HistogramSamples in to remote storage, compare to histograms out for queue managers.</td></tr><tr><td>prometheus_remote_storage_samples_in_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Samples in to remote storage, compare to samples out for queue managers.</td></tr><tr><td>prometheus_remote_storage_string_interner_zero_reference_releases_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>The number of times release has been called for strings that are not interned.</td></tr><tr><td>prometheus_sd_azure_failures_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of Azure service discovery refresh failures.</td></tr><tr><td>prometheus_sd_consul_rpc_duration_seconds</td><td>summary</td><td><code>ip</code>, <code>call</code>, <code>quantile</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code>, <code>endpoint</code></td><td>The duration of a Consul RPC call in seconds.</td></tr><tr><td>prometheus_sd_consul_rpc_duration_seconds_count</td><td>Unknown</td><td><code>ip</code>, <code>call</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code>, <code>endpoint</code></td><td>N/A</td></tr><tr><td>prometheus_sd_consul_rpc_duration_seconds_sum</td><td>Unknown</td><td><code>ip</code>, <code>call</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code>, <code>endpoint</code></td><td>N/A</td></tr><tr><td>prometheus_sd_consul_rpc_failures_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>The number of Consul RPC call failures.</td></tr><tr><td>prometheus_sd_consulagent_rpc_duration_seconds</td><td>summary</td><td><code>ip</code>, <code>call</code>, <code>quantile</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code>, <code>endpoint</code></td><td>The duration of a Consul Agent RPC call in seconds.</td></tr><tr><td>prometheus_sd_consulagent_rpc_duration_seconds_count</td><td>Unknown</td><td><code>ip</code>, <code>call</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code>, <code>endpoint</code></td><td>N/A</td></tr><tr><td>prometheus_sd_consulagent_rpc_duration_seconds_sum</td><td>Unknown</td><td><code>ip</code>, <code>call</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code>, <code>endpoint</code></td><td>N/A</td></tr><tr><td>prometheus_sd_consulagent_rpc_failures_total</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>prometheus_sd_dns_lookup_failures_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>The number of DNS-SD lookup failures.</td></tr><tr><td>prometheus_sd_dns_lookups_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>The number of DNS-SD lookups.</td></tr><tr><td>prometheus_sd_file_read_errors_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>The number of File-SD read errors.</td></tr><tr><td>prometheus_sd_file_scan_duration_seconds</td><td>summary</td><td><code>quantile</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>The duration of the File-SD scan in seconds.</td></tr><tr><td>prometheus_sd_file_scan_duration_seconds_count</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>prometheus_sd_file_scan_duration_seconds_sum</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>prometheus_sd_file_watcher_errors_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>The number of File-SD errors caused by filesystem watch failures.</td></tr><tr><td>prometheus_sd_kubernetes_events_total</td><td>counter</td><td><code>ip</code>, <code>event</code>, <code>ins</code>, <code>job</code>, <code>role</code>, <code>instance</code>, <code>cls</code></td><td>The number of Kubernetes events handled.</td></tr><tr><td>prometheus_target_scrape_pool_exceeded_label_limits_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of times scrape pools hit the label limits, during sync or config reload.</td></tr><tr><td>prometheus_target_scrape_pool_exceeded_target_limit_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of times scrape pools hit the target limit, during sync or config reload.</td></tr><tr><td>prometheus_target_scrape_pool_reloads_failed_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of failed scrape pool reloads.</td></tr><tr><td>prometheus_target_scrape_pool_reloads_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of scrape pool reloads.</td></tr><tr><td>prometheus_target_scrape_pools_failed_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of scrape pool creations that failed.</td></tr><tr><td>prometheus_target_scrape_pools_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of scrape pool creation attempts.</td></tr><tr><td>prometheus_target_scrapes_cache_flush_forced_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>How many times a scrape cache was flushed due to getting big while scrapes are failing.</td></tr><tr><td>prometheus_target_scrapes_exceeded_body_size_limit_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of scrapes that hit the body size limit</td></tr><tr><td>prometheus_target_scrapes_exceeded_sample_limit_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of scrapes that hit the sample limit and were rejected.</td></tr><tr><td>prometheus_target_scrapes_exemplar_out_of_order_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of exemplar rejected due to not being out of the expected order.</td></tr><tr><td>prometheus_target_scrapes_sample_duplicate_timestamp_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of samples rejected due to duplicate timestamps but different values.</td></tr><tr><td>prometheus_target_scrapes_sample_out_of_bounds_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of samples rejected due to timestamp falling outside of the time bounds.</td></tr><tr><td>prometheus_target_scrapes_sample_out_of_order_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Total number of samples rejected due to not being out of the expected order.</td></tr><tr><td>prometheus_template_text_expansion_failures_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>The total number of template text expansion failures.</td></tr><tr><td>prometheus_template_text_expansions_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>The total number of template text expansions.</td></tr><tr><td>prometheus_treecache_watcher_goroutines</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>The current number of watcher goroutines.</td></tr><tr><td>prometheus_treecache_zookeeper_failures_total</td><td>counter</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>The total number of ZooKeeper failures.</td></tr><tr><td>promhttp_metric_handler_errors_total</td><td>counter</td><td><code>ip</code>, <code>cause</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Total number of internal errors encountered by the promhttp metric handler.</td></tr><tr><td>promhttp_metric_handler_requests_in_flight</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Current number of scrapes being served.</td></tr><tr><td>promhttp_metric_handler_requests_total</td><td>counter</td><td><code>ip</code>, <code>ins</code>, <code>code</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>Total number of scrapes by HTTP status code.</td></tr><tr><td>promtail_batch_retries_total</td><td>Unknown</td><td><code>host</code>, <code>ip</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>promtail_build_info</td><td>gauge</td><td><code>ip</code>, <code>version</code>, <code>revision</code>, <code>goversion</code>, <code>branch</code>, <code>ins</code>, <code>goarch</code>, <code>job</code>, <code>tags</code>, <code>instance</code>, <code>cls</code>, <code>goos</code></td><td>A metric with a constant &lsquo;1&rsquo; value labeled by version, revision, branch, goversion from which promtail was built, and the goos and goarch for the build.</td></tr><tr><td>promtail_config_reload_fail_total</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>promtail_config_reload_success_total</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>promtail_dropped_bytes_total</td><td>Unknown</td><td><code>host</code>, <code>ip</code>, <code>ins</code>, <code>job</code>, <code>reason</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>promtail_dropped_entries_total</td><td>Unknown</td><td><code>host</code>, <code>ip</code>, <code>ins</code>, <code>job</code>, <code>reason</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>promtail_encoded_bytes_total</td><td>Unknown</td><td><code>host</code>, <code>ip</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>promtail_file_bytes_total</td><td>gauge</td><td><code>path</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of bytes total.</td></tr><tr><td>promtail_files_active_total</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of active files.</td></tr><tr><td>promtail_mutated_bytes_total</td><td>Unknown</td><td><code>host</code>, <code>ip</code>, <code>ins</code>, <code>job</code>, <code>reason</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>promtail_mutated_entries_total</td><td>Unknown</td><td><code>host</code>, <code>ip</code>, <code>ins</code>, <code>job</code>, <code>reason</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>promtail_read_bytes_total</td><td>gauge</td><td><code>path</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of bytes read.</td></tr><tr><td>promtail_read_lines_total</td><td>Unknown</td><td><code>path</code>, <code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>promtail_request_duration_seconds_bucket</td><td>Unknown</td><td><code>host</code>, <code>ip</code>, <code>ins</code>, <code>job</code>, <code>status_code</code>, <code>le</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>promtail_request_duration_seconds_count</td><td>Unknown</td><td><code>host</code>, <code>ip</code>, <code>ins</code>, <code>job</code>, <code>status_code</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>promtail_request_duration_seconds_sum</td><td>Unknown</td><td><code>host</code>, <code>ip</code>, <code>ins</code>, <code>job</code>, <code>status_code</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>promtail_sent_bytes_total</td><td>Unknown</td><td><code>host</code>, <code>ip</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>promtail_sent_entries_total</td><td>Unknown</td><td><code>host</code>, <code>ip</code>, <code>ins</code>, <code>job</code>, <code>instance</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>promtail_targets_active_total</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>Number of active total.</td></tr><tr><td>promtail_up</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>request_duration_seconds_bucket</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>status_code</code>, <code>route</code>, <code>ws</code>, <code>le</code>, <code>ip</code>, <code>cls</code>, <code>method</code></td><td>N/A</td></tr><tr><td>request_duration_seconds_count</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>status_code</code>, <code>route</code>, <code>ws</code>, <code>ip</code>, <code>cls</code>, <code>method</code></td><td>N/A</td></tr><tr><td>request_duration_seconds_sum</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>status_code</code>, <code>route</code>, <code>ws</code>, <code>ip</code>, <code>cls</code>, <code>method</code></td><td>N/A</td></tr><tr><td>request_message_bytes_bucket</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>route</code>, <code>le</code>, <code>ip</code>, <code>cls</code>, <code>method</code></td><td>N/A</td></tr><tr><td>request_message_bytes_count</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>route</code>, <code>ip</code>, <code>cls</code>, <code>method</code></td><td>N/A</td></tr><tr><td>request_message_bytes_sum</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>route</code>, <code>ip</code>, <code>cls</code>, <code>method</code></td><td>N/A</td></tr><tr><td>response_message_bytes_bucket</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>route</code>, <code>le</code>, <code>ip</code>, <code>cls</code>, <code>method</code></td><td>N/A</td></tr><tr><td>response_message_bytes_count</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>route</code>, <code>ip</code>, <code>cls</code>, <code>method</code></td><td>N/A</td></tr><tr><td>response_message_bytes_sum</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>route</code>, <code>ip</code>, <code>cls</code>, <code>method</code></td><td>N/A</td></tr><tr><td>scrape_duration_seconds</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>scrape_samples_post_metric_relabeling</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>scrape_samples_scraped</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>scrape_series_added</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr><tr><td>tcp_connections</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>protocol</code>, <code>ip</code>, <code>cls</code></td><td>Current number of accepted TCP connections.</td></tr><tr><td>tcp_connections_limit</td><td>gauge</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>protocol</code>, <code>ip</code>, <code>cls</code></td><td>The max number of TCP connections that can be accepted (0 means no limit).</td></tr><tr><td>up</td><td>Unknown</td><td><code>instance</code>, <code>ins</code>, <code>job</code>, <code>ip</code>, <code>cls</code></td><td>N/A</td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-239230a82e0955081c7a4e1b76e4c445>7 - FAQ</h1><div class=lead>Frequently asked questions about Pigsty NODE module</div><hr><h2 id=how-to-configure-ntp-service>How to configure NTP service?</h2><blockquote><p>NTP is critical for various production services. If NTP is not configured, you can use public NTP services or the Chronyd on the admin node as the time standard.</p></blockquote><p>If your nodes already have NTP configured, you can preserve the existing configuration without making any changes by setting <code>node_ntp_enabled</code> to <code>false</code>.</p><p>Otherwise, if you have Internet access, you can use public NTP services such as <code>pool.ntp.org</code>.</p><p>If you don&rsquo;t have Internet access, you can use the following approach to ensure all nodes in the environment are synchronized with the admin node, or use another internal NTP time service.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>node_ntp_servers:                 <span style=color:#8f5902;font-style:italic># NTP servers in /etc/chrony.conf</span>
</span></span><span style=display:flex><span>  - pool cn.pool.ntp.org iburst
</span></span><span style=display:flex><span>  - pool <span style=color:#4e9a06>${</span><span style=color:#000>admin_ip</span><span style=color:#4e9a06>}</span> iburst       <span style=color:#8f5902;font-style:italic># assume non-admin nodes do not have internet access, at least sync with admin node</span>
</span></span></code></pre></div><hr><h2 id=how-to-force-sync-time-on-nodes>How to force sync time on nodes?</h2><p>Use <code>chronyc</code> to sync time. You must configure the NTP service first.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ansible all -b -a <span style=color:#4e9a06>&#39;chronyc -a makestep&#39;</span>     <span style=color:#8f5902;font-style:italic># sync time</span>
</span></span></code></pre></div><p>You can replace <code>all</code> with any group or host IP address to limit the execution scope.</p><hr><h2 id=remote-nodes-are-not-accessible-via-ssh>Remote nodes are not accessible via SSH?</h2><p>If the target machine is hidden behind an SSH jump host, or some customizations prevent direct access using <code>ssh ip</code>, you can use <a href=https://docs.ansible.com/ansible/latest/inventory_guide/connection_details.html>Ansible connection parameters</a> to specify various SSH connection options, such as:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pg-test:
</span></span><span style=display:flex><span>  vars: <span style=color:#ce5c00;font-weight:700>{</span> pg_cluster: pg-test <span style=color:#ce5c00;font-weight:700>}</span>
</span></span><span style=display:flex><span>  hosts:
</span></span><span style=display:flex><span>    10.10.10.11: <span style=color:#ce5c00;font-weight:700>{</span>pg_seq: 1, pg_role: primary, ansible_host: node-1 <span style=color:#ce5c00;font-weight:700>}</span>
</span></span><span style=display:flex><span>    10.10.10.12: <span style=color:#ce5c00;font-weight:700>{</span>pg_seq: 2, pg_role: replica, ansible_port: 22223, ansible_user: admin <span style=color:#ce5c00;font-weight:700>}</span>
</span></span><span style=display:flex><span>    10.10.10.13: <span style=color:#ce5c00;font-weight:700>{</span>pg_seq: 3, pg_role: offline, ansible_port: <span style=color:#0000cf;font-weight:700>22224</span> <span style=color:#ce5c00;font-weight:700>}</span>
</span></span></code></pre></div><hr><h2 id=password-required-for-remote-node-ssh-and-sudo>Password required for remote node SSH and SUDO?</h2><p><strong>When performing deployments and changes</strong>, the admin user used <strong>must</strong> have <code>ssh</code> and <code>sudo</code> privileges for all nodes. Passwordless login is not required.</p><p>You can pass ssh and sudo passwords via the <code>-k|-K</code> parameters when executing playbooks, or even use another user to run playbooks via <code>-e</code><a href=/docs/node/param#connect><code>ansible_host</code></a><code>=&lt;another_user></code>.</p><p>However, Pigsty strongly recommends configuring SSH <strong>passwordless login</strong> with passwordless <code>sudo</code> for the admin user.</p><hr><h2 id=how-to-create-a-dedicated-admin-user-with-an-existing-admin-user>How to create a dedicated admin user with an existing admin user?</h2><p>Use the following command to create a new standard admin user defined by <a href=/docs/node/param#node_admin_username><code>node_admin_username</code></a> using an existing admin user on that node.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./node.yml -k -K -e <span style=color:#000>ansible_user</span><span style=color:#ce5c00;font-weight:700>=</span>&lt;another_admin&gt; -t node_admin
</span></span></code></pre></div><hr><h2 id=how-to-expose-services-using-haproxy-on-nodes>How to expose services using HAProxy on nodes?</h2><p>You can use <a href=/docs/node/param#haproxy_services><code>haproxy_services</code></a> in the configuration to expose services, and use <code>node.yml -t haproxy_config,haproxy_reload</code> to update the configuration.</p><p>Here&rsquo;s an example of exposing a MinIO service: <a href=/docs/minio#expose-service>Expose MinIO Service</a></p><hr><h2 id=why-are-all-my-etcyumreposd-files-gone>Why are all my /etc/yum.repos.d/* files gone?</h2><p>Pigsty builds a local software repository on infra nodes that includes all dependencies. All regular nodes will reference and use the local software repository on Infra nodes according to the default configuration of <a href=/docs/node/param#node_repo_modules><code>node_repo_modules</code></a> as <code>local</code>.</p><p>This design avoids Internet access and enhances installation stability and reliability. All original repo definition files are moved to the <code>/etc/yum.repos.d/backup</code> directory; you can copy them back as needed.</p><p>If you want to preserve the original repo definition files during regular node installation, set <a href=/docs/node/param#node_repo_remove><code>node_repo_remove</code></a> to <code>false</code>.</p><p>If you want to preserve the original repo definition files during Infra node local repo construction, set <a href=/docs/infra/param#repo_remove><code>repo_remove</code></a> to <code>false</code>.</p><hr><h2 id=why-did-my-command-line-prompt-change-how-to-restore-it>Why did my command line prompt change? How to restore it?</h2><p>The shell command line prompt used by Pigsty is specified by the environment variable <code>PS1</code>, defined in the <code>/etc/profile.d/node.sh</code> file.</p><p>If you don&rsquo;t like it and want to modify or restore it, you can remove this file and log in again.</p><hr><h2 id=why-did-my-hostname-change>Why did my hostname change?</h2><p>Pigsty will modify your node hostname in two situations:</p><ul><li><a href=/docs/node/param#nodename_overwrite><code>nodename</code></a> value is explicitly defined (default is empty)</li><li>The <a href=/docs/pgsql><strong><code>PGSQL</code></strong></a> module is declared on the node and the <a href=/docs/node/param#node_id_from_pg><strong><code>node_id_from_pg</code></strong></a> parameter is enabled (default is <code>true</code>)</li></ul><p>If you don&rsquo;t want the hostname to be modified, you can set <a href=/docs/node/param#nodename_overwrite><strong><code>nodename_overwrite</code></strong></a> to <code>false</code> at the global/cluster/instance level (default is <code>true</code>).</p><p>For details, see the <a href=/docs/node/param#node_id><strong><code>NODE_ID</code></strong></a> section.</p><hr><h2 id=what-compatibility-issues-exist-with-tencent-opencloudos>What compatibility issues exist with Tencent OpenCloudOS?</h2><p>The <code>softdog</code> kernel module is not available on OpenCloudOS and needs to be removed from <code>node_kernel_modules</code>. Add the following configuration item to the global variables in the config file to override:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>node_kernel_modules</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000;font-weight:700>[</span><span style=color:#f8f8f8> </span><span style=color:#000>ip_vs, ip_vs_rr, ip_vs_wrr, ip_vs_sh ]</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><hr><h2 id=what-common-issues-exist-on-debian-systems>What common issues exist on Debian systems?</h2><p>When using Pigsty on Debian/Ubuntu systems, you may encounter the following issues:</p><p><strong>Missing locale</strong></p><p>If the system reports locale-related errors, you can fix them with the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>localedef -i en_US -f UTF-8 en_US.UTF-8
</span></span></code></pre></div><p><strong>Missing rsync tool</strong></p><p>Pigsty relies on rsync for file synchronization. If the system doesn&rsquo;t have it installed, you can install it with:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>apt-get install rsync
</span></span></code></pre></div></div></main></div></div><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class="td-footer__left col-6 col-sm-4 order-sm-1"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title="Email - Vonng" aria-label="Email - Vonng"><a target=_blank rel=noopener href=mailto:rh@vonng.com aria-label="Email - Vonng"><i class="fa fa-envelope"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title="GitHub - Vonng" aria-label="GitHub - Vonng"><a target=_blank rel=noopener href=https://github.com/Vonng aria-label="GitHub - Vonng"><i class="fab fa-github"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title="X - Vonng" aria-label="X - Vonng"><a target=_blank rel=noopener href=https://x.com/RonVonng aria-label="X - Vonng"><i class="fab fa-x-twitter"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title="X - Pigsty" aria-label="X - Pigsty"><a target=_blank rel=noopener href=https://x.com/PlGSTY aria-label="X - Pigsty"><i class="fab fa-twitter"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title="LinkedIn - Vonng" aria-label="LinkedIn - Vonng"><a target=_blank rel=noopener href=https://www.linkedin.com/in/vonng/ aria-label="LinkedIn - Vonng"><i class="fab fa-linkedin"></i></a></li></ul></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=Discord aria-label=Discord><a target=_blank rel=noopener href=https://discord.gg/wDzt5VyWEz aria-label=Discord><i class="fab fa-discord"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=Telegram aria-label=Telegram><a target=_blank rel=noopener href=https://t.me/joinchat/gV9zfZraNPM3YjFh aria-label=Telegram><i class="fab fa-telegram"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=WeChat aria-label=WeChat><a target=_blank rel=noopener href=/img/pigsty/pigsty-cc.jpg aria-label=WeChat><i class="fab fa-weixin"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=Discussion aria-label=Discussion><a target=_blank rel=noopener href=https://github.com/orgs/pgsty/discussions aria-label=Discussion><i class="fab fa-discourse"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=Repo aria-label=Repo><a target=_blank rel=noopener href=https://github.com/pgsty/pigsty aria-label=Repo><i class="fab fa-github"></i></a></li></ul></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"><span class=td-footer__copyright>&copy;
2018&ndash;2026
<span class=td-footer__authors>Ruohang Feng</span></span><span class=td-footer__all_rights_reserved>All Rights Reserved</span><span class=ms-2><a href=/docs/about/privacy target=_blank rel=noopener>Privacy Policy</a></span></div></div></div></footer></div><script src=/js/main.min.d20e761d6aa4d2ace0488e45da0e775a8b17300a8430f32fbcfa016e6c9e6eb6.js integrity="sha256-0g52HWqk0qzgSI5F2g53WosXMAqEMPMvvPoBbmyebrY=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script></body></html>