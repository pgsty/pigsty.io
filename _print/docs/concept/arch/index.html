<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js data-theme-init><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=color-scheme content="light dark"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#000000"><style>html{background:Canvas;color:CanvasText}@media(prefers-color-scheme:dark){html{background:#0b0d12;color:#e6e6e6}}html[data-theme-init] *{transition:none!important}</style><script>(function(){const t="td-color-theme",n=localStorage.getItem(t);let e=n||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");e==="auto"&&(e=window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"),document.documentElement.setAttribute("data-bs-theme",e)})()</script><link rel=canonical type=text/html href=https://pigsty.io/docs/concept/arch/><link rel=alternate type=application/rss+xml href=https://pigsty.io/docs/concept/arch/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Architecture | PIGSTY</title><meta name=description content="Pigsty's modular architecture—declarative composition, on-demand customization, flexible deployment."><meta property="og:url" content="https://pigsty.io/docs/concept/arch/"><meta property="og:site_name" content="PIGSTY"><meta property="og:title" content="Architecture"><meta property="og:description" content="Pigsty's modular architecture—declarative composition, on-demand customization, flexible deployment."><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta itemprop=name content="Architecture"><meta itemprop=description content="Pigsty's modular architecture—declarative composition, on-demand customization, flexible deployment."><meta itemprop=dateModified content="2026-01-15T11:51:06+08:00"><meta itemprop=wordCount content="738"><meta itemprop=keywords content="Concept,PIGSTY"><meta name=twitter:card content="summary"><meta name=twitter:title content="Architecture"><meta name=twitter:description content="Pigsty's modular architecture—declarative composition, on-demand customization, flexible deployment."><link rel=preload href=/scss/main.min.f6da858eb22dc48110abb983f095554930a5a614c30190b4e32ee7b08a496331.css as=style integrity="sha256-9tqFjrItxIEQq7mD8JVVSTClphTDAZC04y7nsIpJYzE=" crossorigin=anonymous><link href=/scss/main.min.f6da858eb22dc48110abb983f095554930a5a614c30190b4e32ee7b08a496331.css rel=stylesheet integrity="sha256-9tqFjrItxIEQq7mD8JVVSTClphTDAZC04y7nsIpJYzE=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-LG1V9WTKGE"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-LG1V9WTKGE")}</script></head><body class=td-section><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="td-navbar-container container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"><svg viewBox="0 0 24 24" width="24" height="24"><defs/><g id="32" fill="none" stroke-dasharray="none" fill-opacity="1" stroke-opacity="1" stroke="none"><title>32</title><g id="32_图层_2"><title>图层 2</title><g id="Group_17"><g id="Graphic_16"/><g id="Graphic_15"><path d="M7.666187 11.971335l2.165064-3.75H14.16138l2.165065 3.75-2.165065 3.75H9.831251z" fill="#bbb" fill-opacity=".9526367"/><path d="M7.666187 11.971335l2.165064-3.75H14.16138l2.165065 3.75-2.165065 3.75H9.831251z" stroke="#fff" stroke-linecap="round" stroke-linejoin="round" stroke-width=".6730434"/></g><g id="Graphic_14"><path d="M7.666187 19.474806l2.165064-3.75H14.16138l2.165065 3.75-2.165065 3.75H9.831251z" fill="#de372c" fill-opacity=".8545852"/><path d="M7.666187 19.474806l2.165064-3.75H14.16138l2.165065 3.75-2.165065 3.75H9.831251z" stroke="#fff" stroke-linecap="round" stroke-linejoin="round" stroke-width=".6730434"/></g><g id="Graphic_13"><path d="M14.161476 15.751202l2.165064-3.75h4.33013l2.165064 3.75-2.165064 3.75H16.32654z" fill="#424242" fill-opacity=".9016462"/><path d="M14.161476 15.751202l2.165064-3.75h4.33013l2.165064 3.75-2.165064 3.75H16.32654z" stroke="#fff" stroke-linecap="round" stroke-linejoin="round" stroke-width=".6730434"/></g><g id="Graphic_12"><path d="M14.161476 8.226008 16.32654 4.4760076h4.33013L22.821734 8.226008l-2.165064 3.75H16.32654z" fill="#ffa269" fill-opacity=".8975772"/><path d="M14.161476 8.226008 16.32654 4.4760076h4.33013L22.821734 8.226008l-2.165064 3.75H16.32654z" stroke="#fff" stroke-linecap="round" stroke-linejoin="round" stroke-width=".6730434"/></g><g id="Graphic_11"><path d="M7.666187 4.5 9.831251.75H14.16138L16.326445 4.5 14.16138 8.25H9.831251z" fill="#419edb" fill-opacity=".8979957"/><path d="M7.666187 4.5 9.831251.75H14.16138L16.326445 4.5 14.16138 8.25H9.831251z" stroke="#fff" stroke-linecap="round" stroke-linejoin="round" stroke-width=".6730434"/></g><g id="Graphic_10"><path d="M1.1491742 8.226008 3.3142388 4.4760076H7.644368L9.809432 8.226008l-2.165064 3.75H3.3142388z" fill="#2f6793" fill-opacity=".9002511"/><path d="M1.1491742 8.226008 3.3142388 4.4760076H7.644368L9.809432 8.226008l-2.165064 3.75H3.3142388z" stroke="#fff" stroke-linecap="round" stroke-linejoin="round" stroke-width=".6730434"/></g><g id="Graphic_9"><path d="M1.182071 15.741036l2.1650645-3.75h4.330129l2.1650645 3.75-2.1650645 3.75H3.3471355z" fill="#53ac79" fill-opacity=".9"/><path d="M1.182071 15.741036l2.1650645-3.75h4.330129l2.1650645 3.75-2.1650645 3.75H3.3471355z" stroke="#fff" stroke-linecap="round" stroke-linejoin="round" stroke-width=".6730434"/></g></g></g></g></svg></span><span class=navbar-brand__name>PIGSTY</span></a><div class="td-navbar-nav-scroll td-navbar-nav-scroll--indicator" id=main_navbar><div class="scroll-indicator scroll-left"></div><ul class=navbar-nav><li class=nav-item><a class=nav-link href=/docs/><i class='fa-solid fa-book'></i><span>Docs</span></a></li><li class=nav-item><a class=nav-link href=/blog/><i class="fas fa-blog"></i><span>Blog</span></a></li><li class="nav-item dropdown d-none d-lg-block td-navbar__version-menu"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Ver</a><ul class=dropdown-menu><li><a class=dropdown-item href=https://pigsty.cc>Pigsty 中文网站</a></li><li><a class=dropdown-item href=https://doc.pgsty.com>Pigsty v3.7 Doc</a></li><li><a class=dropdown-item href=https://v34.pigsty.cc>Pigsty v3.4 Doc</a></li><li><a class=dropdown-item href=https://v27.pigsty.cc>Pigsty v2.7 Doc</a></li><li><a class=dropdown-item href=https://v15.pigsty.cc/docs>Pigsty v1.5 Doc</a></li></ul></div></li><li class="nav-item td-navbar__light-dark-menu"><div class="td-light-dark-menu dropdown"><svg class="d-none"><symbol id="check2" viewBox="0 0 16 16"><path d="M13.854 3.646a.5.5.0 010 .708l-7 7a.5.5.0 01-.708.0l-3.5-3.5a.5.5.0 11.708-.708L6.5 10.293l6.646-6.647a.5.5.0 01.708.0z"/></symbol><symbol id="circle-half" viewBox="0 0 16 16"><path d="M8 15A7 7 0 108 1v14zm0 1A8 8 0 118 0a8 8 0 010 16z"/></symbol><symbol id="moon-stars-fill" viewBox="0 0 16 16"><path d="M6 .278a.768.768.0 01.08.858 7.208 7.208.0 00-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527.0 1.04-.055 1.533-.16a.787.787.0 01.81.316.733.733.0 01-.031.893A8.349 8.349.0 018.344 16C3.734 16 0 12.286.0 7.71.0 4.266 2.114 1.312 5.124.06A.752.752.0 016 .278z"/><path d="M10.794 3.148a.217.217.0 01.412.0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217.0 010 .412l-1.162.387A1.734 1.734.0 0011.593 7.69l-.387 1.162a.217.217.0 01-.412.0l-.387-1.162A1.734 1.734.0 009.31 6.593l-1.162-.387a.217.217.0 010-.412l1.162-.387a1.734 1.734.0 001.097-1.097l.387-1.162zM13.863.099a.145.145.0 01.274.0l.258.774c.115.346.386.617.732.732l.774.258a.145.145.0 010 .274l-.774.258a1.156 1.156.0 00-.732.732l-.258.774a.145.145.0 01-.274.0l-.258-.774a1.156 1.156.0 00-.732-.732l-.774-.258a.145.145.0 010-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"/></symbol><symbol id="sun-fill" viewBox="0 0 16 16"><path d="M8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.5.5.0 01.5.5v2a.5.5.0 01-1 0v-2A.5.5.0 018 0zm0 13a.5.5.0 01.5.5v2a.5.5.0 01-1 0v-2A.5.5.0 018 13zm8-5a.5.5.0 01-.5.5h-2a.5.5.0 010-1h2a.5.5.0 01.5.5zM3 8a.5.5.0 01-.5.5h-2a.5.5.0 010-1h2A.5.5.0 013 8zm10.657-5.657a.5.5.0 010 .707l-1.414 1.415a.5.5.0 11-.707-.708l1.414-1.414a.5.5.0 01.707.0zm-9.193 9.193a.5.5.0 010 .707L3.05 13.657a.5.5.0 01-.707-.707l1.414-1.414a.5.5.0 01.707.0zm9.193 2.121a.5.5.0 01-.707.0l-1.414-1.414a.5.5.0 01.707-.707l1.414 1.414a.5.5.0 010 .707zM4.464 4.465a.5.5.0 01-.707.0L2.343 3.05a.5.5.0 11.707-.707l1.414 1.414a.5.5.0 010 .708z"/></symbol></svg>
<button class="btn btn-link nav-link dropdown-toggle d-flex align-items-center" id=bd-theme type=button aria-expanded=false data-bs-toggle=dropdown aria-label="Toggle theme (auto)">
<svg class="bi my-1 theme-icon-active"><use href="#circle-half"/></svg></button><ul class=dropdown-menu aria-labelledby=bd-theme><li><button type=button class="dropdown-item d-flex align-items-center" data-bs-theme-value=light aria-pressed=false>
<svg class="bi me-2 opacity-50"><use href="#sun-fill"/></svg>
Light
<svg class="bi ms-auto d-none"><use href="#check2"/></svg></button></li><li><button type=button class="dropdown-item d-flex align-items-center" data-bs-theme-value=dark aria-pressed=false>
<svg class="bi me-2 opacity-50"><use href="#moon-stars-fill"/></svg>
Dark
<svg class="bi ms-auto d-none"><use href="#check2"/></svg></button></li><li><button type=button class="dropdown-item d-flex align-items-center active" data-bs-theme-value=auto aria-pressed=true>
<svg class="bi me-2 opacity-50"><use href="#circle-half"/></svg>
Auto
<svg class="bi ms-auto d-none"><use href="#check2"/></svg></button></li></ul></div></li><li class=nav-item><a class=nav-link href=# onclick="return switchLang(),!1" title="Switch Language / 切换语言"><i class="fas fa-language"></i></a></li></ul><div class="scroll-indicator scroll-right"></div></div><div class="d-none d-lg-block td-navbar__search"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.d757e151050fd40088751a74721d74f6.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav><script>function switchLang(){var t=window.location.host,e=window.location.pathname+window.location.search+window.location.hash;t.indexOf("pigsty.io")!==-1?window.location.href="https://pigsty.cc"+e:t.indexOf("pigsty.cc")!==-1?window.location.href="https://pigsty.io"+e:window.location.href="https://pigsty.cc"+e}</script></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/concept/arch/>Return to the regular view of this page</a>.</p></div><h1 class=title>Architecture</h1><div class=lead>Pigsty&rsquo;s modular architecture—declarative composition, on-demand customization, flexible deployment.</div><ul><li>1: <a href=#pg-66767bf2f054756113e02343d62b9774>Nodes</a></li><li>2: <a href=#pg-d65561d0ed395a21fc6b7dd2654dc081>Infrastructure</a></li><li>3: <a href=#pg-c777ff17702df77ebc33dadeb81e4174>PGSQL Arch</a></li></ul><div class=content><p>Pigsty uses a <strong>modular architecture</strong> with a <strong>declarative interface</strong>. You can <a href=/docs/deploy/planning#common-solutions><strong>freely combine modules like building blocks</strong></a> as needed.</p><ul><li>Pigsty adopts a <a href=/docs/deploy/planning><strong>modular design</strong></a> that can be freely combined and used on demand (use one or all) to suit different scenarios.</li><li>Pigsty uses <a href=/docs/concept/iac/inventory><strong>config inventory</strong></a> and <a href=/docs/concept/iac/parameter><strong>config parameters</strong></a> to describe the entire deployment environment, implemented via <a href=/docs/setup/playbook><strong>Ansible playbooks</strong></a>.</li><li>Pigsty can run on any <a href=/docs/concept/arch/node><strong>node</strong></a>—physical or virtual—as long as the OS is <a href=/docs/ref/linux><strong>compatible</strong></a>.</li></ul><hr><h2 id=modules>Modules</h2><p>Pigsty uses a modular design with six main default modules: <a href=/docs/pgsql><code>PGSQL</code></a>, <a href=/docs/infra><code>INFRA</code></a>, <a href=/docs/node><code>NODE</code></a>, <a href=/docs/etcd><code>ETCD</code></a>, <a href=/docs/redis><code>REDIS</code></a>, and <a href=/docs/minio><code>MINIO</code></a>.</p><ul><li><a href=/docs/pgsql><code>PGSQL</code></a>: Self-healing HA Postgres clusters powered by Patroni, Pgbouncer, HAproxy, PgBackrest, and more.</li><li><a href=/docs/infra><code>INFRA</code></a>: Local software repo, Nginx, Grafana, Victoria, AlertManager, Blackbox Exporter—the complete observability stack.</li><li><a href=/docs/node><code>NODE</code></a>: Tune nodes to desired state—hostname, timezone, NTP, ssh, sudo, haproxy, docker, vector, keepalived.</li><li><a href=/docs/etcd><code>ETCD</code></a>: Distributed key-value store as DCS for HA Postgres clusters: consensus leader election/config management/service discovery.</li><li><a href=/docs/redis><code>REDIS</code></a>: Redis servers supporting standalone primary-replica, sentinel, and cluster modes with full monitoring.</li><li><a href=/docs/minio><code>MINIO</code></a>: S3-compatible simple object storage that can serve as an optional backup destination for PG databases.</li></ul><p>You can declaratively compose them freely. If you only want host monitoring, installing the <a href=/docs/infra><code>INFRA</code></a> module on infrastructure nodes and the <a href=/docs/node><code>NODE</code></a> module on managed nodes is sufficient.
The <a href=/docs/etcd><code>ETCD</code></a> and <a href=/docs/pgsql><code>PGSQL</code></a> modules are used to build HA PG clusters—installing these modules on multiple nodes automatically forms a high-availability database cluster.
You can reuse Pigsty infrastructure and develop your own modules; <a href=/docs/redis><code>REDIS</code></a> and <a href=/docs/minio><code>MINIO</code></a> can serve as examples. More modules will be added—preliminary support for Mongo and MySQL is already on the roadmap.</p><p>Note that all modules depend strongly on the <code>NODE</code> module: in Pigsty, nodes must first have the <code>NODE</code> module installed to be managed before deploying other modules.
When nodes (by default) use the local software repo for installation, the <code>NODE</code> module has a weak dependency on the <code>INFRA</code> module. Therefore, the admin/infrastructure nodes with the <code>INFRA</code> module complete the bootstrap process in the <a href=/docs/setup/playbook><code>deploy.yml</code></a> playbook, resolving the circular dependency.</p><p><a href=/docs/deploy/sandbox><img src=/img/pigsty/sandbox.png alt=pigsty-sandbox></a></p><hr><h2 id=standalone-installation>Standalone Installation</h2><p>By default, Pigsty installs on a single <strong>node</strong> (physical/virtual machine). The <a href=https://github.com/pgsty/pigsty/blob/main/deploy.yml><code>deploy.yml</code></a> playbook installs <a href=/docs/infra><code>INFRA</code></a>, <a href=/docs/etcd><code>ETCD</code></a>, <a href=/docs/pgsql><code>PGSQL</code></a>, and optionally <a href=/docs/minio><code>MINIO</code></a> modules on the <strong>current</strong> node,
giving you a fully-featured observability stack (Prometheus, Grafana, Loki, AlertManager, PushGateway, BlackboxExporter, etc.), plus a built-in PostgreSQL standalone instance as a CMDB, ready to use out of the box (cluster name <code>pg-meta</code>, database name <code>meta</code>).</p><p>This node now has a complete self-monitoring system, visualization tools, and a Postgres database with PITR auto-configured (HA unavailable since you only have one node). You can use this node as a devbox, for testing, running demos, and data visualization/analysis. Or, use this node as an admin node to deploy and manage more nodes!</p><p><a href=/docs/infra/><img src=/img/pigsty/arch.png alt=pigsty-arch></a></p><hr><h2 id=monitoring>Monitoring</h2><p>The installed <a href=#standalone-installation>standalone meta node</a> can serve as an <strong>admin node</strong> and <strong>monitoring center</strong> to bring more nodes and database servers under its supervision and control.</p><p>Pigsty&rsquo;s monitoring system can be used independently. If you want to install the Prometheus/Grafana observability stack, Pigsty provides best practices!
It offers rich dashboards for <a href=https://demo.pigsty.io/d/node-overview>host nodes</a> and <a href=https://demo.pigsty.io/d/pgsql-overview>PostgreSQL databases</a>.
Whether or not these nodes or PostgreSQL servers are managed by Pigsty, with simple configuration, you immediately have a production-grade monitoring and alerting system, bringing existing hosts and PostgreSQL under management.</p><p><a href=/docs/pgsql/monitor/dashboard><img src=/img/pigsty/dashboard.jpg alt=pigsty-dashboard.jpg></a></p><hr><h2 id=ha-postgresql-clusters>HA PostgreSQL Clusters</h2><p>Pigsty helps you <strong>own</strong> your own production-grade HA PostgreSQL RDS service anywhere.</p><p>To create such an HA PostgreSQL cluster/RDS service, you simply describe it with a short config and run the playbook to create it:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>pg-test</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span><span style=color:#204a87;font-weight:700>hosts</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>10.10.10.11</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span>{<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>pg_seq: 1, pg_role</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>primary }</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>10.10.10.12</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span>{<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>pg_seq: 2, pg_role</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>replica }</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>10.10.10.13</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span>{<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>pg_seq: 3, pg_role</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>replica }</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span><span style=color:#204a87;font-weight:700>vars</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span>{<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>pg_cluster</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>pg-test }</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ bin/pgsql-add pg-test  <span style=color:#8f5902;font-style:italic># Initialize cluster &#39;pg-test&#39;</span>
</span></span></code></pre></div><p>In less than 10 minutes, you&rsquo;ll have a PostgreSQL database cluster with service access, monitoring, backup PITR, and HA fully configured.</p><p><a href=/docs/concept/ha><img src=/img/pigsty/ha.png alt=pigsty-ha.png></a></p><p>Hardware failures are covered by the self-healing HA architecture provided by patroni, etcd, and haproxy—in case of primary failure, automatic failover executes within 45 seconds by default.
Clients don&rsquo;t need to modify config or restart applications: Haproxy uses patroni health checks for traffic distribution, and read-write requests are automatically routed to the new cluster primary, avoiding split-brain issues.
This process is seamless—for example, in case of replica failure or planned switchover, clients experience only a momentary flash of the current query.</p><p>Software failures, human errors, and datacenter-level disasters are covered by pgbackrest and the optional <a href=/docs/minio>MinIO</a> cluster. This provides local/cloud PITR capabilities and, in case of datacenter failure, offers cross-region replication and disaster recovery.</p></div></div><div class=td-content style=page-break-before:always><h1 id=pg-66767bf2f054756113e02343d62b9774>1 - Nodes</h1><div class=lead>A node is an abstraction of hardware/OS resources—physical machines, bare metal, VMs, or containers/pods.</div><p>A <strong>node</strong> is an abstraction of hardware resources and operating systems. It can be a physical machine, bare metal, virtual machine, or container/pod.</p><p>Any machine running a <a href=/docs/ref/linux><strong>Linux OS</strong></a> (with systemd daemon) and standard CPU/memory/disk/network resources can be treated as a node.</p><p>Nodes can have <a href=/docs/ref/module><strong>modules</strong></a> installed. Pigsty has several node types, distinguished by which modules are deployed:</p><table class=full-width><thead><tr><th style=text-align:center>Type</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:center><a href=#regular-node><strong>Regular Node</strong></a></td><td style=text-align:left>A node managed by Pigsty</td></tr><tr><td style=text-align:center><a href=#admin-node><strong>ADMIN Node</strong></a></td><td style=text-align:left>The node that runs Ansible to issue management commands</td></tr><tr><td style=text-align:center><a href=#infra-node><strong>INFRA Node</strong></a></td><td style=text-align:left>Nodes with the <a href=/docs/infra/><strong>INFRA</strong></a> module installed</td></tr><tr><td style=text-align:center><a href=#etcd-node><strong>ETCD Node</strong></a></td><td style=text-align:left>Nodes with the <a href=/docs/etcd/><strong>ETCD</strong></a> module for DCS</td></tr><tr><td style=text-align:center><a href=#minio-node><strong>MINIO Node</strong></a></td><td style=text-align:left>Nodes with the <a href=/docs/minio/><strong>MINIO</strong></a> module for object storage</td></tr><tr><td style=text-align:center><a href=#pgsql-node><strong>PGSQL Node</strong></a></td><td style=text-align:left>Nodes with the <a href=/docs/pgsql/><strong>PGSQL</strong></a> module installed</td></tr><tr><td style=text-align:center>&mldr;</td><td style=text-align:left>Nodes with other modules&mldr;</td></tr></tbody></table><p>In a <a href=/docs/setup/install><strong>singleton</strong></a> Pigsty deployment, multiple roles converge on one node: it serves as the regular node, admin node, infra node, ETCD node, and database node simultaneously.</p><hr><h2 id=regular-node>Regular Node</h2><p>Nodes managed by Pigsty can have modules installed. The <a href=/docs/node/playbook#nodeyml><strong><code>node.yml</code></strong></a> playbook configures nodes to the desired state.
A regular node may run the following services:</p><table class=full-width><thead><tr><th style=text-align:center>Component</th><th style=text-align:center>Port</th><th>Description</th><th>Status</th></tr></thead><tbody><tr><td style=text-align:center><strong><code>node_exporter</code></strong></td><td style=text-align:center><code>9100</code></td><td>Host metrics exporter</td><td>Enabled</td></tr><tr><td style=text-align:center><strong><code>haproxy</code></strong></td><td style=text-align:center><code>9101</code></td><td>HAProxy load balancer (admin port)</td><td>Enabled</td></tr><tr><td style=text-align:center><strong><code>vector</code></strong></td><td style=text-align:center><code>9598</code></td><td>Log collection agent</td><td>Enabled</td></tr><tr><td style=text-align:center><strong><code>docker</code></strong></td><td style=text-align:center><code>9323</code></td><td>Container runtime support</td><td>Optional</td></tr><tr><td style=text-align:center><strong><code>keepalived</code></strong></td><td style=text-align:center><code>n/a</code></td><td>L2 VIP for node cluster</td><td>Optional</td></tr><tr><td style=text-align:center><strong><code>keepalived_exporter</code></strong></td><td style=text-align:center><code>9650</code></td><td>Keepalived status monitor</td><td>Optional</td></tr></tbody></table><p>Here, <code>node_exporter</code> exposes host metrics, <code>vector</code> sends logs to the collection system, and <code>haproxy</code> provides load balancing. These three are enabled by default.
<a href=/docs/docker><strong>Docker</strong></a>, <code>keepalived</code>, and <code>keepalived_exporter</code> are optional and can be enabled as needed.</p><hr><h2 id=admin-node>ADMIN Node</h2><p>A Pigsty deployment has exactly <strong>one admin node</strong>—the node that runs Ansible playbooks and issues control/deployment commands.</p><p>This node has <code>ssh/sudo</code> access to all other nodes. Admin node security is critical; ensure access is strictly controlled.</p><p>During <a href=/docs/setup/install><strong>single-node installation</strong></a> and <a href=/docs/concept/iac/configure><strong>configuration</strong></a>, the current node becomes the admin node.
However, alternatives exist. For example, if your laptop can SSH to all managed nodes and has Ansible installed, it can serve as the admin node—though this isn&rsquo;t recommended for production.</p><p>For instance, you might use your laptop to manage a Pigsty VM in the cloud. In this case, your laptop is the admin node.</p><p>In serious production environments, the admin node is typically 1-2 dedicated DBA machines. In resource-constrained setups, <a href=#infra-node><strong>INFRA nodes</strong></a> often double as admin nodes since all INFRA nodes have Ansible installed by default.</p><hr><h2 id=infra-node>INFRA Node</h2><p>A Pigsty deployment may have <strong>1</strong> or more INFRA nodes; large production environments typically have <strong>2-3</strong>.</p><p>The <strong><code>infra</code></strong> group in the <a href=/docs/concept/iac/inventory><strong>inventory</strong></a> defines which nodes are INFRA nodes. These nodes run the <a href=/docs/infra/><strong>INFRA</strong></a> module with these components:</p><table class=full-width><thead><tr><th style=text-align:center>Component</th><th style=text-align:center>Port</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>nginx</code></td><td style=text-align:center><code>80/443</code></td><td>Web UI, local software repository</td></tr><tr><td style=text-align:center><code>grafana</code></td><td style=text-align:center><code>3000</code></td><td>Visualization platform</td></tr><tr><td style=text-align:center><code>victoriaMetrics</code></td><td style=text-align:center><code>8428</code></td><td>Time-series database (metrics)</td></tr><tr><td style=text-align:center><code>victoriaLogs</code></td><td style=text-align:center><code>9428</code></td><td>Log collection server</td></tr><tr><td style=text-align:center><code>victoriaTraces</code></td><td style=text-align:center><code>10428</code></td><td>Trace collection server</td></tr><tr><td style=text-align:center><code>vmalert</code></td><td style=text-align:center><code>8880</code></td><td>Alerting and derived metrics</td></tr><tr><td style=text-align:center><code>alertmanager</code></td><td style=text-align:center><code>9059</code></td><td>Alert aggregation and routing</td></tr><tr><td style=text-align:center><code>blackbox_exporter</code></td><td style=text-align:center><code>9115</code></td><td>Blackbox probing (ping nodes/VIPs)</td></tr><tr><td style=text-align:center><code>dnsmasq</code></td><td style=text-align:center><code>53</code></td><td>Internal DNS resolution</td></tr><tr><td style=text-align:center><code>chronyd</code></td><td style=text-align:center><code>123</code></td><td>NTP time server</td></tr><tr><td style=text-align:center><code>ansible</code></td><td style=text-align:center><code>-</code></td><td>Playbook execution</td></tr></tbody></table><p>Nginx serves as the module&rsquo;s entry point, providing the web UI and local software repository.
With multiple INFRA nodes, services on each are independent, but you can access all monitoring data sources from any INFRA node&rsquo;s Grafana.</p><p>Note: The <a href=/docs/infra><strong>INFRA</strong></a> module is licensed under <a href=/docs/about/license#pigsty-special-module><strong>AGPLv3</strong></a> due to Grafana.
As an exception, if you only use Nginx/Victoria components without Grafana, you&rsquo;re effectively under <a href=/docs/about/license#pigsty-license><strong>Apache-2.0</strong></a>.</p><hr><h2 id=etcd-node>ETCD Node</h2><p>The <a href=/docs/etcd><strong>ETCD</strong></a> module provides Distributed Consensus Service (DCS) for PostgreSQL high availability.</p><p>The <strong><code>etcd</code></strong> group in the <a href=/docs/concept/iac/inventory><strong>inventory</strong></a> defines ETCD nodes. These nodes run etcd servers on two ports:</p><table class=full-width><thead><tr><th style=text-align:center>Component</th><th style=text-align:center>Port</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>etcd</code></td><td style=text-align:center><code>2379</code></td><td>ETCD key-value store (client port)</td></tr><tr><td style=text-align:center><code>etcd</code></td><td style=text-align:center><code>2380</code></td><td>ETCD cluster peer communication</td></tr></tbody></table><hr><h2 id=minio-node>MINIO Node</h2><p>The <a href=/docs/minio><strong>MINIO</strong></a> module provides optional <a href=/docs/pgsql/backup/repository><strong>backup storage</strong></a> for PostgreSQL.</p><p>The <strong><code>minio</code></strong> group in the inventory defines MinIO nodes. These nodes run MinIO servers on:</p><table class=full-width><thead><tr><th style=text-align:center>Component</th><th style=text-align:center>Port</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>minio</code></td><td style=text-align:center><code>9000</code></td><td>MinIO S3 API endpoint</td></tr><tr><td style=text-align:center><code>minio</code></td><td style=text-align:center><code>9001</code></td><td>MinIO admin console</td></tr></tbody></table><hr><h2 id=pgsql-node>PGSQL Node</h2><p>Nodes with the <a href=/docs/pgsql/><strong>PGSQL</strong></a> module are called PGSQL nodes. Node and PostgreSQL instance have a 1:1 deployment—one PG instance per node.</p><p>PGSQL nodes can borrow <strong>identity</strong> from their PostgreSQL instance—controlled by <a href=/docs/node/param/#node_id_from_pg><code>node_id_from_pg</code></a>, defaulting to <code>true</code>, meaning the node name is set to the PG instance name.</p><p>PGSQL nodes run these additional components beyond <a href=#regular-node><strong>regular node</strong></a> services:</p><table class=full-width><thead><tr><th style=text-align:center>Component</th><th style=text-align:center>Port</th><th>Description</th><th>Status</th></tr></thead><tbody><tr><td style=text-align:center><strong><code>postgres</code></strong></td><td style=text-align:center><code>5432</code></td><td>PostgreSQL database server</td><td>Enabled</td></tr><tr><td style=text-align:center><strong><code>pgbouncer</code></strong></td><td style=text-align:center><code>6432</code></td><td>PgBouncer connection pool</td><td>Enabled</td></tr><tr><td style=text-align:center><strong><code>patroni</code></strong></td><td style=text-align:center><code>8008</code></td><td>Patroni HA management</td><td>Enabled</td></tr><tr><td style=text-align:center><strong><code>pg_exporter</code></strong></td><td style=text-align:center><code>9630</code></td><td>PostgreSQL metrics exporter</td><td>Enabled</td></tr><tr><td style=text-align:center><strong><code>pgbouncer_exporter</code></strong></td><td style=text-align:center><code>9631</code></td><td>PgBouncer metrics exporter</td><td>Enabled</td></tr><tr><td style=text-align:center><strong><code>pgbackrest_exporter</code></strong></td><td style=text-align:center><code>9854</code></td><td>pgBackRest metrics exporter</td><td>Enabled</td></tr><tr><td style=text-align:center><strong><code>vip-manager</code></strong></td><td style=text-align:center><code>n/a</code></td><td>Binds L2 VIP to cluster primary</td><td>Optional</td></tr><tr><td style=text-align:center><strong><code>{{ pg_cluster }}-primary</code></strong></td><td style=text-align:center><code>5433</code></td><td>HAProxy service: pooled read/write</td><td>Enabled</td></tr><tr><td style=text-align:center><strong><code>{{ pg_cluster }}-replica</code></strong></td><td style=text-align:center><code>5434</code></td><td>HAProxy service: pooled read-only</td><td>Enabled</td></tr><tr><td style=text-align:center><strong><code>{{ pg_cluster }}-default</code></strong></td><td style=text-align:center><code>5436</code></td><td>HAProxy service: primary direct connection</td><td>Enabled</td></tr><tr><td style=text-align:center><strong><code>{{ pg_cluster }}-offline</code></strong></td><td style=text-align:center><code>5438</code></td><td>HAProxy service: offline read</td><td>Enabled</td></tr><tr><td style=text-align:center><strong><code>{{ pg_cluster }}-&lt;service></code></strong></td><td style=text-align:center><code>543x</code></td><td>HAProxy service: custom PostgreSQL services</td><td>Custom</td></tr></tbody></table><p>The <code>vip-manager</code> is only enabled when users configure a <strong>PG VIP</strong>.
Additional <a href=/docs/pgsql/service#defining-services><strong>custom services</strong></a> can be defined in <a href=/docs/pgsql/service><strong><code>pg_services</code></strong></a>, exposed via <code>haproxy</code> using additional service ports.</p><hr><h2 id=node-relationships>Node Relationships</h2><p>Regular nodes typically reference an <a href=#infra-node><strong>INFRA node</strong></a> via the <a href=/docs/infra/param/#admin_ip><strong><code>admin_ip</code></strong></a> parameter as their infrastructure provider.
For example, with global <code>admin_ip = 10.10.10.10</code>, all nodes use infrastructure services at this IP.</p><p>Parameters that reference <code>${admin_ip}</code>:</p><table><thead><tr><th style=text-align:left>Parameter</th><th style=text-align:center>Module</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:left><a href=/docs/infra/param/#repo_endpoint><strong><code>repo_endpoint</code></strong></a></td><td style=text-align:center><a href=/docs/infra><strong><code>INFRA</code></strong></a></td><td><code>http://${admin_ip}:80</code></td><td>Software repo URL</td></tr><tr><td style=text-align:left><a href=/docs/infra/param/#repo_upstream><strong><code>repo_upstream</code></strong></a><code>.baseurl</code></td><td style=text-align:center><a href=/docs/infra><strong><code>INFRA</code></strong></a></td><td><code>http://${admin_ip}/pigsty</code></td><td>Local repo baseurl</td></tr><tr><td style=text-align:left><a href=/docs/infra/param/#infra_portal><strong><code>infra_portal</code></strong></a><code>.endpoint</code></td><td style=text-align:center><a href=/docs/infra><strong><code>INFRA</code></strong></a></td><td><code>${admin_ip}:&lt;port></code></td><td>Nginx proxy backend</td></tr><tr><td style=text-align:left><a href=/docs/infra/param/#dns_records><strong><code>dns_records</code></strong></a></td><td style=text-align:center><a href=/docs/infra><strong><code>INFRA</code></strong></a></td><td><code>["${admin_ip} i.pigsty", ...]</code></td><td>DNS records</td></tr><tr><td style=text-align:left><a href=/docs/node/param/#node_default_etc_hosts><strong><code>node_default_etc_hosts</code></strong></a></td><td style=text-align:center><a href=/docs/node><strong><code>NODE</code></strong></a></td><td><code>["${admin_ip} i.pigsty"]</code></td><td>Default static DNS</td></tr><tr><td style=text-align:left><a href=/docs/node/param/#node_etc_hosts><strong><code>node_etc_hosts</code></strong></a></td><td style=text-align:center><a href=/docs/node><strong><code>NODE</code></strong></a></td><td>-</td><td>Custom static DNS</td></tr><tr><td style=text-align:left><a href=/docs/node/param/#node_dns_servers><strong><code>node_dns_servers</code></strong></a></td><td style=text-align:center><a href=/docs/node><strong><code>NODE</code></strong></a></td><td><code>["${admin_ip}"]</code></td><td>Dynamic DNS servers</td></tr><tr><td style=text-align:left><a href=/docs/node/param/#node_ntp_servers><strong><code>node_ntp_servers</code></strong></a></td><td style=text-align:center><a href=/docs/node><strong><code>NODE</code></strong></a></td><td>-</td><td>NTP servers (optional)</td></tr></tbody></table><p>Typically the admin node and INFRA node coincide. With multiple INFRA nodes, the admin node is usually the first one; others serve as backups.</p><p>In large-scale production deployments, you might separate the Ansible admin node from INFRA module nodes.
For example, use 1-2 small dedicated hosts under the DBA team as the control hub (ADMIN nodes), and 2-3 high-spec physical machines as monitoring infrastructure (INFRA nodes).</p><p>Typical node counts by deployment scale:</p><table class=full-width><thead><tr><th style=text-align:center>Scale</th><th style=text-align:center>ADMIN</th><th style=text-align:center>INFRA</th><th style=text-align:center>ETCD</th><th style=text-align:center>MINIO</th><th style=text-align:center>PGSQL</th></tr></thead><tbody><tr><td style=text-align:center>Single-node</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>0</td><td style=text-align:center>1</td></tr><tr><td style=text-align:center>3-node</td><td style=text-align:center>1</td><td style=text-align:center>3</td><td style=text-align:center>3</td><td style=text-align:center>0</td><td style=text-align:center>3</td></tr><tr><td style=text-align:center>Small prod</td><td style=text-align:center>1</td><td style=text-align:center>2</td><td style=text-align:center>3</td><td style=text-align:center>0</td><td style=text-align:center>N</td></tr><tr><td style=text-align:center>Large prod</td><td style=text-align:center>2</td><td style=text-align:center>3</td><td style=text-align:center>5</td><td style=text-align:center>4+</td><td style=text-align:center>N</td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-d65561d0ed395a21fc6b7dd2654dc081>2 - Infrastructure</h1><div class=lead>Infrastructure module architecture, components, and functionality in Pigsty.</div><p>Running production-grade, highly available PostgreSQL clusters typically requires a comprehensive set of infrastructure services (foundation) for support, such as monitoring and alerting, log collection, time synchronization, DNS resolution, and local software repositories.
Pigsty provides the <a href=/docs/infra><strong>INFRA module</strong></a> to address this—it&rsquo;s an <strong>optional module</strong>, but we strongly recommend enabling it.</p><hr><h2 id=overview>Overview</h2><p>The diagram below shows the architecture of a <a href=/docs/setup/install><strong>single-node deployment</strong></a>. The right half represents the components included in the <a href=/docs/infra><strong>INFRA module</strong></a>:</p><table><thead><tr><th style=text-align:left>Component</th><th>Type</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><a href=#nginx><strong>Nginx</strong></a></td><td>Web Server</td><td style=text-align:left>Unified entry for <a href=/docs/setup/webui><strong>WebUI</strong></a>, <a href=#repo><strong>local repo</strong></a>, reverse proxy for internal services</td></tr><tr><td style=text-align:left><a href=#repo><strong>Repo</strong></a></td><td>Software Repo</td><td style=text-align:left>APT/DNF repository with all RPM/DEB packages needed for deployment</td></tr><tr><td style=text-align:left><a href=#grafana><strong>Grafana</strong></a></td><td>Visualization</td><td style=text-align:left>Displays metrics, logs, and traces; hosts dashboards, reports, and custom data apps</td></tr><tr><td style=text-align:left><a href=#victoriametrics><strong>VictoriaMetrics</strong></a></td><td>Time Series DB</td><td style=text-align:left>Scrapes all metrics, Prometheus API compatible, provides VMUI query interface</td></tr><tr><td style=text-align:left><a href=#victorialogs><strong>VictoriaLogs</strong></a></td><td>Log Platform</td><td style=text-align:left>Centralized log storage; all nodes run Vector by default, pushing logs here</td></tr><tr><td style=text-align:left><a href=#victoriatraces><strong>VictoriaTraces</strong></a></td><td>Tracing</td><td style=text-align:left>Collects slow SQL, service traces, and other tracing data</td></tr><tr><td style=text-align:left><a href=#vmalert><strong>VMAlert</strong></a></td><td>Alert Engine</td><td style=text-align:left>Evaluates alerting rules, pushes events to Alertmanager</td></tr><tr><td style=text-align:left><a href=#alertmanager><strong>AlertManager</strong></a></td><td>Alert Manager</td><td style=text-align:left>Aggregates alerts, dispatches notifications via email, Webhook, etc.</td></tr><tr><td style=text-align:left><a href=#blackboxexporter><strong>BlackboxExporter</strong></a></td><td>Blackbox Probe</td><td style=text-align:left>Probes reachability of IPs/VIPs/URLs</td></tr><tr><td style=text-align:left><a href=#dnsmasq><strong>DNSMASQ</strong></a></td><td>DNS Service</td><td style=text-align:left>Provides DNS resolution for domains used within Pigsty [Optional]</td></tr><tr><td style=text-align:left><a href=#chronyd><strong>Chronyd</strong></a></td><td>Time Sync</td><td style=text-align:left>Provides NTP time synchronization to ensure consistent time across nodes [Optional]</td></tr><tr><td style=text-align:left><a href=/docs/concept/sec/ca><strong>CA</strong></a></td><td>Certificate</td><td style=text-align:left>Issues encryption certificates within the environment</td></tr><tr><td style=text-align:left><a href=/docs/setup/playbook><strong>Ansible</strong></a></td><td>Orchestration</td><td style=text-align:left>Batch, declarative, agentless tool for managing large numbers of servers</td></tr></tbody></table><p><a href=/docs/infra/><img src=/img/pigsty/arch.png alt=pigsty-arch></a></p><hr><h2 id=nginx>Nginx</h2><p><strong>Nginx</strong> is the access entry point for all WebUI services in Pigsty, using ports <a href=/docs/infra/param#nginx_port><strong><code>80</code></strong></a> / <a href=/docs/infra/param#nginx_ssl_port><strong><code>443</code></strong></a> for HTTP/HTTPS by default. <a href=https://demo.pigsty.io/><strong>Live Demo</strong></a></p><table class=full-width><thead><tr><th style=text-align:center>IP Access (replace)</th><th style=text-align:center>Domain (HTTP)</th><th style=text-align:center>Domain (HTTPS)</th><th style=text-align:center>Public Demo</th></tr></thead><tbody><tr><td style=text-align:center><a href=http://10.10.10.10><strong><code>http://10.10.10.10</code></strong></a></td><td style=text-align:center><a href=http://i.pigsty><strong><code>http://i.pigsty</code></strong></a></td><td style=text-align:center><a href=https://i.pigsty><strong><code>https://i.pigsty</code></strong></a></td><td style=text-align:center><a href=https://demo.pigsty.io><strong><code>https://demo.pigsty.io</code></strong></a></td></tr></tbody></table><p>Infrastructure components with WebUIs can be exposed uniformly through <strong>Nginx</strong>, such as <strong>Grafana</strong>, <strong>VictoriaMetrics</strong> (VMUI), <strong>AlertManager</strong>,
and <strong>HAProxy</strong> console. Additionally, the <strong>local software repository</strong> and other static resources are served via <strong>Nginx</strong>.</p><p><strong>Nginx</strong> configures local web servers or reverse proxy servers based on definitions in <a href=/docs/infra/param/#infra_portal><strong><code>infra_portal</code></strong></a>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>infra_portal</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span><span style=color:#204a87;font-weight:700>home </span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span>{<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>domain</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>i.pigsty }</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><p>By default, it exposes Pigsty&rsquo;s admin homepage: <code>i.pigsty</code>. Different endpoints on this page proxy different components:</p><table class=full-width><thead><tr><th style=text-align:left>Endpoint</th><th style=text-align:left>Component</th><th style=text-align:left>Native Port</th><th style=text-align:left>Notes</th><th>Public Demo</th></tr></thead><tbody><tr><td style=text-align:left><code>/</code></td><td style=text-align:left><a href=/docs/infra/><strong>Nginx</strong></a></td><td style=text-align:left><code>80/443</code></td><td style=text-align:left>Homepage, local repo, file server</td><td><a href=https://demo.pigsty.io><code>demo.pigsty.io</code></a></td></tr><tr><td style=text-align:left><code>/ui/</code></td><td style=text-align:left><a href=#grafana><strong>Grafana</strong></a></td><td style=text-align:left><code>3000</code></td><td style=text-align:left>Grafana dashboard entry</td><td><a href=https://demo.pigsty.io/ui/><code>demo.pigsty.io/ui/</code></a></td></tr><tr><td style=text-align:left><code>/vmetrics/</code></td><td style=text-align:left><a href=/docs/infra/><strong>VictoriaMetrics</strong></a></td><td style=text-align:left><code>8428</code></td><td style=text-align:left>Time series DB Web UI</td><td><a href=https://demo.pigsty.io/vmetrics/><code>demo.pigsty.io/vmetrics/</code></a></td></tr><tr><td style=text-align:left><code>/vlogs/</code></td><td style=text-align:left><a href=/docs/infra/><strong>VictoriaLogs</strong></a></td><td style=text-align:left><code>9428</code></td><td style=text-align:left>Log DB Web UI</td><td><a href=https://demo.pigsty.io/vlogs/><code>demo.pigsty.io/vlogs/</code></a></td></tr><tr><td style=text-align:left><code>/vtraces/</code></td><td style=text-align:left><a href=/docs/infra/><strong>VictoriaTraces</strong></a></td><td style=text-align:left><code>10428</code></td><td style=text-align:left>Tracing Web UI</td><td><a href=https://demo.pigsty.io/vtraces/><code>demo.pigsty.io/vtraces/</code></a></td></tr><tr><td style=text-align:left><code>/vmalert/</code></td><td style=text-align:left><a href=/docs/infra/><strong>VMAlert</strong></a></td><td style=text-align:left><code>8880</code></td><td style=text-align:left>Alert rule management</td><td><a href=https://demo.pigsty.io/vmalert/><code>demo.pigsty.io/vmalert/</code></a></td></tr><tr><td style=text-align:left><code>/alertmgr/</code></td><td style=text-align:left><a href=/docs/infra/><strong>AlertManager</strong></a></td><td style=text-align:left><code>9059</code></td><td style=text-align:left>Alert management Web UI</td><td><a href=https://demo.pigsty.io/alertmgr/><code>demo.pigsty.io/alertmgr/</code></a></td></tr><tr><td style=text-align:left><code>/blackbox/</code></td><td style=text-align:left><a href=/docs/infra/><strong>Blackbox</strong></a></td><td style=text-align:left><code>9115</code></td><td style=text-align:left>Blackbox probe</td><td></td></tr></tbody></table><p><a href=https://demo.pigsty.io><img src=/img/pigsty/home.png alt></a></p><p>Pigsty allows rich customization of <strong>Nginx</strong> as a local file server or reverse proxy, with self-signed or real HTTPS certificates.</p><p>For more information, see: <a href=/docs/infra/admin/portal/><strong>Tutorial: Nginx—Expose Web Services via Proxy</strong></a> and <a href=/docs/infra/admin/cert><strong>Tutorial: Certbot—Request and Renew HTTPS Certificates</strong></a></p><hr><h2 id=repo>Repo</h2><p>Pigsty creates a <strong>local software repository</strong> on the Infra node during installation to accelerate subsequent software installations. <a href=https://demo.pigsty.io/pigsty/><strong>Live Demo</strong></a></p><p>This repository defaults to the <a href=/docs/infra/param#repo_home><strong><code>/www/pigsty</code></strong></a> directory,
served by <strong>Nginx</strong> and mounted at the <code>/pigsty</code> path:</p><table class=full-width><thead><tr><th style=text-align:center>IP Access (replace)</th><th style=text-align:center>Domain (HTTP)</th><th style=text-align:center>Domain (HTTPS)</th><th style=text-align:center>Public Demo</th></tr></thead><tbody><tr><td style=text-align:center><a href=http://10.10.10.10/pigsty><strong><code>http://10.10.10.10/pigsty</code></strong></a></td><td style=text-align:center><a href=http://i.pigsty/pigsty><strong><code>http://i.pigsty/pigsty</code></strong></a></td><td style=text-align:center><a href=https://i.pigsty/pigsty><strong><code>https://i.pigsty/pigsty</code></strong></a></td><td style=text-align:center><a href=https://demo.pigsty.io/pigsty><strong><code>https://demo.pigsty.io/pigsty</code></strong></a></td></tr></tbody></table><p>Pigsty supports <a href=/docs/setup/offline><strong>offline installation</strong></a>, which essentially pre-copies a prepared local software repository to the target environment.
When Pigsty performs production deployment and needs to create a local software repository, if it finds the <strong><code>/www/pigsty/repo_complete</code></strong> marker file already exists locally, it skips downloading packages from upstream and uses existing packages directly, avoiding internet downloads.</p><p><a href=https://demo.pigsty.io/pigsty/><img src=/img/pigsty/repo.webp alt=repo></a></p><p>For more information, see: <a href=/docs/infra/param/#repo><strong>Config: INFRA - REPO</strong></a></p><hr><h2 id=grafana>Grafana</h2><p><strong>Grafana</strong> is the core component of Pigsty&rsquo;s monitoring system, used for visualizing metrics, logs, and various information. <a href=https://demo.pigsty.io/ui/><strong>Live Demo</strong></a></p><p><strong>Grafana</strong> listens on port <code>3000</code> by default and is proxied via <strong>Nginx</strong> at the <code>/ui</code> path:</p><table class=full-width><thead><tr><th style=text-align:center>IP Access (replace)</th><th style=text-align:center>Domain (HTTP)</th><th style=text-align:center>Domain (HTTPS)</th><th style=text-align:center>Public Demo</th></tr></thead><tbody><tr><td style=text-align:center><a href=http://10.10.10.10/ui><strong><code>http://10.10.10.10/ui</code></strong></a></td><td style=text-align:center><a href=http://i.pigsty/ui><strong><code>http://i.pigsty/ui</code></strong></a></td><td style=text-align:center><a href=https://i.pigsty/ui><strong><code>https://i.pigsty/ui</code></strong></a></td><td style=text-align:center><a href=https://demo.pigsty.io/ui><strong><code>https://demo.pigsty.io/ui</code></strong></a></td></tr></tbody></table><p>Pigsty provides pre-built dashboards based on <strong>VictoriaMetrics</strong> / <strong>Logs</strong> / <strong>Traces</strong>, with one-click drill-down and roll-up via URL jumps for rapid troubleshooting.</p><p><strong>Grafana</strong> can also serve as a low-code visualization platform, so <strong>ECharts</strong>, victoriametrics-datasource, victorialogs-datasource plugins are installed by default,
with <strong>Vector</strong> / <strong>Victoria</strong> datasources registered uniformly as <code>vmetrics-*</code>, <code>vlogs-*</code>, <code>vtraces-*</code> for easy custom dashboard extension.</p><p><img src=/img/dashboard/pigsty.jpg alt=dashboard></p><p>For more information, see: <a href=/docs/infra/param/#grafana><strong>Config: INFRA - GRAFANA</strong></a>.</p><hr><h2 id=victoriametrics>VictoriaMetrics</h2><p><strong>VictoriaMetrics</strong> is Pigsty&rsquo;s time series database, responsible for scraping and storing all monitoring metrics. <a href=https://demo.pigsty.io/vmetrics/><strong>Live Demo</strong></a></p><p>It listens on port <code>8428</code> by default, mounted at <strong>Nginx</strong> <code>/vmetrics</code> path, and also accessible via the <code>p.pigsty</code> domain:</p><table class=full-width><thead><tr><th style=text-align:center>IP Access (replace)</th><th style=text-align:center>Domain (HTTP)</th><th style=text-align:center>Domain (HTTPS)</th><th style=text-align:center>Public Demo</th></tr></thead><tbody><tr><td style=text-align:center><a href=http://10.10.10.10/vmetrics><strong><code>http://10.10.10.10/vmetrics</code></strong></a></td><td style=text-align:center><a href=http://p.pigsty><strong><code>http://p.pigsty</code></strong></a></td><td style=text-align:center><a href=https://i.pigsty/vmetrics><strong><code>https://i.pigsty/vmetrics</code></strong></a></td><td style=text-align:center><a href=https://demo.pigsty.io/vmetrics><strong><code>https://demo.pigsty.io/vmetrics</code></strong></a></td></tr></tbody></table><p><strong>VictoriaMetrics</strong> is fully compatible with the <strong>Prometheus</strong> API, supporting PromQL queries, remote read/write protocols, and the Alertmanager API.
The built-in <strong>VMUI</strong> provides an ad-hoc query interface for exploring metrics data directly, and also serves as a <strong>Grafana</strong> datasource.</p><p><a href=https://demo.pigsty.io/vmetrics/vmui><img src=/img/pigsty/vmetrics.webp alt=vmetrics></a></p><p>For more information, see: <a href=/docs/infra/param/#vmetrics_enabled><strong>Config: INFRA - VMETRICS</strong></a></p><hr><h2 id=victorialogs>VictoriaLogs</h2><p><strong>VictoriaLogs</strong> is Pigsty&rsquo;s log platform, centrally storing structured logs from all nodes. <a href=https://demo.pigsty.io/vlogs/><strong>Live Demo</strong></a></p><p>It listens on port <code>9428</code> by default, mounted at <strong>Nginx</strong> <code>/vlogs</code> path:</p><table class=full-width><thead><tr><th style=text-align:center>IP Access (replace)</th><th style=text-align:center>Domain (HTTP)</th><th style=text-align:center>Domain (HTTPS)</th><th style=text-align:center>Public Demo</th></tr></thead><tbody><tr><td style=text-align:center><a href=http://10.10.10.10/vlogs><strong><code>http://10.10.10.10/vlogs</code></strong></a></td><td style=text-align:center><a href=http://i.pigsty/vlogs><strong><code>http://i.pigsty/vlogs</code></strong></a></td><td style=text-align:center><a href=https://i.pigsty/vlogs><strong><code>https://i.pigsty/vlogs</code></strong></a></td><td style=text-align:center><a href=https://demo.pigsty.io/vlogs><strong><code>https://demo.pigsty.io/vlogs</code></strong></a></td></tr></tbody></table><p>All managed nodes run <strong>Vector</strong> Agent by default, collecting system logs, PostgreSQL logs, Patroni logs, Pgbouncer logs, etc., processing them into structured format and pushing to <strong>VictoriaLogs</strong>.
The built-in Web UI supports log search and filtering, and can be integrated with <strong>Grafana</strong>&rsquo;s victorialogs-datasource plugin for visual analysis.</p><p><a href=https://demo.pigsty.io/vlogs/select/vmui><img src=/img/pigsty/vmlogs.webp alt=vlogs></a></p><p>For more information, see: <a href=/docs/infra/param/#vlogs_enabled><strong>Config: INFRA - VLOGS</strong></a></p><hr><h2 id=victoriatraces>VictoriaTraces</h2><p><strong>VictoriaTraces</strong> is used for collecting trace data and slow SQL records. <a href=https://demo.pigsty.io/vtraces/><strong>Live Demo</strong></a></p><p>It listens on port <code>10428</code> by default, mounted at <strong>Nginx</strong> <code>/vtraces</code> path:</p><table class=full-width><thead><tr><th style=text-align:center>IP Access (replace)</th><th style=text-align:center>Domain (HTTP)</th><th style=text-align:center>Domain (HTTPS)</th><th style=text-align:center>Public Demo</th></tr></thead><tbody><tr><td style=text-align:center><a href=http://10.10.10.10/vtraces><strong><code>http://10.10.10.10/vtraces</code></strong></a></td><td style=text-align:center><a href=http://i.pigsty/vtraces><strong><code>http://i.pigsty/vtraces</code></strong></a></td><td style=text-align:center><a href=https://i.pigsty/vtraces><strong><code>https://i.pigsty/vtraces</code></strong></a></td><td style=text-align:center><a href=https://demo.pigsty.io/vtraces><strong><code>https://demo.pigsty.io/vtraces</code></strong></a></td></tr></tbody></table><p><strong>VictoriaTraces</strong> provides a <strong>Jaeger</strong>-compatible interface for analyzing service call chains and database slow queries.
Combined with <strong>Grafana</strong> dashboards, it enables rapid identification of performance bottlenecks and root cause tracing.</p><p>For more information, see: <a href=/docs/infra/param/#vtraces_enabled><strong>Config: INFRA - VTRACES</strong></a></p><hr><h2 id=vmalert>VMAlert</h2><p><strong>VMAlert</strong> is the alerting rule computation engine, responsible for evaluating alert rules and pushing triggered events to <strong>Alertmanager</strong>. <a href=https://demo.pigsty.io/vmalert/><strong>Live Demo</strong></a></p><p>It listens on port <code>8880</code> by default, mounted at <strong>Nginx</strong> <code>/vmalert</code> path:</p><table class=full-width><thead><tr><th style=text-align:center>IP Access (replace)</th><th style=text-align:center>Domain (HTTP)</th><th style=text-align:center>Domain (HTTPS)</th><th style=text-align:center>Public Demo</th></tr></thead><tbody><tr><td style=text-align:center><a href=http://10.10.10.10/vmalert><strong><code>http://10.10.10.10/vmalert</code></strong></a></td><td style=text-align:center><a href=http://i.pigsty/vmalert><strong><code>http://i.pigsty/vmalert</code></strong></a></td><td style=text-align:center><a href=https://i.pigsty/vmalert><strong><code>https://i.pigsty/vmalert</code></strong></a></td><td style=text-align:center><a href=https://demo.pigsty.io/vmalert><strong><code>https://demo.pigsty.io/vmalert</code></strong></a></td></tr></tbody></table><p><strong>VMAlert</strong> reads metrics data from <strong>VictoriaMetrics</strong> and periodically evaluates alerting rules.
Pigsty provides pre-built alerting rules for PGSQL, NODE, REDIS, and other modules, covering common failure scenarios out of the box.</p><p><a href=https://demo.pigsty.io/vmalert/vmalert/groups><img src=/img/pigsty/vmalert.webp alt=vmalert></a></p><p>For more information, see: <a href=/docs/infra/param/#vmalert_enabled><strong>Config: INFRA - VMALERT</strong></a></p><hr><h2 id=alertmanager>AlertManager</h2><p><strong>AlertManager</strong> handles alert event aggregation, deduplication, grouping, and dispatch. <a href=https://demo.pigsty.io/alertmgr/><strong>Live Demo</strong></a></p><p>It listens on port <code>9059</code> by default, mounted at <strong>Nginx</strong> <code>/alertmgr</code> path, and also accessible via the <code>a.pigsty</code> domain:</p><table class=full-width><thead><tr><th style=text-align:center>IP Access (replace)</th><th style=text-align:center>Domain (HTTP)</th><th style=text-align:center>Domain (HTTPS)</th><th style=text-align:center>Public Demo</th></tr></thead><tbody><tr><td style=text-align:center><a href=http://10.10.10.10/alertmgr><strong><code>http://10.10.10.10/alertmgr</code></strong></a></td><td style=text-align:center><a href=http://a.pigsty><strong><code>http://a.pigsty</code></strong></a></td><td style=text-align:center><a href=https://i.pigsty/alertmgr><strong><code>https://i.pigsty/alertmgr</code></strong></a></td><td style=text-align:center><a href=https://demo.pigsty.io/alertmgr><strong><code>https://demo.pigsty.io/alertmgr</code></strong></a></td></tr></tbody></table><p><strong>AlertManager</strong> supports multiple notification channels: email, Webhook, Slack, PagerDuty, WeChat Work, etc.
Through alert routing rules, differentiated dispatch based on severity level and module type is possible, with support for silencing, inhibition, and other advanced features.</p><p><a href=https://demo.pigsty.io/alertmgr/><img src=/img/pigsty/alertmanager.webp alt=alertmanager></a></p><p>For more information, see: <a href=/docs/infra/param/#alertmanager_enabled><strong>Config: INFRA - AlertManager</strong></a></p><hr><h2 id=blackboxexporter>BlackboxExporter</h2><p><strong>Blackbox Exporter</strong> is used for active probing of target reachability, enabling blackbox monitoring.</p><p>It listens on port <code>9115</code> by default, mounted at <strong>Nginx</strong> <code>/blackbox</code> path:</p><table class=full-width><thead><tr><th style=text-align:center>IP Access (replace)</th><th style=text-align:center>Domain (HTTP)</th><th style=text-align:center>Domain (HTTPS)</th><th style=text-align:center>Public Demo</th></tr></thead><tbody><tr><td style=text-align:center><a href=http://10.10.10.10/blackbox><strong><code>http://10.10.10.10/blackbox</code></strong></a></td><td style=text-align:center><a href=http://i.pigsty/blackbox><strong><code>http://i.pigsty/blackbox</code></strong></a></td><td style=text-align:center><a href=https://i.pigsty/blackbox><strong><code>https://i.pigsty/blackbox</code></strong></a></td><td style=text-align:center><a href=https://demo.pigsty.io/blackbox><strong><code>https://demo.pigsty.io/blackbox</code></strong></a></td></tr></tbody></table><p>It supports multiple probe methods including ICMP Ping, TCP ports, and HTTP/HTTPS endpoints.
Useful for monitoring VIP reachability, service port availability, external dependency health, etc.—an important tool for assessing failure impact scope.</p><p><a href=https://demo.pigsty.io/blackbox/><img src=/img/pigsty/blackbox.webp alt=blackbox></a></p><p>For more information, see: <a href=/docs/infra/param/#blackbox_exporter><strong>Config: INFRA - BLACKBOX</strong></a></p><hr><h2 id=ansible>Ansible</h2><p><strong>Ansible</strong> is Pigsty&rsquo;s core orchestration tool; all deployment, configuration, and management operations are performed through Ansible Playbooks.</p><p>Pigsty automatically installs <strong>Ansible</strong> on the admin node (Infra node) during installation.
It adopts a declarative configuration style and idempotent playbook design: the same playbook can be run repeatedly, and the system automatically converges to the desired state without side effects.</p><p><strong>Ansible</strong>&rsquo;s core advantages:</p><ul><li><strong>Agentless</strong>: Executes remotely via SSH, no additional software needed on target nodes.</li><li><strong>Declarative</strong>: Describes the desired state rather than execution steps; configuration is documentation.</li><li><strong>Idempotent</strong>: Multiple executions produce consistent results; supports retry after partial failures.</li></ul><p>For more information, see: <a href=/docs/setup/playbook><strong>Playbooks: Pigsty Playbook</strong></a></p><hr><h2 id=dnsmasq>DNSMASQ</h2><p><strong>DNSMASQ</strong> provides DNS resolution on <a href=/docs/concept/arch/node#infra-node><strong>INFRA nodes</strong></a>, resolving domain names to their corresponding IP addresses.</p><p>DNSMASQ listens on port <code>53</code> (UDP/TCP) by default, providing DNS resolution for all nodes. Records are stored in the <code>/infra/hosts</code> directory.</p><p>Other modules automatically register their domain names with <strong>DNSMASQ</strong> during deployment, which you can use as needed.
DNS is completely optional—<strong>Pigsty works normally without it</strong>.
Client nodes can configure INFRA nodes as their DNS servers, allowing access to services via domain names without remembering IP addresses.</p><ul><li><a href=/docs/infra/param/#dns_records><strong><code>dns_records</code></strong></a>: Default DNS records written to INFRA nodes</li><li><a href=/docs/node/param/#node_dns_servers><strong><code>node_dns_servers</code></strong></a>: Configure DNS servers for nodes, defaults to INFRA node via <a href=/docs/infra/param/#admin_ip><strong><code>admin_ip</code></strong></a> (can also be <a href=/docs/node/param#node_dns_method><strong>disabled</strong></a>)</li></ul><p>For more information, see: <a href=/docs/infra/param/#dns><strong>Config: INFRA - DNS</strong></a> and <a href=/docs/infra/admin/domain><strong>Tutorial: DNS—Configure Domain Resolution</strong></a></p><hr><h2 id=chronyd>Chronyd</h2><p><strong>Chronyd</strong> provides NTP time synchronization, ensuring consistent clocks across all nodes. It listens on port <code>123</code> (UDP) by default as the time source.</p><p>Time synchronization is critical for distributed systems: log analysis requires aligned timestamps, certificate validation depends on accurate clocks, and <strong>PostgreSQL</strong> streaming replication is sensitive to clock drift.
In isolated network environments, the INFRA node can serve as an internal NTP server with other nodes synchronizing to it.</p><p>In Pigsty, all nodes run chronyd by default for time sync. The default upstream is <a href=/docs/node/param#node_ntp_servers><strong><code>pool.ntp.org</code></strong></a> public NTP servers.
Chronyd is essentially managed by the <a href=/docs/node><strong>Node module</strong></a>, but in isolated networks, you can use <a href=/docs/infra/param/#admin_ip><strong><code>admin_ip</code></strong></a> to point to the INFRA node&rsquo;s Chronyd service as the internal time source.
In this case, the Chronyd service on the <a href=/docs/concept/arch/node#infra-node><strong>INFRA node</strong></a> serves as the internal time synchronization infrastructure.</p><p>For more information, see: <a href=/docs/node/param/#node_time><strong>Config: NODE - TIME</strong></a></p><hr><h2 id=infra-node-vs-regular-node>INFRA Node vs Regular Node</h2><p>In Pigsty, the relationship between nodes and infrastructure is a <strong>weak circular dependency</strong>: node_monitor → infra → node</p><p>The <a href=/docs/node><strong>NODE module</strong></a> itself doesn&rsquo;t depend on the <a href=/docs/infra><strong>INFRA module</strong></a>, but the monitoring functionality (node_monitor) requires the monitoring platform and services provided by the infrastructure module.</p><p>Therefore, in the <a href=/docs/infra/playbook#infrayml><strong><code>infra.yml</code></strong></a> and <a href=/docs/setup/playbook><strong><code>deploy</code></strong></a> playbooks, an &ldquo;interleaved deployment&rdquo; technique is used:</p><ul><li>First, initialize the <a href=/docs/node><strong>NODE module</strong></a> on all <a href=/docs/concept/arch/node#regular-node><strong>regular nodes</strong></a>, but skip monitoring config since infrastructure isn&rsquo;t deployed yet.</li><li>Then, initialize the <a href=/docs/infra><strong>INFRA module</strong></a> on the <a href=/docs/concept/arch/node#infra-node><strong>INFRA node</strong></a>—monitoring is now available.</li><li>Finally, reconfigure monitoring on all <a href=/docs/concept/arch/node#regular-node><strong>regular nodes</strong></a>, connecting to the now-deployed monitoring platform.</li></ul><p>If you don&rsquo;t need &ldquo;one-shot&rdquo; deployment of all nodes, you can use <a href=/docs/setup/config#adding-infrastructure><strong>phased deployment</strong></a>: initialize INFRA nodes first, then regular nodes.</p><h3 id=how-are-nodes-coupled-to-infrastructure>How Are Nodes Coupled to Infrastructure?</h3><p>Regular nodes reference an <a href=#infra-node><strong>INFRA node</strong></a> via the <a href=/docs/infra/param/#admin_ip><strong><code>admin_ip</code></strong></a> parameter as their infrastructure provider.</p><p>For example, when you configure global <code>admin_ip = 10.10.10.10</code>, all nodes will typically use infrastructure services at this IP.</p><p>This design allows quick, batch switching of infrastructure providers. Parameters that <strong>may</strong> reference <code>${admin_ip}</code>:</p><table class=full-width><thead><tr><th style=text-align:left>Parameter</th><th style=text-align:center>Module</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:left><a href=/docs/infra/param/#repo_endpoint><strong><code>repo_endpoint</code></strong></a></td><td style=text-align:center><a href=/docs/infra><strong><code>INFRA</code></strong></a></td><td><code>http://${admin_ip}:80</code></td><td>Software repo URL</td></tr><tr><td style=text-align:left><a href=/docs/infra/param/#repo_upstream><strong><code>repo_upstream</code></strong></a><code>.baseurl</code></td><td style=text-align:center><a href=/docs/infra><strong><code>INFRA</code></strong></a></td><td><code>http://${admin_ip}/pigsty</code></td><td>Local repo baseurl</td></tr><tr><td style=text-align:left><a href=/docs/infra/param/#infra_portal><strong><code>infra_portal</code></strong></a><code>.endpoint</code></td><td style=text-align:center><a href=/docs/infra><strong><code>INFRA</code></strong></a></td><td><code>${admin_ip}:&lt;port></code></td><td>Nginx proxy backend</td></tr><tr><td style=text-align:left><a href=/docs/infra/param/#dns_records><strong><code>dns_records</code></strong></a></td><td style=text-align:center><a href=/docs/infra><strong><code>INFRA</code></strong></a></td><td><code>["${admin_ip} i.pigsty", ...]</code></td><td>DNS records</td></tr><tr><td style=text-align:left><a href=/docs/node/param/#node_default_etc_hosts><strong><code>node_default_etc_hosts</code></strong></a></td><td style=text-align:center><a href=/docs/node><strong><code>NODE</code></strong></a></td><td><code>["${admin_ip} i.pigsty"]</code></td><td>Default static DNS</td></tr><tr><td style=text-align:left><a href=/docs/node/param/#node_etc_hosts><strong><code>node_etc_hosts</code></strong></a></td><td style=text-align:center><a href=/docs/node><strong><code>NODE</code></strong></a></td><td><code>[]</code></td><td>Custom static DNS</td></tr><tr><td style=text-align:left><a href=/docs/node/param/#node_dns_servers><strong><code>node_dns_servers</code></strong></a></td><td style=text-align:center><a href=/docs/node><strong><code>NODE</code></strong></a></td><td><code>["${admin_ip}"]</code></td><td>Dynamic DNS servers</td></tr><tr><td style=text-align:left><a href=/docs/node/param/#node_ntp_servers><strong><code>node_ntp_servers</code></strong></a></td><td style=text-align:center><a href=/docs/node><strong><code>NODE</code></strong></a></td><td><code>["pool pool.ntp.org iburst"]</code></td><td>NTP servers (optional)</td></tr></tbody></table><p>For example, when a node installs software, the <code>local</code> repo points to the Nginx local software repository at <code>admin_ip:80/pigsty</code>. The DNS server also points to <a href=#dnsmasq><strong>DNSMASQ</strong></a> at <code>admin_ip:53</code>.
However, this isn&rsquo;t mandatory—nodes can ignore the <code>local</code> repo and install directly from upstream internet sources (most single-node config templates); DNS servers can also remain unconfigured, as Pigsty has no DNS dependency.</p><hr><h2 id=infra-node-vs-admin-node>INFRA Node vs ADMIN Node</h2><p>The management-initiating <a href=/docs/concept/arch/node#admin-node><strong>ADMIN node</strong></a> typically coincides with the <a href=/docs/concept/arch/node#infra-node><strong>INFRA node</strong></a>.
In <a href=/docs/setup/install><strong>single-node deployment</strong></a>, this is exactly the case. In multi-node deployment with multiple INFRA nodes, the admin node is usually the first in the <code>infra</code> group; others serve as backups.
However, exceptions exist. You might separate them for various reasons:</p><p>For example, in <a href=/docs/deploy><strong>large-scale production deployments</strong></a>, a classic pattern uses 1-2 dedicated management hosts (tiny VMs suffice) belonging to the DBA team
as the control hub, with 2-3 high-spec physical machines (or more!) as monitoring infrastructure. Here, admin nodes are separate from infrastructure nodes.
In this case, the <a href=/docs/infra/param/#admin_ip><strong>admin_ip</strong></a> in your config should point to an INFRA node&rsquo;s IP, not the current ADMIN node&rsquo;s IP.
This is for historical reasons: initially ADMIN and INFRA nodes were tightly coupled concepts, with separation capabilities evolving later, so the parameter name wasn&rsquo;t changed.</p><p>Another common scenario is <a href=/docs/setup><strong>managing cloud nodes locally</strong></a>. For example, you can install Ansible on your laptop and specify cloud nodes as &ldquo;managed targets.&rdquo;
In this case, your laptop acts as the ADMIN node, while cloud servers act as INFRA nodes.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>all</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span><span style=color:#204a87;font-weight:700>children</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>infra</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>   </span>{<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>hosts</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span>{<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>10.10.10.10</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span>{<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>infra_seq: 1 , ansible_host</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>your_ssh_alias } } } </span><span style=color:#f8f8f8> </span><span style=color:#8f5902;font-style:italic># &lt;--- Use ansible_host to point to cloud node (fill in ssh alias)</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>etcd</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>    </span>{<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>hosts</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span>{<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>10.10.10.10</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span>{<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>etcd_seq: 1 } }, vars</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span>{<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>etcd_cluster: etcd } }    # SSH connection will use</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>ssh your_ssh_alias</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>pg-meta</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span>{<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>hosts</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span>{<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>10.10.10.10</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span>{<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>pg_seq: 1, pg_role: primary } }, vars</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span>{<span style=color:#f8f8f8> </span><span style=color:#204a87;font-weight:700>pg_cluster</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>pg-meta } }</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span><span style=color:#204a87;font-weight:700>vars</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>version</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>v4.0.0</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>admin_ip</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#0000cf;font-weight:700>10.10.10.10</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>region</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>default</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><hr><h2 id=multiple-infra-nodes>Multiple INFRA Nodes</h2><p>By default, Pigsty only needs one INFRA node for most requirements. Even if the INFRA module goes down, it won&rsquo;t affect database services on other nodes.</p><p>However, in production environments with high monitoring and alerting requirements, you may want multiple INFRA nodes to improve infrastructure availability.
A common deployment uses two Infra nodes for redundancy, monitoring each other&mldr;
or more nodes to deploy a distributed Victoria cluster for unlimited horizontal scaling.</p><p>Each Infra node is <strong>independent</strong>—Nginx points to services on the local machine.
VictoriaMetrics independently scrapes metrics from all services in the environment,
and logs are pushed to all VictoriaLogs collection endpoints by default.
The only exception is Grafana: every Grafana instance registers all VictoriaMetrics / Logs / Traces / PostgreSQL instances as datasources.
Therefore, each Grafana instance can see complete monitoring data.</p><p>If you modify Grafana—such as adding new dashboards or changing datasource configs—these changes only affect the Grafana instance on that node.
To keep Grafana consistent across all nodes, use a PostgreSQL database as shared storage. See <a href=/docs/infra/admin/grafana><strong>Tutorial: Configure Grafana High Availability</strong></a> for details.</p><p><a href=https://demo.pigsty.io/ui/d/infra-overview><img src=/img/dashboard/infra-overview.webp alt></a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-c777ff17702df77ebc33dadeb81e4174>3 - PGSQL Arch</h1><div class=lead>PostgreSQL module component interactions and data flow.</div><p>The PGSQL module organizes PostgreSQL in production as <strong>clusters</strong>—<strong>logical entities</strong> composed of a group of database <strong>instances</strong> associated by <strong>primary-replica</strong> relationships.</p><hr><h2 id=overview>Overview</h2><p>The <a href=/docs/pgsql><strong>PGSQL module</strong></a> includes the following components, working together to provide production-grade PostgreSQL HA cluster services:</p><table class=full-width><thead><tr><th style=text-align:left>Component</th><th>Type</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><a href=#postgresql><strong><code>postgres</code></strong></a></td><td>Database</td><td style=text-align:left>The world&rsquo;s most advanced open-source relational database, PGSQL core</td></tr><tr><td style=text-align:left><a href=#patroni><strong><code>patroni</code></strong></a></td><td>HA</td><td style=text-align:left>Manages PostgreSQL, coordinates failover, leader election, config changes</td></tr><tr><td style=text-align:left><a href=#pgbouncer><strong><code>pgbouncer</code></strong></a></td><td>Pool</td><td style=text-align:left>Lightweight connection pooling middleware, reduces overhead, adds flexibility</td></tr><tr><td style=text-align:left><a href=#pgbackrest><strong><code>pgbackrest</code></strong></a></td><td>Backup</td><td style=text-align:left>Full/incremental backup and WAL archiving, supports local and object storage</td></tr><tr><td style=text-align:left><a href=#pg_exporter><strong><code>pg_exporter</code></strong></a></td><td>Metrics</td><td style=text-align:left>Exports PostgreSQL monitoring metrics for Prometheus scraping</td></tr><tr><td style=text-align:left><a href=#pgbouncer_exporter><strong><code>pgbouncer_exporter</code></strong></a></td><td>Metrics</td><td style=text-align:left>Exports Pgbouncer connection pool metrics</td></tr><tr><td style=text-align:left><a href=#pgbackrest_exporter><strong><code>pgbackrest_exporter</code></strong></a></td><td>Metrics</td><td style=text-align:left>Exports backup status metrics</td></tr><tr><td style=text-align:left><a href=#vip-manager><strong><code>vip-manager</code></strong></a></td><td>VIP</td><td style=text-align:left>Binds L2 VIP to current primary node for transparent failover [Optional]</td></tr></tbody></table><p>The <a href=#vip-manager><strong><code>vip-manager</code></strong></a> is an on-demand component. Additionally, PGSQL uses components from other modules:</p><table class=full-width><thead><tr><th>Component</th><th style=text-align:left>Module</th><th>Type</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td><a href=#haproxy><strong><code>haproxy</code></strong></a></td><td style=text-align:left><a href=/docs/node><strong>NODE</strong></a></td><td>LB</td><td style=text-align:left>Exposes service ports, routes traffic to primary or replicas</td></tr><tr><td><a href=#vector><strong><code>vector</code></strong></a></td><td style=text-align:left><a href=/docs/node><strong>NODE</strong></a></td><td>Logging</td><td style=text-align:left>Collects PostgreSQL, <a href=#patroni>Patroni</a>, <a href=#pgbouncer>Pgbouncer</a> logs and ships to center</td></tr><tr><td><a href=#etcd><strong><code>etcd</code></strong></a></td><td style=text-align:left><a href=/docs/etcd><strong>ETCD</strong></a></td><td>DCS</td><td style=text-align:left>Distributed consistent store for cluster metadata and leader info</td></tr></tbody></table><p>By analogy, the <a href=#postgresql>PostgreSQL</a> database kernel is the CPU, while the PGSQL module packages it as a complete computer.
<a href=#patroni>Patroni</a> and <a href=#etcd>Etcd</a> form the <a href=#ha-subsystem>HA subsystem</a>, <a href=#pgbackrest>pgBackRest</a> and MinIO form the <a href=#backup-subsystem>backup subsystem</a>.
<a href=#haproxy>HAProxy</a>, <a href=#pgbouncer>Pgbouncer</a>, and <a href=#vip-manager>vip-manager</a> form the <a href=#access-subsystem>access subsystem</a>.
Various Exporters and <a href=#vector>Vector</a> build the <a href=#observability-subsystem>observability subsystem</a>;
finally, you can swap different <a href=/docs/pgsql/kernel><strong>kernel CPUs</strong></a> and <a href=/docs/pgsql/ext><strong>extension cards</strong></a>.</p><p><img src=/img/pigsty/motherboard.gif alt></p><table class=full-width><thead><tr><th style=text-align:left>Subsystem</th><th style=text-align:left>Components</th><th style=text-align:left>Function</th></tr></thead><tbody><tr><td style=text-align:left><a href=#ha-subsystem><strong>HA Subsystem</strong></a></td><td style=text-align:left><a href=#patroni>Patroni</a> + <a href=#etcd>etcd</a></td><td style=text-align:left>Failure detection, auto-failover, config management</td></tr><tr><td style=text-align:left><a href=#access-subsystem><strong>Access Subsystem</strong></a></td><td style=text-align:left><a href=#haproxy>HAProxy</a> + <a href=#pgbouncer>Pgbouncer</a> + <a href=#vip-manager>vip-manager</a></td><td style=text-align:left>Service exposure, load balancing, pooling, VIP</td></tr><tr><td style=text-align:left><a href=#backup-subsystem><strong>Backup Subsystem</strong></a></td><td style=text-align:left><a href=#pgbackrest>pgBackRest</a> (+ MinIO)</td><td style=text-align:left>Full/incremental backup, WAL archiving, PITR</td></tr><tr><td style=text-align:left><a href=#observability-subsystem><strong>Observability Subsystem</strong></a></td><td style=text-align:left><a href=#pg_exporter>pg_exporter</a> / <a href=#pgbouncer_exporter>pgbouncer_exporter</a> / <a href=#pgbackrest_exporter>pgbackrest_exporter</a> + <a href=#vector>Vector</a></td><td style=text-align:left>Metrics collection, log aggregation</td></tr></tbody></table><hr><h2 id=component-interaction>Component Interaction</h2><p><a href=/docs/infra/><img src=/img/pigsty/arch.png alt=pigsty-arch></a></p><ul><li>Cluster DNS is resolved by <a href=/docs/concept/arch/infra#dnsmasq><strong>DNSMASQ</strong></a> on infra nodes</li><li>Cluster VIP is managed by <a href=#vip-manager><strong>vip-manager</strong></a>, which binds <a href=/docs/pgsql/param#pg_vip_address><code>pg_vip_address</code></a> to the cluster primary node.<ul><li><a href=#vip-manager>vip-manager</a> gets cluster leader info written by <a href=#patroni>patroni</a> from the <a href=#etcd>etcd</a> cluster</li></ul></li><li>Cluster services are exposed by <a href=#haproxy><strong>HAProxy</strong></a> on nodes, different services distinguished by node ports (543x).<ul><li>HAProxy port 9101: Monitoring metrics & statistics & admin page</li><li>HAProxy port 5433: Routes to primary <a href=#pgbouncer>pgbouncer</a>: <a href=/docs/pgsql/service/#primary-service>read-write service</a></li><li>HAProxy port 5434: Routes to replica <a href=#pgbouncer>pgbouncer</a>: <a href=/docs/pgsql/service/#replica-service>read-only service</a></li><li>HAProxy port 5436: Routes to primary <a href=#postgresql>postgres</a>: <a href=/docs/pgsql/service/#default-service>default service</a></li><li>HAProxy port 5438: Routes to offline <a href=#postgresql>postgres</a>: <a href=/docs/pgsql/service/#offline-service>offline service</a></li><li><a href=#haproxy>HAProxy</a> routes traffic based on health check info from <a href=#patroni>patroni</a>.</li></ul></li><li><a href=#pgbouncer><strong>Pgbouncer</strong></a> is connection pooling middleware, listening on port 6432 by default, buffering connections, exposing additional metrics, and providing extra flexibility.<ul><li><a href=#pgbouncer>Pgbouncer</a> is stateless and deployed 1:1 with <a href=#postgresql>Postgres</a> via local Unix socket.</li><li>Production traffic (primary/replica) goes through <a href=#pgbouncer>pgbouncer</a> by default (can specify bypass via <a href=/docs/pgsql/param#pg_default_service_dest><code>pg_default_service_dest</code></a>)</li><li>Default/offline services always bypass <a href=#pgbouncer>pgbouncer</a> and connect directly to target <a href=#postgresql>Postgres</a>.</li></ul></li><li><a href=#postgresql><strong>PostgreSQL</strong></a> listens on port 5432, providing relational database services<ul><li>Installing PGSQL module on multiple nodes with the same cluster name automatically forms an HA cluster via streaming replication</li><li><a href=#postgresql>PostgreSQL</a> process is managed by <a href=#patroni>patroni</a> by default.</li></ul></li><li><a href=#patroni><strong>Patroni</strong></a> listens on port 8008 by default, supervising <a href=#postgresql>PostgreSQL</a> server processes<ul><li><a href=#patroni>Patroni</a> starts <a href=#postgresql>Postgres</a> server as child process</li><li><a href=#patroni>Patroni</a> uses <a href=#etcd>etcd</a> as DCS: stores config, failure detection, and leader election.</li><li><a href=#patroni>Patroni</a> provides Postgres info (e.g., primary/replica) via health checks, <a href=#haproxy>HAProxy</a> uses this to distribute traffic</li></ul></li><li><a href=#pg_exporter><strong>pg_exporter</strong></a> exposes <a href=#postgresql>postgres</a> monitoring metrics on port 9630</li><li><a href=#pgbouncer_exporter><strong>pgbouncer_exporter</strong></a> exposes <a href=#pgbouncer>pgbouncer</a> metrics on port 9631</li><li><a href=#pgbackrest><strong>pgBackRest</strong></a> uses local backup repository by default (<code>pgbackrest_method</code> = <code>local</code>)<ul><li>If using <code>local</code> (default), <a href=#pgbackrest>pgBackRest</a> creates local repository under <a href=/docs/pgsql/param#pg_fs_bkup><code>pg_fs_bkup</code></a> on primary node</li><li>If using <code>minio</code>, <a href=#pgbackrest>pgBackRest</a> creates backup repository on dedicated MinIO cluster</li></ul></li><li><a href=#vector><strong>Vector</strong></a> collects Postgres-related logs (postgres, pgbouncer, patroni, pgbackrest)<ul><li><a href=#vector>vector</a> listens on port 9598, also exposes its own metrics to VictoriaMetrics on infra nodes</li><li><a href=#vector>vector</a> sends logs to VictoriaLogs on infra nodes</li></ul></li></ul><hr><h2 id=ha-subsystem>HA Subsystem</h2><p>The <a href=/docs/concept/ha><strong>HA</strong></a> subsystem consists of <a href=#patroni><strong>Patroni</strong></a> and <a href=#etcd><strong>etcd</strong></a>, responsible for PostgreSQL cluster failure detection, automatic failover, and configuration management.</p><p><strong>How it works</strong>: <a href=#patroni>Patroni</a> runs on each node, managing the local <a href=#postgresql>PostgreSQL</a> process and writing cluster state (leader, members, config) to <a href=#etcd>etcd</a>.
When the primary fails, <a href=#patroni>Patroni</a> coordinates election via <a href=#etcd>etcd</a>, promoting the healthiest replica to new primary. The entire process is automatic, with RTO typically under 45 seconds.</p><p><strong>Key Interactions</strong>:</p><ul><li><strong><a href=#postgresql>PostgreSQL</a></strong>: Starts, stops, reloads PG as parent process, controls its lifecycle</li><li><strong><a href=#etcd>etcd</a></strong>: External dependency, writes/watches leader key for distributed consensus and failure detection</li><li><strong><a href=#haproxy>HAProxy</a></strong>: Provides health checks via REST API (<code>:8008</code>), reporting instance role</li><li><strong><a href=#vip-manager>vip-manager</a></strong>: Watches leader key in <a href=#etcd>etcd</a>, auto-migrates VIP</li></ul><p>For more information, see: <a href=/docs/concept/ha/><strong>High Availability</strong></a> and <a href=/docs/pgsql/param/#pg_bootstrap><strong>Config: PGSQL - PG_BOOTSTRAP</strong></a></p><hr><h2 id=access-subsystem>Access Subsystem</h2><p>The access subsystem consists of <a href=#haproxy><strong>HAProxy</strong></a>, <a href=#pgbouncer><strong>Pgbouncer</strong></a>, and <a href=#vip-manager><strong>vip-manager</strong></a>, responsible for service exposure, traffic routing, and connection pooling.</p><p>There are multiple access methods. A typical traffic path is: <code>Client → DNS/VIP → HAProxy (543x) → Pgbouncer (6432) → PostgreSQL (5432)</code></p><table class=full-width><thead><tr><th style=text-align:left>Layer</th><th style=text-align:left>Component</th><th style=text-align:left>Port</th><th style=text-align:left>Role</th></tr></thead><tbody><tr><td style=text-align:left>L2 VIP</td><td style=text-align:left><a href=#vip-manager>vip-manager</a></td><td style=text-align:left>-</td><td style=text-align:left>Binds L2 VIP to primary (optional)</td></tr><tr><td style=text-align:left>L4 Load Bal</td><td style=text-align:left><a href=#haproxy>HAProxy</a></td><td style=text-align:left>543x</td><td style=text-align:left>Service exposure, load balancing, health checks</td></tr><tr><td style=text-align:left>L7 Pool</td><td style=text-align:left><a href=#pgbouncer>Pgbouncer</a></td><td style=text-align:left>6432</td><td style=text-align:left>Connection reuse, session management, transaction pooling</td></tr></tbody></table><p><strong>Service Ports</strong>:</p><ul><li><code>5433</code> primary: Read-write service, routes to primary <a href=#pgbouncer>Pgbouncer</a></li><li><code>5434</code> replica: Read-only service, routes to replica <a href=#pgbouncer>Pgbouncer</a></li><li><code>5436</code> default: Default service, direct to primary (bypasses pool)</li><li><code>5438</code> offline: Offline service, direct to offline replica (ETL/analytics)</li></ul><p><strong>Key Features</strong>:</p><ul><li><a href=#haproxy>HAProxy</a> uses <a href=#patroni>Patroni</a> REST API to determine instance role, auto-routes traffic</li><li><a href=#pgbouncer>Pgbouncer</a> uses transaction-level pooling, absorbs connection spikes, reduces PG connection overhead</li><li><a href=#vip-manager>vip-manager</a> watches <a href=#etcd>etcd</a> leader key, auto-migrates VIP during failover</li></ul><p>For more information, see: <a href=/docs/pgsql/service/><strong>Service Access</strong></a> and <a href=/docs/pgsql/param/#pg_access><strong>Config: PGSQL - PG_ACCESS</strong></a></p><hr><h2 id=backup-subsystem>Backup Subsystem</h2><p>The backup subsystem consists of <a href=#pgbackrest><strong>pgBackRest</strong></a> (optionally with <strong>MinIO</strong> as remote repository), responsible for data backup and point-in-time recovery (<a href=/docs/concept/pitr><strong>PITR</strong></a>).</p><p><strong>Backup Types</strong>:</p><ul><li><strong>Full backup</strong>: Complete database copy</li><li><strong>Incremental/differential backup</strong>: Only backs up changed data blocks</li><li><strong>WAL archiving</strong>: Continuous transaction log archiving, enables any point-in-time recovery</li></ul><p><strong>Storage Backends</strong>:</p><ul><li><code>local</code> (default): Local disk, backups stored at <a href=/docs/pgsql/param#pg_fs_bkup><code>pg_fs_bkup</code></a> mount point</li><li><code>minio</code>: S3-compatible object storage, supports centralized backup management and off-site DR</li></ul><p><strong>Key Interactions</strong>:</p><ul><li><strong><a href=#pgbackrest>pgBackRest</a> → <a href=#postgresql>PostgreSQL</a></strong>: Executes backup commands, manages WAL archiving</li><li><strong><a href=#pgbackrest>pgBackRest</a> → <a href=#patroni>Patroni</a></strong>: Recovery can bootstrap replicas as new primary or standby</li><li><strong><a href=#pgbackrest_exporter>pgbackrest_exporter</a> → Prometheus</strong>: Exports backup status metrics, monitors backup health</li></ul><p>For more information, see: <a href=/docs/concept/pitr/><strong>PITR</strong></a>, <a href=/docs/pgsql/backup/><strong>Backup & Recovery</strong></a>, and <a href=/docs/pgsql/param/#pg_backup><strong>Config: PGSQL - PG_BACKUP</strong></a></p><hr><h2 id=observability-subsystem>Observability Subsystem</h2><p>The observability subsystem consists of three <strong>Exporters</strong> and <a href=#vector><strong>Vector</strong></a>, responsible for metrics collection and log aggregation.</p><table><thead><tr><th style=text-align:left>Component</th><th style=text-align:left>Port</th><th style=text-align:left>Target</th><th style=text-align:left>Key Metrics</th></tr></thead><tbody><tr><td style=text-align:left><a href=#pg_exporter>pg_exporter</a></td><td style=text-align:left><code>9630</code></td><td style=text-align:left><a href=#postgresql>PostgreSQL</a></td><td style=text-align:left>Sessions, transactions, replication lag, buffer hits</td></tr><tr><td style=text-align:left><a href=#pgbouncer_exporter>pgbouncer_exporter</a></td><td style=text-align:left><code>9631</code></td><td style=text-align:left><a href=#pgbouncer>Pgbouncer</a></td><td style=text-align:left>Pool utilization, wait queue, hit rate</td></tr><tr><td style=text-align:left><a href=#pgbackrest_exporter>pgbackrest_exporter</a></td><td style=text-align:left><code>9854</code></td><td style=text-align:left><a href=#pgbackrest>pgBackRest</a></td><td style=text-align:left>Latest backup time, size, type</td></tr><tr><td style=text-align:left><a href=#vector>vector</a></td><td style=text-align:left><code>9598</code></td><td style=text-align:left><a href=#postgresql>postgres</a>/<a href=#patroni>patroni</a>/<a href=#pgbouncer>pgbouncer</a> logs</td><td style=text-align:left>Structured log stream</td></tr></tbody></table><p><strong>Data Flow</strong>:</p><ul><li><strong>Metrics</strong>: Exporter → VictoriaMetrics (INFRA) → Grafana dashboards</li><li><strong>Logs</strong>: <a href=#vector>Vector</a> → VictoriaLogs (INFRA) → Grafana log queries</li></ul><p><a href=#pg_exporter>pg_exporter</a> / <a href=#pgbouncer_exporter>pgbouncer_exporter</a> connect to target services via local Unix socket, decoupled from HA topology. In <a href=/docs/setup/slim><strong>slim install</strong></a> mode, these components can be disabled.</p><p>For more information, see: <a href=/docs/pgsql/param/#pg_monitor><strong>Config: PGSQL - PG_MONITOR</strong></a></p><hr><h2 id=postgresql>PostgreSQL</h2><p><a href=https://www.postgresql.org/><strong>PostgreSQL</strong></a> is the PGSQL module core, listening on port <code>5432</code> by default for relational database services, deployed 1:1 with <a href=/docs/node><strong>nodes</strong></a>.</p><p>Pigsty currently supports PostgreSQL 14-18 (lifecycle major versions), installed via binary packages from the <a href=/docs/repo/pgdg/><strong>PGDG official repo</strong></a>.
Pigsty also allows you to use other <a href=/docs/pgsql/kernel><strong>PG kernel forks</strong></a> to replace the default PostgreSQL kernel,
and install up to <a href=/docs/pgsql/ext><strong>440</strong></a> extension plugins on top of the PG kernel.</p><p><strong>PostgreSQL</strong> processes are managed by default by the <a href=/docs/concept/ha><strong>HA</strong></a> agent—<a href=#patroni><strong>Patroni</strong></a>.
When a cluster has only one node, that instance is the primary; when the cluster has multiple nodes, other instances automatically join as replicas:
through physical replication, syncing data changes from the primary in real-time. Replicas can handle read-only requests and automatically take over when the primary fails.</p><p><a href=/docs/concept/ha><img src=/img/pigsty/ha.png alt=pigsty-ha.png></a></p><p>You can access PostgreSQL directly, or through <a href=#haproxy>HAProxy</a> and <a href=#pgbouncer>Pgbouncer</a> connection pool.</p><p>For more information, see: <a href=/docs/pgsql/param/#pg_bootstrap><strong>Config: PGSQL - PG_BOOTSTRAP</strong></a></p><hr><h2 id=patroni>Patroni</h2><p><a href=https://patroni.readthedocs.io/><strong>Patroni</strong></a> is the PostgreSQL HA control component, listening on port <code>8008</code> by default.</p><p><strong>Patroni</strong> takes over <a href=#postgresql><strong>PostgreSQL</strong></a> startup, shutdown, configuration, and health status, writing leader and member information to <a href=#etcd><strong>etcd</strong></a>.
It handles automatic failover, maintains replication factor, coordinates parameter changes, and provides a REST API for <a href=#haproxy><strong>HAProxy</strong></a>, monitoring, and administrators.</p><p><a href=#haproxy><strong>HAProxy</strong></a> uses <strong>Patroni</strong> health check endpoints to determine instance roles and route traffic to the correct primary or replica.
<a href=#vip-manager><strong>vip-manager</strong></a> monitors the leader key in <a href=#etcd><strong>etcd</strong></a> and automatically migrates the VIP when the primary changes.</p><p><a href=/docs/concept/ha><img src=/img/dashboard/pgsql-patroni.webp alt=patroni></a></p><p>For more information, see: <a href=/docs/pgsql/param/#patroni_mode><strong>Config: PGSQL - PG_BOOTSTRAP</strong></a></p><hr><h2 id=pgbouncer>Pgbouncer</h2><p><a href=https://www.pgbouncer.org/><strong>Pgbouncer</strong></a> is a lightweight connection pooling middleware, listening on port <code>6432</code> by default, deployed 1:1 with <a href=#postgresql>PostgreSQL</a> database and node.</p><p><strong>Pgbouncer</strong> runs statelessly on each instance, connecting to <a href=#postgresql><strong>PostgreSQL</strong></a> via local Unix socket, using Transaction Pooling by default
for pool management, absorbing burst client connections, stabilizing database sessions, reducing lock contention, and significantly improving performance under high concurrency.</p><p>Pigsty routes production traffic (read-write service <code>5433</code> / read-only service <code>5434</code>) through <strong>Pgbouncer</strong> by default,
while only the default service (<code>5436</code>) and offline service (<code>5438</code>) bypass the pool for direct <a href=#postgresql><strong>PostgreSQL</strong></a> connections.</p><p>Pool mode is controlled by <a href=/docs/pgsql/param#pgbouncer_poolmode><code>pgbouncer_poolmode</code></a>, defaulting to <code>transaction</code> (transaction-level pooling).
Connection pooling can be disabled via <a href=/docs/pgsql/param#pgbouncer_enabled><code>pgbouncer_enabled</code></a>.</p><p><a href=https://demo.pigsty.io/ui/d/pgsql-pgbouncer><img src=/img/dashboard/pgsql-pgbouncer.webp alt=pgbouncer.png></a></p><p>For more information, see: <a href=/docs/pgsql/param/#pg_access><strong>Config: PGSQL - PG_ACCESS</strong></a></p><hr><h2 id=pgbackrest>pgBackRest</h2><p><a href=https://pgbackrest.org/><strong>pgBackRest</strong></a> is a professional PostgreSQL backup/recovery tool, one of the strongest in the PG ecosystem, supporting full/incremental/differential backup and WAL archiving.</p><p>Pigsty uses pgBackRest for PostgreSQL <a href=/docs/concept/pitr><strong>PITR</strong></a> capability,
allowing you to roll back clusters to any point within the backup retention window.</p><p><strong>pgBackRest</strong> works with <a href=#postgresql><strong>PostgreSQL</strong></a> to create backup repositories on the primary, executing backup and archive tasks.
By default, it uses local backup repository (<a href=/docs/pgsql/param#pgbackrest_method><strong><code>pgbackrest_method</code></strong></a> = <code>local</code>),
but can be configured for MinIO or other object storage for centralized backup management.</p><p>After initialization, <a href=/docs/pgsql/param#pgbackrest_init_backup><strong><code>pgbackrest_init_backup</code></strong></a> can automatically trigger the first full backup.
Recovery integrates with <a href=#patroni><strong>Patroni</strong></a>, supporting bootstrapping replicas as new primaries or standbys.</p><p><a href=/docs/concept/pitr><img src=/img/dashboard/pgsql-pitr.webp alt=pgbackrest></a></p><p>For more information, see: <a href=/docs/pgsql/backup/><strong>Backup & Recovery</strong></a> and <a href=/docs/pgsql/param/#pg_backup><strong>Config: PGSQL - PG_BACKUP</strong></a></p><hr><h2 id=haproxy>HAProxy</h2><p><a href=http://www.haproxy.org/><strong>HAProxy</strong></a> is the service entry point and load balancer, exposing multiple database service ports.</p><table><thead><tr><th style=text-align:left>Port</th><th style=text-align:left>Service</th><th style=text-align:left>Target</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><code>9101</code></td><td style=text-align:left>Admin</td><td style=text-align:left>-</td><td style=text-align:left>HAProxy statistics and admin page</td></tr><tr><td style=text-align:left><code>5433</code></td><td style=text-align:left>primary</td><td style=text-align:left>Primary <a href=#pgbouncer>Pgbouncer</a></td><td style=text-align:left>Read-write service, routes to primary pool</td></tr><tr><td style=text-align:left><code>5434</code></td><td style=text-align:left>replica</td><td style=text-align:left>Replica <a href=#pgbouncer>Pgbouncer</a></td><td style=text-align:left>Read-only service, routes to replica pool</td></tr><tr><td style=text-align:left><code>5436</code></td><td style=text-align:left>default</td><td style=text-align:left>Primary <a href=#postgresql>Postgres</a></td><td style=text-align:left>Default service, direct to primary (bypasses pool)</td></tr><tr><td style=text-align:left><code>5438</code></td><td style=text-align:left>offline</td><td style=text-align:left>Offline <a href=#postgresql>Postgres</a></td><td style=text-align:left>Offline service, direct to offline replica (ETL/analytics)</td></tr></tbody></table><p><strong>HAProxy</strong> uses <a href=#patroni><strong>Patroni</strong></a> REST API health checks to determine instance roles and route traffic to the appropriate primary or replica.
Service definitions are composed from <a href=/docs/pgsql/param#pg_default_services><strong><code>pg_default_services</code></strong></a> and <a href=/docs/pgsql/param#pg_services><strong><code>pg_services</code></strong></a>.</p><p>A dedicated HAProxy node group can be specified via <a href=/docs/pgsql/param#pg_service_provider><strong><code>pg_service_provider</code></strong></a> to handle higher traffic;
by default, <strong>HAProxy</strong> on local nodes publishes services.</p><p><a href=/docs/concept/ha/svc><img src=/img/dashboard/node-haproxy.webp alt=haproxy></a></p><p>For more information, see: <a href=/docs/pgsql/service/><strong>Service Access</strong></a> and <a href=/docs/pgsql/param/#pg_access><strong>Config: PGSQL - PG_ACCESS</strong></a></p><hr><h2 id=vip-manager>vip-manager</h2><p><a href=https://github.com/cybertec-postgresql/vip-manager><strong>vip-manager</strong></a> binds L2 VIP to the current primary node. This is an optional component; enable it if your network supports L2 VIP.</p><p><strong>vip-manager</strong> runs on each PG node, monitoring the leader key written by <a href=#patroni><strong>Patroni</strong></a> in <a href=#etcd><strong>etcd</strong></a>,
and binds <a href=/docs/pgsql/param#pg_vip_address><strong><code>pg_vip_address</code></strong></a> to the current primary node&rsquo;s network interface.
When cluster failover occurs, <strong>vip-manager</strong> immediately releases the VIP from the old primary and rebinds it on the new primary, switching traffic to the new primary.</p><p>This component is optional, enabled via <a href=/docs/pgsql/param#pg_vip_enabled><strong><code>pg_vip_enabled</code></strong></a>.
When enabled, ensure all nodes are in the same VLAN; otherwise, VIP migration will fail.
Public cloud networks typically don&rsquo;t support L2 VIP; it&rsquo;s recommended only for on-premises and private cloud environments.</p><p><a href=/docs/concept/ha/svc><img src=/img/dashboard/node-vip.webp alt=node-vip></a></p><p>For more information, see: <a href=/docs/pgsql/tutorial/pg-vip/><strong>Tutorial: VIP Configuration</strong></a> and <a href=/docs/pgsql/param/#pg_access><strong>Config: PGSQL - PG_ACCESS</strong></a></p><hr><h2 id=pg_exporter>pg_exporter</h2><p><a href=https://github.com/pgsty/pg_exporter><strong>pg_exporter</strong></a> exports <a href=#postgresql>PostgreSQL</a> monitoring metrics, listening on port <code>9630</code> by default.</p><p><strong>pg_exporter</strong> runs on each PG node, connecting to <a href=#postgresql><strong>PostgreSQL</strong></a> via local Unix socket,
exporting rich metrics covering sessions, buffer hits, replication lag, transaction rates, etc., scraped by <strong>VictoriaMetrics</strong> on INFRA nodes.</p><p>Collection configuration is specified by <a href=/docs/pgsql/param#pg_exporter_config><strong><code>pg_exporter_config</code></strong></a>,
with support for automatic database discovery (<a href=/docs/pgsql/param#pg_exporter_auto_discovery><strong><code>pg_exporter_auto_discovery</code></strong></a>),
and tiered cache strategies via <a href=/docs/pgsql/param#pg_exporter_cache_ttls><strong><code>pg_exporter_cache_ttls</code></strong></a>.</p><p>You can disable this component via parameters; in <a href=/docs/setup/slim><strong>slim install</strong></a>, this component is not enabled.</p><p><a href=https://demo.pigsty.io/ui/d/pgsql-exporter><img src=/img/dashboard/pgsql-exporter.webp alt=pg-exporter></a></p><p>For more information, see: <a href=/docs/pgsql/param/#pg_monitor><strong>Config: PGSQL - PG_MONITOR</strong></a></p><hr><h2 id=pgbouncer_exporter>pgbouncer_exporter</h2><p><strong>pgbouncer_exporter</strong> exports <a href=#pgbouncer>Pgbouncer</a> connection pool metrics, listening on port <code>9631</code> by default.</p><p><code>pgbouncer_exporter</code> uses the same <a href=#pg_exporter><strong>pg_exporter</strong></a> binary but with a dedicated metrics config file, supporting pgbouncer 1.8-1.25+.
<strong>pgbouncer_exporter</strong> reads <a href=#pgbouncer><strong>Pgbouncer</strong></a> statistics views, providing pool utilization, wait queue, and hit rate metrics.</p><p>If <a href=#pgbouncer><strong>Pgbouncer</strong></a> is disabled, this component is also disabled. In <a href=/docs/setup/slim><strong>slim install</strong></a>, this component is not enabled.</p><p>For more information, see: <a href=/docs/pgsql/param/#pg_monitor><strong>Config: PGSQL - PG_MONITOR</strong></a></p><hr><h2 id=pgbackrest_exporter>pgbackrest_exporter</h2><p><strong>pgbackrest_exporter</strong> exports backup status metrics, listening on port <code>9854</code> by default.</p><p><strong>pgbackrest_exporter</strong> parses <a href=#pgbackrest><strong>pgBackRest</strong></a> status, generating metrics for most recent backup time, size, type, etc. Combined with alerting policies, it quickly detects expired or failed backups, ensuring data safety.
Note that when there are many backups or using large network repositories, collection overhead can be significant, so <strong>pgbackrest_exporter</strong> has a default 2-minute collection interval.
In the worst case, you may see the latest backup status in the monitoring system 2 minutes after a backup completes.</p><p>For more information, see: <a href=/docs/pgsql/param/#pg_monitor><strong>Config: PGSQL - PG_MONITOR</strong></a></p><hr><h2 id=etcd>etcd</h2><p><a href=/docs/etcd><strong>etcd</strong></a> is a distributed consistent store (DCS), providing cluster metadata storage and leader election capability for <a href=#patroni><strong>Patroni</strong></a>.</p><p>etcd is deployed and managed by the independent <a href=/docs/etcd><strong>ETCD module</strong></a>, not part of the PGSQL module itself, but critical for PostgreSQL HA.
<a href=#patroni>Patroni</a> writes cluster state, leader info, and config parameters to etcd; all nodes reach consensus through etcd.
<a href=#vip-manager>vip-manager</a> also reads the leader key from etcd to enable automatic VIP migration.</p><p>For more information, see: <a href=/docs/etcd/><strong>ETCD Module</strong></a></p><hr><h2 id=vector>vector</h2><p><a href=https://vector.dev/><strong>Vector</strong></a> is a high-performance log collection component, deployed by the <a href=/docs/node><strong>NODE module</strong></a>, responsible for collecting PostgreSQL-related logs.</p><p>Vector runs on nodes, tracking <a href=#postgresql>PostgreSQL</a>, <a href=#pgbouncer>Pgbouncer</a>, <a href=#patroni>Patroni</a>, and <a href=#pgbackrest>pgBackRest</a> log directories,
sending structured logs to VictoriaLogs on INFRA nodes for centralized storage and querying.</p><p>For more information, see: <a href=/docs/node/><strong>NODE Module</strong></a></p></div></main></div></div><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class="td-footer__left col-6 col-sm-4 order-sm-1"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title="Email - Vonng" aria-label="Email - Vonng"><a target=_blank rel=noopener href=mailto:rh@vonng.com aria-label="Email - Vonng"><i class="fa fa-envelope"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title="GitHub - Vonng" aria-label="GitHub - Vonng"><a target=_blank rel=noopener href=https://github.com/Vonng aria-label="GitHub - Vonng"><i class="fab fa-github"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title="X - Vonng" aria-label="X - Vonng"><a target=_blank rel=noopener href=https://x.com/RonVonng aria-label="X - Vonng"><i class="fab fa-x-twitter"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title="X - Pigsty" aria-label="X - Pigsty"><a target=_blank rel=noopener href=https://x.com/PlGSTY aria-label="X - Pigsty"><i class="fab fa-twitter"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title="LinkedIn - Vonng" aria-label="LinkedIn - Vonng"><a target=_blank rel=noopener href=https://www.linkedin.com/in/vonng/ aria-label="LinkedIn - Vonng"><i class="fab fa-linkedin"></i></a></li></ul></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=Discord aria-label=Discord><a target=_blank rel=noopener href=https://discord.gg/wDzt5VyWEz aria-label=Discord><i class="fab fa-discord"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=Telegram aria-label=Telegram><a target=_blank rel=noopener href=https://t.me/joinchat/gV9zfZraNPM3YjFh aria-label=Telegram><i class="fab fa-telegram"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=WeChat aria-label=WeChat><a target=_blank rel=noopener href=/img/pigsty/pigsty-cc.jpg aria-label=WeChat><i class="fab fa-weixin"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=Discussion aria-label=Discussion><a target=_blank rel=noopener href=https://github.com/orgs/pgsty/discussions aria-label=Discussion><i class="fab fa-discourse"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=Repo aria-label=Repo><a target=_blank rel=noopener href=https://github.com/pgsty/pigsty aria-label=Repo><i class="fab fa-github"></i></a></li></ul></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"><span class=td-footer__copyright>&copy;
2018&ndash;2026
<span class=td-footer__authors>Ruohang Feng</span></span><span class=td-footer__all_rights_reserved>All Rights Reserved</span><span class=ms-2><a href=/docs/about/privacy target=_blank rel=noopener>Privacy Policy</a></span></div></div></div></footer></div><script src=/js/main.min.d20e761d6aa4d2ace0488e45da0e775a8b17300a8430f32fbcfa016e6c9e6eb6.js integrity="sha256-0g52HWqk0qzgSI5F2g53WosXMAqEMPMvvPoBbmyebrY=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script></body></html>