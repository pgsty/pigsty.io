[{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/select/","tags":"","title":"Featured"},{"body":" Main dashboard for a PGSQL cluster: Demo\nPGSQL Cluster is the main dashboard for a single PostgreSQL cluster, providing cluster-level core metrics overview.\n","categories":"","description":"Main dashboard for a PGSQL cluster","excerpt":"Main dashboard for a PGSQL cluster","ref":"/docs/pgsql/dashboard/cluster/pgsql-cluster/","tags":"","title":"PGSQL Cluster"},{"body":" Main dashboard for a single PGSQL database: Demo\nPGSQL Database is the main dashboard for a single PostgreSQL database, providing comprehensive database-level metrics.\n","categories":"","description":"Main dashboard for a single PGSQL database","excerpt":"Main dashboard for a single PGSQL database","ref":"/docs/pgsql/dashboard/database/pgsql-database/","tags":"","title":"PGSQL Database"},{"body":" Main dashboard for a single PGSQL instance: Demo\nPGSQL Instance is the main dashboard for a single PostgreSQL instance, providing comprehensive instance-level metrics.\n","categories":"","description":"Main dashboard for a single PGSQL instance","excerpt":"Main dashboard for a single PGSQL instance","ref":"/docs/pgsql/dashboard/instance/pgsql-instance/","tags":"","title":"PGSQL Instance"},{"body":" Main dashboard for the PGSQL module: Demo\nPGSQL Overview is the main dashboard for the PostgreSQL module, providing a global overview of the entire PGSQL module.\n","categories":"","description":"Main dashboard for the PGSQL module","excerpt":"Main dashboard for the PGSQL module","ref":"/docs/pgsql/dashboard/overview/pgsql-overview/","tags":"","title":"PGSQL Overview"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/_div_pigsty/","tags":"","title":"PIGSTY"},{"body":" Database info from database catalog: Demo\nPGCAT Database shows database-level information retrieved directly from PostgreSQL system catalog.\n","categories":"","description":"Database info retrieved directly from database catalog","excerpt":"Database info retrieved directly from database catalog","ref":"/docs/pgsql/dashboard/database/pgcat-database/","tags":"","title":"PGCAT Database"},{"body":" RDS version of PGSQL Cluster: Demo\nPGRDS Cluster is the RDS version of PGSQL Cluster, focusing on PostgreSQL-native metrics without host-level metrics.\n","categories":"","description":"RDS version of PGSQL Cluster focusing on PostgreSQL-native metrics","excerpt":"RDS version of PGSQL Cluster focusing on PostgreSQL-native metrics","ref":"/docs/pgsql/dashboard/cluster/pgrds-cluster/","tags":"","title":"PGRDS Cluster"},{"body":" RDS version of PGSQL Instance: Demo\nPGRDS Instance is the RDS version of PGSQL Instance, focusing on PostgreSQL-native metrics without host-level metrics.\n","categories":"","description":"RDS version of PGSQL Instance focusing on PostgreSQL-native metrics","excerpt":"RDS version of PGSQL Instance focusing on PostgreSQL-native metrics","ref":"/docs/pgsql/dashboard/instance/pgrds-instance/","tags":"","title":"PGRDS Instance"},{"body":" Global key metrics and alert events for PGSQL: Demo\nPGSQL Alert provides a global overview of core metrics and alert events for PostgreSQL clusters.\n","categories":"","description":"Global key metrics and alert events for PGSQL","excerpt":"Global key metrics and alert events for PGSQL","ref":"/docs/pgsql/dashboard/overview/pgsql-alert/","tags":"","title":"PGSQL Alert"},{"body":" Instance info from database catalog: Demo\nPGCAT Instance shows instance-level information retrieved directly from PostgreSQL system catalog.\n","categories":"","description":"Instance info retrieved directly from database catalog","excerpt":"Instance info retrieved directly from database catalog","ref":"/docs/pgsql/dashboard/instance/pgcat-instance/","tags":"","title":"PGCAT Instance"},{"body":" Session/load/QPS/TPS/locks for PGSQL cluster: Demo\nPGSQL Activity focuses on session activity, load, QPS, TPS, and lock status for a PostgreSQL cluster.\n","categories":"","description":"Session/load/QPS/TPS/locks for PGSQL cluster","excerpt":"Session/load/QPS/TPS/locks for PGSQL cluster","ref":"/docs/pgsql/dashboard/cluster/pgsql-activity/","tags":"","title":"PGSQL Activity"},{"body":" Overview of horizontally sharded PGSQL clusters: Demo\nPGSQL Shard provides cross-shard metric comparison for horizontally sharded PGSQL clusters such as CITUS or GPSQL.\n","categories":"","description":"Overview of horizontally sharded PGSQL clusters","excerpt":"Overview of horizontally sharded PGSQL clusters","ref":"/docs/pgsql/dashboard/overview/pgsql-shard/","tags":"","title":"PGSQL Shard"},{"body":" Table/index access metrics: Demo\nPGSQL Tables shows table and index access metrics for all objects within a single PostgreSQL database.\n","categories":"","description":"Table/index access metrics within a single database","excerpt":"Table/index access metrics within a single database","ref":"/docs/pgsql/dashboard/database/pgsql-tables/","tags":"","title":"PGSQL Tables"},{"body":" Persistence metrics for PGSQL instance: Demo\nPGSQL Persist focuses on persistence-related metrics: WAL generation, XID consumption, checkpoints, archiving, and I/O patterns.\n","categories":"","description":"Persistence metrics - WAL, XID, checkpoint, archive, IO","excerpt":"Persistence metrics - WAL, XID, checkpoint, archive, IO","ref":"/docs/pgsql/dashboard/instance/pgsql-persist/","tags":"","title":"PGSQL Persist"},{"body":" Replication, slots, and pub/sub for PGSQL cluster: Demo\nPGSQL Replication focuses on replication status, replication slots, and logical replication (pub/sub) for a PostgreSQL cluster.\n","categories":"","description":"Replication, slots, and pub/sub for PGSQL cluster","excerpt":"Replication, slots, and pub/sub for PGSQL cluster","ref":"/docs/pgsql/dashboard/cluster/pgsql-replication/","tags":"","title":"PGSQL Replication"},{"body":" Detailed info for a single table: Demo\nPGSQL Table shows detailed metrics for a single table including QPS, response time, index usage, and sequence info.\n","categories":"","description":"Detailed info for a single table (QPS/RT/index/sequence)","excerpt":"Detailed info for a single table (QPS/RT/index/sequence)","ref":"/docs/pgsql/dashboard/database/pgsql-table/","tags":"","title":"PGSQL Table"},{"body":" Detailed table info from catalog: Demo\nPGCAT Table shows detailed table information from database catalog including statistics and bloat analysis.\n","categories":"","description":"Detailed table info from database catalog","excerpt":"Detailed table info from database catalog","ref":"/docs/pgsql/dashboard/database/pgcat-table/","tags":"","title":"PGCAT Table"},{"body":" Detailed metrics for HAProxy: Demo\nPGSQL Proxy shows detailed metrics for a single HAProxy load balancer instance serving PostgreSQL traffic.\n","categories":"","description":"Detailed metrics for a single HAProxy load balancer","excerpt":"Detailed metrics for a single HAProxy load balancer","ref":"/docs/pgsql/dashboard/instance/pgsql-proxy/","tags":"","title":"PGSQL Proxy"},{"body":" Service, proxy, routing, and load balancing for PGSQL cluster: Demo\nPGSQL Service focuses on service endpoints, proxy routing, and load balancing status for a PostgreSQL cluster.\n","categories":"","description":"Service, proxy, routing, and load balancing for PGSQL cluster","excerpt":"Service, proxy, routing, and load balancing for PGSQL cluster","ref":"/docs/pgsql/dashboard/cluster/pgsql-service/","tags":"","title":"PGSQL Service"},{"body":" Database CRUD, slow queries, and table statistics: Demo\nPGSQL Databases focuses on database-level CRUD operations, slow queries, and table statistics across all instances in a cluster.\n","categories":"","description":"Database CRUD, slow queries, and table statistics across all instances","excerpt":"Database CRUD, slow queries, and table statistics across all instances","ref":"/docs/pgsql/dashboard/cluster/pgsql-databases/","tags":"","title":"PGSQL Databases"},{"body":" Metrics overview for Pgbouncer: Demo\nPGSQL Pgbouncer shows connection pooling metrics for a single Pgbouncer instance.\n","categories":"","description":"Metrics overview for a single Pgbouncer connection pooler","excerpt":"Metrics overview for a single Pgbouncer connection pooler","ref":"/docs/pgsql/dashboard/instance/pgsql-pgbouncer/","tags":"","title":"PGSQL Pgbouncer"},{"body":" Detailed info for a query type: Demo\nPGSQL Query shows detailed metrics for a specific query type including QPS and response time distribution.\n","categories":"","description":"Detailed info for a query type (QPS/RT)","excerpt":"Detailed info for a query type (QPS/RT)","ref":"/docs/pgsql/dashboard/database/pgsql-query/","tags":"","title":"PGSQL Query"},{"body":" Query details from database catalog: Demo\nPGCAT Query shows query details from database catalog including SQL text and execution statistics.\n","categories":"","description":"Query details from database catalog","excerpt":"Query details from database catalog","ref":"/docs/pgsql/dashboard/database/pgcat-query/","tags":"","title":"PGCAT Query"},{"body":" HA status and Patroni component status: Demo\nPGSQL Patroni focuses on high-availability status and Patroni component health for a PostgreSQL cluster.\n","categories":"","description":"HA status and Patroni component status for cluster","excerpt":"HA status and Patroni component status for cluster","ref":"/docs/pgsql/dashboard/cluster/pgsql-patroni/","tags":"","title":"PGSQL Patroni"},{"body":" Session and active/idle time metrics: Demo\nPGSQL Session focuses on session statistics and active/idle time distribution for a single PostgreSQL instance.\n","categories":"","description":"Session and active/idle time metrics for a single instance","excerpt":"Session and active/idle time metrics for a single instance","ref":"/docs/pgsql/dashboard/instance/pgsql-session/","tags":"","title":"PGSQL Session"},{"body":" Activity and lock wait info: Demo\nPGCAT Locks shows active sessions and lock wait information from database catalog.\n","categories":"","description":"Activity and lock wait info from database catalog","excerpt":"Activity and lock wait info from database catalog","ref":"/docs/pgsql/dashboard/database/pgcat-locks/","tags":"","title":"PGCAT Locks"},{"body":" PITR context for point-in-time recovery: Demo\nPGSQL PITR provides context information for point-in-time recovery operations, showing backup status and WAL timeline.\n","categories":"","description":"PITR context for point-in-time recovery assistance","excerpt":"PITR context for point-in-time recovery assistance","ref":"/docs/pgsql/dashboard/cluster/pgsql-pitr/","tags":"","title":"PGSQL PITR"},{"body":" Transaction, lock, TPS/QPS metrics: Demo\nPGSQL Xacts focuses on transaction processing, lock activity, and TPS/QPS metrics for a single PostgreSQL instance.\n","categories":"","description":"Transaction, lock, TPS/QPS related metrics","excerpt":"Transaction, lock, TPS/QPS related metrics","ref":"/docs/pgsql/dashboard/instance/pgsql-xacts/","tags":"","title":"PGSQL Xacts"},{"body":" Schema info from database catalog: Demo\nPGCAT Schema shows schema-level information from database catalog including tables, indexes, and sequences.\n","categories":"","description":"Schema info from database catalog","excerpt":"Schema info from database catalog","ref":"/docs/pgsql/dashboard/database/pgcat-schema/","tags":"","title":"PGCAT Schema"},{"body":" Self-monitoring metrics for exporters: Demo\nPGSQL Exporter shows self-monitoring metrics for the Postgres exporter and Pgbouncer exporter components.\n","categories":"","description":"Self-monitoring metrics for Postgres and Pgbouncer exporters","excerpt":"Self-monitoring metrics for Postgres and Pgbouncer exporters","ref":"/docs/pgsql/dashboard/instance/pgsql-exporter/","tags":"","title":"PGSQL Exporter"},{"body":"Quick Reference Action Command Description Create Cluster bin/pgsql-add \u003ccls\u003e Create a new PostgreSQL cluster Expand Cluster bin/pgsql-add \u003ccls\u003e \u003cip...\u003e Add replica to existing cluster Shrink Cluster bin/pgsql-rm \u003ccls\u003e \u003cip...\u003e Remove instance from cluster Remove Cluster bin/pgsql-rm \u003ccls\u003e Destroy entire PostgreSQL cluster Reload Service bin/pgsql-svc \u003ccls\u003e [ip...] Reload cluster load balancer config Reload HBA bin/pgsql-hba \u003ccls\u003e [ip...] Reload cluster HBA access rules Clone Cluster - Clone via standby cluster or PITR For other management tasks, see: HA Management, Manage Users, Manage Databases.\nCreate Cluster To create a new PostgreSQL cluster, first define the cluster in the inventory, then add nodes and initialize:\nScript Playbook Example bin/node-add \u003ccls\u003e # Add nodes in group \u003ccls\u003e ./node.yml -l \u003ccls\u003e # Use Ansible playbook to add nodes in group \u003ccls\u003e bin/pgsql-add pg-test # Add nodes in pg-test group, runs ./node.yml -l pg-test On managed nodes, create the cluster with: (Execute pgsql.yml playbook on \u003ccls\u003e group)\nScript Playbook Example bin/pgsql-add \u003ccls\u003e # Create PostgreSQL cluster \u003ccls\u003e ./pgsql.yml -l \u003ccls\u003e # Use Ansible playbook to create PostgreSQL cluster \u003ccls\u003e bin/pgsql-add pg-test # Create pg-test cluster Example: Create 3-node PG cluster pg-test\nRisk: Re-running create on existing cluster If you re-run create on an existing cluster, Pigsty won’t remove existing data files, but service configs will be overwritten and the cluster will restart! Additionally, if you specified a baseline SQL in database definition, it will re-execute - if it contains delete/overwrite logic, data loss may occur.\nExpand Cluster To add a new replica to an existing PostgreSQL cluster, add the instance definition to inventory: all.children.\u003ccls\u003e.hosts.\npg-test: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } # existing member 10.10.10.12: { pg_seq: 2, pg_role: replica } # existing member 10.10.10.13: { pg_seq: 3, pg_role: replica } # \u003c--- new member vars: { pg_cluster: pg-test } Scaling out is similar to creating a cluster. First add the new node to Pigsty: Add Node:\nScript Playbook Example bin/node-add \u003cip\u003e # Add node with IP \u003cip\u003e ./node.yml -l \u003cip\u003e # Use Ansible playbook to add node \u003cip\u003e bin/node-add 10.10.10.13 # Add node 10.10.10.13, runs ./node.yml -l 10.10.10.13 Then run the following on the new node to scale out (Install PGSQL module on new node with same pg_cluster):\nScript Playbook Example bin/pgsql-add \u003ccls\u003e \u003cip\u003e # Add node \u003cip\u003e to cluster ./pgsql.yml -l \u003cip\u003e # Core: Use Ansible playbook to install PGSQL module on \u003cip\u003e bin/pgsql-add pg-test 10.10.10.13 # Scale out pg-test with node 10.10.10.13 After scaling, you should Reload Service to add the new member to load balancer.\nExample: Add replica 10.10.10.13 to 2-node cluster pg-test\nShrink Cluster To remove a replica from an existing PostgreSQL cluster, remove the instance definition from inventory all.children.\u003ccls\u003e.hosts.\nFirst uninstall PGSQL module from target node (Execute pgsql-rm.yml on \u003cip\u003e):\nScript Playbook Example bin/pgsql-rm \u003ccls\u003e \u003cip\u003e # Remove PostgreSQL instance on \u003cip\u003e from cluster \u003ccls\u003e ./pgsql-rm.yml -l \u003cip\u003e # Use Ansible playbook to remove PostgreSQL instance on \u003cip\u003e bin/pgsql-rm pg-test 10.10.10.13 # Remove 10.10.10.13 from pg-test cluster After removing PGSQL module, optionally remove the node from Pigsty: Remove Node:\nScript Playbook Example bin/node-rm \u003cip\u003e # Remove node \u003cip\u003e from Pigsty management ./node-rm.yml -l \u003cip\u003e # Use Ansible playbook to remove node \u003cip\u003e bin/node-rm 10.10.10.13 # Remove node 10.10.10.13 from Pigsty After scaling in, remove the instance from inventory, then Reload Service to remove it from load balancer.\npg-test: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } 10.10.10.12: { pg_seq: 2, pg_role: replica } 10.10.10.13: { pg_seq: 3, pg_role: replica } # \u003c--- remove after execution vars: { pg_cluster: pg-test } Example: Remove replica 10.10.10.13 from 3-node cluster pg-test\nRemove Cluster To destroy a cluster, uninstall PGSQL module from all nodes (Execute pgsql-rm.yml on \u003ccls\u003e):\nScript Playbook Example bin/pgsql-rm \u003ccls\u003e # Destroy entire PostgreSQL cluster \u003ccls\u003e ./pgsql-rm.yml -l \u003ccls\u003e # Use Ansible playbook to destroy cluster \u003ccls\u003e bin/pgsql-rm pg-test # Destroy pg-test cluster After destroying PGSQL, optionally remove all nodes from Pigsty: Remove Node (optional if other services exist):\nScript Playbook Example bin/node-rm \u003ccls\u003e # Remove all nodes in group \u003ccls\u003e from Pigsty ./node-rm.yml -l \u003ccls\u003e # Use Ansible playbook to remove nodes in group \u003ccls\u003e bin/node-rm pg-test # Remove all pg-test nodes from Pigsty After removal, delete the entire cluster definition from inventory.\npg-test: # remove this cluster definition group hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } 10.10.10.12: { pg_seq: 2, pg_role: replica } 10.10.10.13: { pg_seq: 3, pg_role: replica } vars: { pg_cluster: pg-test } Example: Destroy 3-node PG cluster pg-test\nNote: If pg_safeguard is configured (or globally true), pgsql-rm.yml will abort to prevent accidental removal. Override with playbook command line to force removal. By default, cluster backup repo is deleted with the cluster. To preserve backups (e.g., with centralized repo), set pg_rm_backup=false:\n./pgsql-rm.yml -l pg-meta -e pg_safeguard=false # force remove protected cluster pg-meta ./pgsql-rm.yml -l pg-meta -e pg_rm_backup=false # preserve backup repo during removal Reload Service PostgreSQL clusters expose services via HAProxy on host nodes. When service definitions change, instance weights change, or cluster membership changes (e.g., scale out/scale in, switchover/failover), reload services to update load balancer config.\nTo reload service config on entire cluster or specific instances (Execute pg_service subtask of pgsql.yml on \u003ccls\u003e or \u003cip\u003e):\nScript Playbook Example bin/pgsql-svc \u003ccls\u003e # Reload service config for entire cluster \u003ccls\u003e bin/pgsql-svc \u003ccls\u003e \u003cip...\u003e # Reload service config for specific instances ./pgsql.yml -l \u003ccls\u003e -t pg_service -e pg_reload=true # Reload entire cluster ./pgsql.yml -l \u003cip\u003e -t pg_service -e pg_reload=true # Reload specific instance bin/pgsql-svc pg-test # Reload pg-test cluster service config bin/pgsql-svc pg-test 10.10.10.13 # Reload pg-test 10.10.10.13 instance service config Note: If using dedicated load balancer cluster (pg_service_provider), only reloading cluster primary updates the LB config.\nExample: Reload pg-test cluster service config\nExample: Reload PG Service to Remove Instance Reload HBA When HBA configs change, reload HBA rules to apply. (pg_hba_rules / pgb_hba_rules) If you have role-specific HBA rules or IP ranges referencing cluster member aliases, reload HBA after switchover/scaling.\nTo reload PG and Pgbouncer HBA rules on entire cluster or specific instances (Execute HBA subtasks of pgsql.yml on \u003ccls\u003e or \u003cip\u003e):\nScript Playbook Example bin/pgsql-hba \u003ccls\u003e # Reload HBA rules for entire cluster \u003ccls\u003e bin/pgsql-hba \u003ccls\u003e \u003cip...\u003e # Reload HBA rules for specific instances ./pgsql.yml -l \u003ccls\u003e -t pg_hba,pg_reload,pgbouncer_hba,pgbouncer_reload -e pg_reload=true # Reload entire cluster ./pgsql.yml -l \u003cip\u003e -t pg_hba,pg_reload,pgbouncer_hba,pgbouncer_reload -e pg_reload=true # Reload specific instance bin/pgsql-hba pg-test # Reload pg-test cluster HBA rules bin/pgsql-hba pg-test 10.10.10.13 # Reload pg-test 10.10.10.13 instance HBA rules Example: Reload pg-test cluster HBA rules\nConfig Cluster PostgreSQL config params are managed by Patroni. Initial params are specified by Patroni config template. After cluster init, config is stored in Etcd, dynamically managed and synced by Patroni. Most Patroni config params can be modified via patronictl. Other params (e.g., etcd DCS config, log/RestAPI config) can be updated via subtasks. For example, when etcd cluster membership changes, refresh Patroni config:\n./pgsql.yml -l pg-test -t pg_conf # Update Patroni config file ansible pg-test -b -a 'systemctl reload patroni' # Reload Patroni service You can override Patroni-managed defaults at different levels: specify params per instance, specify params per user, or specify params per database.\nClone Cluster Two ways to clone a cluster: use Standby Cluster, or use Point-in-Time Recovery. The former is simple with no dependencies but only clones latest state; the latter requires centralized backup repository (e.g., MinIO) but can clone to any point within retention period.\nMethod Pros Cons Use Cases Standby Cluster Simple, no dependencies Only clones latest state DR, read-write separation, migration PITR Recover to any point Requires centralized backup Undo mistakes, data audit Clone via Standby Cluster Standby Cluster continuously syncs from upstream cluster via streaming replication - the simplest cloning method. Specify pg_upstream on the new cluster primary to auto-pull data from upstream.\n# pg-test is the original cluster pg-test: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } vars: { pg_cluster: pg-test } # pg-test2 is standby cluster (clone) of pg-test pg-test2: hosts: 10.10.10.12: { pg_seq: 1, pg_role: primary, pg_upstream: 10.10.10.11 } # specify upstream 10.10.10.13: { pg_seq: 2, pg_role: replica } vars: { pg_cluster: pg-test2 } Create standby cluster with:\nScript Playbook bin/pgsql-add pg-test2 # Create standby cluster, auto-clone from upstream pg-test ./pgsql.yml -l pg-test2 # Use Ansible playbook to create standby cluster Standby cluster follows upstream, keeping data in sync. Promote to independent cluster anytime:\nExample: Promote Standby to Independent Cluster Via Config Cluster, remove standby_cluster config to promote:\n$ pg edit-config pg-test2 -standby_cluster: - create_replica_methods: - - basebackup - host: 10.10.10.11 - port: 5432 Apply these changes? [y/N]: y After promotion, pg-test2 becomes independent cluster accepting writes, forked from pg-test.\nExample: Change Replication Upstream If upstream cluster switchover occurs, change standby cluster upstream via Config Cluster:\n$ pg edit-config pg-test2 standby_cluster: create_replica_methods: - basebackup - host: 10.10.10.11 # \u003c--- old upstream + host: 10.10.10.14 # \u003c--- new upstream port: 5432 Apply these changes? [y/N]: y Clone via PITR Point-in-Time Recovery (PITR) allows recovery to any point within backup retention. Requires centralized backup repository (MinIO/S3), but more powerful.\nTo clone via PITR, add pg_pitr param specifying recovery target:\n# Clone new cluster pg-meta2 from pg-meta backup pg-meta2: hosts: { 10.10.10.12: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta2 pg_pitr: cluster: pg-meta # Recover from pg-meta backup time: '2025-01-10 10:00:00+00' # Recover to specific time Execute clone with pgsql-pitr.yml playbook:\nPlaybook CLI ./pgsql-pitr.yml -l pg-meta2 # Clone pg-meta2 from pg-meta backup # Specify PITR options via command line ./pgsql-pitr.yml -l pg-meta2 -e '{\"pg_pitr\": {\"cluster\": \"pg-meta\", \"time\": \"2025-01-10 10:00:00+00\"}}' PITR supports multiple recovery target types:\nTarget Type Example Description Time time: \"2025-01-10 10:00:00+00\" Recover to specific timestamp XID xid: \"250000\" Recover to before/after txn Name name: \"before_migration\" Recover to named restore point LSN lsn: \"0/4001C80\" Recover to specific WAL pos Latest type: \"latest\" Recover to end of WAL archive Post-PITR Processing Recovered cluster has archive_mode disabled to prevent accidental WAL overwrites. If recovered data is correct, enable archiving and perform new full backup:\npsql -c 'ALTER SYSTEM RESET archive_mode; SELECT pg_reload_conf();' pg-backup full # Execute new full backup For detailed PITR usage, see Restore Operations documentation.\n","categories":["Task"],"description":"Create/destroy PostgreSQL clusters, scale existing clusters, and clone clusters.","excerpt":"Create/destroy PostgreSQL clusters, scale existing clusters, and clone …","ref":"/docs/pgsql/admin/cluster/","tags":"","title":"Managing PostgreSQL Clusters"},{"body":"“PostgreSQL In Great STYle”: Postgres, Infras, Graphics, Service, Toolbox, it’s all Yours.\n—— Battery-Included, Local-First PostgreSQL Distribution as an Free \u0026 Open-Source RDS\nGtiHub | Demo | Blog | Discuss | Discord | DeepWiki | Roadmap | 中文文档\nGet Started with the latest release: curl -fsSL https://repo.pigsty.io/get | bash\nAbout: Features | History | Event | Community | Privacy Policy | License | Sponsor | Subscription\nSetup: Install | Offline Install | Preparation | Configuration | Playbook | Provision | Security | FAQ\nConcept: Architecture | Cluster Model | Monitoring | IaC | HA | PITR | Service Access | Security\nReference: Supported Linux | Fire Hierarchy | Parameters | Playbooks | Ports | Comparison | Cost\nModules: PGSQL | INFRA | NODE | ETCD | MINIO | REDIS | FERRET | DOCKER | APP\n","categories":["Reference"],"description":"","excerpt":"“PostgreSQL In Great STYle”: Postgres, Infras, Graphics, Service, …","ref":"/docs/","tags":"","title":"Pigsty Docs v4.0"},{"body":" RTO Timeline Failure Model Phase Best Worst Average Description Lease Expiration ttl - loop ttl ttl - loop/2 Best: crash just before refreshWorst: crash right after refresh Replica Detect 0 loop loop / 2 Best: exactly at check pointWorst: just missed check point Election Promote 0 2 1 Best: direct lock and promoteWorst: API timeout + Promote HAProxy Check (rise-1) × fastinter (rise-1) × fastinter + inter (rise-1) × fastinter + inter/2 Best: state change before checkWorst: state change right after check Key Difference Between Passive and Active Failover:\nScenario Patroni Status Lease Handling Primary Wait Time Active Failover (PG crash) Alive, healthy Actively tries to restart PG, releases lease on timeout primary_start_timeout Passive Failover (Node crash) Dies with node Cannot actively release, must wait for TTL expiration ttl In passive failover scenarios, Patroni dies along with the node and cannot actively release the Leader Key. The lease in DCS can only trigger cluster election after TTL naturally expires.\nTimeline Analysis Phase 1: Lease Expiration The Patroni primary refreshes the Leader Key every loop_wait cycle, resetting TTL to the configured value.\nTimeline: t-loop t t+ttl-loop t+ttl | | | | Last Refresh Failure Best Case Worst Case |←── loop ──→| | | |←──────────── ttl ─────────────────────→| Best case: Failure occurs just before lease refresh (elapsed loop since last refresh), remaining TTL = ttl - loop Worst case: Failure occurs right after lease refresh, must wait full ttl Average case: ttl - loop/2 Texpire={ttl−loopBestttl−loop/2AveragettlWorstT_{expire} = \\begin{cases} ttl - loop \u0026 \\text{Best} \\\\ ttl - loop/2 \u0026 \\text{Average} \\\\ ttl \u0026 \\text{Worst} \\end{cases}Texpire​=⎩⎨⎧​ttl−loopttl−loop/2ttl​BestAverageWorst​Phase 2: Replica Detection Replicas wake up on loop_wait cycles and check the Leader Key status in DCS.\nTimeline: Lease Expired Replica Wakes | | |←── 0~loop ─→| Best case: Replica happens to wake when lease expires, wait 0 Worst case: Replica just entered sleep when lease expires, wait loop Average case: loop/2 Tdetect={0Bestloop/2AverageloopWorstT_{detect} = \\begin{cases} 0 \u0026 \\text{Best} \\\\ loop/2 \u0026 \\text{Average} \\\\ loop \u0026 \\text{Worst} \\end{cases}Tdetect​=⎩⎨⎧​0loop/2loop​BestAverageWorst​Phase 3: Lock Contest \u0026 Promote When replicas detect Leader Key expiration, they start the election process. The replica that acquires the Leader Key executes pg_ctl promote to become the new primary.\nVia REST API, parallel queries to check each replica’s replication position, typically 10ms, hardcoded 2s timeout. Compare WAL positions to determine the best candidate, replicas attempt to create Leader Key (CAS atomic operation) Execute pg_ctl promote to become primary (very fast, typically negligible) Election Flow: ReplicaA ──→ Query replication position ──→ Compare ──→ Contest lock ──→ Success ReplicaB ──→ Query replication position ──→ Compare ──→ Contest lock ──→ Fail Best case: Single replica or immediate lock acquisition and promotion, constant overhead 0.1s Worst case: DCS API call timeout: 2s Average case: 1s constant overhead Telect={0.1Best1Average2WorstT_{elect} = \\begin{cases} 0.1 \u0026 \\text{Best} \\\\ 1 \u0026 \\text{Average} \\\\ 2 \u0026 \\text{Worst} \\end{cases}Telect​=⎩⎨⎧​0.112​BestAverageWorst​Phase 4: Health Check HAProxy detects the new primary online, requiring rise consecutive successful health checks.\nDetection Timeline: New Primary First Check Second Check Third Check (UP) | | | | |←─ 0~inter ─→|←─ fast ─→|←─ fast ─→| Best case: New primary promoted just before check, (rise-1) × fastinter Worst case: New primary promoted right after check, (rise-1) × fastinter + inter Average case: (rise-1) × fastinter + inter/2 Thaproxy={(rise−1)×fastinterBest(rise−1)×fastinter+inter/2Average(rise−1)×fastinter+interWorstT_{haproxy} = \\begin{cases} (rise-1) \\times fastinter \u0026 \\text{Best} \\\\ (rise-1) \\times fastinter + inter/2 \u0026 \\text{Average} \\\\ (rise-1) \\times fastinter + inter \u0026 \\text{Worst} \\end{cases}Thaproxy​=⎩⎨⎧​(rise−1)×fastinter(rise−1)×fastinter+inter/2(rise−1)×fastinter+inter​BestAverageWorst​ RTO Formula Sum all phase times to get total RTO:\nBest Case\nRTOmin=ttl−loop+0.1+(rise−1)×fastinterRTO_{min} = ttl - loop + 0.1 + (rise-1) \\times fastinterRTOmin​=ttl−loop+0.1+(rise−1)×fastinterAverage Case\nRTOavg=ttl+1+inter/2+(rise−1)×fastinterRTO_{avg} = ttl + 1 + inter/2 + (rise-1) \\times fastinterRTOavg​=ttl+1+inter/2+(rise−1)×fastinterWorst Case\nRTOmax=ttl+loop+2+inter+(rise−1)×fastinterRTO_{max} = ttl + loop + 2 + inter + (rise-1) \\times fastinterRTOmax​=ttl+loop+2+inter+(rise−1)×fastinter Model Calculation Substitute the four RTO model parameters into the formulas above:\npg_rto_plan: # [ttl, loop, retry, start, margin, inter, fastinter, downinter, rise, fall] fast: [ 20 ,5 ,5 ,15 ,5 ,'1s' ,'0.5s' ,'1s' ,3 ,3 ] # rto \u003c 30s norm: [ 30 ,5 ,10 ,25 ,5 ,'2s' ,'1s' ,'2s' ,3 ,3 ] # rto \u003c 45s safe: [ 60 ,10 ,20 ,45 ,10 ,'3s' ,'1.5s' ,'3s' ,3 ,3 ] # rto \u003c 90s wide: [ 120 ,20 ,30 ,95 ,15 ,'4s' ,'2s' ,'4s' ,3 ,3 ] # rto \u003c 150s Four Mode Calculation Results (unit: seconds, format: min / avg / max)\nPhase fast norm safe wide Lease Expiration 15 / 17 / 20 25 / 27 / 30 50 / 55 / 60 100 / 110 / 120 Replica Detection 0 / 3 / 5 0 / 3 / 5 0 / 5 / 10 0 / 10 / 20 Lock Contest \u0026 Promote 0 / 1 / 2 0 / 1 / 2 0 / 1 / 2 0 / 1 / 2 Health Check 1 / 2 / 2 2 / 3 / 4 3 / 5 / 6 4 / 6 / 8 Total 16 / 23 / 29 27 / 34 / 41 53 / 66 / 78 104 / 127 / 150 ","categories":["Concept"],"description":"Failover path triggered by node crash causing leader lease expiration and cluster election","excerpt":"Failover path triggered by node crash causing leader lease expiration …","ref":"/docs/concept/ha/failure/passive/","tags":"","title":"Model of Patroni Passive Failure"},{"body":"The INFRA module plays a special role in Pigsty: it’s not a traditional “cluster” but rather a management hub composed of a group of infrastructure nodes, providing core services for the entire Pigsty deployment. Each INFRA node is an autonomous infrastructure service unit running core components like Nginx, Grafana, and VictoriaMetrics, collectively providing observability and management capabilities for managed database clusters.\nThere are two core entities in Pigsty’s INFRA module:\nNode: A server running infrastructure components—can be bare metal, VM, container, or Pod. Component: Various infrastructure services running on nodes, such as Nginx, Grafana, VictoriaMetrics, etc. INFRA nodes typically serve as Admin Nodes, the control plane of Pigsty.\nComponent Composition Each INFRA node runs the following core components:\nComponent Port Description Nginx 80/443 Web portal, local repo, unified reverse proxy Grafana 3000 Visualization platform, dashboards, data apps VictoriaMetrics 8428 Time-series database, Prometheus API compatible VictoriaLogs 9428 Log database, receives structured logs from Vector VictoriaTraces 10428 Trace storage for slow SQL / request tracing VMAlert 8880 Alert rule evaluator based on VictoriaMetrics Alertmanager 9059 Alert aggregation and dispatch Blackbox Exporter 9115 ICMP/TCP/HTTP black-box probing DNSMASQ 53 DNS server for internal domain resolution Chronyd 123 NTP time server These components together form Pigsty’s observability infrastructure.\nExamples Let’s look at a concrete example with a two-node INFRA deployment:\ninfra: hosts: 10.10.10.10: { infra_seq: 1 } 10.10.10.11: { infra_seq: 2 } The above config fragment defines a two-node INFRA deployment:\nGroup Description infra INFRA infrastructure node group Node Description infra-1 10.10.10.10 INFRA node #1 infra-2 10.10.10.11 INFRA node #2 For production environments, deploying at least two INFRA nodes is recommended for infrastructure component redundancy.\nIdentity Parameters Pigsty uses the INFRA_ID parameter group to assign deterministic identities to each INFRA module entity. One parameter is required:\nParameter Type Level Description Format infra_seq int Node INFRA node sequence, required Natural number, starting from 1, unique within group With node sequence assigned at node level, Pigsty automatically generates unique identifiers for each entity based on rules:\nEntity Generation Rule Example Node infra-{{ infra_seq }} infra-1, infra-2 The INFRA module assigns infra-N format identifiers to nodes for distinguishing multiple infrastructure nodes in the monitoring system. However, this doesn’t change the node’s hostname or system identity; nodes still use their existing hostname or IP address for identification.\nService Portal INFRA nodes provide unified web service entry through Nginx. The infra_portal parameter defines services exposed through Nginx.\nThe default configuration only defines the home server:\ninfra_portal: home : { domain: i.pigsty } Pigsty automatically configures reverse proxy endpoints for enabled components (Grafana, VictoriaMetrics, AlertManager, etc.). If you need to access these services via separate domains, you can explicitly add configurations:\ninfra_portal: home : { domain: i.pigsty } grafana : { domain: g.pigsty, endpoint: \"${admin_ip}:3000\", websocket: true } prometheus : { domain: p.pigsty, endpoint: \"${admin_ip}:8428\" } # VMUI alertmanager : { domain: a.pigsty, endpoint: \"${admin_ip}:9059\" } Domain Service Description i.pigsty Home Pigsty homepage g.pigsty Grafana Monitoring dashboard p.pigsty VictoriaMetrics TSDB Web UI a.pigsty Alertmanager Alert management UI Accessing Pigsty services via domain names is recommended over direct IP + port.\nDeployment Scale The number of INFRA nodes depends on deployment scale and HA requirements:\nScale INFRA Nodes Description Dev/Test 1 Single-node deployment, all on one node Small Prod 1-2 Single or dual node, can share with other services Medium Prod 2-3 Dedicated INFRA nodes, redundant components Large Prod 3+ Multiple INFRA nodes, component separation In singleton deployment, INFRA components share the same node with PGSQL, ETCD, etc. In small-scale deployments, INFRA nodes typically also serve as “Admin Node” / backup admin node and local software repository (/www/pigsty). In larger deployments, these responsibilities can be separated to dedicated nodes.\nMonitoring Label System Pigsty’s monitoring system collects metrics from INFRA components themselves. Unlike database modules, each component in the INFRA module is treated as an independent monitoring object, distinguished by the cls (class) label.\nLabel Description Example cls Component type, each forming a “class” nginx ins Instance name, format {component}-{infra_seq} nginx-1 ip INFRA node IP running the component 10.10.10.10 job VictoriaMetrics scrape job, fixed as infra infra Using a two-node INFRA deployment (infra_seq: 1 and infra_seq: 2) as example, component monitoring labels are:\nComponent cls ins Example Port Nginx nginx nginx-1, nginx-2 9113 Grafana grafana grafana-1, grafana-2 3000 VictoriaMetrics vmetrics vmetrics-1, vmetrics-2 8428 VictoriaLogs vlogs vlogs-1, vlogs-2 9428 VictoriaTraces vtraces vtraces-1, vtraces-2 10428 VMAlert vmalert vmalert-1, vmalert-2 8880 Alertmanager alertmanager alertmanager-1, alertmanager-2 9059 Blackbox blackbox blackbox-1, blackbox-2 9115 All INFRA component metrics use a unified job=\"infra\" label, distinguished by the cls label:\nnginx_up{cls=\"nginx\", ins=\"nginx-1\", ip=\"10.10.10.10\", job=\"infra\"} grafana_info{cls=\"grafana\", ins=\"grafana-1\", ip=\"10.10.10.10\", job=\"infra\"} vm_app_version{cls=\"vmetrics\", ins=\"vmetrics-1\", ip=\"10.10.10.10\", job=\"infra\"} vlogs_rows_ingested_total{cls=\"vlogs\", ins=\"vlogs-1\", ip=\"10.10.10.10\", job=\"infra\"} alertmanager_alerts{cls=\"alertmanager\", ins=\"alertmanager-1\", ip=\"10.10.10.10\", job=\"infra\"} ","categories":["Concept"],"description":"Entity-Relationship model for INFRA infrastructure nodes in Pigsty, component composition, and naming conventions.","excerpt":"Entity-Relationship model for INFRA infrastructure nodes in Pigsty, …","ref":"/docs/concept/model/infra/","tags":"","title":"E-R Model of Infra Cluster"},{"body":"Pigsty provides four scenario-based parameter templates by default, which can be specified and used through the pg_conf parameter.\ntiny.yml: Optimized for small nodes, VMs, and small demos (1-8 cores, 1-16GB) oltp.yml: Optimized for OLTP workloads and latency-sensitive applications (4C8GB+) (default template) olap.yml: Optimized for OLAP workloads and throughput (4C8G+) crit.yml: Optimized for data consistency and critical applications (4C8G+) Pigsty adopts different parameter optimization strategies for these four default scenarios, as shown below:\nMemory Parameter Tuning Pigsty automatically detects the system’s memory size and uses it as the basis for setting the maximum number of connections and memory-related parameters.\npg_max_conn: PostgreSQL maximum connections, auto will use recommended values for different scenarios pg_shared_buffer_ratio: Shared buffer memory ratio, default is 0.25 By default, Pigsty uses 25% of memory as PostgreSQL shared buffers, with the remaining 75% as the operating system cache.\nBy default, if the user has not set a pg_max_conn maximum connections value, Pigsty will use defaults according to the following rules:\noltp: 500 (pgbouncer) / 1000 (postgres) crit: 500 (pgbouncer) / 1000 (postgres) tiny: 300 olap: 300 For OLTP and CRIT templates, if the service is not pointing to the pgbouncer connection pool but directly connects to the postgres database, the maximum connections will be doubled to 1000.\nAfter determining the maximum connections, work_mem is calculated from shared memory size / maximum connections and limited to the range of 64MB ~ 1GB.\n{% raw %} {% if pg_max_conn != 'auto' and pg_max_conn|int \u003e= 20 %}{% set pg_max_connections = pg_max_conn|int %}{% else %}{% if pg_default_service_dest|default('postgres') == 'pgbouncer' %}{% set pg_max_connections = 500 %}{% else %}{% set pg_max_connections = 1000 %}{% endif %}{% endif %} {% set pg_max_prepared_transactions = pg_max_connections if 'citus' in pg_libs else 0 %} {% set pg_max_locks_per_transaction = (2 * pg_max_connections)|int if 'citus' in pg_libs or 'timescaledb' in pg_libs else pg_max_connections %} {% set pg_shared_buffers = (node_mem_mb|int * pg_shared_buffer_ratio|float) | round(0, 'ceil') | int %} {% set pg_maintenance_mem = (pg_shared_buffers|int * 0.25)|round(0, 'ceil')|int %} {% set pg_effective_cache_size = node_mem_mb|int - pg_shared_buffers|int %} {% set pg_workmem = ([ ([ (pg_shared_buffers / pg_max_connections)|round(0,'floor')|int , 64 ])|max|int , 1024])|min|int %} {% endraw %} CPU Parameter Tuning In PostgreSQL, there are 4 important parameters related to parallel queries. Pigsty automatically optimizes parameters based on the current system’s CPU cores. In all strategies, the total number of parallel processes (total budget) is usually set to CPU cores + 8, with a minimum of 16, to reserve enough background workers for logical replication and extensions. The OLAP and TINY templates vary slightly based on scenarios.\nOLTP Setting Logic Range Limits max_worker_processes max(100% CPU + 8, 16) CPU cores + 4, minimum 12 max_parallel_workers max(ceil(50% CPU), 2) 1/2 CPU rounded up, minimum 2 max_parallel_maintenance_workers max(ceil(33% CPU), 2) 1/3 CPU rounded up, minimum 2 max_parallel_workers_per_gather min(max(ceil(20% CPU), 2),8) 1/5 CPU rounded down, minimum 2, max 8 OLAP Setting Logic Range Limits max_worker_processes max(100% CPU + 12, 20) CPU cores + 12, minimum 20 max_parallel_workers max(ceil(80% CPU, 2)) 4/5 CPU rounded up, minimum 2 max_parallel_maintenance_workers max(ceil(33% CPU), 2) 1/3 CPU rounded up, minimum 2 max_parallel_workers_per_gather max(floor(50% CPU), 2) 1/2 CPU rounded up, minimum 2 CRIT Setting Logic Range Limits max_worker_processes max(100% CPU + 8, 16) CPU cores + 8, minimum 16 max_parallel_workers max(ceil(50% CPU), 2) 1/2 CPU rounded up, minimum 2 max_parallel_maintenance_workers max(ceil(33% CPU), 2) 1/3 CPU rounded up, minimum 2 max_parallel_workers_per_gather 0, enable as needed TINY Setting Logic Range Limits max_worker_processes max(100% CPU + 4, 12) CPU cores + 4, minimum 12 max_parallel_workers max(ceil(50% CPU) 1) 50% CPU rounded down, minimum 1 max_parallel_maintenance_workers max(ceil(33% CPU), 1) 33% CPU rounded down, minimum 1 max_parallel_workers_per_gather 0, enable as needed Note that the CRIT and TINY templates disable parallel queries by setting max_parallel_workers_per_gather = 0. Users can enable parallel queries as needed by setting this parameter.\nBoth OLTP and CRIT templates additionally set the following parameters, doubling the parallel query cost to reduce the tendency to use parallel queries.\nparallel_setup_cost: 2000 # double from 100 to increase parallel cost parallel_tuple_cost: 0.2 # double from 0.1 to increase parallel cost min_parallel_table_scan_size: 16MB # double from 8MB to increase parallel cost min_parallel_index_scan_size: 1024 # double from 512 to increase parallel cost Note that adjustments to the max_worker_processes parameter only take effect after a restart. Additionally, when a replica’s configuration value for this parameter is higher than the primary’s, the replica will fail to start. This parameter must be adjusted through Patroni configuration management, which ensures consistent primary-replica configuration and prevents new replicas from failing to start during failover.\nStorage Space Parameters Pigsty automatically detects the total space of the disk where the /data/postgres main data directory is located and uses it as the basis for specifying the following parameters:\n{% raw %} min_wal_size: {{ ([pg_size_twentieth, 200])|min }}GB # 1/20 disk size, max 200GB max_wal_size: {{ ([pg_size_twentieth * 4, 2000])|min }}GB # 2/10 disk size, max 2000GB max_slot_wal_keep_size: {{ ([pg_size_twentieth * 6, 3000])|min }}GB # 3/10 disk size, max 3000GB temp_file_limit: {{ ([pg_size_twentieth, 200])|min }}GB # 1/20 of disk size, max 200GB {% endraw %} temp_file_limit defaults to 5% of disk space, capped at 200GB. min_wal_size defaults to 5% of disk space, capped at 200GB. max_wal_size defaults to 20% of disk space, capped at 2TB. max_slot_wal_keep_size defaults to 30% of disk space, capped at 3TB. As a special case, the OLAP template allows 20% for temp_file_limit, capped at 2TB.\nManual Parameter Tuning In addition to using Pigsty’s automatically configured parameters, you can also manually tune PostgreSQL parameters.\nUse the pg edit-config \u003ccluster\u003e command to interactively edit cluster configuration:\npg edit-config pg-meta Or use the -p parameter to directly set parameters:\npg edit-config -p log_min_duration_statement=1000 pg-meta pg edit-config --force -p shared_preload_libraries='timescaledb, pg_cron, pg_stat_statements, auto_explain' pg-meta You can also use the Patroni REST API to modify configuration:\ncurl -u 'postgres:Patroni.API' \\ -d '{\"postgresql\":{\"parameters\": {\"log_min_duration_statement\":200}}}' \\ -s -X PATCH http://10.10.10.10:8008/config | jq . ","categories":["Reference"],"description":"Learn the parameter optimization strategies Pigsty uses for the 4 different PostgreSQL workload scenarios.","excerpt":"Learn the parameter optimization strategies Pigsty uses for the 4 …","ref":"/docs/pgsql/template/tune/","tags":"","title":"Parameter Optimization Policy"},{"body":"PostgreSQL module global overview monitoring dashboards, including:\nPGSQL Overview: Main dashboard for the PGSQL module PGSQL Alert: Global key metrics and alert events for PGSQL PGSQL Shard: Overview of horizontally sharded PGSQL clusters ","categories":"","description":"PostgreSQL module global overview monitoring dashboards","excerpt":"PostgreSQL module global overview monitoring dashboards","ref":"/docs/pgsql/dashboard/overview/","tags":"","title":"Overview"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/pigsty/","tags":"","title":"Pigsty"},{"body":"RPO (Recovery Point Objective) defines the maximum amount of data loss allowed when the primary fails.\nFor scenarios where data integrity is critical, such as financial transactions, RPO = 0 is typically required, meaning no data loss is allowed.\nHowever, stricter RPO targets come at a cost: higher write latency, reduced system throughput, and the risk that replica failures may cause primary unavailability. For typical scenarios, some data loss is acceptable (e.g., up to 1MB) in exchange for higher availability and performance.\nTrade-offs In asynchronous replication scenarios, there is typically some replication lag between replicas and the primary (depending on network and throughput, normally in the range of 10KB-100KB / 100µs-10ms). This means when the primary fails, replicas may not have fully synchronized with the latest data. If a failover occurs, the new primary may lose some unreplicated data.\nThe upper limit of potential data loss is controlled by the pg_rpo parameter, which defaults to 1048576 (1MB), meaning up to 1MiB of data loss can be tolerated during failover.\nWhen the cluster primary fails, if any replica has replication lag within this threshold, Pigsty will automatically promote that replica to be the new primary. However, when all replicas exceed this threshold, Pigsty will refuse [automatic failover] to prevent data loss. Manual intervention is then required to decide whether to wait for the primary to recover (which may never happen) or accept the data loss and force-promote a replica.\nYou need to configure this value based on your business requirements, making a trade-off between availability and consistency. Increasing this value improves the success rate of automatic failover but also increases the upper limit of potential data loss.\nWhen you set pg_rpo = 0, Pigsty enables synchronous replication, ensuring the primary only returns write success after at least one replica has persisted the data. This configuration ensures zero replication lag but introduces significant write latency and reduces overall throughput.\nflowchart LR A([Primary Failure]) --\u003e B{Synchronous\u003cbr/\u003eReplication?} B --\u003e|No| C{Lag \u003c RPO?} B --\u003e|Yes| D{Sync Replica\u003cbr/\u003eAvailable?} C --\u003e|Yes| E[Lossy Auto Failover\u003cbr/\u003eRPO \u003c 1MB] C --\u003e|No| F[Refuse Auto Failover\u003cbr/\u003eWait for Primary Recovery\u003cbr/\u003eor Manual Intervention] D --\u003e|Yes| G[Lossless Auto Failover\u003cbr/\u003eRPO = 0] D --\u003e|No| H{Strict Mode?} H --\u003e|No| C H --\u003e|Yes| F style A fill:#dc3545,stroke:#b02a37,color:#fff style E fill:#F0AD4E,stroke:#146c43,color:#fff style G fill:#198754,stroke:#146c43,color:#fff style F fill:#BE002F,stroke:#565e64,color:#fff Protection Modes Pigsty provides three protection modes to help users make trade-offs under different RPO requirements, similar to Oracle Data Guard protection modes.\nMaximum Performance Default mode, asynchronous replication, transactions commit with only local WAL persistence, no waiting for replicas, replica failures are completely transparent to the primary Primary failure may lose unsent/unreceived WAL (typically \u003c 1MB, normally 10ms/100ms, 10KB/100KB range under normal network conditions) Optimized for performance, suitable for typical business scenarios that tolerate minor data loss during failures Maximum Availability Configured with pg_rpo = 0, enables Patroni synchronous commit mode: synchronous_mode: true Under normal conditions, waits for at least one replica confirmation, achieving zero data loss. When all sync replicas fail, automatically degrades to async mode to continue service Balances data safety and service availability, recommended configuration for production critical business Maximum Protection Uses crit.yml template, enables Patroni strict synchronous mode: synchronous_mode: true / synchronous_mode_strict: true When all sync replicas fail, primary refuses writes to prevent data loss, transactions must be persisted on at least one replica before returning success Suitable for financial transactions, medical records, and other scenarios with extremely high data integrity requirements Name Maximum Performance Maximum Availability Maximum Protection Replication Asynchronous Synchronous Strict Synchronous Data Loss Possible (replication lag) Zero normally, minor when degraded Zero Write Latency Lowest Medium (+1 network RTT) Medium (+1 network RTT) Throughput Highest Reduced Reduced Replica Failure Impact None Auto degrade, service continues Primary stops writes RPO \u003c 1MB = 0 (normal) / \u003c 1MB (degraded) = 0 Use Case Typical business, performance first Critical business, safety first Financial core, compliance first Configuration Default config pg_rpo = 0 pg_conf: crit.yml Implementation The three protection modes differ in how two core Patroni parameters are configured: synchronous_mode and synchronous_mode_strict:\nsynchronous_mode: Whether Patroni enables synchronous replication. If enabled, check if synchronous_mode_strict enables strict synchronous mode. synchronous_mode_strict = false: Default configuration, allows degradation to async mode when replicas fail, primary continues service (Maximum Availability) synchronous_mode_strict = true: Degradation forbidden, primary stops writes until sync replica recovers (Maximum Protection) Mode synchronous_mode synchronous_mode_strict Replication Mode Replica Failure Behavior Max Performance false - Async No impact Max Availability true false Synchronous Auto degrade to async Max Protection true true Strict Synchronous Primary refuses writes Typically, you only need to set the pg_rpo parameter to 0 to enable the synchronous_mode switch, activating Maximum Availability mode. If you use pg_conf = crit.yml template, it additionally enables the synchronous_mode_strict strict mode switch, activating Maximum Protection mode.\nYou can also directly configure these Patroni parameters as needed. Refer to Patroni and PostgreSQL documentation to achieve stronger data protection, such as:\nSpecify the synchronous replica list, configure more sync replicas to improve disaster tolerance, use quorum synchronous commit, or even require all replicas to perform synchronous commit. Configure synchronous_commit: 'remote_apply' to strictly ensure primary-replica read-write consistency. (Oracle Maximum Protection mode is equivalent to remote_write) Recommendations Maximum Performance mode (asynchronous replication) is the default mode used by Pigsty and is sufficient for the vast majority of workloads. Tolerating minor data loss during failures (typically in the range of a few KB to hundreds of KB) in exchange for higher throughput and availability is the recommended configuration for typical business scenarios. In this case, you can adjust the maximum allowed data loss through the pg_rpo parameter to suit different business needs.\nMaximum Availability mode (synchronous replication) is suitable for scenarios with high data integrity requirements that cannot tolerate data loss. In this mode, a minimum of two-node PostgreSQL cluster (one primary, one replica) is required. Set pg_rpo to 0 to enable this mode.\nMaximum Protection mode (strict synchronous replication) is suitable for financial transactions, medical records, and other scenarios with extremely high data integrity requirements. We recommend using at least a three-node cluster (one primary, two replicas), because with only two nodes, if the replica fails, the primary will stop writes, causing service unavailability, which reduces overall system reliability. With three nodes, if only one replica fails, the primary can continue to serve.\n","categories":["Concept"],"description":"Trade-off analysis for RPO (Recovery Point Objective), finding the optimal balance between availability and data loss.","excerpt":"Trade-off analysis for RPO (Recovery Point Objective), finding the …","ref":"/docs/concept/ha/rpo/","tags":"","title":"RPO Trade-offs"},{"body":"","categories":"","description":"Articles about Pigsty, cloud computing, database industry, and notes on PostgreSQL development, administration, and internals","excerpt":"Articles about Pigsty, cloud computing, database industry, and notes …","ref":"/blog/","tags":"","title":"Pigsty Blog Articles"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/cloud/","tags":["Cloud-Exit"],"title":"Cloud"},{"body":"PostgreSQL cluster-level monitoring dashboards, including:\nPGSQL Cluster: Main dashboard for a PGSQL cluster PGRDS Cluster: RDS version of PGSQL Cluster, focusing on PostgreSQL-native metrics PGSQL Activity: Session/load/QPS/TPS/locks for PGSQL cluster PGSQL Replication: Replication, slots, and pub/sub for PGSQL cluster PGSQL Service: Service, proxy, routing, and load balancing for PGSQL cluster PGSQL Databases: Database CRUD, slow queries, and table statistics across all instances PGSQL Patroni: HA status and Patroni component status for cluster PGSQL PITR: PITR context for point-in-time recovery assistance ","categories":"","description":"PostgreSQL cluster-level monitoring dashboards","excerpt":"PostgreSQL cluster-level monitoring dashboards","ref":"/docs/pgsql/dashboard/cluster/","tags":"","title":"Cluster"},{"body":" RTO Timeline Failure Model Item Best Worst Average Description Crash Found 0 loop loop/2 Best: PG crashes right before checkWorst: PG crashes right after check Restart Timeout 0 start start Best: PG recovers instantlyWorst: Wait full start timeout before releasing lease Replica Detect 0 loop loop/2 Best: Right at check pointWorst: Just missed check point Elect Promote 0 2 1 Best: Acquire lock and promote directlyWorst: API timeout + Promote HAProxy Check (rise-1) × fastinter (rise-1) × fastinter + inter (rise-1) × fastinter + inter/2 Best: State changes before checkWorst: State changes right after check Key Difference Between Active and Passive Failure:\nScenario Patroni Status Lease Handling Main Wait Time Active Failure (PG crash) Alive, healthy Actively tries to restart PG, releases lease after timeout primary_start_timeout Passive Failure (node down) Dies with node Cannot actively release, must wait for TTL expiry ttl In active failure scenarios, Patroni remains alive and can actively detect PG crash and attempt restart. If restart succeeds, service self-heals; if timeout expires without recovery, Patroni actively releases the Leader Key, triggering cluster election.\nTiming Analysis Phase 1: Failure Detection Patroni checks PostgreSQL status every loop_wait cycle (via pg_isready or process check).\nTimeline: Last check PG crash Next check | | | |←── 0~loop ──→| | Best case: PG crashes right before Patroni check, detected immediately, wait 0 Worst case: PG crashes right after check, wait for next cycle, wait loop Average case: loop/2 Tdetect={0Bestloop/2AverageloopWorstT_{detect} = \\begin{cases} 0 \u0026 \\text{Best} \\\\ loop/2 \u0026 \\text{Average} \\\\ loop \u0026 \\text{Worst} \\end{cases}Tdetect​=⎩⎨⎧​0loop/2loop​BestAverageWorst​Phase 2: Restart Timeout After Patroni detects PG crash, it attempts to restart PostgreSQL. This phase has two possible outcomes:\nTimeline: Crash detected Restart attempt Success/Timeout | | | |←──── 0 ~ start ─────────────────────→| Path A: Self-healing Success (Best case)\nPG restarts successfully, service recovers No failover triggered, extremely short RTO Wait time: 0 (relative to Failover path) Path B: Failover Required (Average/Worst case)\nPG still not recovered after primary_start_timeout Patroni actively releases Leader Key Wait time: start Trestart={0Best (self-healing success)startAverage (failover required)startWorstT_{restart} = \\begin{cases} 0 \u0026 \\text{Best (self-healing success)} \\\\ start \u0026 \\text{Average (failover required)} \\\\ start \u0026 \\text{Worst} \\end{cases}Trestart​=⎩⎨⎧​0startstart​Best (self-healing success)Average (failover required)Worst​ Note: Average case assumes failover is required. If PG can quickly self-heal, overall RTO will be significantly lower.\nPhase 3: Standby Detection Standbys wake up on loop_wait cycle and check Leader Key status in DCS. When primary Patroni releases the Leader Key, standbys discover this and begin election.\nTimeline: Lease released Standby wakes | | |←── 0~loop ──────→| Best case: Standby wakes right when lease is released, wait 0 Worst case: Standby just went to sleep when lease released, wait loop Average case: loop/2 Tstandby={0Bestloop/2AverageloopWorstT_{standby} = \\begin{cases} 0 \u0026 \\text{Best} \\\\ loop/2 \u0026 \\text{Average} \\\\ loop \u0026 \\text{Worst} \\end{cases}Tstandby​=⎩⎨⎧​0loop/2loop​BestAverageWorst​Phase 4: Lock \u0026 Promote After standbys discover Leader Key vacancy, election begins. The standby that acquires the Leader Key executes pg_ctl promote to become the new primary.\nVia REST API, parallel queries to check each standby’s replication position, typically 10ms, hardcoded 2s timeout. Compare WAL positions to determine best candidate, standbys attempt to create Leader Key (CAS atomic operation) Execute pg_ctl promote to become primary (very fast, typically negligible) Election process: StandbyA ──→ Query replication position ──→ Compare ──→ Try lock ──→ Success StandbyB ──→ Query replication position ──→ Compare ──→ Try lock ──→ Fail Best case: Single standby or direct lock acquisition and promote, constant overhead 0.1s Worst case: DCS API call timeout: 2s Average case: 1s constant overhead Telect={0.1Best1Average2WorstT_{elect} = \\begin{cases} 0.1 \u0026 \\text{Best} \\\\ 1 \u0026 \\text{Average} \\\\ 2 \u0026 \\text{Worst} \\end{cases}Telect​=⎩⎨⎧​0.112​BestAverageWorst​Phase 5: Health Check HAProxy detects new primary online, requires rise consecutive successful health checks.\nCheck timeline: New primary First check Second check Third check (UP) | | | | |←─ 0~inter ──→|←─── fast ────→|←─── fast ────→| Best case: New primary comes up right at check time, (rise-1) × fastinter Worst case: New primary comes up right after check, (rise-1) × fastinter + inter Average case: (rise-1) × fastinter + inter/2 Thaproxy={(rise−1)×fastinterBest(rise−1)×fastinter+inter/2Average(rise−1)×fastinter+interWorstT_{haproxy} = \\begin{cases} (rise-1) \\times fastinter \u0026 \\text{Best} \\\\ (rise-1) \\times fastinter + inter/2 \u0026 \\text{Average} \\\\ (rise-1) \\times fastinter + inter \u0026 \\text{Worst} \\end{cases}Thaproxy​=⎩⎨⎧​(rise−1)×fastinter(rise−1)×fastinter+inter/2(rise−1)×fastinter+inter​BestAverageWorst​ RTO Formula Sum all phase times to get total RTO:\nBest Case (PG instant self-healing)\nRTOmin=0+0+0+0.1+(rise−1)×fastinter≈(rise−1)×fastinterRTO_{min} = 0 + 0 + 0 + 0.1 + (rise-1) \\times fastinter \\approx (rise-1) \\times fastinterRTOmin​=0+0+0+0.1+(rise−1)×fastinter≈(rise−1)×fastinterAverage Case (Failover required)\nRTOavg=loop+start+1+inter/2+(rise−1)×fastinterRTO_{avg} = loop + start + 1 + inter/2 + (rise-1) \\times fastinterRTOavg​=loop+start+1+inter/2+(rise−1)×fastinterWorst Case\nRTOmax=loop×2+start+2+inter+(rise−1)×fastinterRTO_{max} = loop \\times 2 + start + 2 + inter + (rise-1) \\times fastinterRTOmax​=loop×2+start+2+inter+(rise−1)×fastinter Model Calculation Substituting the four RTO model parameters into the formulas above:\npg_rto_plan: # [ttl, loop, retry, start, margin, inter, fastinter, downinter, rise, fall] fast: [ 20 ,5 ,5 ,15 ,5 ,'1s' ,'0.5s' ,'1s' ,3 ,3 ] # rto \u003c 30s norm: [ 30 ,5 ,10 ,25 ,5 ,'2s' ,'1s' ,'2s' ,3 ,3 ] # rto \u003c 45s safe: [ 60 ,10 ,20 ,45 ,10 ,'3s' ,'1.5s' ,'3s' ,3 ,3 ] # rto \u003c 90s wide: [ 120 ,20 ,30 ,95 ,15 ,'4s' ,'2s' ,'4s' ,3 ,3 ] # rto \u003c 150s Calculation Results for Four Modes (unit: seconds, format: min / avg / max)\nPhase fast norm safe wide Failure Detection 0 / 3 / 5 0 / 3 / 5 0 / 5 / 10 0 / 10 / 20 Restart Timeout 0 / 15 / 15 0 / 25 / 25 0 / 45 / 45 0 / 95 / 95 Standby Detection 0 / 3 / 5 0 / 3 / 5 0 / 5 / 10 0 / 10 / 20 Lock \u0026 Promote 0 / 1 / 2 0 / 1 / 2 0 / 1 / 2 0 / 1 / 2 Health Check 1 / 2 / 2 2 / 3 / 4 3 / 5 / 6 4 / 6 / 8 Total 1 / 24 / 29 2 / 35 / 41 3 / 61 / 73 4 / 122 / 145 Comparison with Passive Failure Phase Active Failure (PG crash) Passive Failure (node down) Description Detection Mechanism Patroni active detection TTL passive expiry Active detection discovers failure faster Core Wait start ttl start is usually less than ttl, but requires additional failure detection time Lease Handling Active release Passive expiry Active release is more timely Self-healing Possible Yes No Active detection can attempt local recovery RTO Comparison (Average case):\nMode Active Failure (PG crash) Passive Failure (node down) Difference fast 24s 23s +1s norm 35s 34s +1s safe 61s 66s -5s wide 122s 127s -5s Analysis: In fast and norm modes, active failure RTO is slightly higher than passive failure because it waits for primary_start_timeout (start); but in safe and wide modes, since start \u003c ttl - loop, active failure is actually faster. However, active failure has the possibility of self-healing, with potentially extremely short RTO in best case scenarios.\n","categories":["Concept"],"description":"PostgreSQL primary process crashes while Patroni stays alive and attempts restart, triggering failover after timeout","excerpt":"PostgreSQL primary process crashes while Patroni stays alive and …","ref":"/docs/concept/ha/failure/active/","tags":"","title":"Model of Patroni Active Failure"},{"body":"oltp.yml is Pigsty’s default config template, optimized for online transaction processing (OLTP). Designed for 4-128 core CPUs with high concurrency, low latency, and high throughput.\nPair with node_tune = oltp for OS-level tuning.\nUse Cases OLTP template is ideal for:\nE-commerce: Order processing, inventory, user transactions Social apps: User feeds, messaging, following relationships Gaming backends: Player data, leaderboards, game state SaaS applications: Multi-tenant business systems Web apps: CRUD-intensive workloads Workload characteristics:\nMany short transactions (millisecond-level) High concurrent connections (hundreds to thousands) Read/write ratio typically 7:3 to 9:1 Latency-sensitive, requires fast response High data consistency requirements Usage oltp.yml is the default template, no explicit specification needed:\npg-oltp: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } 10.10.10.12: { pg_seq: 2, pg_role: replica } vars: pg_cluster: pg-oltp # pg_conf: oltp.yml # PostgreSQL config template (default) # node_tune: oltp # OS tuning template (default) Or explicitly specify:\npg-oltp: vars: pg_conf: oltp.yml # PostgreSQL config template node_tune: oltp # OS tuning template Parameter Details Connection Management max_connections: 500/1000 # depends on pgbouncer usage superuser_reserved_connections: 10 When pg_default_service_dest is pgbouncer, max_connections is set to 500 When traffic connects directly to PostgreSQL, max_connections is set to 1000 Override via pg_max_conn parameter Memory Config OLTP template memory allocation strategy:\nParameter Formula Description shared_buffers mem × pg_shared_buffer_ratio Default ratio 0.25 maintenance_work_mem shared_buffers × 25% For VACUUM, CREATE INDEX work_mem 64MB - 1GB Based on shared_buffers/max_connections effective_cache_size total mem - shared_buffers Estimated cache memory work_mem calculation:\nwork_mem = min(max(shared_buffers / max_connections, 64MB), 1GB) Ensures each connection has sufficient sort/hash memory without over-allocation.\nParallel Query OLTP template moderately limits parallel queries to prevent resource contention:\nmax_worker_processes: cpu + 8 (min 16) max_parallel_workers: 50% × cpu (min 2) max_parallel_workers_per_gather: 20% × cpu (2-8) max_parallel_maintenance_workers: 33% × cpu (min 2) Parallel cost estimates are increased to favor serial execution:\nparallel_setup_cost: 2000 # 2x default (1000) parallel_tuple_cost: 0.2 # 2x default (0.1) min_parallel_table_scan_size: 16MB # 2x default (8MB) min_parallel_index_scan_size: 1024 # 2x default (512) WAL Config min_wal_size: disk/20 (max 200GB) max_wal_size: disk/5 (max 2000GB) max_slot_wal_keep_size: disk×3/10 (max 3000GB) wal_buffers: 16MB wal_writer_delay: 20ms wal_writer_flush_after: 1MB commit_delay: 20 commit_siblings: 10 checkpoint_timeout: 15min checkpoint_completion_target: 0.80 Balances data safety and write performance.\nVacuum Config vacuum_cost_delay: 20ms # sleep after each vacuum round vacuum_cost_limit: 2000 # cost limit per vacuum round autovacuum_max_workers: 3 autovacuum_naptime: 1min autovacuum_vacuum_scale_factor: 0.08 # 8% table change triggers vacuum autovacuum_analyze_scale_factor: 0.04 # 4% table change triggers analyze autovacuum_freeze_max_age: 1000000000 Conservative vacuum settings avoid impacting online transaction performance.\nQuery Optimization random_page_cost: 1.1 # SSD optimized effective_io_concurrency: 200 # SSD concurrent IO default_statistics_target: 400 # Statistics precision Enables planner to generate better query plans.\nLogging \u0026 Monitoring log_min_duration_statement: 100 # log queries \u003e 100ms log_statement: ddl # log DDL statements log_checkpoints: on log_lock_waits: on log_temp_files: 1024 # log temp files \u003e 1MB log_autovacuum_min_duration: 1s track_io_timing: on track_functions: all track_activity_query_size: 8192 Client Timeouts deadlock_timeout: 50ms idle_in_transaction_session_timeout: 10min 10-minute idle transaction timeout prevents zombie transactions holding locks.\nExtension Config shared_preload_libraries: 'pg_stat_statements, auto_explain' # auto_explain auto_explain.log_min_duration: 1s auto_explain.log_analyze: on auto_explain.log_verbose: on auto_explain.log_timing: on auto_explain.log_nested_statements: true # pg_stat_statements pg_stat_statements.max: 10000 pg_stat_statements.track: all pg_stat_statements.track_utility: off pg_stat_statements.track_planning: off Template Comparison Feature OLTP OLAP CRIT max_connections 500-1000 500 500-1000 work_mem 64MB-1GB 64MB-8GB 64MB-1GB Parallel query Moderate limit Aggressive Disabled Vacuum intensity Conservative Aggressive Conservative Txn timeout 10min Disabled 1min Slow query threshold 100ms 1000ms 100ms Why OLTP over OLAP? Queries are mostly simple point/range lookups Transaction response time requires milliseconds High concurrent connections No complex analytical queries Why OLTP over CRIT? Small probability of data loss acceptable (async replication) Complete audit logs not required Better write performance desired Performance Tuning Tips Connection Pooling For high concurrency, use PgBouncer connection pool:\npg-oltp: vars: pg_default_service_dest: pgbouncer # default pgbouncer_poolmode: transaction # transaction-level pooling Read Separation Use read replicas to share read load:\npg-oltp: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } 10.10.10.12: { pg_seq: 2, pg_role: replica } 10.10.10.13: { pg_seq: 3, pg_role: replica } Monitoring Metrics Focus on these metrics:\nConnections: Active/waiting connection counts Transaction rate: TPS, commit/rollback ratio Response time: Query latency percentiles (p50/p95/p99) Lock waits: Lock wait time, deadlock counts Replication lag: Replica delay time and bytes References pg_conf: PostgreSQL config template selection node_tune: OS tuning template, should match pg_conf OLAP Template: Analytics template comparison CRIT Template: Critical business template comparison TINY Template: Micro instance template comparison Cluster Config: PostgreSQL cluster type configuration High Availability: HA architecture design ","categories":["Reference"],"description":"PostgreSQL config template optimized for online transaction processing workloads","excerpt":"PostgreSQL config template optimized for online transaction processing …","ref":"/docs/pgsql/template/oltp/","tags":"","title":"OLTP Template"},{"body":"The PGSQL module organizes PostgreSQL in production as clusters—logical entities composed of a group of database instances associated by primary-replica relationships.\nEach cluster is an autonomous business unit consisting of at least one primary instance, exposing capabilities through services.\nThere are four core entities in Pigsty’s PGSQL module:\nCluster: An autonomous PostgreSQL business unit serving as the top-level namespace for other entities. Service: A named abstraction that exposes capabilities, routes traffic, and exposes services using node ports. Instance: A single PostgreSQL server consisting of running processes and database files on a single node. Node: A hardware resource abstraction running Linux + Systemd environment—can be bare metal, VM, container, or Pod. Along with two business entities—“Database” and “Role”—these form the complete logical view as shown below:\nExamples Let’s look at two concrete examples. Using the four-node Pigsty sandbox, there’s a three-node pg-test cluster:\npg-test: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } 10.10.10.12: { pg_seq: 2, pg_role: replica } 10.10.10.13: { pg_seq: 3, pg_role: replica } vars: { pg_cluster: pg-test } The above config fragment defines a high-availability PostgreSQL cluster with these related entities:\nCluster Description pg-test PostgreSQL 3-node HA cluster Instance Description pg-test-1 PostgreSQL instance #1, default primary pg-test-2 PostgreSQL instance #2, initial replica pg-test-3 PostgreSQL instance #3, initial replica Service Description pg-test-primary Read-write service (routes to primary pgbouncer) pg-test-replica Read-only service (routes to replica pgbouncer) pg-test-default Direct read-write service (routes to primary postgres) pg-test-offline Offline read service (routes to dedicated postgres) Node Description node-1 10.10.10.11 Node #1, hosts pg-test-1 PG instance node-2 10.10.10.12 Node #2, hosts pg-test-2 PG instance node-3 10.10.10.13 Node #3, hosts pg-test-3 PG instance Identity Parameters Pigsty uses the PG_ID parameter group to assign deterministic identities to each PGSQL module entity. Three parameters are required:\nParameter Type Level Description Format pg_cluster string Cluster PG cluster name, required Valid DNS name, regex [a-zA-Z0-9-]+ pg_seq int Instance PG instance number, required Natural number, starting from 0 or 1, unique within cluster pg_role enum Instance PG instance role, required Enum: primary, replica, offline With cluster name defined at cluster level and instance number/role assigned at instance level, Pigsty automatically generates unique identifiers for each entity based on rules:\nEntity Generation Rule Example Instance {{ pg_cluster }}-{{ pg_seq }} pg-test-1, pg-test-2, pg-test-3 Service {{ pg_cluster }}-{{ pg_role }} pg-test-primary, pg-test-replica, pg-test-offline Node Explicitly specified or borrowed from PG pg-test-1, pg-test-2, pg-test-3 Because Pigsty adopts a 1:1 exclusive deployment model for nodes and PG instances, by default the host node identifier borrows from the PG instance identifier (node_id_from_pg). You can also explicitly specify nodename to override, or disable nodename_overwrite to use the current default.\nSharding Identity Parameters When using multiple PostgreSQL clusters (sharding) to serve the same business, two additional identity parameters are used: pg_shard and pg_group.\nIn this case, this group of PostgreSQL clusters shares the same pg_shard name with their own pg_group numbers, like this Citus cluster:\nIn this case, pg_cluster cluster names are typically composed of: {{ pg_shard }}{{ pg_group }}, e.g., pg-citus0, pg-citus1, etc.\nall: children: pg-citus0: # citus shard 0 hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: { pg_cluster: pg-citus0 , pg_group: 0 } pg-citus1: # citus shard 1 hosts: { 10.10.10.11: { pg_seq: 1, pg_role: primary } } vars: { pg_cluster: pg-citus1 , pg_group: 1 } pg-citus2: # citus shard 2 hosts: { 10.10.10.12: { pg_seq: 1, pg_role: primary } } vars: { pg_cluster: pg-citus2 , pg_group: 2 } pg-citus3: # citus shard 3 hosts: { 10.10.10.13: { pg_seq: 1, pg_role: primary } } vars: { pg_cluster: pg-citus3 , pg_group: 3 } Pigsty provides dedicated monitoring dashboards for horizontal sharding clusters, making it easy to compare performance and load across shards, but this requires using the above entity naming convention.\nThere are also other identity parameters for special scenarios, such as pg_upstream for specifying backup clusters/cascading replication upstream, gp_role for Greenplum cluster identity, pg_exporters for external monitoring instances, pg_offline_query for offline query instances, etc. See PG_ID parameter docs.\nMonitoring Label System Pigsty provides an out-of-box monitoring system that uses the above identity parameters to identify various PostgreSQL entities.\npg_up{cls=\"pg-test\", ins=\"pg-test-1\", ip=\"10.10.10.11\", job=\"pgsql\"} pg_up{cls=\"pg-test\", ins=\"pg-test-2\", ip=\"10.10.10.12\", job=\"pgsql\"} pg_up{cls=\"pg-test\", ins=\"pg-test-3\", ip=\"10.10.10.13\", job=\"pgsql\"} For example, the cls, ins, ip labels correspond to cluster name, instance name, and node IP—the identifiers for these three core entities. They appear along with the job label in all native monitoring metrics collected by VictoriaMetrics and VictoriaLogs log streams.\nThe job name for collecting PostgreSQL metrics is fixed as pgsql; The job name for monitoring remote PG instances is fixed as pgrds. The job name for collecting PostgreSQL CSV logs is fixed as postgres; The job name for collecting pgbackrest logs is fixed as pgbackrest, other PG components collect logs via job: syslog.\nAdditionally, some entity identity labels appear in specific entity-related monitoring metrics, such as:\ndatname: Database name, if a metric belongs to a specific database. relname: Table name, if a metric belongs to a specific table. idxname: Index name, if a metric belongs to a specific index. funcname: Function name, if a metric belongs to a specific function. seqname: Sequence name, if a metric belongs to a specific sequence. query: Query fingerprint, if a metric belongs to a specific query. ","categories":["Concept"],"description":"Entity-Relationship model for PostgreSQL clusters in Pigsty, including E-R diagram, entity definitions, and naming conventions.","excerpt":"Entity-Relationship model for PostgreSQL clusters in Pigsty, including …","ref":"/docs/concept/model/pgsql/","tags":"","title":"E-R Model of PostgreSQL Cluster"},{"body":"","categories":"","description":"Everything about PostgreSQL - development, administration, internals, ecosystem, tools, extensions, and best practices","excerpt":"Everything about PostgreSQL - development, administration, internals, …","ref":"/blog/pg/","tags":"","title":"PostgreSQL"},{"body":"RTO (Recovery Time Objective) defines the maximum time required for the system to restore write capability when the primary fails.\nFor critical transaction systems where availability is paramount, the shortest possible RTO is typically required, such as under one minute.\nHowever, shorter RTO comes at a cost: increased false failover risk. Network jitter may be misinterpreted as a failure, leading to unnecessary failovers. For cross-datacenter/cross-region deployments, RTO requirements are typically relaxed (e.g., 1-2 minutes) to reduce false failover risk.\nTrade-offs The upper limit of unavailability during failover is controlled by the pg_rto parameter. Pigsty provides four preset RTO modes: fast, norm, safe, wide, each optimized for different network conditions and deployment scenarios. The default is norm mode (~45 seconds). You can also specify the RTO upper limit directly in seconds, and the system will automatically map to the closest mode.\nWhen the primary fails, the entire recovery process involves multiple phases: Patroni detects the failure, DCS lock expires, new primary election, promote execution, HAProxy detects the new primary. Reducing RTO means shortening the timeout for each phase, which makes the cluster more sensitive to network jitter, thereby increasing false failover risk.\nYou need to choose the appropriate mode based on actual network conditions, balancing recovery speed and false failover risk. The worse the network quality, the more conservative mode you should choose; the better the network quality, the more aggressive mode you can choose.\nflowchart LR A([Primary Failure]) --\u003e B{Patroni\u003cbr/\u003eDetected?} B --\u003e|PG Crash| C[Attempt Local Restart] B --\u003e|Node Down| D[Wait TTL Expiration] C --\u003e|Success| E([Local Recovery]) C --\u003e|Fail/Timeout| F[Release Leader Lock] D --\u003e F F --\u003e G[Replica Election] G --\u003e H[Execute Promote] H --\u003e I[HAProxy Detects] I --\u003e J([Service Restored]) style A fill:#dc3545,stroke:#b02a37,color:#fff style E fill:#198754,stroke:#146c43,color:#fff style J fill:#198754,stroke:#146c43,color:#fff Four Modes Pigsty provides four RTO modes to help users make trade-offs under different network conditions.\nName fast norm safe wide Use Case Same rack Same datacenter (default) Same region, cross-DC Cross-region/continent Network \u003c 1ms, very stable 1-5ms, normal 10-50ms, cross-DC 100-200ms, public network Target RTO 30s 45s 90s 150s False Failover Risk Higher Medium Lower Very Low Configuration pg_rto: fast pg_rto: norm pg_rto: safe pg_rto: wide fast: Same Rack/Switch Suitable for scenarios with extremely low network latency (\u003c 1ms) and very stable networks, such as same-rack or same-switch deployments Average RTO: 14s, worst case: 29s, TTL only 20s, check interval 5s Highest network quality requirements, any jitter may trigger failover, higher false failover risk norm: Same Datacenter (Default) Default mode, suitable for same-datacenter deployment, network latency 1-5ms, normal quality, reasonable packet loss rate Average RTO: 21s, worst case: 43s, TTL is 30s, provides reasonable tolerance window Balances recovery speed and stability, suitable for most production environments safe: Same Region, Cross-Datacenter Suitable for same-region/same-area cross-datacenter deployment, network latency 10-50ms, occasional jitter possible Average RTO: 43s, worst case: 91s, TTL is 60s, longer tolerance window Primary restart wait time is longer (60s), gives more local recovery opportunities, lower false failover risk wide: Cross-Region/Continent Suitable for cross-region or even cross-continent deployment, network latency 100-200ms, possible public-network-level packet loss Average RTO: 92s, worst case: 207s, TTL is 120s, very wide tolerance window Sacrifices recovery speed for extremely low false failover rate, suitable for geo-disaster recovery scenarios RTO Timeline Patroni / PG HA has two critical failure paths. For detailed RTO timing analysis, see: Active Failure Detection and Passive Lease Expiration.\nImplementation The four RTO modes differ in how the following 10 Patroni and HAProxy HA-related parameters are configured.\nComponent Parameter fast norm safe wide Description patroni ttl 20 30 60 120 Leader lock TTL (seconds) loop_wait 5 5 10 20 HA loop check interval (seconds) retry_timeout 5 10 20 30 DCS operation retry timeout (seconds) primary_start_timeout 15 25 45 95 Primary restart wait time (seconds) safety_margin 5 5 10 15 Watchdog safety margin (seconds) haproxy inter 1s 2s 3s 4s Normal state check interval fastinter 0.5s 1s 1.5s 2s State transition check interval downinter 1s 2s 3s 4s DOWN state check interval rise 3 3 3 3 Consecutive successes to mark UP fall 3 3 3 3 Consecutive failures to mark DOWN Patroni Parameters ttl: Leader lock TTL. Primary must renew within this time, otherwise lock expires and triggers election. Directly determines passive failure detection delay. loop_wait: Patroni main loop interval. Each loop performs one health check and state sync, affects failure discovery timeliness. retry_timeout: DCS operation retry timeout. During network partition, Patroni retries continuously within this period; after timeout, primary actively demotes to prevent split-brain. primary_start_timeout: Wait time for Patroni to attempt local restart after PG crash. After timeout, releases Leader lock and triggers failover. safety_margin: Watchdog safety margin. Ensures sufficient time to trigger system restart during failures, avoiding split-brain. HAProxy Parameters inter: Health check interval in normal state, used when service status is stable. fastinter: Check interval during state transition, uses shorter interval to accelerate confirmation when state change detected. downinter: Check interval in DOWN state, uses this interval to probe recovery after service marked DOWN. rise: Consecutive successes required to mark UP. After new primary comes online, must pass rise consecutive checks before receiving traffic. fall: Consecutive failures required to mark DOWN. Service must fail fall consecutive times before being marked DOWN. Key Constraint Patroni core constraint: Ensures primary can complete demotion before TTL expires, preventing split-brain.\nloop_wait+2×retry_timeout≤ttlloop\\_wait + 2 \\times retry\\_timeout \\leq ttlloop_wait+2×retry_timeout≤ttl Recommendations fast mode is suitable for scenarios with extremely high RTO requirements, but requires sufficiently good network quality (latency \u003c 1ms, very low packet loss). Recommended only for same-rack or same-switch deployments, and should be thoroughly tested in production before enabling.\nnorm mode (default) is Pigsty’s default configuration, sufficient for the vast majority of same-datacenter deployments. An average recovery time of 21 seconds is within acceptable range while providing a reasonable tolerance window to avoid false failovers from network jitter.\nsafe mode is suitable for same-city cross-datacenter deployments with higher network latency or occasional jitter. The longer tolerance window effectively prevents false failovers from network jitter, making it the recommended configuration for cross-datacenter disaster recovery.\nwide mode is suitable for cross-region or even cross-continent deployments with high network latency and possible public-network-level packet loss. In such scenarios, stability is more important than recovery speed, so an extremely wide tolerance window ensures very low false failover rate.\nScenario Recommended Mode Rationale Dev/Test environment fast Quick feedback, low impact from false failover Same-datacenter production norm Default choice, well-balanced Same-city active-active/cross-DC DR safe Tolerates network jitter, reduces false failover Geo-DR/cross-country deployment wide Adapts to high-latency public network, very low false failover rate Uncertain network quality safe Conservative choice, avoids false failover Typically you only need to set pg_rto to the mode name, and Pigsty will automatically configure Patroni and HAProxy parameters. For backward compatibility, Pigsty still supports configuring RTO directly in seconds, but the effect is equivalent to specifying norm mode.\nThe mode configuration actually loads the corresponding parameter set from pg_rto_plan. You can modify or override this configuration to implement custom RTO strategies.\npg_rto_plan: # [ttl, loop, retry, start, margin, inter, fastinter, downinter, rise, fall] fast: [ 20 ,5 ,5 ,15 ,5 ,'1s' ,'0.5s' ,'1s' ,3 ,3 ] # rto \u003c 30s norm: [ 30 ,5 ,10 ,25 ,5 ,'2s' ,'1s' ,'2s' ,3 ,3 ] # rto \u003c 45s safe: [ 60 ,10 ,20 ,45 ,10 ,'3s' ,'1.5s' ,'3s' ,3 ,3 ] # rto \u003c 90s wide: [ 120 ,20 ,30 ,95 ,15 ,'4s' ,'2s' ,'4s' ,3 ,3 ] # rto \u003c 150s ","categories":["Concept"],"description":"Trade-off analysis for RTO (Recovery Time Objective), finding the optimal balance between recovery speed and false failover risk.","excerpt":"Trade-off analysis for RTO (Recovery Time Objective), finding the …","ref":"/docs/concept/ha/rto/","tags":"","title":"RTO Trade-offs"},{"body":"Quick Start Pigsty uses declarative management: first define users in the inventory, then use bin/pgsql-user \u003ccls\u003e \u003cusername\u003e to create or modify.\npg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta pg_users: [{ name: dbuser_app, password: 'DBUser.App', pgbouncer: true }] # \u003c--- Define user list here! Script Playbook Example bin/pgsql-user \u003ccls\u003e \u003cusername\u003e # Create/modify \u003cusername\u003e user on \u003ccls\u003e cluster ./pgsql-user.yml -l pg-meta -e username=dbuser_app # Use playbook to create/modify user bin/pgsql-user pg-meta dbuser_app # Create/modify dbuser_app user on pg-meta cluster For complete user definition reference, see User Configuration. For access permissions, see ACL: Role Privileges.\nNote: User name cannot be modified after creation. To rename, delete the old user and create new one.\nAction Command Description Create User bin/pgsql-user \u003ccls\u003e \u003cuser\u003e Create new business user or role Modify User bin/pgsql-user \u003ccls\u003e \u003cuser\u003e Modify existing user properties Delete User bin/pgsql-user \u003ccls\u003e \u003cuser\u003e Safe delete user (requires state: absent) Create User Users defined in pg_users are auto-created during PostgreSQL cluster creation in the pg_user task.\nTo create a new user on an existing cluster, add user definition to all.children.\u003ccls\u003e.pg_users, then execute:\nScript Playbook Example bin/pgsql-user \u003ccls\u003e \u003cusername\u003e # Create user \u003cusername\u003e ./pgsql-user.yml -l \u003ccls\u003e -e username=\u003cusername\u003e # Use Ansible playbook bin/pgsql-user pg-meta dbuser_app # Create dbuser_app user in pg-meta cluster Example: Create business user dbuser_app\n#all.children.pg-meta.vars.pg_users: - name: dbuser_app password: DBUser.App pgbouncer: true roles: [dbrole_readwrite] comment: application user for myapp Result: Creates dbuser_app user on primary, sets password, grants dbrole_readwrite role, adds to Pgbouncer pool, reloads Pgbouncer config on all instances.\nRecommendation: Use playbook For manual user creation, you must ensure Pgbouncer user list sync yourself.\nModify User Same command as create - playbook is idempotent. When target user exists, Pigsty modifies properties to match config.\nScript Playbook Example bin/pgsql-user \u003ccls\u003e \u003cuser\u003e # Modify user \u003cuser\u003e properties ./pgsql-user.yml -l \u003ccls\u003e -e username=\u003cuser\u003e # Idempotent, can repeat bin/pgsql-user pg-meta dbuser_app # Modify dbuser_app to match config Immutable properties: User name can’t be modified after creation - requires delete and recreate.\nAll other properties can be modified. Common examples:\nModify password: Update password field. Logging is temporarily disabled during password change to prevent leakage.\n- name: dbuser_app password: NewSecretPassword # New password Modify privilege attributes: Configure boolean flags for user privileges.\n- name: dbuser_app superuser: false # Superuser (use carefully!) createdb: true # Allow CREATE DATABASE createrole: false # Allow CREATE ROLE inherit: true # Auto-inherit role privileges replication: false # Allow streaming replication bypassrls: false # Bypass row-level security connlimit: 50 # Connection limit, -1 unlimited Modify expiration: Use expire_in for relative expiry (N days), or expire_at for absolute date. expire_in takes priority and recalculates on each playbook run - good for temp users needing periodic renewal.\n- name: temp_user expire_in: 30 # Expires in 30 days (relative) - name: contractor_user expire_at: '2024-12-31' # Expires on date (absolute) - name: permanent_user expire_at: 'infinity' # Never expires Modify role membership: Use roles array with simple or extended format. Role membership is additive - won’t remove undeclared existing roles. Use state: absent to explicitly revoke.\n- name: dbuser_app roles: - dbrole_readwrite # Simple form: grant role - { name: dbrole_admin, admin: true } # With ADMIN OPTION - { name: pg_monitor, set: false } # PG16+: disallow SET ROLE - { name: old_role, state: absent } # Revoke role membership Manage user parameters: Use parameters dict for user-level params, generates ALTER USER ... SET. Use DEFAULT to reset.\n- name: dbuser_analyst parameters: work_mem: '256MB' statement_timeout: '5min' search_path: 'analytics,public' log_statement: DEFAULT # Reset to default Connection pool config: Set pgbouncer: true to add user to pool. Optional pool_mode and pool_connlimit.\n- name: dbuser_app pgbouncer: true # Add to pool pool_mode: transaction # Pool mode pool_connlimit: 50 # Max user connections Delete User To delete a user, set state to absent and execute:\nScript Playbook Example bin/pgsql-user \u003ccls\u003e \u003cuser\u003e # Delete \u003cuser\u003e (config must have state: absent) ./pgsql-user.yml -l \u003ccls\u003e -e username=\u003cuser\u003e # Use Ansible playbook bin/pgsql-user pg-meta dbuser_old # Delete dbuser_old (config has state: absent) Config example:\npg_users: - name: dbuser_old state: absent Deletion process: Uses pg-drop-role script for safe deletion; auto-disables login and terminates connections; transfers database/tablespace ownership to postgres; handles object ownership in all databases; revokes all role memberships; creates audit log; removes from Pgbouncer and reloads config.\nProtection: These system users cannot be deleted and are auto-skipped: postgres (superuser), replicator (or pg_replication_username), dbuser_dba (or pg_admin_username), dbuser_monitor (or pg_monitor_username).\nSafe Deletion Pigsty uses pg-drop-role for safe deletion, auto-handling owned databases, tablespaces, schemas, tables, etc. Terminates active connections, transfers ownership to postgres, creates audit log at /tmp/pg_drop_role_\u003cuser\u003e_\u003ctimestamp\u003e.log. No manual dependency handling needed.\nManual Deletion For manual user deletion, use pg-drop-role script directly:\n# Check dependencies (read-only) pg-drop-role dbuser_old --check # Preview deletion (don't execute) pg-drop-role dbuser_old --dry-run -v # Delete user, transfer objects to postgres pg-drop-role dbuser_old # Force delete (terminate connections) pg-drop-role dbuser_old --force # Delete user, transfer to specific user pg-drop-role dbuser_old dbuser_new Common Use Cases Common user configuration examples:\nBasic business user\n- name: dbuser_app password: DBUser.App pgbouncer: true roles: [dbrole_readwrite] comment: application user Read-only user\n- name: dbuser_readonly password: DBUser.Readonly pgbouncer: true roles: [dbrole_readonly] Admin user (can execute DDL)\n- name: dbuser_admin password: DBUser.Admin pgbouncer: true pool_mode: session roles: [dbrole_admin] parameters: log_statement: 'all' Temp user (expires in 30 days)\n- name: temp_contractor password: TempPassword expire_in: 30 roles: [dbrole_readonly] Role (no login, for permission grouping)\n- name: custom_role login: false comment: custom role for special permissions User with advanced role options (PG16+)\n- name: dbuser_special password: DBUser.Special pgbouncer: true roles: - dbrole_readwrite - { name: dbrole_admin, admin: true } - { name: pg_monitor, set: false } - { name: pg_execute_server_program, inherit: false } Query Users Common SQL queries for user info:\nList all users\nSELECT rolname, rolsuper, rolinherit, rolcreaterole, rolcreatedb, rolcanlogin, rolreplication, rolbypassrls, rolconnlimit, rolvaliduntil FROM pg_roles WHERE rolname NOT LIKE 'pg_%' ORDER BY rolname; View user role membership\nSELECT r.rolname AS member, g.rolname AS role, m.admin_option, m.set_option, m.inherit_option FROM pg_auth_members m JOIN pg_roles r ON r.oid = m.member JOIN pg_roles g ON g.oid = m.roleid WHERE r.rolname = 'dbuser_app'; View user-level parameters\nSELECT rolname, setconfig FROM pg_db_role_setting s JOIN pg_roles r ON r.oid = s.setrole WHERE s.setdatabase = 0; View expiring users\nSELECT rolname, rolvaliduntil, rolvaliduntil - CURRENT_TIMESTAMP AS time_remaining FROM pg_roles WHERE rolvaliduntil IS NOT NULL AND rolvaliduntil \u003c CURRENT_TIMESTAMP + INTERVAL '30 days' ORDER BY rolvaliduntil; Connection Pool Management Connection pool params in user definitions are applied to Pgbouncer when creating/modifying users.\nUsers with pgbouncer: true are added to /etc/pgbouncer/userlist.txt. User-level pool params (pool_mode, pool_connlimit) are configured via /etc/pgbouncer/useropts.txt.\nUse postgres OS user with pgb alias to access Pgbouncer admin database. For more pool management, see Pgbouncer Management.\n","categories":["Task"],"description":"User management - create, modify, delete users, manage role membership, connection pool config","excerpt":"User management - create, modify, delete users, manage role …","ref":"/docs/pgsql/admin/user/","tags":"","title":"Managing PostgreSQL Users"},{"body":"","categories":["Database"],"description":"Articles about the database industry - trends, news, research, concepts, and best practices","excerpt":"Articles about the database industry - trends, news, research, …","ref":"/blog/db/","tags":"","title":"Database"},{"body":"Quick Start Pigsty uses declarative management: first define databases in the inventory, then use bin/pgsql-db \u003ccls\u003e \u003cdbname\u003e to create or modify.\npg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta pg_databases: [{ name: some_db }] # \u003c--- Define database list here! Script Playbook Example bin/pgsql-db \u003ccls\u003e \u003cdbname\u003e # Create/modify \u003cdbname\u003e database on \u003ccls\u003e cluster ./pgsql-db.yml -l pg-meta -e dbname=some_db # Use playbook to create/modify database bin/pgsql-db pg-meta some_db # Create/modify some_db database on pg-meta cluster For complete database definition reference, see Database Configuration. For access permissions, see ACL: Database Privileges.\nNote: Some parameters can only be specified at creation time. Modifying these requires recreating the database (use state: recreate).\nAction Command Description Create Database bin/pgsql-db \u003ccls\u003e \u003cdb\u003e Create new business database Modify Database bin/pgsql-db \u003ccls\u003e \u003cdb\u003e Modify existing database properties Delete Database bin/pgsql-db \u003ccls\u003e \u003cdb\u003e Delete database (requires state: absent) Rebuild Database bin/pgsql-db \u003ccls\u003e \u003cdb\u003e Drop and recreate (requires state: recreate) Clone Database bin/pgsql-db \u003ccls\u003e \u003cdb\u003e Clone database using template Create Database Databases defined in pg_databases are auto-created during PostgreSQL cluster creation in the pg_db task.\nTo create a new database on an existing cluster, add database definition to all.children.\u003ccls\u003e.pg_databases, then execute:\nScript Playbook Example bin/pgsql-db \u003ccls\u003e \u003cdbname\u003e # Create database \u003cdbname\u003e ./pgsql-db.yml -l \u003ccls\u003e -e dbname=\u003cdbname\u003e # Use Ansible playbook bin/pgsql-db pg-meta myapp # Create myapp database in pg-meta cluster Example: Create business database myapp\n#all.children.pg-meta.vars.pg_databases: - name: myapp owner: dbuser_myapp schemas: [app] extensions: - { name: pg_trgm } - { name: btree_gin } comment: my application database Result: Creates myapp database on primary, sets owner to dbuser_myapp, creates app schema, enables pg_trgm and btree_gin extensions. Database is auto-added to Pgbouncer pool and registered as Grafana datasource.\nRecommendation: Use playbook For manual database creation, you must ensure Pgbouncer pool and Grafana datasource sync yourself.\nModify Database Same command as create - playbook is idempotent when no baseline SQL is defined.\nWhen target database exists, Pigsty modifies properties to match config. However, some properties can only be set at creation.\nScript Playbook Example bin/pgsql-db \u003ccls\u003e \u003cdb\u003e # Modify database \u003cdb\u003e properties ./pgsql-db.yml -l \u003ccls\u003e -e dbname=\u003cdb\u003e # Idempotent, can repeat bin/pgsql-db pg-meta myapp # Modify myapp database to match config Immutable properties: These can’t be modified after creation, require state: recreate:\nname (database name), template, strategy (clone strategy) encoding, locale/lc_collate/lc_ctype, locale_provider/icu_locale/icu_rules/builtin_locale All other properties can be modified. Common examples:\nModify owner: Update owner field, executes ALTER DATABASE ... OWNER TO and grants permissions.\n- name: myapp owner: dbuser_new_owner # New owner Modify connection limit: Use connlimit to limit max connections.\n- name: myapp connlimit: 100 # Max 100 connections Revoke public connect: Setting revokeconn: true revokes PUBLIC CONNECT privilege, allowing only owner, DBA, monitor, and replication users.\n- name: myapp owner: dbuser_myapp revokeconn: true # Revoke PUBLIC CONNECT Manage parameters: Use parameters dict for database-level params, generates ALTER DATABASE ... SET. Use special value DEFAULT to reset.\n- name: myapp parameters: work_mem: '256MB' maintenance_work_mem: '512MB' statement_timeout: '30s' search_path: DEFAULT # Reset to default Manage schemas: Use schemas array with simple or extended format. Use state: absent to drop (CASCADE).\n- name: myapp schemas: - app # Simple form - { name: core, owner: dbuser_myapp } # Specify owner - { name: deprecated, state: absent } # Drop schema Manage extensions: Use extensions array with simple or extended format. Use state: absent to uninstall (CASCADE).\n- name: myapp extensions: - postgis # Simple form - { name: vector, schema: public } # Specify schema - { name: pg_trgm, state: absent } # Uninstall extension CASCADE Warning Dropping schemas or uninstalling extensions uses CASCADE, deleting all dependent objects. Understand impact before executing.\nConnection pool config: By default all databases are added to Pgbouncer. Configure pgbouncer, pool_mode, pool_size, pool_reserve, pool_connlimit.\n- name: myapp pgbouncer: true # Add to pool (default true) pool_mode: transaction # Pool mode: transaction/session/statement pool_size: 64 # Default pool size pool_connlimit: 100 # Max database connections Delete Database To delete a database, set state to absent and execute:\nScript Playbook Example bin/pgsql-db \u003ccls\u003e \u003cdb\u003e # Delete \u003cdb\u003e (config must have state: absent) ./pgsql-db.yml -l \u003ccls\u003e -e dbname=\u003cdb\u003e # Use Ansible playbook bin/pgsql-db pg-meta olddb # Delete olddb (config has state: absent) Config example:\npg_databases: - name: olddb state: absent Deletion process: If is_template: true, first executes ALTER DATABASE ... IS_TEMPLATE false; uses DROP DATABASE ... WITH (FORCE) (PG13+) to force drop and terminate all connections; removes from Pgbouncer pool; unregisters from Grafana datasource.\nProtection: System databases postgres, template0, template1 cannot be deleted. Deletion only runs on primary - streaming replication syncs to replicas.\nDanger Warning Database deletion is irreversible - permanently deletes all data. Before executing: ensure recent backup exists, confirm no business uses the database, notify stakeholders. Pigsty is not responsible for any data loss from database deletion. Use at your own risk.\nRebuild Database recreate state rebuilds database (drop then create):\nScript Playbook Example bin/pgsql-db \u003ccls\u003e \u003cdb\u003e # Rebuild \u003cdb\u003e (config must have state: recreate) ./pgsql-db.yml -l \u003ccls\u003e -e dbname=\u003cdb\u003e # Use Ansible playbook bin/pgsql-db pg-meta testdb # Rebuild testdb (config has state: recreate) Config example:\npg_databases: - name: testdb state: recreate owner: dbuser_test baseline: test_init.sql # Execute after rebuild Use cases: Test environment reset, clear dev database, modify immutable properties (encoding, locale), restore to initial state.\nDifference from manual DROP + CREATE: Single command; auto-preserves Pgbouncer and Grafana config; auto-loads baseline init script.\nClone Database Clone PostgreSQL databases using PG template mechanism. During cloning, no active connections to template database are allowed.\nScript Playbook Example bin/pgsql-db \u003ccls\u003e \u003cdb\u003e # Clone \u003cdb\u003e (config must specify template) ./pgsql-db.yml -l \u003ccls\u003e -e dbname=\u003cdb\u003e # Use Ansible playbook bin/pgsql-db pg-meta meta_dev # Clone meta_dev (config has template: meta) Config example:\npg_databases: - name: meta # Source database - name: meta_dev template: meta # Use meta as template strategy: FILE_COPY # PG15+ clone strategy, instant on PG18 Instant Clone (PG18+): If using PostgreSQL 18+, Pigsty defaults file_copy_method. With strategy: FILE_COPY, database clone completes in ~200ms without copying data files. E.g., cloning 30GB database: normal takes 18s, instant takes 200ms.\nManual clone: Ensure all connections to template are terminated:\nSELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = 'meta'; CREATE DATABASE meta_dev TEMPLATE meta STRATEGY FILE_COPY; Limitations: Instant clone only available on supported filesystems (xfs, brtfs, zfs, apfs); don’t use postgres database as template; in high-concurrency environments, all template connections must be cleared within clone window (~200ms).\nConnection Pool Management Connection pool params in database definitions are applied to Pgbouncer when creating/modifying databases.\nBy default all databases are added to Pgbouncer pool (pgbouncer: true). Databases are added to /etc/pgbouncer/database.txt. Database-level pool params (pool_mode, pool_size, etc.) are configured via this file.\nUse postgres OS user with pgb alias to access Pgbouncer admin database. For more pool management, see Pgbouncer Management.\n","categories":["Task"],"description":"Database management - create, modify, delete, rebuild, and clone databases using templates","excerpt":"Database management - create, modify, delete, rebuild, and clone …","ref":"/docs/pgsql/admin/db/","tags":"","title":"Managing PostgreSQL Databases"},{"body":"Patroni failures can be classified into 10 categories by failure target, and further consolidated into five categories based on detection path, which are detailed in this section.\n# Failure Scenario Description Final Path 1 PG process crash crash, OOM killed Active Detection 2 PG connection refused max_connections Active Detection 3 PG zombie Process alive but unresponsive Active Detection (timeout) 4 Patroni process crash kill -9, OOM Passive Detection 5 Patroni zombie Process alive but stuck Watchdog 6 Node down Power outage, hardware failure Passive Detection 7 Node zombie IO hang, CPU starvation Watchdog 8 Primary ↔ DCS network failure Firewall, switch failure Network Partition 9 Storage failure Disk failure, disk full, mount failure Active Detection or Watchdog 10 Manual switchover Switchover/Failover Manual Trigger However, for RTO calculation purposes, all failures ultimately converge to two paths. This section explores the upper bound, lower bound, and average RTO for these two scenarios.\nPassive election triggered after Patroni loses contact Patroni actively detects failure and triggers switchover flowchart LR A([Primary Failure]) --\u003e B{Patroni\u003cbr/\u003eDetected?} B --\u003e|PG Crash| C[Attempt Local Restart] B --\u003e|Node Down| D[Wait TTL Expiration] C --\u003e|Success| E([Local Recovery]) C --\u003e|Fail/Timeout| F[Release Leader Lock] D --\u003e F F --\u003e G[Replica Election] G --\u003e H[Execute Promote] H --\u003e I[HAProxy Detects] I --\u003e J([Service Restored]) style A fill:#dc3545,stroke:#b02a37,color:#fff style E fill:#198754,stroke:#146c43,color:#fff style J fill:#198754,stroke:#146c43,color:#fff ","categories":["Concept"],"description":"Detailed analysis of worst-case, best-case, and average RTO calculation logic and results across three classic failure detection/recovery paths","excerpt":"Detailed analysis of worst-case, best-case, and average RTO …","ref":"/docs/concept/ha/failure/","tags":"","title":"Failure Model"},{"body":"PostgreSQL instance-level monitoring dashboards, including:\nPGSQL Instance: Main dashboard for a single PGSQL instance PGRDS Instance: RDS version of PGSQL Instance, focusing on PostgreSQL-native metrics PGCAT Instance: Instance info retrieved directly from database catalog PGSQL Persist: Persistence metrics: WAL, XID, checkpoint, archive, IO PGSQL Proxy: Detailed metrics for a single HAProxy load balancer PGSQL Pgbouncer: Metrics overview for a single Pgbouncer connection pooler PGSQL Session: Session and active/idle time metrics for a single instance PGSQL Xacts: Transaction, lock, TPS/QPS related metrics PGSQL Exporter: Self-monitoring metrics for Postgres and Pgbouncer exporters ","categories":"","description":"PostgreSQL instance-level monitoring dashboards","excerpt":"PostgreSQL instance-level monitoring dashboards","ref":"/docs/pgsql/dashboard/instance/","tags":"","title":"Instance"},{"body":"olap.yml is optimized for online analytical processing (OLAP). Designed for 4-128 core CPUs with support for large queries, high parallelism, relaxed timeouts, and aggressive vacuum.\nPair with node_tune = olap for OS-level tuning.\nUse Cases OLAP template is ideal for:\nData warehouses: Historical data storage, multidimensional analysis BI reports: Complex report queries, dashboard data sources ETL processing: Data extraction, transformation, loading Data analysis: Ad-hoc queries, data exploration HTAP mixed workloads: Analytical replicas Workload characteristics:\nComplex queries (seconds to minutes) Low concurrent connections (tens to hundreds) Read-intensive, writes typically batch operations Throughput-sensitive, tolerates higher latency Scans large data volumes Usage Specify pg_conf = olap.yml in cluster definition:\npg-olap: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } 10.10.10.12: { pg_seq: 2, pg_role: replica } vars: pg_cluster: pg-olap pg_conf: olap.yml # PostgreSQL analytics template node_tune: olap # OS analytics tuning Use olap.yml template for dedicated offline replicas:\npg-mixed: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } 10.10.10.12: { pg_seq: 2, pg_role: replica } 10.10.10.13: { pg_seq: 3, pg_role: offline, pg_conf: olap.yml } # offline analytics replica vars: pg_cluster: pg-mixed pg_conf: oltp.yml # primary and online replicas use OLTP node_tune: oltp # OS OLTP tuning Parameter Details Connection Management max_connections: 500 superuser_reserved_connections: 10 OLAP scenarios typically don’t need many connections; 500 is sufficient for most analytical workloads.\nMemory Config OLAP template uses more aggressive memory allocation:\nParameter Formula Description shared_buffers mem × pg_shared_buffer_ratio Default ratio 0.25 maintenance_work_mem shared_buffers × 50% Faster index creation and VACUUM work_mem 64MB - 8GB Larger sort/hash memory effective_cache_size total mem - shared_buffers Estimated cache memory work_mem calculation (differs from OLTP):\nwork_mem = min(max(shared_buffers / max_connections, 64MB), 8GB) Larger work_mem allows bigger sort and hash operations in memory, avoiding disk spill.\nLocks \u0026 Transactions max_locks_per_transaction: 2-4x maxconn # OLTP: 1-2x OLAP queries may involve more tables (partitions, many JOINs), requiring more lock slots.\nParallel Query OLAP template aggressively enables parallel queries:\nmax_worker_processes: cpu + 12 (min 20) # OLTP: cpu + 8 max_parallel_workers: 80% × cpu (min 2) # OLTP: 50% max_parallel_workers_per_gather: 50% × cpu # OLTP: 20% (max 8) max_parallel_maintenance_workers: 33% × cpu Parallel cost estimates use defaults to favor parallel plans:\n# parallel_setup_cost: 1000 # default, not doubled # parallel_tuple_cost: 0.1 # default, not doubled Partition-wise optimization enabled:\nenable_partitionwise_join: on # smart partition JOIN enable_partitionwise_aggregate: on # smart partition aggregation IO Config (PG17+) io_workers: 50% × cpu (4-32) # OLTP: 25% (4-16) More IO workers support parallel large table scans.\nWAL Config min_wal_size: disk/20 (max 200GB) max_wal_size: disk/5 (max 2000GB) max_slot_wal_keep_size: disk×3/10 (max 3000GB) temp_file_limit: disk/5 (max 2000GB) # OLTP: disk/20 Larger temp_file_limit allows bigger intermediate results to spill to disk.\nVacuum Config OLAP template uses aggressive vacuum settings:\nvacuum_cost_delay: 10ms # OLTP: 20ms, faster vacuum vacuum_cost_limit: 10000 # OLTP: 2000, more work per round autovacuum_max_workers: 3 autovacuum_naptime: 1min autovacuum_vacuum_scale_factor: 0.08 autovacuum_analyze_scale_factor: 0.04 Analytical databases often have bulk writes requiring aggressive vacuum to reclaim space.\nQuery Optimization random_page_cost: 1.1 effective_io_concurrency: 200 default_statistics_target: 1000 # OLTP: 400, more precise stats Higher default_statistics_target provides more accurate query plans, crucial for complex analytics.\nLogging \u0026 Monitoring log_min_duration_statement: 1000 # OLTP: 100ms, relaxed threshold log_statement: ddl log_checkpoints: on log_lock_waits: on log_temp_files: 1024 log_autovacuum_min_duration: 1s track_io_timing: on track_cost_delay_timing: on # PG18+, track vacuum cost delay track_functions: all track_activity_query_size: 8192 Client Timeouts deadlock_timeout: 50ms idle_in_transaction_session_timeout: 0 # OLTP: 10min, disabled Analytical queries may need to hold transactions for extended periods, so idle timeout is disabled.\nKey Differences from OLTP Parameter OLAP OLTP Reason max_connections 500 500-1000 Fewer analytical connections work_mem limit 8GB 1GB Support larger in-memory sorts maintenance_work_mem 50% buffer 25% buffer Faster index creation max_locks_per_transaction 2-4x 1-2x More tables in queries max_parallel_workers 80% cpu 50% cpu Aggressive parallelism max_parallel_workers_per_gather 50% cpu 20% cpu Aggressive parallelism parallel_setup_cost 1000 2000 Default, encourages parallel parallel_tuple_cost 0.1 0.2 Default, encourages parallel enable_partitionwise_join on off Partition optimization enable_partitionwise_aggregate on off Partition optimization vacuum_cost_delay 10ms 20ms Aggressive vacuum vacuum_cost_limit 10000 2000 Aggressive vacuum temp_file_limit 1/5 disk 1/20 disk Allow larger temp files io_workers 50% cpu 25% cpu More parallel IO log_min_duration_statement 1000ms 100ms Relaxed slow query threshold default_statistics_target 1000 400 More precise stats idle_in_transaction_session_timeout Disabled 10min Allow long transactions Performance Tuning Tips With TimescaleDB OLAP template works great with TimescaleDB:\npg-timeseries: vars: pg_conf: olap.yml pg_libs: 'timescaledb, pg_stat_statements, auto_explain' pg_extensions: - timescaledb With pg_duckdb For ultimate analytical performance, combine with pg_duckdb:\npg-analytics: vars: pg_conf: olap.yml pg_libs: 'pg_duckdb, pg_stat_statements, auto_explain' Columnar Storage Consider columnar storage extensions:\npg_extensions: - citus_columnar # or pg_mooncake Resource Isolation For mixed workloads, isolate analytics to dedicated replicas:\npg-mixed: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } # OLTP writes 10.10.10.12: { pg_seq: 2, pg_role: replica } # OLTP reads 10.10.10.13: { pg_seq: 3, pg_role: offline } # OLAP analytics vars: pg_cluster: pg-mixed Monitoring Metrics Focus on these metrics:\nQuery time: Long query execution time distribution Parallelism: Parallel worker utilization Temp files: Temp file size and count Disk IO: Sequential and index scan IO volume Cache hit ratio: shared_buffers and OS cache hit rates References pg_conf: PostgreSQL config template selection node_tune: OS tuning template, should match pg_conf OLTP Template: Transaction template comparison CRIT Template: Critical business template comparison TINY Template: Micro instance template comparison Offline Replica: Dedicated analytics instances ","categories":["Reference"],"description":"PostgreSQL config template optimized for online analytical processing workloads","excerpt":"PostgreSQL config template optimized for online analytical processing …","ref":"/docs/pgsql/template/olap/","tags":"","title":"OLAP Template"},{"body":"crit.yml is optimized for critical/financial workloads. Designed for 4-128 core CPUs with forced sync replication, data checksums, full audit logging, and strict security. Trades performance for maximum data safety.\nPair with node_tune = crit for OS-level tuning, optimizing dirty page management.\nUse Cases CRIT template is ideal for:\nFinancial transactions: Bank transfers, payment settlement, securities trading Core accounting: General ledger systems, accounting systems Compliance audit: Businesses requiring complete operation records Critical business: Any scenario that cannot tolerate data loss Requirements:\nZero data loss (RPO = 0) Data integrity verification Complete audit logs Strict security policies Acceptable performance trade-offs Usage Specify pg_conf = crit.yml in cluster definition:\npg-finance: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } 10.10.10.12: { pg_seq: 2, pg_role: replica } 10.10.10.13: { pg_seq: 3, pg_role: replica } vars: pg_cluster: pg-finance pg_conf: crit.yml # PostgreSQL critical template node_tune: crit # OS critical tuning Recommendation: Critical clusters should have at least 3 nodes to maintain sync replication when one node fails.\nCore Features Forced Sync Replication CRIT template forces sync replication regardless of pg_rpo setting:\nsynchronous_mode: true # forced on, ignores pg_rpo Every transaction commit waits for at least one replica confirmation, ensuring RPO = 0 (zero data loss).\nCost: Write latency increases (typically 1-5ms depending on network).\nForced Data Checksums CRIT template forces data checksums regardless of pg_checksum setting:\ninitdb: - data-checksums # forced on, ignores pg_checksum Data checksums detect silent disk corruption (bit rot), critical for financial data.\nDisabled Parallel Query CRIT template disables parallel query gather operations:\nmax_parallel_workers_per_gather: 0 # parallel queries disabled Parallel cost estimates are also increased:\nparallel_setup_cost: 2000 parallel_tuple_cost: 0.2 min_parallel_table_scan_size: 16MB min_parallel_index_scan_size: 1024 Reason: Parallel queries may cause unstable latency. For latency-sensitive financial transactions, predictable stable performance is more important.\nParameter Details Connection Management max_connections: 500/1000 # depends on pgbouncer usage superuser_reserved_connections: 10 Same as OLTP template.\nMemory Config Parameter Formula Description shared_buffers mem × pg_shared_buffer_ratio Default ratio 0.25 maintenance_work_mem shared_buffers × 25% For VACUUM, CREATE INDEX work_mem 64MB - 1GB Same as OLTP effective_cache_size total mem - shared_buffers Estimated cache memory WAL Config (Key Differences) wal_writer_delay: 10ms # OLTP: 20ms, more frequent flush wal_writer_flush_after: 0 # OLTP: 1MB, immediate flush, no buffer idle_replication_slot_timeout: 3d # OLTP: 7d, stricter slot cleanup wal_writer_flush_after: 0 ensures every WAL write flushes to disk immediately, minimizing data loss risk.\nReplication Config (PG15-) vacuum_defer_cleanup_age: 500000 # PG15 and below only Preserves 500K recent transactions from vacuum cleanup, providing more catchup buffer for replicas.\nAudit Logging (Key Differences) CRIT template enables full connection audit:\nPostgreSQL 18+:\nlog_connections: 'receipt,authentication,authorization' PostgreSQL 17 and below:\nlog_connections: 'on' log_disconnections: 'on' Records complete connection lifecycle:\nConnection receipt Authentication process Authorization result Disconnection Query Logging log_min_duration_statement: 100 # log queries \u003e 100ms log_statement: ddl # log all DDL track_activity_query_size: 32768 # OLTP: 8192, capture full queries 32KB track_activity_query_size ensures capturing complete long query text.\nStatistics Tracking track_io_timing: on track_cost_delay_timing: on # PG18+, track vacuum cost delay track_functions: all track_activity_query_size: 32768 Client Timeouts (Key Differences) idle_in_transaction_session_timeout: 1min # OLTP: 10min, stricter 1-minute idle transaction timeout quickly releases zombie transactions holding locks.\nExtension Config shared_preload_libraries: '$libdir/passwordcheck, pg_stat_statements, auto_explain' Note: CRIT template loads passwordcheck by default, enforcing password complexity.\nKey Differences from OLTP Parameter CRIT OLTP Reason synchronous_mode Forced true Depends on pg_rpo Zero data loss data-checksums Forced on Optional Data integrity max_parallel_workers_per_gather 0 20% cpu Stable latency wal_writer_delay 10ms 20ms More frequent flush wal_writer_flush_after 0 1MB Immediate flush idle_replication_slot_timeout 3d 7d Stricter cleanup idle_in_transaction_session_timeout 1min 10min Quick lock release track_activity_query_size 32KB 8KB Complete query capture log_connections Full logging Auth only Audit compliance log_disconnections on off Audit compliance passwordcheck Enabled Not enabled Password security vacuum_defer_cleanup_age 500000 0 Replica catchup buffer Performance Impact Using CRIT template has these impacts:\nIncreased Write Latency Sync replication adds 1-5ms write latency (network-dependent):\nAsync replication: commit -\u003e local flush -\u003e return to client Sync replication: commit -\u003e local flush -\u003e wait replica confirm -\u003e return to client Reduced Write Throughput Due to replica confirmation wait, write TPS may drop 10-30%.\nMore Stable Query Latency With parallel queries disabled, query latency is more predictable without parallel startup overhead variance.\nSlightly Increased Resource Overhead More frequent WAL flushes and complete audit logs add extra IO overhead.\nHA Configuration Minimum Recommended Setup pg-critical: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } 10.10.10.12: { pg_seq: 2, pg_role: replica } 10.10.10.13: { pg_seq: 3, pg_role: replica } vars: pg_cluster: pg-critical pg_conf: crit.yml # PostgreSQL critical template node_tune: crit # OS critical tuning 3-node setup ensures sync replication continues when one node fails.\nCross-DC Deployment For financial-grade disaster recovery:\npg-critical: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary, pg_weight: 100 } # DC A 10.10.10.12: { pg_seq: 2, pg_role: replica, pg_weight: 100 } # DC A 10.20.10.13: { pg_seq: 3, pg_role: replica, pg_weight: 0 } # DC B (standby) vars: pg_cluster: pg-critical pg_conf: crit.yml # PostgreSQL critical template node_tune: crit # OS critical tuning Quorum Commit For higher consistency, configure multiple sync replicas:\n$ pg edit-config pg-critical synchronous_mode: true synchronous_node_count: 2 # require 2 replica confirmations Security Hardening Tips Password Policy CRIT template has passwordcheck enabled; further configure:\n-- Set password encryption ALTER SYSTEM SET password_encryption = 'scram-sha-256'; Audit Extension Consider pgaudit for detailed auditing:\npg_libs: 'pg_stat_statements, auto_explain, pgaudit' pg_parameters: pgaudit.log: 'ddl, role, write' Network Isolation Ensure database network is isolated; use HBA rules to restrict access.\nMonitoring Metrics For critical clusters, focus on:\nReplication lag: Sync lag should be near zero Transaction commit time: p99 latency Lock waits: Long lock waits may impact business Checkpoints: Checkpoint duration and frequency WAL generation rate: Predict disk space needs References pg_conf: PostgreSQL config template selection node_tune: OS tuning template, should match pg_conf pg_rpo: Recovery point objective parameter OLTP Template: Transaction template comparison OLAP Template: Analytics template comparison TINY Template: Micro instance template comparison Sync Standby: Sync replication configuration Quorum Commit: Higher consistency level ","categories":["Reference"],"description":"PostgreSQL config template optimized for critical/financial workloads with data safety and audit compliance","excerpt":"PostgreSQL config template optimized for critical/financial workloads …","ref":"/docs/pgsql/template/crit/","tags":"","title":"CRIT Template"},{"body":"PostgreSQL database-level monitoring dashboards, including:\nPGSQL Database: Main dashboard for a single PGSQL database PGCAT Database: Database info retrieved directly from database catalog PGSQL Tables: Table/index access metrics within a single database PGSQL Table: Detailed info for a single table (QPS/RT/index/sequence…) PGCAT Table: Detailed table info from database catalog PGSQL Query: Detailed info for a query type (QPS/RT) PGCAT Query: Query details from database catalog PGCAT Locks: Activity and lock wait info from database catalog PGCAT Schema: Schema info from database catalog ","categories":"","description":"PostgreSQL database-level monitoring dashboards","excerpt":"PostgreSQL database-level monitoring dashboards","ref":"/docs/pgsql/dashboard/database/","tags":"","title":"Database"},{"body":"The ETCD module organizes ETCD in production as clusters—logical entities composed of a group of ETCD instances associated through the Raft consensus protocol.\nEach cluster is an autonomous distributed key-value storage unit consisting of at least one ETCD instance, exposing service capabilities through client ports.\nThere are three core entities in Pigsty’s ETCD module:\nCluster: An autonomous ETCD service unit serving as the top-level namespace for other entities. Instance: A single ETCD server process running on a node, participating in Raft consensus. Node: A hardware resource abstraction running Linux + Systemd environment, implicitly declared. Compared to PostgreSQL clusters, the ETCD cluster model is simpler, without Services or complex Role distinctions. All ETCD instances are functionally equivalent, electing a Leader through the Raft protocol while others become Followers. During scale-out intermediate states, non-voting Learner instance members are also allowed.\nExamples Let’s look at a concrete example with a three-node ETCD cluster:\netcd: hosts: 10.10.10.10: { etcd_seq: 1 } 10.10.10.11: { etcd_seq: 2 } 10.10.10.12: { etcd_seq: 3 } vars: etcd_cluster: etcd The above config fragment defines a three-node ETCD cluster with these related entities:\nCluster Description etcd ETCD 3-node HA cluster Instance Description etcd-1 ETCD instance #1 etcd-2 ETCD instance #2 etcd-3 ETCD instance #3 Node Description 10.10.10.10 Node #1, hosts etcd-1 instance 10.10.10.11 Node #2, hosts etcd-2 instance 10.10.10.12 Node #3, hosts etcd-3 instance Identity Parameters Pigsty uses the ETCD parameter group to assign deterministic identities to each ETCD module entity. Two parameters are required:\nParameter Type Level Description Format etcd_cluster string Cluster ETCD cluster name, required Valid DNS name, defaults to fixed etcd etcd_seq int Instance ETCD instance number, required Natural number, starting from 1, unique within cluster With cluster name defined at cluster level and instance number assigned at instance level, Pigsty automatically generates unique identifiers for each entity based on rules:\nEntity Generation Rule Example Instance {{ etcd_cluster }}-{{ etcd_seq }} etcd-1, etcd-2, etcd-3 The ETCD module does not assign additional identity to host nodes; nodes are identified by their existing hostname or IP address.\nPorts \u0026 Protocols Each ETCD instance listens on the following two ports:\nPort Parameter Purpose 2379 etcd_port Client port, accessed by Patroni, vip-manager, etc. 2380 etcd_peer_port Peer communication port, used for Raft consensus ETCD clusters enable TLS encrypted communication by default and use RBAC authentication mechanism. Clients need correct certificates and passwords to access ETCD services.\nCluster Size As a distributed coordination service, ETCD cluster size directly affects availability, requiring more than half (quorum) of nodes to be alive to maintain service.\nCluster Size Quorum Fault Tolerance Use Case 1 node 1 0 Dev, test, demo 3 nodes 2 1 Small-medium production 5 nodes 3 2 Large-scale production Therefore, even-numbered ETCD clusters are meaningless, and clusters over five nodes are uncommon. Typical sizes are single-node, three-node, and five-node.\nMonitoring Label System Pigsty provides an out-of-box monitoring system that uses the above identity parameters to identify various ETCD entities.\netcd_up{cls=\"etcd\", ins=\"etcd-1\", ip=\"10.10.10.10\", job=\"etcd\"} etcd_up{cls=\"etcd\", ins=\"etcd-2\", ip=\"10.10.10.11\", job=\"etcd\"} etcd_up{cls=\"etcd\", ins=\"etcd-3\", ip=\"10.10.10.12\", job=\"etcd\"} For example, the cls, ins, ip labels correspond to cluster name, instance name, and node IP—the identifiers for these three core entities. They appear along with the job label in all ETCD monitoring metrics collected by VictoriaMetrics. The job name for collecting ETCD metrics is fixed as etcd.\n","categories":["Concept"],"description":"Entity-Relationship model for ETCD clusters in Pigsty, including E-R diagram, entity definitions, and naming conventions.","excerpt":"Entity-Relationship model for ETCD clusters in Pigsty, including E-R …","ref":"/docs/concept/model/etcd/","tags":"","title":"E-R Model of Etcd Cluster"},{"body":"Overview Pigsty uses Patroni to manage PostgreSQL clusters. It handles config changes, status checks, switchover, restart, reinit replicas, and more.\nTo use Patroni for management, you need one of the following identities:\nFrom INFRA node as admin user, managing all clusters in the environment. From PGSQL node as pg_dbsu (default postgres), managing the current cluster only. Patroni provides patronictl CLI for management. Pigsty provides a wrapper alias pg to simplify operations.\nUsing patronictl via pg alias pg () { local patroni_conf=\"/infra/conf/patronictl.yml\"; if [ ! -r ${patroni_conf} ]; then patroni_conf=\"/etc/patroni/patroni.yml\"; if [ ! -r ${patroni_conf} ]; then echo \"error: patronictl config not found\"; return 1; fi; fi; patronictl -c ${patroni_conf} \"$@\" } Available Commands Command Function Description edit-config Edit Config Interactively edit cluster Patroni/PostgreSQL config list List Status List cluster members and their status switchover Switchover Switch primary role to specified replica (planned) failover Failover Force failover to specified replica (emergency) restart Restart Restart PostgreSQL instance to apply restart-required params reload Reload Reload Patroni config (no restart needed) reinit Reinit Replica Reinitialize replica (wipe data and re-clone) pause Pause Auto-Failover Pause Patroni automatic failover resume Resume Auto-Failover Resume Patroni automatic failover history View History Show cluster failover history show-config Show Config Display current cluster config (read-only) query Execute Query Execute SQL query on cluster members topology View Topology Display cluster replication topology version View Version Display Patroni version info remove Remove Member Remove cluster member from DCS (dangerous) Edit Config Use edit-config to interactively edit cluster Patroni and PostgreSQL config. This opens an editor to modify config stored in DCS, automatically applying changes to all members. You can change Patroni params (ttl, loop_wait, synchronous_mode, etc.) and PostgreSQL params in postgresql.parameters.\npg edit-config \u003ccls\u003e # Interactive edit cluster config pg edit-config \u003ccls\u003e --force # Skip confirmation and apply directly pg edit-config \u003ccls\u003e -p \u003ck\u003e=\u003cv\u003e # Modify PostgreSQL param (--pg shorthand) pg edit-config \u003ccls\u003e -s \u003ck\u003e=\u003cv\u003e # Modify Patroni param (--set shorthand) Common config modification examples:\n# Modify PostgreSQL param: slow query threshold (prompts for confirmation) pg edit-config pg-test -p log_min_duration_statement=1000 # Modify PostgreSQL param, skip confirmation pg edit-config pg-test -p log_min_duration_statement=1000 --force # Modify multiple PostgreSQL params pg edit-config pg-test -p work_mem=256MB -p maintenance_work_mem=1GB --force # Modify Patroni params: increase failure detection window (increase RTO) pg edit-config pg-test -s loop_wait=15 -s ttl=60 --force # Modify Patroni param: enable synchronous replication mode pg edit-config pg-test -s synchronous_mode=true --force # Modify Patroni param: enable strict synchronous mode (require at least one sync replica for writes) pg edit-config pg-test -s synchronous_mode_strict=true --force # Modify restart-required params (need pg restart after) pg edit-config pg-test -p shared_buffers=4GB --force pg edit-config pg-test -p shared_preload_libraries='timescaledb, pg_stat_statements' --force pg edit-config pg-test -p max_connections=200 --force Some params require PostgreSQL restart to take effect. Use pg list to check - instances marked with * need restart. Then use pg restart to apply. You can also use curl or programs to call Patroni REST API:\n# View current config curl -s 10.10.10.11:8008/config | jq . # Modify params via API (requires auth) curl -u 'postgres:Patroni.API' \\ -d '{\"postgresql\":{\"parameters\": {\"log_min_duration_statement\":200}}}' \\ -s -X PATCH http://10.10.10.11:8008/config | jq . List Status Use list to view cluster members and status. Output shows each instance’s name, host, role, state, timeline, and replication lag. This is the most commonly used command for checking cluster health.\npg list \u003ccls\u003e # List specified cluster status pg list # List all clusters (on admin node) pg list \u003ccls\u003e -e # Show extended info (--extended) pg list \u003ccls\u003e -t # Show timestamp (--timestamp) pg list \u003ccls\u003e -f json # Output as JSON (--format) pg list \u003ccls\u003e -W 5 # Refresh every 5 seconds (--watch) Example output:\n+ Cluster: pg-test (7322261897169354773) -----+----+--------------+ | Member | Host | Role | State | TL | Lag in MB | +-----------+-------------+---------+---------+----+--------------+ | pg-test-1 | 10.10.10.11 | Leader | running | 1 | | | pg-test-2 | 10.10.10.12 | Replica | running | 1 | 0 | | pg-test-3 | 10.10.10.13 | Replica | running | 1 | 0 | +-----------+-------------+---------+---------+----+--------------+ Column descriptions: Member is instance name, composed of pg_cluster-pg_seq; Host is instance IP; Role is role type - Leader (primary), Replica, Sync Standby, Standby Leader (cascade primary); State is running state - running, streaming, in archive recovery, starting, stopped, etc.; TL is timeline number, incremented after each switchover; Lag in MB is replication lag in MB (not shown for primary).\nInstances requiring restart show * after the name:\n+ Cluster: pg-test (7322261897169354773) -------+----+--------------+ | Member | Host | Role | State | TL | Lag in MB | +-------------+-------------+---------+---------+----+--------------+ | pg-test-1 * | 10.10.10.11 | Leader | running | 1 | | | pg-test-2 * | 10.10.10.12 | Replica | running | 1 | 0 | +-------------+-------------+---------+---------+----+--------------+ Switchover Use switchover for planned primary-replica switchover. Switchover is graceful: Patroni ensures replica is fully synced, demotes primary, then promotes target replica. Takes seconds with brief write unavailability. Use for primary host maintenance, upgrades, or migrating primary to better nodes.\npg switchover \u003ccls\u003e # Interactive switchover, prompts for target replica pg switchover \u003ccls\u003e --leader \u003cold\u003e # Specify current primary name pg switchover \u003ccls\u003e --candidate \u003cnew\u003e # Specify target replica name pg switchover \u003ccls\u003e --scheduled \u003ctime\u003e # Scheduled switchover, format: 2024-12-01T03:00 pg switchover \u003ccls\u003e --force # Skip confirmation Before switchover, ensure all replicas are healthy (running or streaming), replication lag is acceptable, and stakeholders are notified.\n# Interactive switchover (recommended, shows topology and prompts for selection) $ pg switchover pg-test Current cluster topology + Cluster: pg-test (7322261897169354773) -----+----+--------------+ | Member | Host | Role | State | TL | Lag in MB | +-----------+-------------+---------+---------+----+--------------+ | pg-test-1 | 10.10.10.11 | Leader | running | 1 | | | pg-test-2 | 10.10.10.12 | Replica | running | 1 | 0 | | pg-test-3 | 10.10.10.13 | Replica | running | 1 | 0 | +-----------+-------------+---------+---------+----+--------------+ Primary [pg-test-1]: Candidate ['pg-test-2', 'pg-test-3'] []: pg-test-2 When should the switchover take place (e.g. 2024-01-01T12:00) [now]: Are you sure you want to switchover cluster pg-test, demoting current leader pg-test-1? [y/N]: y # Non-interactive switchover (specify primary and candidate) pg switchover pg-test --leader pg-test-1 --candidate pg-test-2 --force # Scheduled switchover (at 3 AM, for maintenance window) pg switchover pg-test --leader pg-test-1 --candidate pg-test-2 --scheduled \"2024-12-01T03:00\" After switchover, use pg list to confirm new cluster topology.\nFailover Use failover for emergency failover. Unlike switchover, failover is for when primary is unavailable. It directly promotes a replica without waiting for original primary confirmation. Since replicas may not be fully synced, failover may cause minor data loss. Use switchover for non-emergency situations.\npg failover \u003ccls\u003e # Interactive failover pg failover \u003ccls\u003e --leader \u003cold\u003e # Specify original primary (for verification, optional) pg failover \u003ccls\u003e --candidate \u003cnew\u003e # Specify replica to promote pg failover \u003ccls\u003e --force # Skip confirmation Failover examples:\n# Interactive failover $ pg failover pg-test Candidate ['pg-test-2', 'pg-test-3'] []: pg-test-2 Are you sure you want to failover cluster pg-test? [y/N]: y Successfully failed over to \"pg-test-2\" # Non-interactive failover (for emergencies) pg failover pg-test --candidate pg-test-2 --force # Specify original primary for verification (errors if name mismatch) pg failover pg-test --leader pg-test-1 --candidate pg-test-2 --force Switchover vs Failover: Switchover is for planned maintenance, requires original primary online, ensures full sync before switching, no data loss; Failover is for emergency recovery, original primary can be offline, directly promotes replica, may lose unsynced data. Use Switchover for daily maintenance/upgrades; use Failover only when primary is completely down and unrecoverable.\nRestart Use restart to restart PostgreSQL instances, typically to apply restart-required param changes. Patroni coordinates restarts - for full cluster restart, it uses rolling restart: replicas first, then primary, minimizing downtime.\npg restart \u003ccls\u003e # Restart all instances in cluster pg restart \u003ccls\u003e \u003cmember\u003e # Restart specific instance pg restart \u003ccls\u003e --role leader # Restart primary only pg restart \u003ccls\u003e --role replica # Restart all replicas pg restart \u003ccls\u003e --pending # Restart only instances marked for restart pg restart \u003ccls\u003e --scheduled \u003ctime\u003e # Scheduled restart pg restart \u003ccls\u003e --timeout \u003csec\u003e # Set restart timeout (seconds) pg restart \u003ccls\u003e --force # Skip confirmation After modifying restart-required params (shared_buffers, shared_preload_libraries, max_connections, max_worker_processes, etc.), use this command.\n# Check which instances need restart (marked with *) $ pg list pg-test + Cluster: pg-test (7322261897169354773) -------+----+--------------+ | Member | Host | Role | State | TL | Lag in MB | +-------------+-------------+---------+---------+----+--------------+ | pg-test-1 * | 10.10.10.11 | Leader | running | 1 | | | pg-test-2 * | 10.10.10.12 | Replica | running | 1 | 0 | +-------------+-------------+---------+---------+----+--------------+ # Restart single replica pg restart pg-test pg-test-2 # Restart entire cluster (rolling restart, replicas then primary) pg restart pg-test --force # Restart only pending instances pg restart pg-test --pending --force # Restart all replicas only pg restart pg-test --role replica --force # Scheduled restart (for maintenance window) pg restart pg-test --scheduled \"2024-12-01T03:00\" # Set restart timeout to 300 seconds pg restart pg-test --timeout 300 --force Reload Use reload to reload Patroni config without restarting PostgreSQL. This re-reads config files and applies non-restart params via pg_reload_conf(). Lighter than restart - doesn’t interrupt connections or running queries.\npg reload \u003ccls\u003e # Reload entire cluster config pg reload \u003ccls\u003e \u003cmember\u003e # Reload specific instance config pg reload \u003ccls\u003e --role leader # Reload primary only pg reload \u003ccls\u003e --role replica # Reload all replicas pg reload \u003ccls\u003e --force # Skip confirmation Most PostgreSQL params work via reload. Only postmaster-context params (shared_buffers, max_connections, shared_preload_libraries, archive_mode, etc.) require restart.\n# Reload entire cluster pg reload pg-test # Reload single instance pg reload pg-test pg-test-1 # Force reload, skip confirmation pg reload pg-test --force Reinit Replica Use reinit to reinitialize a replica. This deletes all data on the replica and performs fresh pg_basebackup from primary. Use when replica data is corrupted, replica is too far behind (WAL already purged), or replica config needs reset.\npg reinit \u003ccls\u003e \u003cmember\u003e # Reinitialize specified replica pg reinit \u003ccls\u003e \u003cmember\u003e --force # Skip confirmation pg reinit \u003ccls\u003e \u003cmember\u003e --wait # Wait for rebuild to complete Warning: This operation deletes all data on target instance! Can only be run on replicas, not primary.\n# Reinitialize replica (prompts for confirmation) $ pg reinit pg-test pg-test-2 Are you sure you want to reinitialize members pg-test-2? [y/N]: y Success: reinitialize for member pg-test-2 # Force reinitialize, skip confirmation pg reinit pg-test pg-test-2 --force # Reinitialize and wait for completion pg reinit pg-test pg-test-2 --force --wait During rebuild, use pg list to check progress. Replica state shows creating replica:\n+ Cluster: pg-test (7322261897169354773) --------------+----+------+ | Member | Host | Role | State | TL | Lag | +-----------+-------------+---------+------------------+----+------+ | pg-test-1 | 10.10.10.11 | Leader | running | 2 | | | pg-test-2 | 10.10.10.12 | Replica | creating replica | | ? | +-----------+-------------+---------+------------------+----+------+ Pause Use pause to pause Patroni automatic failover. When paused, Patroni won’t auto-promote replicas even if primary fails. Use for planned maintenance windows (prevent accidental triggers), debugging (prevent cluster state changes), or manual switchover timing control.\npg pause \u003ccls\u003e # Pause automatic failover pg pause \u003ccls\u003e --wait # Pause and wait for all members to confirm Warning: During pause, cluster won’t auto-recover if primary fails! Remember to resume after maintenance.\n# Pause automatic failover $ pg pause pg-test Success: cluster management is paused # Check cluster status (shows Maintenance mode: on) $ pg list pg-test + Cluster: pg-test (7322261897169354773) -----+----+--------------+ | Member | Host | Role | State | TL | Lag in MB | +-----------+-------------+---------+---------+----+--------------+ | pg-test-1 | 10.10.10.11 | Leader | running | 1 | | | pg-test-2 | 10.10.10.12 | Replica | running | 1 | 0 | +-----------+-------------+---------+---------+----+--------------+ Maintenance mode: on Resume Use resume to resume Patroni automatic failover. Execute immediately after maintenance to ensure cluster auto-recovers on primary failure.\npg resume \u003ccls\u003e # Resume automatic failover pg resume \u003ccls\u003e --wait # Resume and wait for all members to confirm # Resume automatic failover $ pg resume pg-test Success: cluster management is resumed # Confirm resumed (Maintenance mode prompt disappears) $ pg list pg-test History Use history to view cluster failover history. Each switchover (auto or manual) creates a new timeline record.\npg history \u003ccls\u003e # Show failover history pg history \u003ccls\u003e -f json # Output as JSON pg history \u003ccls\u003e -f yaml # Output as YAML $ pg history pg-test +----+-----------+------------------------------+---------------------------+ | TL | LSN | Reason | Timestamp | +----+-----------+------------------------------+---------------------------+ | 1 | 0/5000060 | no recovery target specified | 2024-01-15T10:30:00+08:00 | | 2 | 0/6000000 | switchover to pg-test-2 | 2024-01-20T14:00:00+08:00 | | 3 | 0/7000028 | failover to pg-test-1 | 2024-01-25T09:15:00+08:00 | +----+-----------+------------------------------+---------------------------+ Column descriptions: TL is timeline number, incremented after each switchover, distinguishes primary histories; LSN is Log Sequence Number at switchover, marks WAL position; Reason is switchover reason - switchover to xxx (manual), failover to xxx (failure), or no recovery target specified (init); Timestamp is when switchover occurred.\nShow Config Use show-config to view current cluster config stored in DCS. This is read-only; use edit-config to modify.\npg show-config \u003ccls\u003e # Show cluster config $ pg show-config pg-test loop_wait: 10 maximum_lag_on_failover: 1048576 postgresql: parameters: archive_command: pgbackrest --stanza=pg-test archive-push %p max_connections: 100 shared_buffers: 256MB log_min_duration_statement: 1000 use_pg_rewind: true use_slots: true retry_timeout: 10 ttl: 30 synchronous_mode: false Query Use query to quickly execute SQL on cluster members. Convenient for debugging - for complex production queries, use psql or applications.\npg query \u003ccls\u003e -c \"\u003csql\u003e\" # Execute on primary pg query \u003ccls\u003e -c \"\u003csql\u003e\" -m \u003cmember\u003e # Execute on specific instance (--member) pg query \u003ccls\u003e -c \"\u003csql\u003e\" -r leader # Execute on primary (--role) pg query \u003ccls\u003e -c \"\u003csql\u003e\" -r replica # Execute on all replicas pg query \u003ccls\u003e -f \u003cfile\u003e # Execute SQL from file pg query \u003ccls\u003e -c \"\u003csql\u003e\" -U \u003cuser\u003e # Specify username (--username) pg query \u003ccls\u003e -c \"\u003csql\u003e\" -d \u003cdb\u003e # Specify database (--dbname) pg query \u003ccls\u003e -c \"\u003csql\u003e\" --format json # Output as JSON # Check primary connection count pg query pg-test -c \"SELECT count(*) FROM pg_stat_activity\" # Check PostgreSQL version pg query pg-test -c \"SELECT version()\" # Check replication status on all replicas pg query pg-test -c \"SELECT pg_is_in_recovery(), pg_last_wal_replay_lsn()\" -r replica # Execute on specific instance pg query pg-test -c \"SELECT pg_is_in_recovery()\" -m pg-test-2 # Use specific user and database pg query pg-test -c \"SELECT current_user, current_database()\" -U postgres -d postgres # Output as JSON pg query pg-test -c \"SELECT * FROM pg_stat_replication\" --format json Topology Use topology to view cluster replication topology as a tree. More intuitive than list for showing primary-replica relationships, especially for cascading replication.\npg topology \u003ccls\u003e # Show replication topology $ pg topology pg-test + Cluster: pg-test (7322261897169354773) -------+----+--------------+ | Member | Host | Role | State | TL | Lag in MB | +-------------+-------------+---------+---------+----+--------------+ | pg-test-1 | 10.10.10.11 | Leader | running | 1 | | | + pg-test-2 | 10.10.10.12 | Replica | running | 1 | 0 | | + pg-test-3 | 10.10.10.13 | Replica | running | 1 | 0 | +-------------+-------------+---------+---------+----+--------------+ In cascading replication, topology clearly shows replication hierarchy - e.g., pg-test-3 replicates from pg-test-2, which replicates from primary pg-test-1.\nVersion Use version to view patronictl version.\npg version # Show patronictl version $ pg version patronictl version 4.1.0 Remove Use remove to remove cluster or member metadata from DCS. This is dangerous - only removes DCS metadata, doesn’t stop PostgreSQL or delete data files. Misuse may cause cluster state inconsistency.\npg remove \u003ccls\u003e # Remove entire cluster metadata from DCS Normally you don’t need this command. To properly remove clusters/instances, use Pigsty’s bin/pgsql-rm script or pgsql-rm.yml playbook. Only consider remove for: orphaned DCS metadata (node physically removed but metadata remains), or cluster destroyed via other means requiring metadata cleanup.\n# Remove entire cluster metadata (requires multiple confirmations) $ pg remove pg-test Please confirm the cluster name to remove: pg-test You are about to remove all information in DCS for pg-test, please type: \"Yes I am aware\": Yes I am aware ","categories":["Task"],"description":"Manage PostgreSQL cluster HA with Patroni, including config changes, status check, switchover, restart, and reinit replica.","excerpt":"Manage PostgreSQL cluster HA with Patroni, including config changes, …","ref":"/docs/pgsql/admin/patroni/","tags":"","title":"Patroni HA Management"},{"body":"The MinIO module organizes MinIO in production as clusters—logical entities composed of a group of distributed MinIO instances, collectively providing highly available object storage services.\nEach cluster is an autonomous S3-compatible object storage unit consisting of at least one MinIO instance, exposing service capabilities through the S3 API port.\nThere are three core entities in Pigsty’s MinIO module:\nCluster: An autonomous MinIO service unit serving as the top-level namespace for other entities. Instance: A single MinIO server process running on a node, managing local disk storage. Node: A hardware resource abstraction running Linux + Systemd environment, implicitly declared. Additionally, MinIO has the concept of Storage Pool, used for smooth cluster scaling. A cluster can contain multiple storage pools, each composed of a group of nodes and disks.\nDeployment Modes MinIO supports three main deployment modes for different scenarios:\nMode Code Description Use Case Single-Node Single-Drive SNSD Single node, single data directory or disk Dev, test, demo Single-Node Multi-Drive SNMD Single node, multiple disks, typically 4+ Resource-constrained small deployments Multi-Node Multi-Drive MNMD Multiple nodes, multiple disks per node Production recommended SNSD mode can use any directory as storage for quick experimentation; SNMD and MNMD modes require real disk mount points, otherwise startup is refused.\nExamples Let’s look at a concrete multi-node multi-drive example with a four-node MinIO cluster:\nminio: hosts: 10.10.10.10: { minio_seq: 1 } 10.10.10.11: { minio_seq: 2 } 10.10.10.12: { minio_seq: 3 } 10.10.10.13: { minio_seq: 4 } vars: minio_cluster: minio minio_data: '/data{1...4}' minio_node: '${minio_cluster}-${minio_seq}.pigsty' The above config fragment defines a four-node MinIO cluster with four disks per node:\nCluster Description minio MinIO 4-node HA cluster Instance Description minio-1 MinIO instance #1, managing 4 disks minio-2 MinIO instance #2, managing 4 disks minio-3 MinIO instance #3, managing 4 disks minio-4 MinIO instance #4, managing 4 disks Node Description 10.10.10.10 Node #1, hosts minio-1 instance 10.10.10.11 Node #2, hosts minio-2 instance 10.10.10.12 Node #3, hosts minio-3 instance 10.10.10.13 Node #4, hosts minio-4 instance Identity Parameters Pigsty uses the MINIO parameter group to assign deterministic identities to each MinIO module entity. Two parameters are required:\nParameter Type Level Description Format minio_cluster string Cluster MinIO cluster name, required Valid DNS name, defaults to minio minio_seq int Instance MinIO instance number, required Natural number, starting from 1, unique within cluster With cluster name defined at cluster level and instance number assigned at instance level, Pigsty automatically generates unique identifiers for each entity based on rules:\nEntity Generation Rule Example Instance {{ minio_cluster }}-{{ minio_seq }} minio-1, minio-2, minio-3, minio-4 The MinIO module does not assign additional identity to host nodes; nodes are identified by their existing hostname or IP address. The minio_node parameter generates node names for MinIO cluster internal use (written to /etc/hosts for cluster discovery), not host node identity.\nCore Configuration Parameters Beyond identity parameters, the following parameters are critical for MinIO cluster configuration:\nParameter Type Description minio_data path Data directory, use {x...y} for multi-drive minio_node string Node name pattern for multi-node deployment minio_domain string Service domain, defaults to sss.pigsty These parameters together determine MinIO’s core config MINIO_VOLUMES:\nSNSD: Direct minio_data value, e.g., /data/minio SNMD: Expanded minio_data directories, e.g., /data{1...4} MNMD: Combined minio_node and minio_data, e.g., https://minio-{1...4}.pigsty:9000/data{1...4} Ports \u0026 Services Each MinIO instance listens on the following ports:\nPort Parameter Purpose 9000 minio_port S3 API service port 9001 minio_admin_port Web admin console port MinIO enables HTTPS encrypted communication by default (controlled by minio_https). This is required for backup tools like pgBackREST to access MinIO.\nMulti-node MinIO clusters can be accessed through any node. Best practice is to use a load balancer (e.g., HAProxy + VIP) for unified access point.\nResource Provisioning After MinIO cluster deployment, Pigsty automatically creates the following resources (controlled by minio_provision):\nDefault Buckets (defined by minio_buckets):\nBucket Purpose pgsql PostgreSQL pgBackREST backup storage meta Metadata storage, versioning enabled data General data storage Default Users (defined by minio_users):\nUser Default Password Policy Purpose pgbackrest S3User.Backup pgsql PostgreSQL backup dedicated user s3user_meta S3User.Meta meta Access meta bucket s3user_data S3User.Data data Access data bucket pgbackrest is used for PostgreSQL cluster backups; s3user_meta and s3user_data are reserved users not actively used.\nMonitoring Label System Pigsty provides an out-of-box monitoring system that uses the above identity parameters to identify various MinIO entities.\nminio_up{cls=\"minio\", ins=\"minio-1\", ip=\"10.10.10.10\", job=\"minio\"} minio_up{cls=\"minio\", ins=\"minio-2\", ip=\"10.10.10.11\", job=\"minio\"} minio_up{cls=\"minio\", ins=\"minio-3\", ip=\"10.10.10.12\", job=\"minio\"} minio_up{cls=\"minio\", ins=\"minio-4\", ip=\"10.10.10.13\", job=\"minio\"} For example, the cls, ins, ip labels correspond to cluster name, instance name, and node IP—the identifiers for these three core entities. They appear along with the job label in all MinIO monitoring metrics collected by VictoriaMetrics. The job name for collecting MinIO metrics is fixed as minio.\n","categories":["Concept"],"description":"Entity-Relationship model for MinIO clusters in Pigsty, including E-R diagram, entity definitions, and naming conventions.","excerpt":"Entity-Relationship model for MinIO clusters in Pigsty, including E-R …","ref":"/docs/concept/model/minio/","tags":"","title":"E-R Model of MinIO Cluster"},{"body":"Overview Pigsty uses Pgbouncer as PostgreSQL connection pooling middleware, listening on port 6432 by default, proxying access to local PostgreSQL on port 5432.\nThis is an optional component. If you don’t have massive connections or need transaction pooling and query metrics, you can disable it, connect directly to the database, or keep it unused.\nUser \u0026 Database Management Pgbouncer users and databases are auto-managed by Pigsty, applying database config and user config when creating databases and creating users.\nDatabase Management: Databases defined in pg_databases are auto-added to Pgbouncer by default. Set pgbouncer: false to exclude specific databases.\npg_databases: - name: mydb # Added to connection pool by default pool_mode: transaction # Database-level pool mode pool_size: 64 # Default pool size - name: internal pgbouncer: false # Excluded from connection pool User Management: Users defined in pg_users need explicit pgbouncer: true to be added to connection pool user list.\npg_users: - name: dbuser_app password: DBUser.App pgbouncer: true # Add to connection pool user list pool_mode: transaction # User-level pool mode Service Management In Pigsty, PostgreSQL cluster Primary Service and Replica Service default to Pgbouncer port 6432. To bypass connection pool and access PostgreSQL directly, customize pg_services, or set pg_default_service_dest to postgres.\nConfig Management Pgbouncer config files are in /etc/pgbouncer/, generated and managed by Pigsty:\nFile Description pgbouncer.ini Main config, pool-level params database.txt Database list, database-level params userlist.txt User password list useropts.txt User-level pool params pgb_hba.conf HBA access control rules Pigsty auto-manages database.txt and userlist.txt, updating them when creating databases or creating users.\nYou can manually edit config then RELOAD to apply:\n# Edit config $ vim /etc/pgbouncer/pgbouncer.ini # Reload via systemctl $ sudo systemctl reload pgbouncer # Reload as pg_dbsu / postgres user $ pgb -c \"RELOAD;\" Pool Management Pgbouncer runs as the same dbsu as PostgreSQL, default postgres OS user. Pigsty provides pgb alias for easy management:\nalias pgb=\"psql -p 6432 -d pgbouncer -U postgres\" Use pgb on database nodes to connect to Pgbouncer admin console for management commands and monitoring queries.\n$ pgb pgbouncer=# SHOW POOLS; pgbouncer=# SHOW CLIENTS; pgbouncer=# SHOW SERVERS; Command Function Description PAUSE Pause Pause database, wait for txn completion then disconnect RESUME Resume Resume database paused by PAUSE/KILL/SUSPEND DISABLE Disable Reject new client connections for database ENABLE Enable Allow new client connections for database RECONNECT Reconnect Gracefully close and rebuild server connections KILL Kill Immediately disconnect all client and server connections KILL_CLIENT Kill Client Terminate specific client connection SUSPEND Suspend Flush buffers and stop listening, for online restart SHUTDOWN Shutdown Shutdown Pgbouncer process RELOAD Reload Reload config files WAIT_CLOSE Wait Close Wait for server connections to close after RECONNECT/RELOAD Monitor Commands Monitor View pool status, clients, servers, etc. PAUSE Use PAUSE to pause database connections. Pgbouncer waits for active txn/session to complete based on pool mode, then disconnects server connections. New client requests are blocked until RESUME.\nPAUSE [db]; -- Pause specified database, or all if not specified Typical use cases:\nOnline backend database switch (e.g., update connection target after switchover) Maintenance operations requiring all connections disconnected Combined with SUSPEND for Pgbouncer online restart $ pgb -c \"PAUSE mydb;\" # Pause mydb database $ pgb -c \"PAUSE;\" # Pause all databases After pause, SHOW DATABASES shows paused status:\npgbouncer=# SHOW DATABASES; name | host | port | database | ... | paused | disabled ----------+-----------+------+----------+-----+--------+---------- mydb | /var/run | 5432 | mydb | ... | 1 | 0 RESUME Use RESUME to restore databases paused by PAUSE, KILL, or SUSPEND, allowing new connections and resuming normal service.\nRESUME [db]; -- Resume specified database, or all if not specified $ pgb -c \"RESUME mydb;\" # Resume mydb database $ pgb -c \"RESUME;\" # Resume all databases DISABLE Use DISABLE to disable a database, rejecting all new client connection requests. Existing connections are unaffected.\nDISABLE db; -- Disable specified database (database name required) Typical use cases:\nTemporarily offline a database for maintenance Block new connections for safe database migration Gradually decommission a database being removed $ pgb -c \"DISABLE mydb;\" # Disable mydb, new connections rejected ENABLE Use ENABLE to enable a database previously disabled by DISABLE, accepting new client connections again.\nENABLE db; -- Enable specified database (database name required) $ pgb -c \"ENABLE mydb;\" # Enable mydb, allow new connections RECONNECT Use RECONNECT to gracefully rebuild server connections. Pgbouncer closes connections when released back to pool, creating new ones when needed.\nRECONNECT [db]; -- Rebuild server connections for database, or all if not specified Typical use cases:\nRefresh connections after backend database IP change Reroute traffic after switchover Rebuild connections after DNS update $ pgb -c \"RECONNECT mydb;\" # Rebuild mydb server connections $ pgb -c \"RECONNECT;\" # Rebuild all server connections After RECONNECT, use WAIT_CLOSE to wait for old connections to fully release.\nKILL Use KILL to immediately disconnect all client and server connections for a database. Unlike PAUSE, KILL doesn’t wait for transaction completion - forces immediate disconnect.\nKILL [db]; -- Kill all connections for database, or all (except admin) if not specified $ pgb -c \"KILL mydb;\" # Force disconnect all mydb connections $ pgb -c \"KILL;\" # Force disconnect all database connections (except admin) After KILL, new connections are blocked until RESUME.\nKILL_CLIENT Use KILL_CLIENT to terminate a specific client connection. Client ID can be obtained from SHOW CLIENTS output.\nKILL_CLIENT id; -- Terminate client connection with specified ID # View client connections $ pgb -c \"SHOW CLIENTS;\" # Terminate specific client (assuming ptr column shows ID 0x1234567890) $ pgb -c \"KILL_CLIENT 0x1234567890;\" SUSPEND Use SUSPEND to suspend Pgbouncer. Flushes all socket buffers and stops listening until RESUME.\nSUSPEND; -- Suspend Pgbouncer SUSPEND is mainly for Pgbouncer online restart (zero-downtime upgrade):\n# 1. Suspend current Pgbouncer $ pgb -c \"SUSPEND;\" # 2. Start new Pgbouncer process (with -R option to take over sockets) $ pgbouncer -R /etc/pgbouncer/pgbouncer.ini # 3. New process takes over, old process exits automatically SHUTDOWN Use SHUTDOWN to shut down Pgbouncer process. Multiple shutdown modes supported:\nSHUTDOWN; -- Immediate shutdown SHUTDOWN WAIT_FOR_SERVERS; -- Wait for server connections to release SHUTDOWN WAIT_FOR_CLIENTS; -- Wait for clients to disconnect (zero-downtime rolling restart) Mode Description SHUTDOWN Immediately shutdown Pgbouncer WAIT_FOR_SERVERS Stop accepting new connections, wait for server release WAIT_FOR_CLIENTS Stop accepting new connections, wait for all clients disconnect, for rolling restart $ pgb -c \"SHUTDOWN WAIT_FOR_CLIENTS;\" # Graceful shutdown, wait for clients RELOAD Use RELOAD to reload Pgbouncer config files. Dynamically updates most config params without process restart.\nRELOAD; -- Reload config files $ pgb -c \"RELOAD;\" # Reload via admin console $ systemctl reload pgbouncer # Reload via systemd $ kill -SIGHUP $(cat /var/run/pgbouncer/pgbouncer.pid) # Reload via signal Pigsty provides playbook task to reload Pgbouncer config:\n./pgsql.yml -l \u003ccls\u003e -t pgbouncer_reload # Reload cluster Pgbouncer config WAIT_CLOSE Use WAIT_CLOSE to wait for server connections to finish closing. Typically used after RECONNECT or RELOAD to ensure old connections are fully released.\nWAIT_CLOSE [db]; -- Wait for server connections to close, or all if not specified # Complete connection rebuild flow $ pgb -c \"RECONNECT mydb;\" $ pgb -c \"WAIT_CLOSE mydb;\" # Wait for old connections to release Monitoring Pgbouncer provides rich SHOW commands for monitoring pool status:\nCommand Description SHOW HELP Show available commands SHOW DATABASES Show database config and status SHOW POOLS Show pool statistics SHOW CLIENTS Show client connection list SHOW SERVERS Show server connection list SHOW USERS Show user config SHOW STATS Show statistics (requests, bytes) SHOW STATS_TOTALS Show cumulative statistics SHOW STATS_AVERAGES Show average statistics SHOW CONFIG Show current config params SHOW MEM Show memory usage SHOW DNS_HOSTS Show DNS cached hostnames SHOW DNS_ZONES Show DNS cached zones SHOW SOCKETS Show open socket info SHOW ACTIVE_SOCKETS Show active sockets SHOW LISTS Show internal list counts SHOW FDS Show file descriptor usage SHOW STATE Show Pgbouncer running state SHOW VERSION Show Pgbouncer version Common monitoring examples:\n# View pool status $ pgb -c \"SHOW POOLS;\" # View client connections $ pgb -c \"SHOW CLIENTS;\" # View server connections $ pgb -c \"SHOW SERVERS;\" # View statistics $ pgb -c \"SHOW STATS;\" # View database status $ pgb -c \"SHOW DATABASES;\" For more monitoring command details, see Pgbouncer official docs.\nUnix Signals Pgbouncer supports Unix signal control, useful when admin console is unavailable:\nSignal Equivalent Command Description SIGHUP RELOAD Reload config files SIGTERM SHUTDOWN WAIT_FOR_CLIENTS Graceful shutdown, wait clients SIGINT SHUTDOWN WAIT_FOR_SERVERS Graceful shutdown, wait servers SIGQUIT SHUTDOWN Immediate shutdown SIGUSR1 PAUSE Pause all databases SIGUSR2 RESUME Resume all databases # Reload config via signal $ kill -SIGHUP $(cat /var/run/pgbouncer/pgbouncer.pid) # Graceful shutdown via signal $ kill -SIGTERM $(cat /var/run/pgbouncer/pgbouncer.pid) # Pause via signal $ kill -SIGUSR1 $(cat /var/run/pgbouncer/pgbouncer.pid) # Resume via signal $ kill -SIGUSR2 $(cat /var/run/pgbouncer/pgbouncer.pid) Traffic Switching Pigsty provides pgb-route utility function to quickly switch Pgbouncer traffic to other nodes for zero-downtime migration:\n# Definition (already in /etc/profile.d/pg-alias.sh) function pgb-route(){ local ip=${1-'\\/var\\/run\\/postgresql'} sed -ie \"s/host=[^[:space:]]\\+/host=${ip}/g\" /etc/pgbouncer/pgbouncer.ini cat /etc/pgbouncer/pgbouncer.ini } # Usage: Route traffic to 10.10.10.12 $ pgb-route 10.10.10.12 $ pgb -c \"RECONNECT; WAIT_CLOSE;\" Complete zero-downtime switching flow:\n# 1. Modify route target $ pgb-route 10.10.10.12 # 2. Reload config $ pgb -c \"RELOAD;\" # 3. Rebuild connections and wait for old connections to release $ pgb -c \"RECONNECT;\" $ pgb -c \"WAIT_CLOSE;\" ","categories":["Task"],"description":"Manage Pgbouncer connection pool, including pause, resume, disable, enable, reconnect, kill, and reload operations.","excerpt":"Manage Pgbouncer connection pool, including pause, resume, disable, …","ref":"/docs/pgsql/admin/pgbouncer/","tags":"","title":"Pgbouncer Connection Pooling"},{"body":"tiny.yml is optimized for micro instances and resource-constrained environments. Designed for 1-3 core CPUs with minimal resource usage, conservative memory allocation, and disabled parallel queries.\nPair with node_tune = tiny for OS-level tuning.\nUse Cases TINY template is ideal for:\nDev/test: Local development, CI/CD testing Low-spec VMs: 1-2 core CPU, 1-4GB RAM cloud instances Edge computing: Raspberry Pi, embedded devices Demos: Quick Pigsty experience Personal projects: Resource-limited blogs, small apps Resource constraints:\n1-3 CPU cores 1-8 GB RAM Limited disk space May share resources with other services Usage Specify pg_conf = tiny.yml in cluster definition:\npg-dev: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } vars: pg_cluster: pg-dev pg_conf: tiny.yml # PostgreSQL micro instance template node_tune: tiny # OS micro instance tuning Single-node development:\npg-local: hosts: 127.0.0.1: { pg_seq: 1, pg_role: primary } vars: pg_cluster: pg-local pg_conf: tiny.yml # PostgreSQL micro instance template node_tune: tiny # OS micro instance tuning Parameter Details Connection Management max_connections: 250 # OLTP: 500-1000, reduced connection overhead superuser_reserved_connections: 10 Micro instances don’t need many concurrent connections; 250 is sufficient for dev/test.\nMemory Config TINY template uses conservative memory allocation:\nParameter Formula Description shared_buffers mem × pg_shared_buffer_ratio Default ratio 0.25 maintenance_work_mem shared_buffers × 25% For VACUUM, CREATE INDEX work_mem 16MB - 256MB Smaller sort/hash memory effective_cache_size total mem - shared_buffers Estimated cache memory work_mem calculation (differs from OLTP):\nwork_mem = min(max(shared_buffers / max_connections, 16MB), 256MB) Smaller work_mem limit (256MB vs OLTP’s 1GB) prevents memory exhaustion.\nParallel Query (Fully Disabled) TINY template completely disables parallel queries:\nmax_worker_processes: cpu + 4 (min 12) # OLTP: cpu + 8 max_parallel_workers: 50% × cpu (min 1) # OLTP: 50% (min 2) max_parallel_workers_per_gather: 0 # parallel queries disabled max_parallel_maintenance_workers: 33% × cpu (min 1) max_parallel_workers_per_gather: 0 ensures queries won’t spawn parallel workers, avoiding resource contention on low-core systems.\nIO Config (PG17+) io_workers: 3 # fixed value, OLTP: 25% cpu (4-16) Fixed low IO worker count suitable for resource-constrained environments.\nVacuum Config vacuum_cost_delay: 20ms vacuum_cost_limit: 2000 autovacuum_max_workers: 2 # OLTP: 3, one fewer worker autovacuum_naptime: 1min # autovacuum_vacuum_scale_factor uses default # autovacuum_analyze_scale_factor uses default Fewer autovacuum workers reduce background resource usage.\nQuery Optimization random_page_cost: 1.1 effective_io_concurrency: 200 default_statistics_target: 200 # OLTP: 400, lower precision saves space Lower default_statistics_target reduces pg_statistic table size.\nLogging Config log_min_duration_statement: 100 # same as OLTP log_statement: ddl log_checkpoints: on log_lock_waits: on log_temp_files: 1024 # log_connections uses default (no extra logging) TINY template doesn’t enable extra connection logging to reduce log volume.\nClient Timeouts deadlock_timeout: 50ms idle_in_transaction_session_timeout: 10min # same as OLTP Extension Config shared_preload_libraries: 'pg_stat_statements, auto_explain' pg_stat_statements.max: 2500 # OLTP: 10000, reduced memory usage pg_stat_statements.track: all pg_stat_statements.track_utility: off pg_stat_statements.track_planning: off pg_stat_statements.max reduced from 10000 to 2500, saving ~75% memory.\nKey Differences from OLTP Parameter TINY OLTP Reason max_connections 250 500-1000 Reduce connection overhead work_mem limit 256MB 1GB Prevent memory exhaustion max_worker_processes cpu+4 cpu+8 Fewer background processes max_parallel_workers_per_gather 0 20% cpu Disable parallel queries autovacuum_max_workers 2 3 Reduce background load default_statistics_target 200 400 Save space pg_stat_statements.max 2500 10000 Reduce memory usage io_workers 3 25% cpu Fixed low value Resource Estimates TINY template resource usage by configuration:\n1 Core 1GB RAM shared_buffers: ~256MB work_mem: ~16MB maintenance_work_mem: ~64MB max_connections: 250 max_worker_processes: ~12 PostgreSQL process memory: ~400-600MB\n2 Core 4GB RAM shared_buffers: ~1GB work_mem: ~32MB maintenance_work_mem: ~256MB max_connections: 250 max_worker_processes: ~12 PostgreSQL process memory: ~1.5-2GB\n4 Core 8GB RAM Consider using OLTP template instead:\npg-small: vars: pg_conf: oltp.yml # 4C8G can use OLTP template Performance Tuning Tips Further Resource Reduction For extremely constrained resources:\npg_parameters: max_connections: 100 # further reduce shared_buffers: 128MB # further reduce maintenance_work_mem: 32MB work_mem: 8MB Disable Unnecessary Extensions pg_libs: 'pg_stat_statements' # keep only essential extensions Disable Unnecessary Features pg_parameters: track_io_timing: off # disable IO timing tracking track_functions: none # disable function tracking Use External Connection Pool Even on micro instances, PgBouncer significantly improves concurrency:\npg-tiny: vars: pg_conf: tiny.yml pg_default_service_dest: pgbouncer pgbouncer_poolmode: transaction Cloud Platform Recommendations AWS t3.micro: 1 vCPU, 1GB RAM - suitable for TINY t3.small: 2 vCPU, 2GB RAM - suitable for TINY t3.medium: 2 vCPU, 4GB RAM - consider OLTP Alibaba Cloud ecs.t6-c1m1.small: 1 vCPU, 1GB RAM - suitable for TINY ecs.t6-c1m2.small: 1 vCPU, 2GB RAM - suitable for TINY ecs.t6-c1m4.small: 1 vCPU, 4GB RAM - suitable for TINY Tencent Cloud SA2.SMALL1: 1 vCPU, 1GB RAM - suitable for TINY SA2.SMALL2: 1 vCPU, 2GB RAM - suitable for TINY SA2.SMALL4: 1 vCPU, 4GB RAM - suitable for TINY Edge Device Deployment Raspberry Pi 4 pg-pi: hosts: 192.168.1.100: { pg_seq: 1, pg_role: primary } vars: pg_cluster: pg-pi pg_conf: tiny.yml # PostgreSQL micro instance template node_tune: tiny # OS micro instance tuning pg_storage_type: SSD # SSD storage recommended Docker Container pg-docker: hosts: 172.17.0.2: { pg_seq: 1, pg_role: primary } vars: pg_cluster: pg-docker pg_conf: tiny.yml # PostgreSQL micro instance template node_tune: tiny # OS micro instance tuning Upgrading to OLTP When your application grows and needs more resources, easily upgrade to OLTP template:\nUpgrade VM specs (4 core 8GB+) Modify cluster config: pg-growing: vars: pg_conf: oltp.yml # change from tiny.yml to oltp.yml node_tune: oltp # change from tiny to oltp Reconfigure cluster or redeploy References pg_conf: PostgreSQL config template selection node_tune: OS tuning template, should match pg_conf OLTP Template: Transaction template, upgrade for 4C8G+ OLAP Template: Analytics template CRIT Template: Critical business template Single-Node Install: Pigsty single-node installation guide ","categories":["Reference"],"description":"PostgreSQL config template optimized for micro instances and resource-constrained environments","excerpt":"PostgreSQL config template optimized for micro instances and …","ref":"/docs/pgsql/template/tiny/","tags":"","title":"TINY Template"},{"body":"The Redis module organizes Redis in production as clusters—logical entities composed of a group of Redis instances deployed on one or more nodes.\nEach cluster is an autonomous high-performance cache/storage unit consisting of at least one Redis instance, exposing service capabilities through ports.\nThere are three core entities in Pigsty’s Redis module:\nCluster: An autonomous Redis service unit serving as the top-level namespace for other entities. Instance: A single Redis server process running on a specific port on a node. Node: A hardware resource abstraction running Linux + Systemd environment, can host multiple Redis instances, implicitly declared. Unlike PostgreSQL, Redis uses a single-node multi-instance deployment model: one physical/virtual machine node typically deploys multiple Redis instances to fully utilize multi-core CPUs. Therefore, nodes and instances have a 1:N relationship. Additionally, production typically advises against Redis instances with memory \u003e 12GB.\nOperating Modes Redis has three different operating modes, specified by the redis_mode parameter:\nMode Code Description HA Mechanism Standalone standalone Classic master-replica, default mode Requires Sentinel Sentinel sentinel HA monitoring and auto-failover for standalone Multi-node quorum Native Cluster cluster Redis native distributed cluster, no sentinel needed Built-in auto-failover Standalone: Default mode, replication via replica_of parameter. Requires additional Sentinel cluster for HA. Sentinel: Stores no business data, dedicated to monitoring standalone Redis clusters for auto-failover; multi-node itself provides HA. Native Cluster: Data auto-sharded across multiple primaries, each can have multiple replicas, built-in HA, no sentinel needed. Examples Let’s look at concrete examples for each mode:\nStandalone Cluster Classic master-replica on a single node:\nredis-ms: hosts: 10.10.10.10: redis_node: 1 redis_instances: 6379: { } 6380: { replica_of: '10.10.10.10 6379' } vars: redis_cluster: redis-ms redis_password: 'redis.ms' redis_max_memory: 64MB Cluster Description redis-ms Redis standalone cluster Node Description redis-ms-1 10.10.10.10 Node #1, hosts 2 instances Instance Description redis-ms-1-6379 Primary instance, listening on port 6379 redis-ms-1-6380 Replica instance, port 6380, replicates from 6379 Sentinel Cluster Three sentinel instances on a single node for monitoring standalone clusters. Sentinel clusters specify monitored standalone clusters via redis_sentinel_monitor:\nredis-sentinel: hosts: 10.10.10.11: redis_node: 1 redis_instances: { 26379: {}, 26380: {}, 26381: {} } vars: redis_cluster: redis-sentinel redis_password: 'redis.sentinel' redis_mode: sentinel redis_max_memory: 16MB redis_sentinel_monitor: - { name: redis-ms, host: 10.10.10.10, port: 6379, password: redis.ms, quorum: 2 } Native Cluster A Redis native distributed cluster with two nodes and six instances (minimum spec: 3 primaries, 3 replicas):\nredis-test: hosts: 10.10.10.12: { redis_node: 1, redis_instances: { 6379: {}, 6380: {}, 6381: {} } } 10.10.10.13: { redis_node: 2, redis_instances: { 6379: {}, 6380: {}, 6381: {} } } vars: redis_cluster: redis-test redis_password: 'redis.test' redis_mode: cluster redis_max_memory: 32MB This creates a 3 primary 3 replica native Redis cluster.\nCluster Description redis-test Redis native cluster (3P3R) Instance Description redis-test-1-6379 Instance on node 1, port 6379 redis-test-1-6380 Instance on node 1, port 6380 redis-test-1-6381 Instance on node 1, port 6381 redis-test-2-6379 Instance on node 2, port 6379 redis-test-2-6380 Instance on node 2, port 6380 redis-test-2-6381 Instance on node 2, port 6381 Node Description redis-test-1 10.10.10.12 Node #1, hosts 3 instances redis-test-2 10.10.10.13 Node #2, hosts 3 instances Identity Parameters Pigsty uses the REDIS parameter group to assign deterministic identities to each Redis module entity. Three parameters are required:\nParameter Type Level Description Format redis_cluster string Cluster Redis cluster name, required Valid DNS name, regex [a-z][a-z0-9-]* redis_node int Node Redis node number, required Natural number, starting from 1, unique within cluster redis_instances dict Node Redis instance definition, required JSON object, key is port, value is instance config With cluster name defined at cluster level and node number/instance definition assigned at node level, Pigsty automatically generates unique identifiers for each entity:\nEntity Generation Rule Example Instance {{ redis_cluster }}-{{ redis_node }}-{{ port }} redis-ms-1-6379, redis-ms-1-6380 The Redis module does not assign additional identity to host nodes; nodes are identified by their existing hostname or IP address. redis_node is used for instance naming, not host node identity.\nInstance Definition redis_instances is a JSON object with port number as key and instance config as value:\nredis_instances: 6379: { } # Primary instance, no extra config 6380: { replica_of: '10.10.10.10 6379' } # Replica, specify upstream primary 6381: { replica_of: '10.10.10.10 6379' } # Replica, specify upstream primary Each Redis instance listens on a unique port within the node. You can choose any port number, but avoid system reserved ports (\u003c 1024) or conflicts with Pigsty used ports. The replica_of parameter sets replication relationship in standalone mode, format '\u003cip\u003e \u003cport\u003e', specifying upstream primary address and port.\nAdditionally, each Redis node runs a Redis Exporter collecting metrics from all local instances:\nPort Parameter Purpose 9121 redis_exporter_port Redis Exporter port Redis’s single-node multi-instance deployment model has some limitations:\nNode Exclusive: A node can only belong to one Redis cluster, not assigned to different clusters simultaneously. Port Unique: Redis instances on the same node must use different ports to avoid conflicts. Password Shared: Multiple instances on the same node cannot have different passwords (redis_exporter limitation). Manual HA: Standalone Redis clusters require additional Sentinel configuration for auto-failover. Monitoring Label System Pigsty provides an out-of-box monitoring system that uses the above identity parameters to identify various Redis entities.\nredis_up{cls=\"redis-ms\", ins=\"redis-ms-1-6379\", ip=\"10.10.10.10\", job=\"redis\"} redis_up{cls=\"redis-ms\", ins=\"redis-ms-1-6380\", ip=\"10.10.10.10\", job=\"redis\"} For example, the cls, ins, ip labels correspond to cluster name, instance name, and node IP—the identifiers for these three core entities. They appear along with the job label in all Redis monitoring metrics collected by VictoriaMetrics. The job name for collecting Redis metrics is fixed as redis.\n","categories":["Concept"],"description":"Entity-Relationship model for Redis clusters in Pigsty, including E-R diagram, entity definitions, and naming conventions.","excerpt":"Entity-Relationship model for Redis clusters in Pigsty, including E-R …","ref":"/docs/concept/model/redis/","tags":"","title":"E-R Model of Redis Cluster"},{"body":"Overview Pigsty’s PGSQL module consists of multiple components, each running as a systemd service on nodes. (pgbackrest is an exception)\nUnderstanding these components and their management is essential for maintaining production PostgreSQL clusters.\nComponent Port Service Name Description Patroni 8008 patroni HA manager, manages PostgreSQL lifecycle PostgreSQL 5432 postgres Placeholder service, not used, for emergency Pgbouncer 6432 pgbouncer Connection pooling middleware, traffic entry PgBackRest - - pgBackRest has no daemon service HAProxy 543x haproxy Load balancer, exposes database services pg_exporter 9630 pg_exporter PostgreSQL metrics exporter pgbouncer_exporter 9631 pgbouncer_exporter Pgbouncer metrics exporter vip-manager - vip-manager Optional, manages L2 VIP address floating Important Do NOT use systemctl directly to manage PostgreSQL service. PostgreSQL is managed by Patroni - use patronictl commands instead. Direct PostgreSQL operations may cause Patroni state inconsistency and trigger unexpected failover. The postgres service is an emergency escape hatch when Patroni fails.\nQuick Reference Operation Command Start systemctl start \u003cservice\u003e Stop systemctl stop \u003cservice\u003e Restart systemctl restart \u003cservice\u003e Reload systemctl reload \u003cservice\u003e Status systemctl status \u003cservice\u003e Logs journalctl -u \u003cservice\u003e -f Enable systemctl enable \u003cservice\u003e Disable systemctl disable \u003cservice\u003e Common service names: patroni, pgbouncer, haproxy, pg_exporter, pgbouncer_exporter, vip-manager\nPatroni Patroni is PostgreSQL’s HA manager, handling startup, shutdown, failure detection, and automatic failover. It’s the core PGSQL module component. PostgreSQL process is managed by Patroni - don’t use systemctl to manage postgres service directly.\nStart Patroni\nsystemctl start patroni # Start Patroni (also starts PostgreSQL) After starting, Patroni auto-launches PostgreSQL. On first start, behavior depends on role:\nPrimary: Initialize or recover data directory Replica: Clone data from primary and establish replication Stop Patroni\nsystemctl stop patroni # Stop Patroni (also stops PostgreSQL) Stopping Patroni gracefully shuts down PostgreSQL. Note: If this is primary and auto-failover isn’t paused, may trigger failover.\nRestart Patroni\nsystemctl restart patroni # Restart Patroni (also restarts PostgreSQL) Restart causes brief service interruption. For production, use pg restart for rolling restart.\nReload Patroni\nsystemctl reload patroni # Reload Patroni config Reload re-reads config file and applies hot-reloadable params to PostgreSQL.\nView Status \u0026 Logs\nsystemctl status patroni # View Patroni service status journalctl -u patroni -f # Real-time Patroni logs journalctl -u patroni -n 100 --no-pager # Last 100 lines Config file: /etc/patroni/patroni.yml\nBest Practice: Use patronictl instead of systemctl to manage PostgreSQL clusters.\nPgbouncer Pgbouncer is a lightweight PostgreSQL connection pooling middleware. Business traffic typically goes through Pgbouncer (6432) rather than directly to PostgreSQL (5432) for connection reuse and database protection.\nStart Pgbouncer\nsystemctl start pgbouncer Stop Pgbouncer\nsystemctl stop pgbouncer Note: Stopping Pgbouncer disconnects all pooled business connections.\nRestart Pgbouncer\nsystemctl restart pgbouncer Restart disconnects all existing connections. For config changes only, use reload.\nReload Pgbouncer\nsystemctl reload pgbouncer Reload re-reads config files (user list, pool params, etc.) without disconnecting existing connections.\nView Status \u0026 Logs\nsystemctl status pgbouncer journalctl -u pgbouncer -f Config files:\nMain config: /etc/pgbouncer/pgbouncer.ini HBA rules: /etc/pgbouncer/pgb_hba.conf User list: /etc/pgbouncer/userlist.txt Database list: /etc/pgbouncer/database.txt Admin Console\npsql -p 6432 -U postgres -d pgbouncer # Connect to Pgbouncer admin console Common admin commands:\nSHOW POOLS; -- View pool status SHOW CLIENTS; -- View client connections SHOW SERVERS; -- View backend server connections SHOW STATS; -- View statistics RELOAD; -- Reload config PAUSE; -- Pause all pools RESUME; -- Resume all pools HAProxy HAProxy is a high-performance load balancer that routes traffic to correct PostgreSQL instances. Pigsty uses HAProxy to expose services, routing traffic based on role (primary/replica) and health status.\nStart HAProxy\nsystemctl start haproxy Stop HAProxy\nsystemctl stop haproxy Note: Stopping HAProxy disconnects all load-balanced connections.\nRestart HAProxy\nsystemctl restart haproxy Reload HAProxy\nsystemctl reload haproxy HAProxy supports graceful reload without disconnecting existing connections. Use reload for config changes.\nView Status \u0026 Logs\nsystemctl status haproxy journalctl -u haproxy -f Config file: /etc/haproxy/haproxy.cfg\nAdmin Interface\nHAProxy provides a web admin interface, default port 9101:\nhttp://\u003cnode_ip\u003e:9101/haproxy Default auth: username admin, password configured by haproxy_admin_password.\npg_exporter pg_exporter is PostgreSQL’s Prometheus metrics exporter for collecting database performance metrics.\nStart pg_exporter\nsystemctl start pg_exporter Stop pg_exporter\nsystemctl stop pg_exporter After stopping, Prometheus can’t collect PostgreSQL metrics from this instance.\nRestart pg_exporter\nsystemctl restart pg_exporter View Status \u0026 Logs\nsystemctl status pg_exporter journalctl -u pg_exporter -f Config file: /etc/pg_exporter.yml\nVerify Metrics\ncurl -s localhost:9630/metrics | head -20 pgbouncer_exporter pgbouncer_exporter is Pgbouncer’s Prometheus metrics exporter.\nStart/Stop/Restart\nsystemctl start pgbouncer_exporter systemctl stop pgbouncer_exporter systemctl restart pgbouncer_exporter View Status \u0026 Logs\nsystemctl status pgbouncer_exporter journalctl -u pgbouncer_exporter -f Verify Metrics\ncurl -s localhost:9631/metrics | head -20 vip-manager vip-manager is an optional component for managing L2 VIP address floating. When pg_vip_enabled is enabled, vip-manager binds VIP to current primary node.\nStart vip-manager\nsystemctl start vip-manager Stop vip-manager\nsystemctl stop vip-manager After stopping, VIP address is released from current node.\nRestart vip-manager\nsystemctl restart vip-manager View Status \u0026 Logs\nsystemctl status vip-manager journalctl -u vip-manager -f Config file: /etc/default/vip-manager\nVerify VIP Binding\nip addr show # Check network interfaces, verify VIP binding pg list \u003ccls\u003e # Confirm primary location Startup Order \u0026 Dependencies Recommended PGSQL module component startup order:\n1. patroni # Start Patroni first (auto-starts PostgreSQL) 2. pgbouncer # Then start connection pool 3. haproxy # Start load balancer 4. pg_exporter # Start metrics exporters 5. pgbouncer_exporter 6. vip-manager # Finally start VIP manager (if enabled) Stop order should be reversed. Pigsty playbooks handle these dependencies automatically.\nBatch Start All Services\nsystemctl start patroni pgbouncer haproxy pg_exporter pgbouncer_exporter Batch Stop All Services\nsystemctl stop pgbouncer_exporter pg_exporter haproxy pgbouncer patroni Common Troubleshooting Service Startup Failure\nsystemctl status \u003cservice\u003e # View service status journalctl -u \u003cservice\u003e -n 50 # View recent logs journalctl -u \u003cservice\u003e --since \"5 min ago\" # Last 5 minutes logs Patroni Won’t Start\nSymptom Possible Cause Solution Can’t connect to etcd etcd cluster unavailable Check etcd service status Data dir permission error File ownership not postgres chown -R postgres:postgres /pg/data Port in use Leftover PostgreSQL process pg_ctl stop -D /pg/data or kill Pgbouncer Won’t Start\nSymptom Possible Cause Solution Config syntax error INI format error Check /etc/pgbouncer/pgbouncer.ini Port in use Port 6432 already used lsof -i :6432 userlist.txt permissions Incorrect file permissions chmod 600 /etc/pgbouncer/userlist.txt HAProxy Won’t Start\nSymptom Possible Cause Solution Config syntax error haproxy.cfg format error haproxy -c -f /etc/haproxy/haproxy.cfg Port in use Service port conflict lsof -i :5433 Related Documentation Patroni Management: Manage PostgreSQL HA with patronictl Cluster Management: Create, scale, destroy clusters Service Configuration: HAProxy service definition and config Monitoring System: PostgreSQL monitoring and alerting ","categories":["Task"],"description":"Use systemctl to manage PostgreSQL cluster component services - start, stop, restart, reload, and status check.","excerpt":"Use systemctl to manage PostgreSQL cluster component services - start, …","ref":"/docs/pgsql/admin/component/","tags":"","title":"Managing PostgreSQL Component Services"},{"body":"Pigsty uses crontab to manage scheduled tasks for routine backups, freezing aging transactions, and reorganizing bloated tables and indexes.\nQuick Reference Operation Quick Command Description Configure Cron Jobs ./pgsql.yml -t pg_crontab -l \u003ccls\u003e Apply pg_crontab config View Cron Jobs crontab -l View as postgres user Physical Backup pg-backup [full|diff|incr] Execute backup with pgBackRest Transaction Freeze pg-vacuum [database...] Freeze aging transactions, prevent XID wraparound Bloat Maintenance pg-repack [database...] Online reorganize bloated tables and indexes For other management tasks, see: Backup Management, Monitoring System, HA Management.\nConfigure Cron Jobs Use the pg_crontab parameter to configure cron jobs for the PostgreSQL database superuser (pg_dbsu, default postgres).\nExample Configuration\nThe following pg-meta cluster configures a daily full backup at 1:00 AM, while pg-test configures weekly full backup on Monday with incremental backups on other days.\npg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta pg_crontab: - '00 01 * * * /pg/bin/pg-backup' pg-test: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } 10.10.10.12: { pg_seq: 2, pg_role: replica } vars: pg_cluster: pg-test pg_crontab: - '00 01 * * 1 /pg/bin/pg-backup full' - '00 01 * * 2,3,4,5,6,7 /pg/bin/pg-backup' Recommended Maintenance Schedule\npg_crontab: - '00 01 * * * /pg/bin/pg-backup full' # Daily full backup at 1:00 AM - '00 03 * * 0 /pg/bin/pg-vacuum' # Weekly vacuum freeze on Sunday at 3:00 AM - '00 04 * * 1 /pg/bin/pg-repack' # Weekly repack on Monday at 4:00 AM Task Frequency Timing Description pg-backup Daily Early morning Full or incremental backup, depending on business needs pg-vacuum Weekly Sunday early morning Freeze aging transactions, prevent XID wraparound pg-repack Weekly/Monthly Off-peak hours Reorganize bloated tables/indexes, reclaim space Primary Only Execution The pg-backup, pg-vacuum, and pg-repack scripts automatically detect the current node role. Only the primary will actually execute; replicas will exit directly. Therefore, you can safely configure the same cron jobs on all nodes, and after failover, the new primary will automatically continue executing maintenance tasks.\nApply Cron Jobs Cron jobs are automatically written to the default location for the corresponding OS distribution when the pgsql.yml playbook executes (the pg_crontab task):\nEL (RHEL/Rocky/Alma): /var/spool/cron/postgres Debian/Ubuntu: /var/spool/cron/crontabs/postgres Playbook Manual ./pgsql.yml -l pg-meta -t pg_crontab # Apply pg_crontab config to specified cluster ./pgsql.yml -l 10.10.10.10 -t pg_crontab # Target specific host only # Edit cron jobs as postgres user sudo -u postgres crontab -e # Or edit crontab file directly sudo vi /var/spool/cron/postgres # EL series sudo vi /var/spool/cron/crontabs/postgres # Debian/Ubuntu Each playbook execution will fully overwrite the cron job configuration.\nView Cron Jobs Execute the following command as the pg_dbsu OS user to view cron jobs:\ncrontab -l # Pigsty Managed Crontab for postgres SHELL=/bin/bash PATH=/usr/pgsql/bin:/pg/bin:/usr/local/bin:/usr/bin:/usr/sbin:/bin:/sbin MAILTO=\"\" 00 01 * * * /pg/bin/pg-backup If you’re not familiar with crontab syntax, refer to Crontab Guru for explanations.\npg-backup pg-backup is Pigsty’s physical backup script based on pgBackRest, supporting full, differential, and incremental backup modes.\nBasic Usage\npg-backup # Execute incremental backup (default), auto full if no existing full backup pg-backup full # Execute full backup pg-backup diff # Execute differential backup (based on most recent full backup) pg-backup incr # Execute incremental backup (based on most recent any backup) Backup Types\nType Parameter Description Full Backup full Complete backup of all data, only this backup needed for recovery Differential diff Backup changes since last full backup, recovery needs full + diff Incremental incr Backup changes since last any backup, recovery needs complete chain Execution Requirements\nScript must run on primary as postgres user Script auto-detects current node role, exits (exit 1) when run on replica Auto-retrieves stanza name from /etc/pgbackrest/pgbackrest.conf Common Cron Configurations\nDaily Full Weekly Full + Daily Incr Weekly Full + Daily Diff pg_crontab: - '00 01 * * * /pg/bin/pg-backup full' # Daily full backup at 1:00 AM pg_crontab: - '00 01 * * 1 /pg/bin/pg-backup full' # Monday full backup - '00 01 * * 2,3,4,5,6,7 /pg/bin/pg-backup' # Other days incremental pg_crontab: - '00 01 * * 1 /pg/bin/pg-backup full' # Monday full backup - '00 01 * * 2,3,4,5,6,7 /pg/bin/pg-backup diff' # Other days differential For more backup and recovery operations, see the Backup Management section.\npg-vacuum pg-vacuum is Pigsty’s transaction freeze script for executing VACUUM FREEZE operations to prevent database shutdown from transaction ID (XID) wraparound.\nBasic Usage\nBasic Options Manual SQL pg-vacuum # Freeze aging tables in all databases pg-vacuum mydb # Process specified database only pg-vacuum mydb1 mydb2 # Process multiple databases pg-vacuum -n mydb # Dry run mode, display only without executing pg-vacuum -a 80000000 mydb # Use custom age threshold (default 100M) pg-vacuum -r 50 mydb # Use custom aging ratio threshold (default 40%) -- Execute VACUUM FREEZE on entire database VACUUM FREEZE; -- Execute VACUUM FREEZE on specific table VACUUM FREEZE schema.table_name; Command Options\nOption Description Default -h, --help Show help message - -n, --dry-run Dry run mode, display only false -a, --age Age threshold, tables exceeding need freeze 100000000 -r, --ratio Aging ratio threshold, full freeze if exceeded (%) 40 Logic\nCheck database datfrozenxid age, skip database if below threshold Calculate aging page ratio (percentage of table pages exceeding age threshold of total pages) If aging ratio \u003e 40%, execute full database VACUUM FREEZE ANALYZE Otherwise, only execute VACUUM FREEZE ANALYZE on tables exceeding age threshold Script sets vacuum_cost_limit = 10000 and vacuum_cost_delay = 1ms to control I/O impact.\nExecution Requirements\nScript must run on primary as postgres user Uses file lock /tmp/pg-vacuum.lock to prevent concurrent execution Auto-skips template0, template1, postgres system databases Common Cron Configuration\npg_crontab: - '00 03 * * 0 /pg/bin/pg-vacuum' # Weekly Sunday at 3:00 AM pg-repack pg-repack is Pigsty’s bloat maintenance script based on the pg_repack extension for online reorganization of bloated tables and indexes.\nBasic Usage\nBasic Options Manual pg-repack # Reorganize bloated tables and indexes in all databases pg-repack mydb # Reorganize specified database only pg-repack mydb1 mydb2 # Reorganize multiple databases pg-repack -n mydb # Dry run mode, display only without executing pg-repack -t mydb # Reorganize tables only pg-repack -i mydb # Reorganize indexes only pg-repack -T 30 -j 4 mydb # Custom lock timeout (seconds) and parallelism # Use pg_repack command directly to reorganize specific table pg_repack dbname -t schema.table # Use pg_repack command directly to reorganize specific index pg_repack dbname -i schema.index Command Options\nOption Description Default -h, --help Show help message - -n, --dry-run Dry run mode, display only false -t, --table Reorganize tables only false -i, --index Reorganize indexes only false -T, --timeout Lock wait timeout (seconds) 10 -j, --jobs Parallel jobs 2 Auto-Selection Thresholds\nScript auto-selects objects to reorganize based on table/index size and bloat ratio:\nTable Bloat Thresholds\nSize Range Bloat Threshold Max Count \u003c 256MB \u003e 40% 64 256MB - 2GB \u003e 30% 16 2GB - 8GB \u003e 20% 4 8GB - 64GB \u003e 15% 1 Index Bloat Thresholds\nSize Range Bloat Threshold Max Count \u003c 128MB \u003e 40% 64 128MB - 1GB \u003e 35% 16 1GB - 8GB \u003e 30% 4 8GB - 64GB \u003e 20% 1 Tables/indexes over 64GB are skipped with a warning and require manual handling.\nExecution Requirements\nScript must run on primary as postgres user Requires pg_repack extension installed (installed by default in Pigsty) Requires pg_table_bloat and pg_index_bloat views in monitor schema Uses file lock /tmp/pg-repack.lock to prevent concurrent execution Auto-skips template0, template1, postgres system databases Lock Waiting Normal reads/writes are not affected during reorganization, but the final switch moment requires acquiring AccessExclusive lock on the table, blocking all access. For high-throughput workloads, recommend running during off-peak hours or maintenance windows.\nCommon Cron Configuration\npg_crontab: - '00 04 * * 1 /pg/bin/pg-repack' # Weekly Monday at 4:00 AM You can confirm database bloat through Pigsty’s PGCAT Database - Table Bloat panel and select high-bloat tables and indexes for reorganization.\nFor more details see: Managing Relation Bloat\nRemove Cron Jobs When using the pgsql-rm.yml playbook to remove a PostgreSQL cluster, it automatically deletes the postgres user’s crontab file.\n./pgsql-rm.yml -l \u003ccls\u003e -t pg_crontab # Remove cron jobs only ./pgsql-rm.yml -l \u003ccls\u003e # Remove entire cluster (including cron jobs) Related Documentation Backup Management: PostgreSQL backup and recovery Monitoring System: PostgreSQL monitoring and alerting Cluster Management: Cluster creation, scaling, and teardown Patroni Management: HA cluster management ","categories":["Admin"],"description":"Configure crontab to schedule PostgreSQL backups, vacuum freeze, and bloat maintenance tasks","excerpt":"Configure crontab to schedule PostgreSQL backups, vacuum freeze, and …","ref":"/docs/pgsql/admin/crontab/","tags":"","title":"Manage PostgreSQL Cron Jobs"},{"body":"Quick Start Pigsty provides 440+ extensions. Using extensions involves four steps: Download, Install, Configure, Enable.\npg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta pg_extensions: [ postgis, timescaledb, pgvector ] # \u003c--- Install extension packages pg_libs: 'timescaledb, pg_stat_statements, auto_explain' # \u003c--- Configure preload extensions pg_databases: - name: meta extensions: [ postgis, timescaledb, vector ] # \u003c--- Enable in database Script Playbook Example bin/pgsql-ext \u003ccls\u003e # Install extensions defined in config on \u003ccls\u003e cluster bin/pgsql-ext \u003ccls\u003e [ext...] # Install extensions specified on command line ./pgsql.yml -l pg-meta -t pg_ext # Use playbook to install extensions bin/pgsql-ext pg-meta # Install defined extensions on pg-meta cluster bin/pgsql-ext pg-meta pg_duckdb pg_mooncake # Install specified extensions For complete extension reference, see Extensions. For available extensions, see Extension Catalog.\nAction Command Description Download Extensions ./infra.yml -t repo_build Download extensions to local repo Install Extensions bin/pgsql-ext \u003ccls\u003e Install extension packages on cluster Configure Extensions pg edit-config \u003ccls\u003e -p Add to preload libs (requires restart) Enable Extensions psql -c 'CREATE EXT ...' Create extension objects in database Update Extensions ALTER EXTENSION UPDATE Update packages and extension objects Remove Extensions DROP EXTENSION Drop extension objects, uninstall pkgs Install Extensions Extensions defined in pg_extensions are auto-installed during PostgreSQL cluster creation in the pg_extension task.\nTo install extensions on an existing cluster, add extensions to all.children.\u003ccls\u003e.pg_extensions, then execute:\nScript Playbook Example bin/pgsql-ext \u003ccls\u003e # Install extensions on \u003ccls\u003e cluster ./pgsql.yml -l \u003ccls\u003e -t pg_extension # Use Ansible playbook bin/pgsql-ext pg-meta # Install extensions defined in config on pg-meta Example: Install PostGIS, TimescaleDB and PGVector on cluster\n#all.children.pg-meta.vars: pg_extensions: [ postgis, timescaledb, pgvector ] Result: Installs extension packages on all cluster nodes. Pigsty auto-translates package aliases to actual package names for OS and PG version.\nEnsure repos available before install Before installing, ensure nodes have correct repos configured - extensions downloaded to local repo, or upstream repos configured.\nManual Install If you don’t want to use Pigsty config to manage extensions, pass extension list directly on command line:\nScript Playbook bin/pgsql-ext pg-meta pg_duckdb pg_mooncake # Install specified extensions on pg-meta ./pgsql.yml -l pg-meta -t pg_ext -e '{\"pg_extensions\": [\"pg_duckdb\", \"pg_mooncake\"]}' You can also use pig package manager CLI to install extensions on single node, with auto package alias resolution.\npig install postgis timescaledb # Install multiple extensions pig install pgvector -v 17 # Install for specific PG major version ansible pg-test -b -a 'pig install pg_duckdb' # Batch install on cluster with Ansible You can also use OS package manager directly (apt/dnf), but you must know the exact RPM/DEB package name for your OS/PG:\n# EL systems (RHEL, Rocky, Alma, Oracle Linux) sudo yum install -y pgvector_17* # Debian / Ubuntu sudo apt install -y postgresql-17-pgvector Download Extensions To install extensions, ensure node’s extension repos contain the extension:\nStandalone install: No worries, upstream repos already added to node. Offline install: No worries, most extensions included in offline package, few require online install. Production multi-node deployment with local repo: depends - if extension was in repo_packages / repo_extra_packages when creating local repo, it’s already downloaded. Otherwise download first or configure upstream repos for online install. Pigsty’s default config auto-downloads mainstream extensions during installation. For additional extensions, add to repo_extra_packages and rebuild repo:\nrepo_extra_packages: [ pgvector, postgis, timescaledb ] Script Playbook make repo # Shortcut = repo-build + node-repo make repo-build # Rebuild Infra repo (download packages and deps) make node-repo # Refresh node repo cache, update Infra repo reference ./deploy.yml -t repo_build,node_repo # Execute both tasks at once ./infra.yml -t repo_build # Re-download packages to local repo ./node.yml -t node_repo # Refresh node repo cache Configure Repos You can also let all nodes use upstream repos directly (not recommended for production), skipping download and installing from upstream extension repos:\n./node.yml -t node_repo -e node_repo_modules=node,pgsql # Add PGDG and Pigsty upstream repos Configure Extensions Some extensions require preloading to shared_preload_libraries, requiring database restart after modification.\nUse pg_libs as its default value to configure preload extensions, but this only takes effect during cluster init - later modifications are ineffective.\npg-meta: vars: pg_cluster: pg-meta pg_libs: 'timescaledb, pg_stat_statements, auto_explain' # Preload extensions pg_extensions: [ timescaledb, postgis, pgvector ] # Install packages For existing clusters, refer to Modify Config to modify shared_preload_libraries:\npg edit-config pg-meta --force -p shared_preload_libraries='timescaledb, pg_stat_statements, auto_explain' pg restart pg-meta # Modify pg-meta params and restart to apply Ensure extension packages are correctly installed before adding preload config. If extension in shared_preload_libraries doesn’t exist or fails to load, PostgreSQL won’t start. Also, manage cluster config changes through Patroni - avoid using ALTER SYSTEM or pg_parameters to modify instance config separately. If primary and replica configs differ, it may cause startup failure or replication interruption.\nEnable Extensions After installing packages, execute CREATE EXTENSION in database to use extension features.\nEnable during cluster init\nDeclare extensions to enable in database definition via extensions array:\npg_databases: - name: meta extensions: - vector # Simple form - { name: postgis, schema: public } # Specify schema Manual enable\nSQL psql Playbook CREATE EXTENSION vector; -- Create extension CREATE EXTENSION postgis SCHEMA public; -- Specify schema CREATE EXTENSION IF NOT EXISTS vector; -- Idempotent creation CREATE EXTENSION postgis_topology CASCADE; -- Auto-install dependencies psql -d meta -c 'CREATE EXTENSION vector;' # Create extension in meta database psql -d meta -c 'CREATE EXTENSION postgis SCHEMA public;' # Specify schema # After modifying database definition, use playbook to enable extensions bin/pgsql-db pg-meta meta # Creating/modifying database auto-enables defined extensions Result: Creates extension objects (functions, types, operators, index methods, etc.) in database, enabling use of extension features.\nUpdate Extensions Extension updates involve two layers: package update and extension object update.\nUpdate packages\npig yum apt pig update pgvector # Update extension with pig sudo yum update pgvector_18 # EL sudo apt upgrade postgresql-18-pgvector # Debian/Ubuntu Update extension objects\n-- View upgradeable extensions SELECT name, installed_version, default_version FROM pg_available_extensions WHERE installed_version IS NOT NULL AND installed_version \u003c\u003e default_version; -- Update extension to latest version ALTER EXTENSION vector UPDATE; -- Update to specific version ALTER EXTENSION vector UPDATE TO '0.8.1'; Update Notes Backup database before updating extensions. Preloaded extensions may require PostgreSQL restart after update. Some extension version upgrades may be incompatible - check extension docs.\nRemove Extensions Removing extensions involves two layers: drop extension objects and uninstall packages.\nDrop extension objects\nDROP EXTENSION vector; -- Drop extension DROP EXTENSION vector CASCADE; -- Cascade drop (drops dependent objects) Remove from preload\nFor preloaded extensions, remove from shared_preload_libraries and restart:\npg edit-config pg-meta --force -p shared_preload_libraries='pg_stat_statements, auto_explain' pg restart pg-meta # Restart to apply config Uninstall packages (optional)\npig yum apt pig remove pgvector # Uninstall with pig sudo yum remove pgvector_17* # EL systems sudo apt remove postgresql-17-pgvector # Debian/Ubuntu CASCADE Warning Using CASCADE to drop extensions also drops all objects depending on that extension (tables, indexes, views, etc.). Check dependencies before executing.\nQuery Extensions Common SQL queries for extension info:\nView enabled extensions\nSELECT extname, extversion, nspname AS schema FROM pg_extension e JOIN pg_namespace n ON e.extnamespace = n.oid ORDER BY extname; View available extensions\nSELECT name, default_version, installed_version, comment FROM pg_available_extensions WHERE installed_version IS NOT NULL -- Only show installed ORDER BY name; Check if extension is available\nSELECT * FROM pg_available_extensions WHERE name = 'vector'; View extension dependencies\nSELECT e.extname, d.refobjid::regclass AS depends_on FROM pg_extension e JOIN pg_depend d ON d.objid = e.oid WHERE d.deptype = 'e' AND e.extname = 'postgis_topology'; View extension objects\nSELECT classid::regclass, objid, deptype FROM pg_depend WHERE refobjid = (SELECT oid FROM pg_extension WHERE extname = 'vector'); psql shortcuts\n\\dx # List enabled extensions \\dx+ vector # Show extension details Add Repos To install directly from upstream, manually add repos.\nUsing Pigsty playbook\n./node.yml -t node_repo -e node_repo_modules=node,pgsql # Add PGDG and Pigsty repos ./node.yml -t node_repo -e node_repo_modules=node,pgsql,local # Including local repo YUM repos (EL systems)\n# Pigsty repo curl -fsSL https://repo.pigsty.io/key | sudo tee /etc/pki/rpm-gpg/RPM-GPG-KEY-pigsty \u003e/dev/null curl -fsSL https://repo.pigsty.io/yum/repo | sudo tee /etc/yum.repos.d/pigsty.repo \u003e/dev/null # China mainland mirror curl -fsSL https://repo.pigsty.cc/key | sudo tee /etc/pki/rpm-gpg/RPM-GPG-KEY-pigsty \u003e/dev/null curl -fsSL https://repo.pigsty.cc/yum/repo | sudo tee /etc/yum.repos.d/pigsty.repo \u003e/dev/null APT repos (Debian/Ubuntu)\ncurl -fsSL https://repo.pigsty.io/key | sudo gpg --dearmor -o /etc/apt/keyrings/pigsty.gpg sudo tee /etc/apt/sources.list.d/pigsty.list \u003e /dev/null \u003c\u003cEOF deb [signed-by=/etc/apt/keyrings/pigsty.gpg] https://repo.pigsty.io/apt/infra generic main deb [signed-by=/etc/apt/keyrings/pigsty.gpg] https://repo.pigsty.io/apt/pgsql $(lsb_release -cs) main EOF sudo apt update # China mainland mirror: replace repo.pigsty.io with repo.pigsty.cc FAQ Difference between extension name and package name\nName Description Example Extension name Name used with CREATE EXTENSION vector Package alias Standardized name in Pigsty config pgvector Package name Actual OS package name pgvector_17* or postgresql-17-pgvector Preloaded extension prevents startup\nIf extension in shared_preload_libraries doesn’t exist or fails to load, PostgreSQL won’t start. Solutions:\nEnsure extension package is correctly installed Or remove extension from shared_preload_libraries (edit /pg/data/postgresql.conf) Extension dependencies\nSome extensions depend on others, requiring sequential creation or using CASCADE:\nCREATE EXTENSION postgis; -- Create base extension first CREATE EXTENSION postgis_topology; -- Then create dependent extension -- Or CREATE EXTENSION postgis_topology CASCADE; -- Auto-create dependencies Extension version incompatibility\nView extension versions supported by current PostgreSQL:\nSELECT * FROM pg_available_extension_versions WHERE name = 'vector'; Related Resources Extensions: Detailed extension management documentation Extension Catalog: Browse 440+ available extensions pig Package Manager: Extension installation CLI tool Database Management: Enable extensions in databases ","categories":["Task"],"description":"Extension management - download, install, configure, enable, update, and remove extensions","excerpt":"Extension management - download, install, configure, enable, update, …","ref":"/docs/pgsql/admin/ext/","tags":"","title":"Managing PostgreSQL Extensions"},{"body":"Quick Start PostgreSQL version upgrades fall into two types: minor version upgrade and major version upgrade, with very different risk and complexity.\nType Example Downtime Data Compatibility Risk Minor upgrade 17.2 → 17.3 Seconds (rolling) Fully compatible Low Major upgrade 17 → 18 Minutes Requires data dir upgrade Medium Minor Major Extension # Rolling upgrade: replicas first, then primary ansible \u003ccls\u003e -b -a 'yum upgrade -y postgresql17*' pg restart --role replica --force \u003ccls\u003e pg switchover \u003ccls\u003e pg restart \u003ccls\u003e \u003cold-primary\u003e --force # Recommended: Logical replication migration bin/pgsql-add pg-new # Create new version cluster # Configure logical replication to sync data... # Switch traffic to new cluster ansible \u003ccls\u003e -b -a 'yum upgrade -y postgis36_17*' psql -c 'ALTER EXTENSION postgis UPDATE;' For detailed online migration process, see Online Migration documentation.\nAction Description Risk Minor Version Upgrade Update packages, rolling restart Low Minor Version Downgrade Rollback to previous minor version Low Major Version Upgrade Logical replication or pg_upgrade Medium Extension Upgrade Upgrade extension packages and objects Low Minor Version Upgrade Minor version upgrades (e.g., 17.2 → 17.3) are the most common upgrade scenario, typically for security patches and bug fixes. Data directory is fully compatible, completed via rolling restart.\nStrategy: Recommended rolling upgrade: upgrade replicas first, then switchover to upgrade original primary - minimizes service interruption.\n1. Update repo → 2. Upgrade replica packages → 3. Restart replicas 4. Switchover → 5. Upgrade original primary packages → 6. Restart original primary Step 1: Prepare packages\nEnsure local repo has latest PostgreSQL packages and refresh node cache:\nRepo EL Debian cd ~/pigsty ./infra.yml -t repo_upstream # Add upstream repos (needs internet) ./infra.yml -t repo_build # Rebuild local repo ansible \u003ccls\u003e -b -a 'yum clean all' ansible \u003ccls\u003e -b -a 'yum makecache' ansible \u003ccls\u003e -b -a 'apt clean' ansible \u003ccls\u003e -b -a 'apt update' Step 2: Upgrade replicas\nUpgrade packages on all replicas and verify version:\nEL Debian ansible \u003ccls\u003e -b -a 'yum upgrade -y postgresql17*' ansible \u003ccls\u003e -b -a '/usr/pgsql/bin/pg_ctl --version' ansible \u003ccls\u003e -b -a 'apt install -y postgresql-17' ansible \u003ccls\u003e -b -a '/usr/lib/postgresql/17/bin/pg_ctl --version' Restart all replicas to apply new version:\npg restart --role replica --force \u003ccls\u003e Step 3: Switchover\nExecute switchover to transfer primary role to upgraded replica:\npg switchover \u003ccls\u003e # Or non-interactive: pg switchover --leader \u003cold-primary\u003e --candidate \u003cnew-primary\u003e --scheduled=now --force \u003ccls\u003e Step 4: Upgrade original primary\nOriginal primary is now replica - upgrade packages and restart:\nEL Debian ansible \u003cold-primary-ip\u003e -b -a 'yum upgrade -y postgresql17*' ansible \u003cold-primary-ip\u003e -b -a 'apt install -y postgresql-17' pg restart \u003ccls\u003e \u003cold-primary-name\u003e --force Step 5: Verify\nConfirm all instances have consistent version:\npg list \u003ccls\u003e pg query \u003ccls\u003e -c \"SELECT version()\" Minor Version Downgrade In rare cases (e.g., new version introduces bugs), may need to downgrade PostgreSQL to previous version.\nStep 1: Get old version packages\nEL Refresh Cache cd ~/pigsty; ./infra.yml -t repo_upstream # Add upstream repos cd /www/pigsty; repotrack postgresql17-*-17.1 # Download specific version packages cd ~/pigsty; ./infra.yml -t repo_create # Rebuild repo metadata ansible \u003ccls\u003e -b -a 'yum clean all' ansible \u003ccls\u003e -b -a 'yum makecache' Step 2: Execute downgrade\nEL Debian ansible \u003ccls\u003e -b -a 'yum downgrade -y postgresql17*' ansible \u003ccls\u003e -b -a 'apt install -y postgresql-17=17.1*' Step 3: Restart cluster\npg restart --force \u003ccls\u003e Major Version Upgrade Major version upgrades (e.g., 17 → 18) involve data format changes, requiring specialized tools for data migration.\nMethod Downtime Complexity Use Case Logical Replication Migration Seconds (switch) High Production, minimal downtime required pg_upgrade In-Place Upgrade Minutes~Hours Medium Test env, smaller data Recommended Approach For production, we recommend logical replication migration: create new version cluster, sync data via logical replication, then blue-green switch. Shortest downtime and rollback-ready. See Online Migration.\nLogical Replication Migration Logical replication is the recommended approach for production major version upgrades. Core steps:\n1. Create new version target cluster → 2. Configure logical replication → 3. Verify data consistency 4. Switch app traffic to new cluster → 5. Decommission old cluster Step 1: Create new version cluster\npg-meta-new: hosts: 10.10.10.12: { pg_seq: 1, pg_role: primary } vars: pg_cluster: pg-meta-new pg_version: 18 # New version bin/pgsql-add pg-meta-new Step 2: Configure logical replication\n-- Source cluster (old version) primary: create publication CREATE PUBLICATION upgrade_pub FOR ALL TABLES; -- Target cluster (new version) primary: create subscription CREATE SUBSCRIPTION upgrade_sub CONNECTION 'host=10.10.10.11 port=5432 dbname=mydb user=replicator password=xxx' PUBLICATION upgrade_pub; Step 3: Wait for sync completion\n-- Target cluster: check subscription status SELECT * FROM pg_stat_subscription; -- Source cluster: check replication slot LSN SELECT slot_name, confirmed_flush_lsn FROM pg_replication_slots; Step 4: Switch traffic\nAfter confirming data sync complete: stop app writes to source → wait for final sync → switch app connections to new cluster → drop subscription, decommission source.\n-- Target cluster: drop subscription DROP SUBSCRIPTION upgrade_sub; For detailed migration process, see Online Migration documentation.\npg_upgrade In-Place Upgrade pg_upgrade is PostgreSQL’s official major version upgrade tool, suitable for test environments or scenarios accepting longer downtime.\nImportant Warning In-place upgrade causes longer downtime and is difficult to rollback. For production, prefer logical replication migration.\nStep 1: Install new version packages\n./pgsql.yml -l \u003ccls\u003e -t pg_pkg -e pg_version=18 Step 2: Stop Patroni\npg pause \u003ccls\u003e # Pause auto-failover systemctl stop patroni # Stop Patroni (stops PostgreSQL) Step 3: Run pg_upgrade\nsudo su - postgres mkdir -p /data/postgres/pg-meta-18/data # Pre-check (-c parameter: check only, don't execute) /usr/pgsql-18/bin/pg_upgrade \\ -b /usr/pgsql-17/bin -B /usr/pgsql-18/bin \\ -d /data/postgres/pg-meta-17/data \\ -D /data/postgres/pg-meta-18/data \\ -v -c # Execute upgrade /usr/pgsql-18/bin/pg_upgrade \\ -b /usr/pgsql-17/bin -B /usr/pgsql-18/bin \\ -d /data/postgres/pg-meta-17/data \\ -D /data/postgres/pg-meta-18/data \\ --link -j 8 -v Step 4: Update links and start\nrm -rf /usr/pgsql \u0026\u0026 ln -s /usr/pgsql-18 /usr/pgsql rm -rf /pg \u0026\u0026 ln -s /data/postgres/pg-meta-18 /pg # Edit /etc/patroni/patroni.yml to update paths systemctl start patroni pg resume \u003ccls\u003e Step 5: Post-processing\n/usr/pgsql-18/bin/vacuumdb --all --analyze-in-stages ./delete_old_cluster.sh # Cleanup script generated by pg_upgrade Extension Upgrade When upgrading PostgreSQL version, typically also need to upgrade related extensions.\nUpgrade extension packages\nEL Debian ansible \u003ccls\u003e -b -a 'yum upgrade -y postgis36_17 timescaledb-2-postgresql-17* pgvector_17*' ansible \u003ccls\u003e -b -a 'apt install -y postgresql-17-postgis-3 postgresql-17-pgvector' Upgrade extension objects\nAfter package upgrade, execute extension upgrade in database:\n-- View upgradeable extensions SELECT name, installed_version, default_version FROM pg_available_extensions WHERE installed_version IS NOT NULL AND installed_version \u003c\u003e default_version; -- Upgrade extensions ALTER EXTENSION postgis UPDATE; ALTER EXTENSION timescaledb UPDATE; ALTER EXTENSION vector UPDATE; -- Check extension versions SELECT extname, extversion FROM pg_extension; Extension Compatibility Before major version upgrade, confirm all extensions support target PostgreSQL version. Some extensions may require uninstall/reinstall - check extension documentation.\nImportant Notes Backup first: Always perform complete backup before any upgrade Test verify: Verify upgrade process in test environment first Extension compatibility: Confirm all extensions support target version Rollback plan: Prepare rollback plan, especially for major upgrades Monitor closely: Monitor database performance and error logs after upgrade Document: Record all operations and issues during upgrade Related Documentation Online Migration: Zero-downtime migration using logical replication Patroni Management: Manage cluster with patronictl Cluster Management: Cluster creation, scaling, destruction Backup Recovery: PostgreSQL backup and recovery Extension Management: Extension installation and management ","categories":["Task"],"description":"Version upgrade - minor version rolling upgrade, major version migration, extension upgrade","excerpt":"Version upgrade - minor version rolling upgrade, major version …","ref":"/docs/pgsql/admin/upgrade/","tags":"","title":"Upgrading PostgreSQL Major/Minor Versions"},{"body":"","categories":["Tutorial"],"description":"Learn about Pigsty itself in every aspect - features, history, license, privacy policy, community events, and news.","excerpt":"Learn about Pigsty itself in every aspect - features, history, …","ref":"/docs/about/","tags":"","title":"About"},{"body":" “PostgreSQL In Great STYle”: Postgres, Infras, Graphics, Service, Toolbox, it’s all Yours.\n—— Battery-included, local-first PostgreSQL distribution, open-source RDS alternative\nValue Propositions Extensibility: Powerful extensions out-of-the-box: deep integration of PostGIS, TimescaleDB, Citus, PGVector and 440+ plugins with Oracle / SQL Server compatible kernels. Reliability: Quickly create high-availability, self-healing PostgreSQL clusters with auto-configured point-in-time recovery, access control, self-signed CA and SSL, ensuring rock-solid data. Observability: Based on Prometheus \u0026 Grafana modern observability stack, providing stunning monitoring best practices. Modular design, can be used independently: Gallery \u0026 Demo. Availability: Deliver stable, reliable, auto-routed, transaction-pooled, read-write separated high-performance database services, with flexible access modes via HAProxy, Pgbouncer, and VIP. Maintainability: Easy to use, Infrastructure as Code, Management SOPs, auto-tuning, local software repository, Vagrant sandbox and Terraform templates, zero-downtime migration solutions. Composability: Modular architecture design, reusable Infra, various optional modules: Redis, MinIO, ETCD, FerretDB, DuckDB, Docker, Supabase. Overview Pigsty is a better local open-source RDS for PostgreSQL alternative:\nBattery-Included RDS: From kernel to RDS distribution, providing production-grade PG database services for versions 13-18 on EL/Debian/Ubuntu. Rich Extensions: Providing unparalleled 440+ extensions with out-of-the-box distributed, time-series, geospatial, graph, vector, multi-modal database capabilities. Flexible Modular Architecture: Flexible composition, free extension: Redis/Etcd/MinIO/Mongo; can be used independently to monitor existing RDS/hosts/databases. Stunning Observability: Based on modern observability stack Prometheus/Grafana, providing stunning, unparalleled database observability capabilities. Battle-Tested Reliability: Self-healing high-availability architecture: automatic failover on hardware failure, seamless traffic switching. With auto-configured PITR as safety net for accidental data deletion! Easy to Use and Maintain: Declarative API, GitOps ready, foolproof operation, Database/Infra-as-Code and management SOPs encapsulating management complexity! Solid Security Practices: Encryption and backup all included, with built-in basic ACL best practices. As long as hardware and keys are secure, you don’t need to worry about database security! Broad Application Scenarios: Low-code data application development, or use preset Docker Compose templates to spin up massive software using PostgreSQL with one click! Open-Source Free Software: Own better database services at less than 1/10 the cost of cloud databases! Truly “own” your data and achieve autonomy! PostgreSQL integrates ecosystem tools and best practices:\nOut-of-the-box PostgreSQL distribution, deeply integrating 440+ extension plugins for geospatial, time-series, distributed, graph, vector, search, and AI! Runs on bare operating systems without container support, supporting mainstream operating systems: EL 8/9/10, Ubuntu 22.04/24.04, and Debian 12/13. Based on patroni, haproxy, and etcd, creating a self-healing high-availability architecture: automatic failover on hardware failure, seamless traffic switching. Based on pgBackRest and optional MinIO clusters providing out-of-the-box PITR point-in-time recovery, serving as a safety net for software defects and accidental data deletion. Based on Ansible providing declarative APIs to abstract complexity, greatly simplifying daily operations management in a Database-as-Code manner. Pigsty has broad applications, can be used as complete application runtime, develop demo data/visualization applications, and massive software using PG can be spun up with Docker templates. Provides Vagrant-based local development and testing sandbox environment, and Terraform-based cloud auto-deployment solutions, keeping development, testing, and production environments consistent. Deploy and monitor dedicated Redis (primary-replica, sentinel, cluster), MinIO, Etcd, Haproxy, MongoDB (FerretDB) clusters Battery-Included RDS Get production-grade PostgreSQL database services locally immediately!\nPostgreSQL is a near-perfect database kernel, but it needs more tools and systems to become a good enough database service (RDS). Pigsty helps PostgreSQL make this leap. Pigsty solves various challenges you’ll encounter when using PostgreSQL: kernel extension installation, connection pooling, load balancing, service access, high availability / automatic failover, log collection, metrics monitoring, alerting, backup recovery, PITR, access control, parameter tuning, security encryption, certificate issuance, NTP, DNS, parameter tuning, configuration management, CMDB, management playbooks… You no longer need to worry about these details!\nPigsty supports PostgreSQL 13 ~ 18 mainline kernels and other compatible forks, running on EL / Debian / Ubuntu and compatible OS distributions, available on x86_64 and ARM64 chip architectures, without container support required. Besides database kernels and many out-of-the-box extension plugins, Pigsty also provides complete infrastructure and runtime required for database services, as well as local sandbox / production environment / cloud IaaS auto-deployment solutions.\nPigsty can bootstrap an entire environment from bare metal with one click, reaching the last mile of software delivery. Ordinary developers and operations engineers can quickly get started and manage databases part-time, building enterprise-grade RDS services without database experts!\nRich Extensions Hyper-converged multi-modal, use PostgreSQL for everything, one PG to replace all databases!\nPostgreSQL’s soul lies in its rich extension ecosystem, and Pigsty uniquely deeply integrates 440+ extensions from the PostgreSQL ecosystem, providing you with an out-of-the-box hyper-converged multi-modal database!\nExtensions can create synergistic effects, producing 1+1 far greater than 2 results. You can use PostGIS for geospatial data, TimescaleDB for time-series/event stream data analysis, and Citus to upgrade it in-place to a distributed geospatial-temporal database; You can use PGVector to store and search AI embeddings, ParadeDB for ElasticSearch-level full-text search, and simultaneously use precise SQL, full-text search, and fuzzy vector for hybrid search. You can also achieve dedicated OLAP database/data lakehouse analytical performance through Hydra, duckdb_fdw, pg_analytics, pg_duckdb and other analytical extensions.\nUsing PostgreSQL as a single component to replace MySQL, Kafka, ElasticSearch, MongoDB, and big data analytics stacks has become a best practice — a single database choice can significantly reduce system complexity, greatly improve development efficiency and agility, achieving remarkable software/hardware and development/operations cost reduction and efficiency improvement.\nFlexible Modular Architecture Flexible composition, free extension, multi-database support, monitor existing RDS/hosts/databases\nComponents in Pigsty are abstracted as independently deployable modules, which can be freely combined to address varying requirements. The INFRA module comes with a complete modern monitoring stack, while the NODE module tunes nodes to desired state and brings them under management. Installing the PGSQL module on multiple nodes automatically forms a high-availability database cluster based on primary-replica replication, while the ETCD module provides consensus and metadata storage for database high availability.\nBeyond these four core modules, Pigsty also provides a series of optional feature modules: The MINIO module can provide local object storage capability and serve as a centralized database backup repository. The REDIS module can provide auxiliary services for databases in standalone primary-replica, sentinel, or native cluster modes. The DOCKER module can be used to spin up stateless application software.\nAdditionally, Pigsty provides PG-compatible / derivative kernel support. You can use Babelfish for MS SQL Server compatibility, IvorySQL for Oracle compatibility, OpenHaloDB for MySQL compatibility, and OrioleDB for ultimate OLTP performance.\nFurthermore, you can use FerretDB for MongoDB compatibility, Supabase for Firebase compatibility, and PolarDB to meet domestic compliance requirements. More professional/pilot modules will be continuously introduced to Pigsty, such as GPSQL, KAFKA, DUCKDB, VICTORIA, TIGERBEETLE, KUBERNETES, CONSUL, JUPYTER, GREENPLUM, CLOUDBERRY, MYSQL, …\nStunning Observability Using modern open-source observability stack, providing unparalleled monitoring best practices!\nPigsty provides best practices for monitoring based on the open-source Grafana / Prometheus modern observability stack: Grafana for visualization, VictoriaMetrics for metrics collection, VictoriaLogs for log collection and querying, Alertmanager for alert notifications. Blackbox Exporter for checking service availability. The entire system is also designed for one-click deployment as the out-of-the-box INFRA module.\nAny component managed by Pigsty is automatically brought under monitoring, including host nodes, load balancer HAProxy, database Postgres, connection pool Pgbouncer, metadata store ETCD, KV cache Redis, object storage MinIO, …, and the entire monitoring infrastructure itself. Numerous Grafana monitoring dashboards and preset alert rules will qualitatively improve your system observability capabilities. Of course, this system can also be reused for your application monitoring infrastructure, or for monitoring existing database instances or RDS.\nWhether for failure analysis or slow query optimization, capacity assessment or resource planning, Pigsty provides comprehensive data support, truly achieving data-driven operations. In Pigsty, over three thousand types of monitoring metrics are used to describe all aspects of the entire system, and are further processed, aggregated, analyzed, refined, and presented in intuitive visualization modes. From global overview dashboards to CRUD details of individual objects (tables, indexes, functions) in a database instance, everything is visible at a glance. You can drill down, roll up, or jump horizontally freely, browsing current system status and historical trends, and predicting future evolution.\nAdditionally, Pigsty’s monitoring system module can be used independently — to monitor existing host nodes and database instances, or cloud RDS services. With just one connection string and one command, you can get the ultimate PostgreSQL observability experience.\nVisit the Screenshot Gallery and Online Demo for more details.\nBattle-Tested Reliability Out-of-the-box high availability and point-in-time recovery capabilities ensure your database is rock-solid!\nFor table/database drops caused by software defects or human error, Pigsty provides out-of-the-box PITR point-in-time recovery capability, enabled by default without additional configuration. As long as storage space allows, base backups and WAL archiving based on pgBackRest give you the ability to quickly return to any point in the past. You can use local directories/disks, or dedicated MinIO clusters or S3 object storage services to retain longer recovery windows, according to your budget.\nMore importantly, Pigsty makes high availability and self-healing the standard for PostgreSQL clusters. The high-availability self-healing architecture based on patroni, etcd, and haproxy lets you handle hardware failures with ease: RTO \u003c 30s for primary failure automatic failover (configurable), with zero data loss RPO = 0 in consistency-first mode. As long as any instance in the cluster survives, the cluster can provide complete service, and clients only need to connect to any node in the cluster to get full service.\nPigsty includes built-in HAProxy load balancers for automatic traffic switching, providing DNS/VIP/LVS and other access methods for clients. Failover and active switchover are almost imperceptible to the business side except for brief interruptions, and applications don’t need to modify connection strings or restart. The minimal maintenance window requirements bring great flexibility and convenience: you can perform rolling maintenance and upgrades on the entire cluster without application coordination. The feature that hardware failures can wait until the next day to handle lets developers, operations, and DBAs sleep well. Many large organizations and core institutions have been using Pigsty in production for extended periods. The largest deployment has 25K CPU cores and 200+ PostgreSQL ultra-large instances; in this deployment case, dozens of hardware failures and various incidents occurred over six to seven years, DBAs changed several times, but still maintained availability higher than 99.999%.\nEasy to Use and Maintain Infra as Code, Database as Code, declarative APIs encapsulate database management complexity.\nPigsty provides services through declarative interfaces, elevating system controllability to a new level: users tell Pigsty “what kind of database cluster I want” through configuration inventories, without worrying about how to do it. In effect, this is similar to CRDs and Operators in K8S, but Pigsty can be used for databases and infrastructure on any node: whether containers, virtual machines, or physical machines.\nWhether creating/destroying clusters, adding/removing replicas, or creating new databases/users/services/extensions/whitelist rules, you only need to modify the configuration inventory and run the idempotent playbooks provided by Pigsty, and Pigsty adjusts the system to your desired state. Users don’t need to worry about configuration details — Pigsty automatically tunes based on machine hardware configuration. You only need to care about basics like cluster name, how many instances on which machines, what configuration template to use: transaction/analytics/critical/tiny — developers can also self-serve. But if you’re willing to dive into the rabbit hole, Pigsty also provides rich and fine-grained control parameters to meet the demanding customization needs of the most meticulous DBAs.\nBeyond that, Pigsty’s own installation and deployment is also one-click foolproof, with all dependencies pre-packaged, requiring no internet access during installation. The machine resources needed for installation can also be automatically obtained through Vagrant or Terraform templates, allowing you to spin up a complete Pigsty deployment from scratch on a local laptop or cloud VM in about ten minutes. The local sandbox environment can run on a 1-core 2GB micro VM, providing the same functional simulation as production environments, usable for development, testing, demos, and learning.\nSolid Security Practices Encryption and backup all included. As long as hardware and keys are secure, you don’t need to worry about database security.\nPigsty is designed for high-standard, demanding enterprise scenarios, adopting industry-leading security best practices to protect your data security (confidentiality/integrity/availability). The default configuration’s security is sufficient to meet compliance requirements for most scenarios.\nPigsty creates self-signed CAs (or uses your provided CA) to issue certificates and encrypt network communication. Sensitive management pages and API endpoints that need protection are password-protected. Database backups use AES encryption, database passwords use scram-sha-256 encryption, and plugins are provided to enforce password strength policies. Pigsty provides an out-of-the-box, easy-to-use, easily extensible ACL model, providing read/write/admin/ETL permission distinctions, with HBA rule sets following the principle of least privilege, ensuring system confidentiality through multiple layers of protection.\nPigsty enables database checksums by default to avoid silent data corruption, with replicas providing bad block fallback. Provides CRIT zero-data-loss configuration templates, using watchdog to ensure HA fencing as a fallback. You can audit database operations through the audit plugin, with all system and database logs collected for reference to meet compliance requirements.\nPigsty correctly configures SELinux and firewall settings, and follows the principle of least privilege in designing OS user groups and file permissions, ensuring system security baselines meet compliance requirements. Security is also uncompromised for auxiliary optional components like Etcd and MinIO — both use RBAC models and TLS encrypted communication, ensuring overall system security.\nA properly configured system easily passes Level 3 security certification. As long as you follow security best practices, deploy on internal networks with properly configured security groups and firewalls, database security will no longer be your pain point.\nBroad Application Scenarios Use preset Docker templates to spin up massive software using PostgreSQL with one click!\nIn various data-intensive applications, the database is often the trickiest part. For example, the core difference between GitLab Enterprise and Community Edition is the underlying PostgreSQL database monitoring and high availability. If you already have a good enough local PG RDS, you can refuse to pay for software’s homemade database components.\nPigsty provides the Docker module and many out-of-the-box Compose templates. You can use Pigsty-managed high-availability PostgreSQL (as well as Redis and MinIO) as backend storage, spinning up these software in stateless mode with one click: GitLab, Gitea, Wiki.js, NocoDB, Odoo, Jira, Confluence, Harbor, Mastodon, Discourse, KeyCloak, etc. If your application needs a reliable PostgreSQL database, Pigsty is perhaps the simplest way to get one.\nPigsty also provides application development toolsets closely related to PostgreSQL: PGAdmin4, PGWeb, ByteBase, PostgREST, Kong, as well as EdgeDB, FerretDB, Supabase — these “upper-layer databases” using PostgreSQL as storage. More wonderfully, you can build interactive data applications quickly in a low-code manner based on the Grafana and Postgres built into Pigsty, and even use Pigsty’s built-in ECharts panels to create more expressive interactive visualization works.\nPigsty provides a powerful runtime for your AI applications. Your agents can leverage PostgreSQL and the powerful capabilities of the observability world in this environment to quickly build data-driven intelligent agents.\nOpen-Source Free Software Pigsty is free software open-sourced under AGPLv3, watered by the passion of PostgreSQL-loving community members\nPigsty is completely open-source and free software, allowing you to run enterprise-grade PostgreSQL database services at nearly pure hardware cost without database experts. For comparison, database vendors’ “enterprise database services” and public cloud vendors’ RDS charge premiums several to over ten times the underlying hardware resources as “service fees.”\nMany users choose the cloud precisely because they can’t handle databases themselves; many users use RDS because there’s no other choice. We will break cloud vendors’ monopoly, providing users with a cloud-neutral, better open-source RDS alternative: Pigsty follows PostgreSQL upstream closely, with no vendor lock-in, no annoying “licensing fees,” no node count limits, and no data collection. All your core assets — data — can be “autonomously controlled,” in your own hands.\nPigsty itself aims to replace tedious manual database operations with database autopilot software, but even the best software can’t solve all problems. There will always be some rare, low-frequency edge cases requiring expert intervention. This is why we also provide professional subscription services to provide safety nets for enterprise users who need them. Subscription consulting fees of tens of thousands are less than one-thirtieth of a top DBA’s annual salary, completely eliminating your concerns and putting costs where they really matter. For community users, we also contribute with love, providing free support and daily Q\u0026A.\n","categories":["Reference"],"description":"Pigsty's value propositions and highlight features.","excerpt":"Pigsty's value propositions and highlight features.","ref":"/docs/about/feature/","tags":"","title":"Features"},{"body":" Release Strategy Pigsty uses semantic versioning: \u003cmajor\u003e.\u003cminor\u003e.\u003cpatch\u003e. Alpha/Beta/RC versions will have suffixes like -a1, -b1, -c1 appended to the version number.\nMajor version updates signify incompatible foundational changes and major new features; minor version updates typically indicate regular feature updates and small API changes; patch version updates mean bug fixes and package version updates.\nPigsty plans to release one major version update per year. Minor version updates usually follow PostgreSQL’s minor version update rhythm, catching up within a month at the latest after a new PostgreSQL version is released. Pigsty typically plans 4-6 minor versions per year. For complete release history, please refer to Release Notes.\nDeploy with Specific Version Numbers Pigsty develops using the main trunk branch. Please always use Releases with version numbers.\nUnless you know what you’re doing, do not use GitHub’s main branch. Always check out and use a specific version.\nFeatures Under Consideration A sufficiently good command-line management tool ARM architecture support for infrastructure components Add more extensions for PostgreSQL More preset scenario-based configuration templates Fully migrate software repository and installation download sources to Cloudflare Deploy and monitor highly available Kubernetes clusters using SealOS! Use VictoriaMetrics to replace Prometheus for time-series data storage Monitor and deploy MySQL databases Monitor databases in Kubernetes Provide richer Docker application templates PGLite browser-side support Here are our Active Issues and Roadmap.\nExtensions and Packages For the extension support roadmap, you can find it here: https://pgext.cloud/e/roadmap\nUnder Consideration walminer is_jsonb_valid https://github.com/furstenheim/is_jsonb_valid pg_kafka https://github.com/xstevens/pg_kafka pg_jieba https://github.com/jaiminpan/pg_jieba pg_paxos https://github.com/microsoft/pg_paxos OneSparse https://github.com/OneSparse/OneSparse PipelineDB https://github.com/pipelinedb/pipelinedb SQL Firewall https://github.com/uptimejp/sql_firewall zcurve https://github.com/bmuratshin/zcurve PG dot net https://github.com/Brick-Abode/pldotnet/releases pg_scws: https://github.com/jaiminpan/pg_scws themsis: https://github.com/cossacklabs/pg_themis pgspeck https://github.com/johto/pgspeck lsm3 https://github.com/postgrespro/lsm3 monq https://github.com/postgrespro/monq pg_badplan https://github.com/trustly/pg_badplan pg_recall https://github.com/mreithub/pg_recall pgfsm https://github.com/michelp/pgfsm pg_trgm pro https://github.com/postgrespro/pg_trgm_pro pgsql-fio: https://github.com/csimsek/pgsql-fio Not Considering for Now pg_tier: not ready due to incomplete dep parquet_s3_fdw parquet_s3_fdw: not ready due to compiler version pg_top: not ready due to cmake error timestamp9: not ready due to compiler error pg_tier obsolete pg_timeseries, we already have timescaledb pg_quack, we already have a pg_lakehouse pg_telemetry, we already have better observability pgx_ulid, https://github.com/pksunkara/pgx_ulid, already covered by pg_idkit (MIT, but RUST) embedding: obsolete FEAT zson https://github.com/postgrespro/zson MIT C (too old) GIS pghydro https://github.com/pghydro/pghydro C GPL-2.0 6.6 (no makefile) https://github.com/Zeleo/pg_natural_sort_order (too old) https://github.com/postgrespro/pg_query_state https://github.com/no0p/pgsampler pg_lz4 https://github.com/zilder/pg_lz4 pg_amqp https://github.com/omniti-labs/pg_amqp tinyint https://github.com/umitanuki/tinyint-postgresql pg_blkchain https://github.com/blkchain/pg_blkchain hashtypes https://github.com/pandrewhk/hashtypes foreign_table_exposer https://github.com/komamitsu/foreign_table_exposer ldap_fdw https://github.com/guedes/ldap_fdw pg_backtrace https://github.com/postgrespro/pg_backtrace connection_limits https://github.com/tvondra/connection_limits fixeddecimal https://github.com/2ndQuadrant/fixeddecimal ","categories":["Reference"],"description":"Future feature planning, new feature release schedule, and todo list.","excerpt":"Future feature planning, new feature release schedule, and todo list.","ref":"/docs/about/roadmap/","tags":"","title":"Roadmap"},{"body":" Historical Origins The Pigsty project began in 2018-2019, originating from Tantan. Tantan is an internet dating app — China’s Tinder, now acquired by Momo. Tantan was a Nordic-style startup with a Swedish engineering founding team.\nTantan had excellent technical taste, using PostgreSQL and Go as its core technology stack. The entire Tantan system architecture was modeled after Instagram, designed entirely around the PostgreSQL database. Up to several million daily active users, millions of TPS, and hundreds of TB of data, the data component used only PostgreSQL. Almost all business logic was implemented using PG stored procedures — even including 100ms recommendation algorithms!\nThis atypical development model of deeply using PostgreSQL features placed extremely high demands on the capabilities of engineers and DBAs. And Pigsty is the open-source project we forged in this real-world large-scale, high-standard database cluster scenario — embodying our experience and best practices as top PostgreSQL experts.\nDevelopment Process In the beginning, Pigsty did not have the vision, goals, and scope it has today. It aimed to provide a PostgreSQL monitoring system for our own use. We surveyed all available solutions — open-source, commercial, cloud-based, datadog, pgwatch, etc. — and none could meet our observability needs. So we decided to build one ourselves based on Grafana and Prometheus. This became Pigsty’s predecessor and prototype. Pigsty as a monitoring system was quite impressive, helping us solve countless management problems.\nSubsequently, developers wanted such a monitoring system on their local development machines, so we used Ansible to write provisioning playbooks, transforming this system from a one-time construction task into reusable, replicable software. The new functionality allowed users to use Vagrant and Terraform, using Infrastructure as Code to quickly spin up local DevBox development machines or production environment servers, automatically completing PostgreSQL and monitoring system deployment.\nNext, we redesigned the production environment PostgreSQL architecture, introducing Patroni and pgBackRest to solve database high availability and point-in-time recovery issues. We developed a zero-downtime migration solution based on logical replication, rolling upgrading two hundred production database clusters to the latest major version through blue-green deployment. And we incorporated these capabilities into Pigsty.\nPigsty is software we made for ourselves. As client users ourselves, we know exactly what we need and won’t slack on our own requirements. The greatest benefit of “eating dog food” is that we are both developers and users — therefore we know exactly what we need and won’t slack on our own requirements.\nWe solved problem after problem, depositing the solutions into Pigsty. Pigsty’s positioning also gradually evolved from a monitoring system into an out-of-the-box PostgreSQL database distribution. Therefore, at this stage, we decided to open-source Pigsty and began a series of technical sharing and publicity, and external users from various industries began using Pigsty and providing feedback.\nFull-Time Entrepreneurship In 2022, the Pigsty project received seed funding from Miracle Plus, initiated by Dr. Qi Lu, allowing me to work on this full-time.\nAs an open-source project, Pigsty has developed quite well. In these two years of full-time entrepreneurship, Pigsty’s GitHub stars have multiplied from a few hundred to 3,700; it made the HN front page, and growth began snowballing; In the OSSRank open-source rankings, Pigsty ranks 22nd among PostgreSQL ecosystem projects, the highest among Chinese-led projects.\nPreviously, Pigsty could only run on CentOS 7, but now it basically covers all mainstream Linux distributions (EL, Debian, Ubuntu). Supported PG major versions cover 13-18, maintaining, collecting, and integrating 440 extension plugins in the PG ecosystem. Among these, I personally maintain over half of the extension plugins, providing out-of-the-box RPM/DEB packages. Including Pigsty itself, “based on open source, giving back to open source,” this is making some contribution to the PG ecosystem.\nPigsty’s positioning has also continuously evolved from a PostgreSQL database distribution to an open-source cloud database alternative. It truly benchmarks against cloud vendors’ entire cloud database brands.\nRebel Against Public Clouds Public cloud vendors like AWS, Azure, GCP, and Aliyun have provided many conveniences for startups, but they are closed-source and force users to rent infrastructure at exorbitant fees.\nWe believe that excellent database services, like excellent database kernels, should be accessible to every user, rather than requiring expensive rental from cyber lords.\nCloud computing’s agility and elasticity are great, but it should be free, open-source, inclusive, and local-first — We believe the cloud computing universe needs a solution representing open-source values that returns infrastructure control to users without sacrificing the benefits of the cloud.\nTherefore, we are also leading a movement and battle to exit the cloud, as rebels against public clouds, to reshape the industry’s values.\nOur Vision I hope that in the future world, everyone will have the de facto right to freely use excellent services, rather than being confined to a few cyber lord public cloud giants’ territories as cyber tenants or even cyber serfs.\nThis is exactly what Pigsty aims to do — a better, free and open-source RDS alternative. Allowing users to spin up database services better than cloud RDS anywhere (including cloud servers) with one click.\nPigsty is a complete complement to PostgreSQL, and a spicy mockery of cloud databases. Its original meaning is “pigsty,” but it’s also an acronym for Postgres In Great STYle, meaning “PostgreSQL in its full glory.”\nPigsty itself is completely free and open-source software. We purely rely on providing consulting and services to sustain operations. A well-built system may run for years without encountering problems needing a “safety net,” but database problems, once they occur, are never small issues. Often, expert experience can turn decay into magic with a word, and we provide such services to clients in need — we believe this is a more just, reasonable, and sustainable model.\nAbout the Team I am Feng Ruohang, the author of Pigsty. The vast majority of Pigsty’s code was developed by me alone, with individual features contributed by the community.\nIndividual heroism still exists in the software field. Only unique individuals can create unique works — I hope Pigsty can become such a work.\nIf you’re interested in me, here’s my personal homepage: https://vonng.com/\n“Modb Interview with Feng Ruohang” (Chinese)\n“Post-90s, Quit to Start Business, Says Will Crush Cloud Databases” (Chinese)\n","categories":["Reference"],"description":"The origin and motivation of the Pigsty project, its development history, and future goals and vision.","excerpt":"The origin and motivation of the Pigsty project, its development …","ref":"/docs/about/history/","tags":"","title":"History"},{"body":" Recent News 2025-11-29: Pigsty won the PostgreSQL Magneto Award!\nThe 8th Conference of PostgreSQL Ecosystem (Hangzhou, China) Topics: “A World-Grade Postgres Meta Distribution”, AI database considerations, PostgreSQL delivery best practices Pigsty v3.4.1 Released! OpenHalo \u0026 OrioleDB support, MySQL compatibility, pgAdmin improvements\nRelease Notes: v3.4.1 Pigsty v3.4.0 Released! Better backups, automatic Certbot certificates, Ivory cross-platform, AGE extension, APP improvements\nRelease Notes: v3.4.0 Pigsty v3.3.0 Released! 404 extensions, Odoo/Dify/Supabase app templates, DocumentDB support\nRelease Notes: v3.3.0 Pigsty 3.2.2 Released!\nPigsty 3.2.1 Released!\nPigsty 3.2.0 Released!\nPostgreSQL Package Manager pig Released!\nANNOUNCE pig: The Postgres Extension Wizard PIG: A New Package Manager for PostgreSQL Extensions Pigsty 3.1.0 Released with complete PostgreSQL 17.2 extension support\nBlog Post: “Pigsty v3.1: Self-hosted Supabase, PG17 Default, MinIO Improvements, ARM \u0026 Ubuntu24 Support” PostgreSQL Official News: “Pigsty v3.1 Release: PG17, Duck Extensions, Self-hosting Supabase, ARM \u0026 Ubuntu24” Release Notes: v3.1.0 Postgres Weekly Issue 579: https://postgresweekly.com/issues/579 Pigsty 3.0.4 Released! Extension catalog and repository, PG17 extension compilation, Supabase self-hosting optimization\nRelease Notes: v3.0.4 Pigsty 3.0.3 Released! Official PostgreSQL 17 support, Etcd operations and monitoring optimization\nRelease Notes: v3.0.3 Pigsty 3.0.2 Released! Slim installation mode, PolarDB 15 support, routine bug fixes (2024-09-07)\nRelease Notes: v3.0.2 Pigsty 3.0.1 Released! Oracle compatibility, Patroni 4 support, routine bug fixes (2024-08-31)\nRelease Notes: v3.0.1 Pigsty 3.0.0 Released! 333 extensions, replaceable kernels, complete RDS service!\nRelease Notes: v3.0.0 Feature Introduction: Pigsty v3.0.0 News: Pigsty’s supplementary Yum/APT repositories provide 254 additional ready-to-use binary RPM/DEB extensions!\nPostgreSQL Official: Pigsty Supplementary APT/YUM Repository with 254 additional PostgreSQL Extensions! PGCon.Dev 2024 Trip Report!\nPigsty v2.7 Released!\nPostgreSQL Official: Pigsty v2.7 Released, free RDS PG with 255 extensions available Postgres Weekly: [https://postgresweekly.com/issues/556) Pigsty Blog: Pigsty v2.7: The Great Integration Pigsty v2.6 Released!\nPostgreSQL Official: Pigsty, Battery-included PostgreSQL Distro \u0026 Free RDS Alternative, v2.6 released! Postgres Planet (X): https://twitter.com/PostgreSQL/status/1765323952669290515 Postgres Weekly: https://postgresweekly.com/issues/545 Pigsty Blog: Pigsty v2.6: PG Challenges OLAP The name of this project always makes me grin: PIGSTY is actually an acronym, standing for Postgres In Great STYle! It’s a Postgres distribution that includes lots of components and tools out of the box in areas like availability, deployment, and observability. The latest release pushes everything up to Postgres 16.2 standards and introduces new ParadeDB and DuckDB FDW extensions.\nConferences \u0026 Talks Date Type Event Topic 2025-11-29 Award\u0026Talk The 8th Conf of PG Ecosystem (Hangzhou) PostgreSQL Magneto Award, A World-Grade Postgres Meta Distribution 2025-05-16 Lightning PGConf.Dev 2025, Montreal Extension Delivery: Make your PGEXT accessible to users 2025-05-12 Keynote PGEXT.DAY, PGCon.Dev 2025 The Missing Package Manager and Extension Repo for PostgreSQL Ecosystem 2025-04-19 Workshop PostgreSQL Database Technology Summit Using Pigsty to Deploy PG Ecosystem Partners: Dify, Odoo, Supabase 2025-04-11 Live Host OSCHINA Data Intelligence Talk Is the Viral MCP Hype or Revolutionary? 2025-01-15 Live Stream Open Source Veterans \u0026 Newcomers Episode 4 PostgreSQL Extensions Devouring DB World? PG Package Manager pig \u0026 Self-hosted RDS 2025-01-09 Award OSCHINA 2024 Outstanding Contribution Expert Outstanding Contribution Expert Award 2025-01-06 Panel China PostgreSQL Database Ecosystem Conference PostgreSQL Extensions are Devouring the Database World 2024-11-23 Podcast Tech Hotpot Podcast From the Linux Foundation: Why the Recent Focus on ‘Chokepoints’? 2024-08-21 Interview Blue Tech Wave Interview with Feng Ruohang: Simplifying PG Management 2024-08-15 Tech Summit GOTC Global Open Source Technology Summit PostgreSQL AI/ML/RAG Extension Ecosystem and Best Practices 2024-07-12 Keynote 13th PG China Technical Conference The Future of Database World: Extensions, Service, and Postgres 2024-05-31 Unconference PGCon.Dev 2024 Global PG Developer Conference Built-in Prometheus Metrics Exporter 2024-05-28 Seminar PGCon.Dev 2024 Extension Summit Extension in Core \u0026 Binary Packing 2024-05-10 Live Debate Three-way Talk: Cloud Mudslide Series Episode 3 Is Public Cloud a Scam? 2024-04-17 Live Debate Three-way Talk: Cloud Mudslide Series Episode 2 Are Cloud Databases a Tax on Intelligence? 2024-04-16 Panel Cloudflare Immerse Shenzhen Cyber Bodhisattva Panel Discussion 2024-04-12 Tech Summit 2024 Data Technology Carnival Pigsty: Solving PostgreSQL Operations Challenges 2024-03-31 Live Debate Three-way Talk: Cloud Mudslide Series Episode 1 Luo Selling Cloud While We’re Moving Off Cloud? 2024-01-24 Live Host OSCHINA Open Source Talk Episode 9 Will DBAs Be Eliminated by Cloud? 2023-12-20 Live Debate Open Source Talk Episode 7 To Cloud or Not: Cost Cutting or Value Creation? 2023-11-24 Tech Summit Vector Databases in the LLM Era Panel: New Future of Vector Databases in the AI Age 2023-09-08 Interview Motianlun Feature Interview Feng Ruohang: A Tech Enthusiast Who Makes Great Open Source Founders 2023-08-16 Tech Summit DTCC 2023 DBA Night: PostgreSQL vs MySQL Open Source License Issues 2023-08-09 Live Debate Open Source Talk Episode 1 MySQL vs PostgreSQL: Which is World’s No.1? 2023-07-01 Tech Summit SACC 2023 Workshop 8: FinOps Practice: Cloud Cost Management \u0026 Optimization 2023-05-12 Meetup PostgreSQL China Wenzhou Meetup PG With DB4AI: Vector Database PGVECTOR \u0026 AI4DB: Self-Driving Database Pigsty 2023-04-08 Tech Summit Database Carnival 2023 A Better Open Source RDS Alternative: Pigsty 2023-04-01 Tech Summit PostgreSQL China Xi’an Meetup PG High Availability \u0026 Disaster Recovery Best Practices 2023-03-23 Live Stream Bytebase x Pigsty Best Practices for Managing PostgreSQL: Bytebase x Pigsty 2023-03-04 Tech Summit PostgreSQL China Conference Challenging RDS, Pigsty v2.0 Release 2023-02-01 Tech Summit DTCC 2022 Open Source RDS Alternative: Battery-Included, Self-Driving Database Distro Pigsty 2022-07-21 Live Debate Cloud Swallows Open Source Can Open Source Strike Back Against Cloud? 2022-07-04 Interview Creator’s Story Post-90s Developer Quits to Start Up, Aiming to Challenge Cloud Databases 2022-06-28 Live Stream Bass’s Roundtable DBA’s Gospel: SQL Audit Best Practices 2022-06-12 Demo Day MiraclePlus S22 Demo Day User-Friendly Cost-Effective Database Distribution Pigsty 2022-06-05 Live Stream PG Chinese Community Sharing Pigsty v1.5 Quick Start, New Features \u0026 Production Cluster Setup ","categories":["Reference"],"description":"News and events related to Pigsty and PostgreSQL, including latest announcements!","excerpt":"News and events related to Pigsty and PostgreSQL, including latest …","ref":"/docs/about/event/","tags":"","title":"News \u0026 Events"},{"body":" GitHub Our GitHub repository is: https://github.com/pgsty/pigsty. Please give us a ⭐️ star!\nWe welcome anyone to submit new Issues or create Pull Requests, propose feature suggestions, and contribute to Pigsty.\nPlease note that for issues related to Pigsty documentation, please submit Issues in the github.com/pgsty/pigsty.cc repository.\nWeChat Groups Chinese users are mainly active in WeChat groups. Currently, there are seven active groups. Groups 1-4 are full; for other groups, you need to add the assistant’s WeChat to be invited.\nTo join the WeChat community, search for “Pigsty小助手” (WeChat ID: pigsty-cc), note or send “加群” (join group), and the assistant will invite you to the group.\nInternational Community Telegram: https://t.me/joinchat/gV9zfZraNPM3YjFh\nDiscord: https://discord.gg/j5pG8qfKxU\nYou can also contact me via email: rh@vonng.com\nCommunity Help When you encounter problems using Pigsty, you can seek help from the community. The more information you provide, the more likely you are to get help from the community.\nPlease refer to the Community Help Guide and provide as much information as possible so that community members can help you solve the problem. Here is a reference template for asking for help:\nWhat happened? (Required)\nPigsty version and OS version (Required)\n$ grep version pigsty.yml $ cat /etc/os-release $ uname -a Some cloud providers have customized standard OS distributions. You can tell us which cloud provider’s OS image you are using. If you have customized and modified the environment after installing the OS, or if there are specific security rules and firewall configurations in your LAN, please also inform us when asking questions.\nPigsty configuration file\nPlease don’t forget to redact any sensitive information: passwords, internal keys, sensitive configurations, etc.\ncat ~/pigsty/pigsty.yml What did you expect to happen?\nPlease describe what should happen under normal circumstances, and how the actual situation differs from expectations.\nHow to reproduce this issue?\nPlease tell us in as much detail as possible how to reproduce this issue.\nMonitoring screenshots\nIf you are using the monitoring system provided by Pigsty, you can provide relevant screenshots.\nError logs\nPlease provide logs related to the error as much as possible. Please do not paste content like “Failed to start xxx service” that has no informational value.\nYou can query logs from Grafana / VictoriaLogs, or get logs from the following locations:\nSyslog: /var/log/messages (rhel) or /var/log/syslog (debian) Postgres: /pg/log/postgres/* Patroni: /pg/log/patroni/* Pgbouncer: /pg/log/pgbouncer/* Pgbackrest: /pg/log/pgbackrest/* journalctl -u patroni journalctl -u \u003cservice name\u003e Have you searched Issues/Website/FAQ?\nIn the FAQ, we provide answers to many common questions. Please check before asking.\nYou can also search for related issues from GitHub Issues and Discussions:\nPigsty FAQ Github Issues Pigsty Discussions Is there any other information we need to know?\nThe more information and context you provide, the more likely we can help you solve the problem.\n","categories":["Reference"],"description":"Pigsty is a Build in Public project. We are very active on GitHub, and Chinese users are mainly active in WeChat groups.","excerpt":"Pigsty is a Build in Public project. We are very active on GitHub, and …","ref":"/docs/about/community/","tags":"","title":"Join the Community"},{"body":" Pigsty Software When you install Pigsty software, if you use offline package installation in a network-isolated environment, we will not receive any data about you.\nIf you choose online installation, when downloading related packages, our servers or cloud provider servers will automatically log the visiting machine’s IP address and/or hostname in the logs, along with the package names you downloaded.\nWe will not share this information with other organizations unless required by law. (Honestly, we’d have to be really bored to look at this stuff.)\nPigsty’s primary domain is: pigsty.io. For mainland China, please use the registered mirror site pigsty.cc.\nPigsty Website When you visit our website, our servers will automatically log your IP address and/or hostname in Nginx logs.\nWe will only store information such as your email address, name, and location when you decide to send us such information by completing a survey or registering as a user on one of our websites.\nWe collect this information to help us improve website content, customize web page layouts, and contact people for technical and support purposes. We will not share your email address with other organizations unless required by law.\nThis website uses Google Analytics, a web analytics service provided by Google, Inc. (“Google”). Google Analytics uses “cookies,” which are text files placed on your computer to help the website analyze how users use the site.\nThe information generated by the cookie about your use of the website (including your IP address) will be transmitted to and stored by Google on servers in the United States. Google will use this information to evaluate your use of the website, compile reports on website activity for website operators, and provide other services related to website activity and internet usage. Google may also transfer this information to third parties if required by law or where such third parties process the information on Google’s behalf. Google will not associate your IP address with any other data held by Google. You may refuse the use of cookies by selecting the appropriate settings on your browser, however, please note that if you do this, you may not be able to use the full functionality of this website. By using this website, you consent to the processing of data about you by Google in the manner and for the purposes set out above.\nIf you have any questions or comments about this policy, or request deletion of personal data, you can contact us by sending an email to rh@pigsty.io\n","categories":["Reference"],"description":"What user data does Pigsty software and website collect, and how will we process your data and protect your privacy?","excerpt":"What user data does Pigsty software and website collect, and how will …","ref":"/docs/about/privacy/","tags":"","title":"Privacy Policy"},{"body":"Official License: https://github.com/pgsty/pigsty/blob/main/LICENSE\nLicense Summary Pigsty uses Apache-2.0, AGPLv3 for two optional modules, and CC BY 4.0 for documentation.\nPigsty Core The Pigsty core is licensed under Apache License 2.0.\nApache-2.0 is a permissive open-source license. You may freely use, modify, and distribute the software for commercial purposes without opening your own source code or adopting the same license.\nWhat This License Grants What This License Does NOT Grant License Conditions Commercial use Trademark use Include license and copyright notice Modification Liability \u0026 warranty State changes Distribution Patent grant Private use Pigsty Optional Modules The INFRA and MINIO modules are licensed under GNU Affero General Public License v3.0 (AGPLv3).\nAGPLv3 does not affect regular users: using the software is not “distribution,” so your business code using Pigsty need not be open-sourced.\nAGPLv3 obligations apply only when you “distribute” these modules or modifications as part or all of a software/service offering.\nWhat This License Grants What This License Does NOT Grant License Conditions Commercial use Trademark use Include license and prominent notice Modification Liability \u0026 warranty Maintain open-source status Distribution Disclose source code Patent grant Network use is distribution Private use Use same license These modules are optional — avoid them completely to evade AGPLv3 requirements. If used, AGPLv3 compliance is straightforward since Grafana and MinIO already use AGPLv3.\nFiles and directories under AGPL-3.0:\nFiles and Directories Description roles/infra/ Infrastructure module (Grafana integration) roles/minio/ MinIO object storage module (optional backup repository) files/grafana/ Grafana dashboard definitions infra.yml INFRA module installation playbook infra-rm.yml INFRA module removal playbook minio.yml MinIO module installation playbook minio-rm.yml MinIO module removal playbook Pigsty Documentation Pigsty documentation sites (pigsty.cc, pigsty.io, pgsty.com) use Creative Commons Attribution 4.0 International (CC BY 4.0).\nCC BY 4.0 permits free sharing and adaptation with appropriate credit, a license link, and indication of changes.\nWhat This License Grants What This License Does NOT Grant License Conditions Commercial use Trademark use Attribution Modification Liability \u0026 warranty Indicate changes Distribution Patent grant Provide license link Private use SBOM Inventory Open-source software used or related to the Pigsty project.\nFor PostgreSQL extension plugin licenses, refer to PostgreSQL Extension License List.\nModule Software Name License Purpose \u0026 Description Necessity PGSQL PostgreSQL PostgreSQL License PostgreSQL kernel Required PGSQL patroni MIT License PostgreSQL high availability Required ETCD etcd Apache License 2.0 HA consensus and distributed config storage Required INFRA Ansible GPLv3 Executes playbooks and management commands Required INFRA Nginx BSD-2 Exposes Web UI and serves local repo Recommended PGSQL pgbackrest MIT License PITR backup/recovery management Recommended PGSQL pgbouncer ISC License PostgreSQL connection pooling Recommended PGSQL vip-manager BSD 2-Clause License Automatic L2 VIP binding to PG primary Recommended PGSQL pg_exporter Apache License 2.0 PostgreSQL and PgBouncer monitoring Recommended NODE node_exporter Apache License 2.0 Host node monitoring metrics Recommended NODE haproxy HAPROXY’s License (GPLv2) Load balancing and service exposure Recommended INFRA Grafana AGPLv3 Database visualization platform Recommended INFRA VictoriaMetrics Apache License 2.0 TSDB, metric collection, alerting Recommended INFRA VictoriaLogs Apache License 2.0 Centralized log collection, storage, query Recommended INFRA DNSMASQ GPLv2 / GPLv3 DNS resolution and cluster name lookup Recommended MINIO MinIO AGPLv3 S3-compatible object storage service Optional NODE keepalived MIT License VIP binding on node clusters Optional REDIS Redis Redis License (BSD-3) Cache service, locked at 7.2.6 Optional REDIS Redis Exporter MIT License Redis monitoring Optional MONGO FerretDB Apache License 2.0 MongoDB compatibility over PostgreSQL Optional DOCKER docker-ce Apache License 2.0 Container management Optional CLOUD SealOS Apache License 2.0 Fast K8S cluster deployment and packaging Optional DUCKDB DuckDB MIT High-performance analytics Optional External Vagrant Business Source License 1.1 Local test environment VMs Optional External Terraform Business Source License 1.1 One-click cloud resource provisioning Optional External Virtualbox GPLv2 Virtual machine management software Optional Necessity Levels:\nRequired: Essential core capabilities, no option to disable Recommended: Enabled by default, can be disabled via configuration Optional: Not enabled by default, can be enabled via configuration Apache-2.0 License Text Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright (C) 2018-2026 Ruohang Feng, @Vonng (rh@vonng.com) Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. AGPLv3 License Text GNU AFFERO GENERAL PUBLIC LICENSE Version 3, 19 November 2007 Copyright (C) 2007 Free Software Foundation, Inc. \u003chttps://fsf.org/\u003e Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Preamble The GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software. The licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users. When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things. Developers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software. A secondary benefit of defending all users' freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate. Many developers of free software are heartened and encouraged by the resulting cooperation. However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public. The GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community. It requires the operator of a network server to provide the source code of the modified version running there to the users of that server. Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version. An older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals. This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license. The precise terms and conditions for copying, distribution and modification follow. TERMS AND CONDITIONS 0. Definitions. \"This License\" refers to version 3 of the GNU Affero General Public License. \"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks. \"The Program\" refers to any copyrightable work licensed under this License. Each licensee is addressed as \"you\". \"Licensees\" and \"recipients\" may be individuals or organizations. To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work. A \"covered work\" means either the unmodified Program or a work based on the Program. To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well. To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying. An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion. 1. Source Code. The \"source code\" for a work means the preferred form of the work for making modifications to it. \"Object code\" means any non-source form of a work. A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language. The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it. The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work. The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source. The Corresponding Source for a work in source code form is that same work. 2. Basic Permissions. All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law. You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you. Conveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary. 3. Protecting Users' Legal Rights From Anti-Circumvention Law. No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures. When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures. 4. Conveying Verbatim Copies. You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program. You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee. 5. Conveying Modified Source Versions. You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions: a) The work must carry prominent notices stating that you modified it, and giving a relevant date. b) The work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to \"keep intact all notices\". c) You must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it. d) If the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so. A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate. 6. Conveying Non-Source Forms. You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways: a) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange. b) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge. c) Convey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b. d) Convey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements. e) Convey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d. A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work. A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product. \"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made. If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM). The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network. Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying. 7. Additional Terms. \"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions. When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission. Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms: a) Disclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or b) Requiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or c) Prohibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or d) Limiting the use for publicity purposes of names of licensors or authors of that material; or e) Declining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or f) Requiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors. All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying. If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms. Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way. 8. Termination. You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11). However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation. Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice. Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10. 9. Acceptance Not Required for Having Copies. You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so. 10. Automatic Licensing of Downstream Recipients. Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License. An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts. You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it. 11. Patents. A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor's \"contributor version\". A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License. Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version. In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party. If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid. If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it. A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007. Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law. 12. No Surrender of Others' Freedom. If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program. 13. Remote Network Interaction; Use with the GNU General Public License. Notwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software. This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph. Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License. 14. Revised Versions of this License. The Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU Affero General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation. If the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program. Later license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version. 15. Disclaimer of Warranty. THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. 16. Limitation of Liability. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. 17. Interpretation of Sections 15 and 16. If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee. END OF TERMS AND CONDITIONS How to Apply These Terms to Your New Programs If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms. To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found. Copyright (C) 2018-2026 Ruohang Feng, Author of Pigsty This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program. If not, see \u003chttps://www.gnu.org/licenses/\u003e. Also add information on how to contact you by electronic and paper mail. If your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source. For example, if your program is a web application, its interface could display a \"Source\" link that leads users to an archive of the code. There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements. You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see \u003chttps://www.gnu.org/licenses/\u003e. ","categories":["Reference"],"description":"Pigsty's open-source licenses — Apache-2.0, AGPLv3, and CC BY 4.0","excerpt":"Pigsty's open-source licenses — Apache-2.0, AGPLv3, and CC BY 4.0","ref":"/docs/about/license/","tags":"","title":"License"},{"body":" Sponsor Us Pigsty is a free and open-source software, passionately developed by PostgreSQL community members, aiming to integrate the power of the PostgreSQL ecosystem and promote the widespread adoption of PostgreSQL. If our work has helped you, please consider sponsoring or supporting our project:\nSponsor us directly with financial support - express your sincere support in the most direct and powerful way! Consider purchasing our Technical Support Services. We can provide professional PostgreSQL high-availability cluster deployment and maintenance services, making your budget worthwhile! Share your Pigsty use cases and experiences through articles, talks, and videos. Allow us to mention your organization in “Users of Pigsty.” Recommend/refer our project and services to friends, colleagues, and clients in need. Follow our WeChat Official Account and share relevant technical articles to groups and your social media. Angel Investors Pigsty is a project invested by Miracle Plus (formerly YC China) S22. We thank Miracle Plus and Dr. Qi Lu for their support of this project!\nSponsors Special thanks to Vercel for sponsoring pigsty and hosting the Pigsty website.\n","categories":["Reference"],"description":"Pigsty sponsors and investors list - thank you for your support of this project!","excerpt":"Pigsty sponsors and investors list - thank you for your support of …","ref":"/docs/about/sponsor/","tags":["Sponsor"],"title":"Sponsor Us"},{"body":"According to Google Analytics PV and download statistics, Pigsty currently has approximately 100,000 users, with half from mainland China and half from other regions globally. They span across multiple industries including internet, cloud computing, finance, autonomous driving, manufacturing, tech innovation, ISV, and defense. If you are using Pigsty and are willing to share your case and Logo with us, please contact us - we offer one free consultation session as a token of appreciation.\nInternet Tantan: 200+ physical machines for PostgreSQL and Redis services\nBilibili: Supporting PostgreSQL innovative business\nCloud Vendors Bitdeer: Providing PG DBaaS\nOracle OCI: Using Pigsty to deliver PostgreSQL clusters\nFinance AirWallex: Monitoring 200+ GCP PostgreSQL databases\nMedia \u0026 Entertainment Media Storm: Self-hosted PG RDS / Victoria Metrics\nAutonomous Driving Momenta: Autonomous driving, managing self-hosted PostgreSQL clusters\nManufacturing Huafon Group: Using Pigsty to deliver PostgreSQL clusters as chemical industry time-series data warehouse\nTech Innovation Beijing Lingwu Technology: Migrating PostgreSQL from cloud to self-hosted\nMotphys: Self-hosted PostgreSQL supporting GitLab\nSailong Biotech: Self-hosted Supabase\nHangzhou Lingma Technology: Self-hosted PostgreSQL\nISV Inner Mongolia Haode Tianmu Technology Co., Ltd.\nShanghai Yuanfang\nDSG\n","categories":["Reference"],"description":"Pigsty customer and application cases across various domains and industries","excerpt":"Pigsty customer and application cases across various domains and …","ref":"/docs/about/case/","tags":"","title":"User Cases"},{"body":"Pigsty is a portable, extensible open-source PostgreSQL distribution for building production-grade database services in local environments with declarative configuration and automation. It has a vast ecosystem providing a complete set of tools, scripts, and best practices to bring PostgreSQL to enterprise-grade RDS service levels.\nPigsty’s name comes from PostgreSQL In Great STYle, also understood as Postgres, Infras, Graphics, Service, Toolbox, it’s all Yours—a self-hosted PostgreSQL solution with graphical monitoring that’s all yours. You can find the source code on GitHub, visit the official documentation for more information, or experience the Web UI in the online demo.\nWhy Pigsty? What Can It Do? PostgreSQL is a sufficiently perfect database kernel, but it needs more tools and systems to become a truly excellent database service. In production environments, you need to manage every aspect of your database: high availability, backup recovery, monitoring alerts, access control, parameter tuning, extension installation, connection pooling, load balancing…\nWouldn’t it be easier if all this complex operational work could be automated? This is precisely why Pigsty was created.\nPigsty provides:\nOut-of-the-Box PostgreSQL Distribution\nPigsty deeply integrates 440+ extensions from the PostgreSQL ecosystem, providing out-of-the-box distributed, time-series, geographic, spatial, graph, vector, search, and other multi-modal database capabilities. From kernel to RDS distribution, providing production-grade database services for versions 13-18 on EL/Debian/Ubuntu.\nSelf-Healing High Availability Architecture\nA high availability architecture built on Patroni, Etcd, and HAProxy enables automatic failover for hardware failures with seamless traffic handoff. Primary failure recovery time RTO \u003c 45s, data recovery point RPO ≈ 0. You can perform rolling maintenance and upgrades on the entire cluster without application coordination.\nComplete Point-in-Time Recovery Capability\nBased on pgBackRest and optional MinIO cluster, providing out-of-the-box PITR point-in-time recovery capability. Giving you the ability to quickly return to any point in time, protecting against software defects and accidental data deletion.\nFlexible Service Access and Traffic Management\nThrough HAProxy, Pgbouncer, and VIP, providing flexible service access patterns for read-write separation, connection pooling, and automatic routing. Delivering stable, reliable, auto-routing, transaction-pooled high-performance database services.\nStunning Observability\nA modern observability stack based on Prometheus and Grafana provides unparalleled monitoring best practices. Over three thousand types of monitoring metrics describe every aspect of the system, from global dashboards to CRUD operations on individual objects.\nDeclarative Configuration Management\nFollowing the Infrastructure as Code philosophy, using declarative configuration to describe the entire environment. You just tell Pigsty “what kind of database cluster you want” without worrying about how to implement it—the system automatically adjusts to the desired state.\nModular Architecture Design\nA modular architecture design that can be freely combined to suit different scenarios. Beyond the core PostgreSQL module, it also provides optional modules for Redis, MinIO, Etcd, FerretDB, and support for various PG-compatible kernels.\nSolid Security Best Practices\nIndustry-leading security best practices: self-signed CA certificate encryption, AES encrypted backups, scram-sha-256 encrypted passwords, out-of-the-box ACL model, HBA rule sets following the principle of least privilege, ensuring data security.\nSimple and Easy Deployment\nAll dependencies are pre-packaged for one-click installation in environments without internet access. Local sandbox environments can run on micro VMs with 1 core and 2GB RAM, providing functionality identical to production environments. Provides Vagrant-based local sandboxes and Terraform-based cloud deployments.\nWhat Pigsty Is Not Pigsty is not a traditional, all-encompassing PaaS (Platform as a Service) system.\nPigsty doesn’t provide basic hardware resources. It runs on nodes you provide, whether bare metal, VMs, or cloud instances, but it doesn’t create or manage these resources itself (though it provides Terraform templates to simplify cloud resource preparation).\nPigsty is not a container orchestration system. It runs directly on the operating system, not requiring Kubernetes or Docker as infrastructure. Of course, it can coexist with these systems and provides a Docker module for running stateless applications.\nPigsty is not a general database management tool. It focuses on PostgreSQL and its ecosystem. While it also supports peripheral components like Redis, Etcd, and MinIO, the core is always built around PostgreSQL.\nPigsty won’t lock you in. It’s built on open-source components, doesn’t modify the PostgreSQL kernel, and introduces no proprietary protocols. You can continue using your well-managed PostgreSQL clusters anytime without Pigsty.\nPigsty doesn’t restrict how you should or shouldn’t build your database services. For example:\nPigsty provides good parameter defaults and configuration templates, but you can override any parameter. Pigsty provides a declarative API, but you can still use underlying tools (Ansible, Patroni, pgBackRest, etc.) for manual management. Pigsty can manage the complete lifecycle, or you can use only its monitoring system to observe existing database instances or RDS. Pigsty provides a different level of abstraction than the hardware layer—it works at the database service layer, focusing on how to deliver PostgreSQL at its best, rather than reinventing the wheel.\nEvolution of PostgreSQL Deployment To understand Pigsty’s value, let’s review the evolution of PostgreSQL deployment approaches.\nManual Deployment Era In traditional deployment, DBAs needed to manually install and configure PostgreSQL, manually set up replication, manually configure monitoring, and manually handle failures. The problems with this approach are obvious:\nLow efficiency: Each instance requires repeating many manual operations, prone to errors. Lack of standardization: Databases configured by different DBAs can vary greatly, making maintenance difficult. Poor reliability: Failure handling depends on manual intervention, with long recovery times and susceptibility to human error. Weak observability: Lack of unified monitoring, making problem discovery and diagnosis difficult. Managed Database Era To solve these problems, cloud providers offer managed database services (RDS). Cloud RDS does solve some operational issues, but also brings new challenges:\nHigh cost: Managed services typically charge multiples to dozens of times hardware cost as “service fees.” Vendor lock-in: Migration is difficult, tied to specific cloud platforms. Limited functionality: Cannot use certain advanced features, extensions are restricted, parameter tuning is limited. Data sovereignty: Data stored in the cloud, reducing autonomy and control. Local RDS Era Pigsty represents a third approach: building database services in local environments that match or exceed cloud RDS.\nPigsty combines the advantages of both approaches:\nHigh automation: One-click deployment, automatic configuration, self-healing failures—as convenient as cloud RDS. Complete autonomy: Runs on your own infrastructure, data completely in your own hands. Extremely low cost: Run enterprise-grade database services at near-pure-hardware costs. Complete functionality: Unlimited use of PostgreSQL’s full capabilities and ecosystem extensions. Open architecture: Based on open-source components, no vendor lock-in, free to migrate anytime. This approach is particularly suitable for:\nPrivate and hybrid clouds: Enterprises needing to run databases in local environments. Cost-sensitive users: Organizations looking to reduce database TCO. High-security scenarios: Critical data requiring complete autonomy and control. PostgreSQL power users: Scenarios requiring advanced features and rich extensions. Development and testing: Quickly setting up databases locally that match production environments. What’s Next Now that you understand Pigsty’s basic concepts, you can:\nView System Architecture to understand Pigsty’s modular design Learn about Cluster Model to understand how Pigsty organizes database clusters Study High Availability mechanisms to master self-healing principles Explore Point-in-Time Recovery to learn how to handle data deletion Research Service Access to understand stable database service delivery Experience Infrastructure as Code to feel the magic of declarative configuration Or directly start Quick Start to deploy your first Pigsty environment in minutes ","categories":["Concept"],"description":"Understand Pigsty's core concepts, architecture design, and principles. Master high availability, backup recovery, security compliance, and other key capabilities.","excerpt":"Understand Pigsty's core concepts, architecture design, and …","ref":"/docs/concept/","tags":"","title":"Concepts"},{"body":"Pigsty aims to unite the power of the PostgreSQL ecosystem and help users make the most of the world’s most popular database, PostgreSQL, with self-driving database management software.\nWhile Pigsty itself has already resolved many issues in PostgreSQL usage, achieving truly enterprise-grade service quality requires expert support and comprehensive coverage from the original provider. We deeply understand the importance of professional commercial support for enterprise customers. Therefore, Pigsty Enterprise Edition provides a series of value-added services on top of the open-source version, helping users better utilize PostgreSQL and Pigsty for customers to choose according to their needs.\nIf you have any of the following needs, please consider Pigsty subscription service:\nRunning databases in critical scenarios requiring strict SLA guarantees and comprehensive coverage. Need comprehensive support for complex issues related to Pigsty and PostgreSQL. Seeking guidance on PostgreSQL/Pigsty production environment best practices. Want experts to help interpret monitoring dashboards, analyze and identify performance bottlenecks and fault root causes, and provide recommendations. Need to plan database architectures that meet security/disaster recovery/compliance requirements based on existing resources and business needs. Need to migrate from other databases to PostgreSQL, or migrate and transform legacy instances. Building an observability system, data dashboards, and visualization applications based on the Prometheus/Grafana technology stack. Migrating off cloud and seeking open-source alternatives to RDS for PostgreSQL - cloud-neutral, vendor lock-in-free solutions. Want professional support for Redis/ETCD/MinIO, as well as extensions like TimescaleDB/Citus. Want to avoid AGPL v3 license restrictions that mandate derivative works to use the same open-source license, for secondary development and OEM branding. Want to sell Pigsty as SaaS/PaaS/DBaaS, or provide technical services/consulting/cloud services based on this distribution. Subscription Plans In addition to the Open Source Edition, Pigsty offers two different subscription service tiers: Professional Edition and Enterprise Edition, which you can choose based on your actual situation and needs.\nPigsty Open Source Edition (OSS) Free and Open Source No scale limit, no warranty License: AGPLv3\nPG Support: 18\nArchitecture Support: x86_64\nOS Support: Latest minor versions of three families EL 9.4 Debian 12.7 Ubuntu 22.04.5 Features: Core Modules\nSLA: No SLA commitment\nCommunity support Q\u0026A:\nWeChat discussion groups GitHub Issues Discord Channel Support: No person-day support option\nRepository: Global Cloudflare hosted repository\nSelf-sufficient open source veterans Pigsty Professional Edition (PRO) Starting Price: ¥150,000 / year Default choice for regular users License: Commercial License\nPG Support: 17, 18\nArchitecture Support: x86_64, Arm64\nOS Support: Five families major/minor versions EL 8 / 9 compatible Debian 12 Ubuntu 22 / 24 Features: All Modules (except 信创)\nSLA: Response within business hours\nExpert consulting services:\nSoftware bug fixes Complex issue analysis Expert ticket support Support: 1 person-day included per year\nDelivery: Standard offline software package\nRepository: China mainland mirror sites\nDefault choice for regular users Pigsty Enterprise Edition (ENTERPRISE) Starting Price: ¥400,000 / year Critical scenarios with strict SLA License: Commercial License\nPG Support: 12 - 18+\nArchitecture Support: x86_64, Arm64\nOS Support: Customized on demand EL, Debian, Ubuntu Cloud Linux operating systems Domestic OS and ARM Features: All Modules\nSLA: 7 x 24 (\u003c 1h)\nEnterprise-level expert consulting services:\nSoftware bug fixes Complex issue analysis Expert Q\u0026A support Backup compliance advice Upgrade path support Performance bottleneck identification Annual architecture review Extension plugin integration DBaaS \u0026 OEM use cases Support: 2 person-days included per year\nRepository: China mainland mirror sites\nDelivery: Customized offline software package\n信创: PolarDB-O support\nCritical scenarios with strict SLA Pigsty Open Source Edition (OSS) Pigsty Open Source Edition uses the AGPLv3 license, provides complete core functionality, requires no fees, but does not guarantee any warranty service. If you find defects in Pigsty, we welcome you to submit an Issue on Github.\nIf you are a regular end user (i.e., users other than public cloud providers and database vendors), we actually enforce the more permissive Apache 2.0 license - even if you perform secondary development on Pigsty, we will not pursue this.\nFor the open source version, we provide pre-built standard offline software packages for PostgreSQL 18 on the latest minor versions of three specific operating system distributions: EL 9.4, Debian 12.7, Ubuntu 22.04.5 (as support for open source, we also provide Debian 12 Arm64 offline software packages).\nUsing the Pigsty open source version allows junior development/operations engineers to have 70%+ of the capabilities of professional DBAs. Even without database experts, they can easily set up a highly available, high-performance, easy-to-maintain, secure and reliable PostgreSQL database cluster.\nCode OS Distribution Version x86_64 Arm64 PG17 PG16 PG15 PG14 PG13 EL9 RHEL 9 / Rocky9 / Alma9 el9.x86_64 U22 Ubuntu 22.04 (jammy) u22.x86_64 D12 Debian 12 (bookworm) d12.x86_64 d12.aarch64 = Primary support, = Optional support\nPigsty Professional Edition (PRO) Professional Edition Subscription: Starting Price ¥150,000 / year Pigsty Professional Edition subscription provides complete functional modules and warranty for Pigsty itself. For defects in PostgreSQL itself and extension plugins, we will make our best efforts to provide feedback and fixes through the PostgreSQL global developer community.\nPigsty Professional Edition is built on the open source version, fully compatible with all features of the open source version, and provides additional functional modules and broader database/operating system version compatibility options: we will provide build options for all minor versions of five mainstream operating system distributions.\nPigsty Professional Edition includes support for the latest two PostgreSQL major versions (18, 17), providing all available extension plugins in both major versions, ensuring you can smoothly migrate to the latest PostgreSQL major version through rolling upgrades.\nPigsty Professional Edition subscription allows you to use China mainland mirror site software repositories, accessible without VPN/proxy; we will also customize offline software installation packages for your exact operating system major/minor version, ensuring normal installation and delivery in air-gapped environments, achieving autonomous and controllable deployment.\nPigsty Professional Edition subscription provides standard expert consulting services, including complex issue analysis, DBA Q\u0026A support, backup compliance advice, etc. We commit to responding to your issues within business hours (5x8), and provide 1 person-day support per year, with optional person-day add-on options.\nPigsty Professional Edition uses a commercial license and provides written contractual exemption from AGPLv3 open source obligations. Even if you perform secondary development on Pigsty and violate the AGPLv3 license by not open-sourcing, we will not pursue this.\nPigsty Professional Edition starting price is ¥150,000 / year, equivalent to the annual fee for 9 vCPU AWS high-availability RDS PostgreSQL, or a junior operations engineer with a monthly salary of 10,000 yuan.\nCode OS Distribution Version x86_64 Arm64 PG17 PG16 PG15 PG14 PG13 EL9 RHEL 9 / Rocky9 / Alma9 el9.x86_64 el9.aarch64 EL8 RHEL 8 / Rocky8 / Alma8 / Anolis8 el8.x86_64 el8.aarch64 U24 Ubuntu 24.04 (noble) u24.x86_64 u24.aarch64 U22 Ubuntu 22.04 (jammy) u22.x86_64 u22.aarch64 D12 Debian 12 (bookworm) d12.x86_64 d12.aarch64 Pigsty Enterprise Edition Enterprise Edition Subscription: Starting Price ¥400,000 / year Pigsty Enterprise Edition subscription includes all service content provided by the Pigsty Professional Edition subscription, plus the following value-added service items:\nPigsty Enterprise Edition subscription provides the broadest range of database/operating system version support, including extended support for EOL operating systems (EL7, U20, D11), domestic operating systems, cloud vendor operating systems, and EOL database major versions (from PG 13 onwards), as well as full support for Arm64 architecture chips.\nPigsty Enterprise Edition subscription provides 信创 (domestic innovation) and localization solutions, allowing you to use PolarDB v2.0 (this kernel license needs to be purchased separately) kernel to replace the native PostgreSQL kernel to meet domestic compliance requirements.\nPigsty Enterprise Edition subscription provides higher-standard enterprise-level consulting services, committing to 7x24 with (\u003c 1h) response time SLA, and can provide more types of consulting support: version upgrades, performance bottleneck identification, annual architecture review, extension plugin integration, etc.\nPigsty Enterprise Edition subscription includes 2 person-days of support per year, with optional person-day add-on options, for resolving more complex and time-consuming issues.\nPigsty Enterprise Edition allows you to use Pigsty for DBaaS purposes, building cloud database services for external sales.\nPigsty Enterprise Edition starting price is ¥400,000 / year, equivalent to the annual fee for 24 vCPU AWS high-availability RDS, or an operations expert with a monthly salary of 30,000 yuan.\nCode OS Distribution Version x86_64 PG17 PG16 PG15 PG14 PG13 PG12 Arm64 PG17 PG16 PG15 PG14 PG13 PG12 EL9 RHEL 9 / Rocky9 / Alma9 el9.x86_64 el9.arm64 EL8 RHEL 8 / Rocky8 / Alma8 / Anolis8 el8.x86_64 el8.arm64 U24 Ubuntu 24.04 (noble) u24.x86_64 u24.arm64 U22 Ubuntu 22.04 (jammy) u22.x86_64 u22.arm64 D12 Debian 12 (bookworm) d12.x86_64 d12.arm64 D11 Debian 11 (bullseye) d12.x86_64 d11.arm64 U20 Ubuntu 20.04 (focal) d12.x86_64 u20.arm64 EL7 RHEL7 / CentOS7 / UOS … d12.x86_64 el7.arm64 Pigsty Subscription Notes Feature Differences\nPigsty Professional/Enterprise Edition includes the following additional features compared to the open source version:\nCommand Line Management Tool: Unlock the full functionality of the Pigsty command line tool (pig) System Customization Capability: Provide pre-built offline installation packages for exact mainstream Linux operating system distribution major/minor versions Offline Installation Capability: Complete Pigsty installation in environments without Internet access (air-gapped environments) Multi-version PG Kernel: Allow users to freely specify and install PostgreSQL major versions within the lifecycle (13 - 17) Kernel Replacement Capability: Allow users to use other PostgreSQL-compatible kernels to replace the native PG kernel, and the ability to install these kernels offline Babelfish: Provides Microsoft SQL Server wire protocol-level compatibility IvorySQL: Based on PG, provides Oracle syntax/type/stored procedure compatibility PolarDB PG: Provides support for open-source PolarDB for PostgreSQL kernel PolarDB O: 信创 database, Oracle-compatible kernel that meets domestic compliance requirements (Enterprise Edition subscription only) Extension Support Capability: Provides out-of-the-box installation for 440 available PG Extensions for PG 13-18 on mainstream operating systems. Complete Functional Modules: Provides all functional modules: Supabase: Reliably self-host production-grade open-source Firebase MinIO: Enterprise PB-level object storage planning and self-hosting DuckDB: Provides comprehensive DuckDB support, and PostgreSQL + DuckDB OLAP extension plugin support Kafka: Provides high-availability Kafka cluster deployment and monitoring Kubernetes, VictoriaMetrics \u0026 VictoriaLogs Domestic Operating System Support: Provides domestic 信创 operating system support options (Enterprise Edition subscription only) Domestic ARM Architecture Support: Provides domestic ARM64 architecture support options (Enterprise Edition subscription only) China Mainland Mirror Repository: Smooth installation without VPN, providing domestic YUM/APT repository mirrors and DockerHub access proxy. Chinese Interface Support: Monitoring system Chinese interface support (Beta) Payment Model\nPigsty subscription uses an annual payment model. After signing the contract, the one-year validity period is calculated from the contract date. If payment is made before the subscription contract expires, it is considered automatic renewal. Consecutive subscriptions have discounts. The first renewal (second year) enjoys a 95% discount, the second and subsequent renewals enjoy a 90% discount on subscription fees, and one-time subscriptions for three years or more enjoy an overall 85% discount.\nAfter the annual subscription contract terminates, you can choose not to renew the subscription service. Pigsty will no longer provide software updates, technical support, and consulting services, but you can continue to use the already installed version of Pigsty Professional Edition software. If you subscribed to Pigsty professional services and choose not to renew, when re-subscribing you do not need to make up for the subscription fees during the interruption period, but all discounts and benefits will be reset.\nPigsty’s pricing strategy ensures value for money - you can immediately get top DBA’s database architecture construction solutions and management best practices, with their consulting support and comprehensive coverage; while the cost is highly competitive compared to hiring database experts full-time or using cloud databases. Here are market references for enterprise-level database professional service pricing:\nAWS RDS for PostgreSQL High Availability Edition: ¥1,160 ~ ¥1,582 / (vCPU·month), equivalent to 14K ~ 19K/year (per vCPU) Alibaba Cloud RDS for PostgreSQL High Availability Edition: ¥270 ~ ¥432 / (vCPU·month), equivalent to 3K ~ 5K/year (per vCPU) EDB PostgreSQL Cloud Database Enterprise Edition: $183.3 / (vCPU·month), equivalent to 16K/year (per vCPU) Fujitsu Enterprise PostgreSQL Kubernetes: $3200 / (Core·year), equivalent to 12K/year (per vCPU) Oracle Annual Service Fee: (Enterprise $47,500 + Rac $23,000) * 22% per year, equivalent to 28K/year (per vCPU) The fair price for decent database professional services is 10,000 ~ 20,000 yuan / year, with the billing unit being vCPU, i.e., one CPU thread (1 Intel core = 2 vCPU threads). Pigsty provides top-tier PostgreSQL expert services in China and adopts a per-node billing model. On commonly seen high-core-count server nodes, it brings users an unparalleled cost reduction and efficiency improvement experience.\nPigsty Expert Services In addition to Pigsty subscription, Pigsty also provides on-demand Pigsty x PostgreSQL expert services - industry-leading database experts available for consultation.\nExpert Advisor: ¥300,000 / three years Within three years, provides 10 complex case handling sessions related to PostgreSQL and Pigsty, and unlimited Q\u0026A. Expert Support: ¥30,000 / person·day Industry-leading expert on-site support, available for architecture consultation, fault analysis, problem troubleshooting, database health checks, monitoring interpretation, migration assessment, teaching and training, cloud migration/de-cloud consultation, and other continuous time-consuming scenarios. Expert Consultation: ¥3,000 / case Consult on any questions you want to know about Pigsty, PostgreSQL, databases, cloud computing, AI... Database veterans, cloud computing maverick sharing industry-leading insights, cognition, and judgment. Quick Consultation: ¥300 / question Get a quick diagnostic opinion and response to questions related to PostgreSQL / Pigsty / databases, not exceeding 5 minutes. Contact Information Please send an email to rh@vonng.com. Users in mainland China are welcome to add WeChat ID RuohangFeng.\n","categories":["Reference"],"description":"Pigsty Professional/Enterprise subscription service - When you encounter difficulties related to PostgreSQL and Pigsty, our subscription service provides you with comprehensive support.","excerpt":"Pigsty Professional/Enterprise subscription service - When you …","ref":"/docs/about/service/","tags":"","title":"Subscription"},{"body":" What is Pigsty, and what is it not? Pigsty is a PostgreSQL database distribution, a local-first open-source RDS cloud database solution. Pigsty is not a Database Management System (DBMS), but rather a tool, distribution, solution, and best practice for managing DBMS.\nAnalogy: The database is the car, then the DBA is the driver, RDS is the taxi service, and Pigsty is the autonomous driving software.\nWhat problem does Pigsty solve? The ability to use databases well is extremely scarce: either hire database experts at high cost to self-build (hire drivers), or rent RDS from cloud vendors at sky-high prices (hail a taxi), but now you have a new option: Pigsty (autonomous driving). Pigsty helps users use databases well: allowing users to self-build higher-quality and more efficient local cloud database services at less than 1/10 the cost of RDS, without a DBA!\nWho are Pigsty’s target users? Pigsty has two typical target user groups. The foundation is medium to large companies building ultra-large-scale enterprise/production-grade PostgreSQL RDS / DBaaS services. Through extreme customizability, Pigsty can meet the most demanding database management needs and provide enterprise-level support and service guarantees.\nAt the same time, Pigsty also provides “out-of-the-box” PG RDS self-building solutions for individual developers, small and medium enterprises lacking DBA capabilities, and the open-source community.\nWhy can Pigsty help you use databases well? Pigsty embodies the experience and best practices of top experts refined in the most complex and largest-scale client PostgreSQL scenarios, productized into replicable software: Solving extension installation, high availability, connection pooling, monitoring, backup and recovery, parameter optimization, IaC batch management, one-click installation, automated operations, and many other issues at once. Avoiding many pitfalls in advance and preventing repeated mistakes.\nWhy is Pigsty better than RDS? Pigsty provides a feature set and infrastructure support far beyond RDS, including 440 extension plugins and 8+ kernel support. Pigsty provides a unique professional-grade monitoring system in the PG ecosystem, along with architectural best practices battle-tested in complex scenarios, simple and easy to use.\nMoreover, forged in top-tier client scenarios like Tantan, Apple, and Alibaba, continuously nurtured with passion and love, its depth and maturity are incomparable to RDS’s one-size-fits-all approach.\nWhy is Pigsty cheaper than RDS? Pigsty allows you to use 10 ¥/core·month pure hardware resources to run 400¥-1400¥/core·month RDS cloud databases, and save the DBA’s salary. Typically, the total cost of ownership (TCO) of a large-scale Pigsty deployment can be over 90% lower than RDS.\nPigsty can simultaneously reduce software licensing/services/labor costs. Self-building requires no additional staff, allowing you to spend costs where it matters most.\nHow does Pigsty help developers? Pigsty integrates the most comprehensive extensions in the PG ecosystem (440), providing an All-in-PG solution: a single component replacing specialized components like Redis, Kafka, MySQL, ES, vector databases, OLAP / big data analytics.\nGreatly improving R\u0026D efficiency and agility while reducing complexity costs, and developers can achieve self-service management and autonomous DevOps with Pigsty’s support, without needing a DBA.\nHow does Pigsty help operations? Pigsty’s self-healing high-availability architecture ensures hardware failures don’t need immediate handling, letting ops and DBAs sleep well; monitoring aids problem analysis and performance optimization; IaC enables automated management of ultra-large-scale clusters.\nOperations can moonlight as DBAs with Pigsty’s support, while DBAs can skip the system building phase, saving significant work hours and focusing on high-value work, or relaxing, learning PG.\nWho is the author of Pigsty? Pigsty is primarily developed by Feng Ruohang alone, an open-source contributor, database expert, and evangelist who has focused on PostgreSQL for 10 years, formerly at Alibaba, Tantan, and Apple, a full-stack expert. Now the founder of a one-person company, providing professional consulting services.\nHe is also a tech KOL, the founder of the top WeChat database personal account “非法加冯” (Illegally Add Feng), with 60,000+ followers across all platforms.\nWhat is Pigsty’s ecosystem position and influence? Pigsty is the most influential Chinese open-source project in the global PostgreSQL ecosystem, with about 100,000 users, half from overseas. Pigsty is also one of the most active open-source projects in the PostgreSQL ecosystem, currently dominating in extension distribution and monitoring systems.\nPGEXT.Cloud is a PostgreSQL extension repository maintained by Pigsty, with the world’s largest PostgreSQL extension distribution volume. It has become an upstream software supply chain for multiple international PostgreSQL vendors.\nPigsty is currently one of the major distributions in the PostgreSQL ecosystem and a challenger to cloud vendor RDS, now widely used in defense, government, healthcare, internet, finance, manufacturing, and other industries.\nWhat scale of customers is Pigsty suitable for? Pigsty originated from the need for ultra-large-scale PostgreSQL automated management but has been deeply optimized for ease of use. Individual developers and small-medium enterprises lacking professional DBA capabilities can also easily get started.\nThe largest deployment is 25K vCPU, 4.5 million QPS, 6+ years; the smallest deployment can run completely on a 1c1g VM for Demo / Devbox use.\nWhat capabilities does Pigsty provide? Pigsty focuses on integrating the PostgreSQL ecosystem and providing PostgreSQL best practices, but also supports a series of open-source software that works well with PostgreSQL. For example:\nEtcd, Redis, MinIO, DuckDB, Prometheus FerretDB, Babelfish, IvorySQL, PolarDB, OrioleDB OpenHalo, Supabase, Greenplum, Dify, Odoo, … What scenarios is Pigsty suitable for? Running large-scale PostgreSQL clusters for business Self-building RDS, object storage, cache, data warehouse, Supabase, … Self-building enterprise applications like Odoo, Dify, Wiki, GitLab Running monitoring infrastructure, monitoring existing databases and hosts Using multiple PG extensions in combination Dashboard development and interactive data application demos, data visualization, web building Is Pigsty open source and free? Pigsty is 100% open-source software + free software. Under the premise of complying with the open-source license, you can use it freely and for various commercial purposes.\nWe value software freedom. For non-DBaaS / OEM use cases, we enforce a more relaxed equivalent Apache 2.0 license. Please see the license for more details.\nDoes Pigsty provide commercial support? Pigsty software itself is open-source and free, and provides commercial subscriptions for all budgets, providing quality assurance for Pigsty \u0026 PostgreSQL. Subscriptions provide broader OS/PG/chip architecture support ranges, as well as expert consulting and support. Pigsty commercial subscriptions deliver industry-leading management/technical experience/solutions, helping you save valuable time, shouldering risks for you, and providing a safety net for difficult problems.\nDoes Pigsty support domestic innovation (信创)? Pigsty software itself is not a database and is not subject to domestic innovation catalog restrictions, and already has multiple military use cases. However, the Pigsty open-source edition does not provide any form of domestic innovation support. Commercial subscription provides domestic innovation solutions in cooperation with Alibaba Cloud, supporting the use of PolarDB-O with domestic innovation qualifications (requires separate purchase) as the RDS kernel, capable of running on domestic innovation OS/chip environments.\nCan Pigsty run as a multi-tenant DBaaS? If you use the Pigsty Infra module and distribute or operate it as part of a public cloud database service (DBaaS), you may use it for this purpose under the premise of complying with the AGPLv3 license — open-sourcing derivative works under the same license.\nWe reserve the right to hold public cloud/database vendors accountable for violating the AGPLv3 license. If you do not wish to open-source derivative works, we recommend purchasing the Pigsty Enterprise Edition subscription plan, which provides clear authorization for this use case and exemption from Pigsty’s AGPLv3 open-source obligations.\nCan Pigsty’s Logo be rebranded as your own product? When redistributing Pigsty, you must retain copyright notices, patent notices, trademark notices, and attribution notices from the original work, and attach prominent change descriptions in modified files while preserving the content of the LICENSE file. Under these premises, you can replace PIGSTY’s Logo and trademark, but you must not promote it as “your own original work.” We provide commercial licensing support for OEM and rebranding in the enterprise edition.\nPigsty’s Business Entity Pigsty is a project invested by Miracle Plus S22. The original entity Panji Cloud Data (Beijing) Technology Co., Ltd. has been liquidated and divested of the Pigsty business.\nPigsty is currently independently operated and maintained by author Feng Ruohang. The business entities are:\nHainan Zhuxia Cloud Data Co., Ltd. / 91460000MAE6L87B94 Haikou Longhua Piji Data Center / 92460000MAG0XJ569B Haikou Longhua Yuehang Technology Center / 92460000MACCYGBQ1N PIGSTY® and PGSTY® are registered trademarks of Haikou Longhua Yuehang Technology Center.\n","categories":["Reference"],"description":"Answers to frequently asked questions about the Pigsty project itself.","excerpt":"Answers to frequently asked questions about the Pigsty project itself.","ref":"/docs/about/faq/","tags":["FAQ","Introduction","Positioning","Business"],"title":"FAQ"},{"body":"Pigsty uses a modular architecture with a declarative interface. You can freely combine modules like building blocks as needed.\nPigsty adopts a modular design that can be freely combined and used on demand (use one or all) to suit different scenarios. Pigsty uses config inventory and config parameters to describe the entire deployment environment, implemented via Ansible playbooks. Pigsty can run on any node—physical or virtual—as long as the OS is compatible. Modules Pigsty uses a modular design with six main default modules: PGSQL, INFRA, NODE, ETCD, REDIS, and MINIO.\nPGSQL: Self-healing HA Postgres clusters powered by Patroni, Pgbouncer, HAproxy, PgBackrest, and more. INFRA: Local software repo, Nginx, Grafana, Victoria, AlertManager, Blackbox Exporter—the complete observability stack. NODE: Tune nodes to desired state—hostname, timezone, NTP, ssh, sudo, haproxy, docker, vector, keepalived. ETCD: Distributed key-value store as DCS for HA Postgres clusters: consensus leader election/config management/service discovery. REDIS: Redis servers supporting standalone primary-replica, sentinel, and cluster modes with full monitoring. MINIO: S3-compatible simple object storage that can serve as an optional backup destination for PG databases. You can declaratively compose them freely. If you only want host monitoring, installing the INFRA module on infrastructure nodes and the NODE module on managed nodes is sufficient. The ETCD and PGSQL modules are used to build HA PG clusters—installing these modules on multiple nodes automatically forms a high-availability database cluster. You can reuse Pigsty infrastructure and develop your own modules; REDIS and MINIO can serve as examples. More modules will be added—preliminary support for Mongo and MySQL is already on the roadmap.\nNote that all modules depend strongly on the NODE module: in Pigsty, nodes must first have the NODE module installed to be managed before deploying other modules. When nodes (by default) use the local software repo for installation, the NODE module has a weak dependency on the INFRA module. Therefore, the admin/infrastructure nodes with the INFRA module complete the bootstrap process in the deploy.yml playbook, resolving the circular dependency.\nStandalone Installation By default, Pigsty installs on a single node (physical/virtual machine). The deploy.yml playbook installs INFRA, ETCD, PGSQL, and optionally MINIO modules on the current node, giving you a fully-featured observability stack (Prometheus, Grafana, Loki, AlertManager, PushGateway, BlackboxExporter, etc.), plus a built-in PostgreSQL standalone instance as a CMDB, ready to use out of the box (cluster name pg-meta, database name meta).\nThis node now has a complete self-monitoring system, visualization tools, and a Postgres database with PITR auto-configured (HA unavailable since you only have one node). You can use this node as a devbox, for testing, running demos, and data visualization/analysis. Or, use this node as an admin node to deploy and manage more nodes!\nMonitoring The installed standalone meta node can serve as an admin node and monitoring center to bring more nodes and database servers under its supervision and control.\nPigsty’s monitoring system can be used independently. If you want to install the Prometheus/Grafana observability stack, Pigsty provides best practices! It offers rich dashboards for host nodes and PostgreSQL databases. Whether or not these nodes or PostgreSQL servers are managed by Pigsty, with simple configuration, you immediately have a production-grade monitoring and alerting system, bringing existing hosts and PostgreSQL under management.\nHA PostgreSQL Clusters Pigsty helps you own your own production-grade HA PostgreSQL RDS service anywhere.\nTo create such an HA PostgreSQL cluster/RDS service, you simply describe it with a short config and run the playbook to create it:\npg-test: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } 10.10.10.12: { pg_seq: 2, pg_role: replica } 10.10.10.13: { pg_seq: 3, pg_role: replica } vars: { pg_cluster: pg-test } $ bin/pgsql-add pg-test # Initialize cluster 'pg-test' In less than 10 minutes, you’ll have a PostgreSQL database cluster with service access, monitoring, backup PITR, and HA fully configured.\nHardware failures are covered by the self-healing HA architecture provided by patroni, etcd, and haproxy—in case of primary failure, automatic failover executes within 45 seconds by default. Clients don’t need to modify config or restart applications: Haproxy uses patroni health checks for traffic distribution, and read-write requests are automatically routed to the new cluster primary, avoiding split-brain issues. This process is seamless—for example, in case of replica failure or planned switchover, clients experience only a momentary flash of the current query.\nSoftware failures, human errors, and datacenter-level disasters are covered by pgbackrest and the optional MinIO cluster. This provides local/cloud PITR capabilities and, in case of datacenter failure, offers cross-region replication and disaster recovery.\n","categories":["Concept"],"description":"Pigsty's modular architecture—declarative composition, on-demand customization, flexible deployment.","excerpt":"Pigsty's modular architecture—declarative composition, on-demand …","ref":"/docs/concept/arch/","tags":"","title":"Architecture"},{"body":"The current stable version is v4.0.0-c1, GA expected before 2026-01-31.\nVersion Release Date Summary Release Page v4.0.0-c1 2026-01-07 Infra overhaul, security hardening, agent capabilities, fork \u0026 clone v4.0.0-c1 v3.7.0 2025-12-02 PG18 default, 437 extensions, EL10 \u0026 Debian 13 support, PGEXT.CLOUD v3.7.0 v3.6.1 2025-08-15 Routine PG minor updates, PGDG China mirror, EL10/D13 stubs v3.6.1 v3.6.0 2025-07-30 pgactive, MinIO/ETCD improvements, simplified install, config cleanup v3.6.0 v3.5.0 2025-06-16 PG18 beta, 421 extensions, monitoring upgrade, code refactor v3.5.0 v3.4.1 2025-04-05 OpenHalo \u0026 OrioleDB, MySQL compatibility, pgAdmin improvements v3.4.1 v3.4.0 2025-03-30 Backup improvements, auto certs, AGE, IvorySQL all platforms v3.4.0 v3.3.0 2025-02-24 404 extensions, extension directory, App playbook, Nginx customization v3.3.0 v3.2.2 2025-01-23 390 extensions, Omnigres, Mooncake, Citus 13 \u0026 PG17 support v3.2.2 v3.2.1 2025-01-12 350 extensions, Ivory4, Citus enhancements, Odoo template v3.2.1 v3.2.0 2024-12-24 Extension CLI, Grafana enhancements, ARM64 extension completion v3.2.0 v3.1.0 2024-11-24 PG17 default, config simplification, Ubuntu24 \u0026 ARM support v3.1.0 v3.0.4 2024-10-30 PG17 extensions, OLAP suite, pg_duckdb v3.0.4 v3.0.3 2024-09-27 PostgreSQL 17, Etcd improvements, IvorySQL 3.4, PostGIS 3.5 v3.0.3 v3.0.2 2024-09-07 Mini install mode, PolarDB 15 support, monitoring view updates v3.0.2 v3.0.1 2024-08-31 Routine bug fixes, Patroni 4 support, Oracle compatibility improvements v3.0.1 v3.0.0 2024-08-25 333 extensions, pluggable kernels, MSSQL/Oracle/PolarDB compatibility v3.0.0 v2.7.0 2024-05-20 Extension explosion, 20+ new powerful extensions, Docker apps v2.7.0 v2.6.0 2024-02-28 PG16 as default, ParadeDB \u0026 DuckDB extensions introduced v2.6.0 v2.5.1 2023-12-01 Routine minor update, PG16 key extension support v2.5.1 v2.5.0 2023-09-24 Ubuntu/Debian support: bullseye, bookworm, jammy, focal v2.5.0 v2.4.1 2023-09-24 Supabase/PostgresML support with graphql, jwt, pg_net, vault v2.4.1 v2.4.0 2023-09-14 PG16, RDS monitoring, new extensions: FTS/graph/HTTP/embedding v2.4.0 v2.3.1 2023-09-01 PGVector with HNSW, PG16 RC1, doc refresh, Chinese docs, bug fixes v2.3.1 v2.3.0 2023-08-20 Node VIP, FerretDB, NocoDB, MySQL stub, CVE fixes v2.3.0 v2.2.0 2023-08-04 Dashboard \u0026 provisioning overhaul, UOS compatibility v2.2.0 v2.1.0 2023-06-10 PostgreSQL 12-16beta support v2.1.0 v2.0.2 2023-03-31 Added pgvector support, fixed MinIO CVE v2.0.2 v2.0.1 2023-03-21 v2 bug fixes, security enhancements, Grafana upgrade v2.0.1 v2.0.0 2023-02-28 Major architecture upgrade, compatibility/security/maintainability v2.0.0 v1.5.1 2022-06-18 Grafana security hotfix v1.5.1 v1.5.0 2022-05-31 Docker application support v1.5.0 v1.4.1 2022-04-20 Bug fixes \u0026 full English documentation translation v1.4.1 v1.4.0 2022-03-31 MatrixDB support, separated INFRA/NODES/PGSQL/REDIS modules v1.4.0 v1.3.0 2021-11-30 PGCAT overhaul \u0026 PGSQL enhancement \u0026 Redis beta support v1.3.0 v1.2.0 2021-11-03 Default PGSQL version upgraded to 14 v1.2.0 v1.1.0 2021-10-12 Homepage, JupyterLab, PGWEB, Pev2 \u0026 pgbadger v1.1.0 v1.0.0 2021-07-26 v1 GA, Monitoring System Overhaul v1.0.0 v0.9.0 2021-04-04 Pigsty GUI, CLI, Logging Integration v0.9.0 v0.8.0 2021-03-28 Service Provision v0.8.0 v0.7.0 2021-03-01 Monitor only deployment v0.7.0 v0.6.0 2021-02-19 Architecture Enhancement v0.6.0 v0.5.0 2021-01-07 Database Customize Template v0.5.0 v0.4.0 2020-12-14 PostgreSQL 13 Support, Official Documentation v0.4.0 v0.3.0 2020-10-22 Provisioning Solution GA v0.3.0 v0.2.0 2020-07-10 PGSQL Monitoring v6 GA v0.2.0 v0.1.0 2020-06-20 Validation on Testing Environment v0.1.0 v0.0.5 2020-08-19 Offline Installation Mode v0.0.5 v0.0.4 2020-07-27 Refactor playbooks into Ansible roles v0.0.4 v0.0.3 2020-06-22 Interface enhancement v0.0.3 v0.0.2 2020-04-30 First Commit v0.0.2 v0.0.1 2019-05-15 POC v0.0.1 v4.0.0-c1 curl https://pigsty.io/get | bash -s v4.0.0 Highlights\nObservability Revolution: Prometheus → VictoriaMetrics (10x perf), Loki+Promtail → VictoriaLogs+Vector Security Hardening: Auto-generated passwords, etcd RBAC, firewall/SELinux modes, permission tightening Database Management: pg_databases state (create/absent/recreate), instant clone with strategy PITR \u0026 Fork: /pg/bin/pg-fork for instant CoW cloning, enhanced pg-pitr with pre-backup Multi-Cloud Terraform: AWS, Azure, GCP, Hetzner, DigitalOcean, Linode, Vultr, TencentCloud templates AI Agent: Add support for claude code, opencode and uv License: AGPL-3.0 → Apache-2.0 244 commits, 554 files changed, +94,508 / -41,374 lines\nInfra Software Versions\nPackage Version Package Version grafana 12.3.1 victoria-metrics 1.132.0 victoria-logs 1.43.1 vector 0.52.0 alertmanager 0.30.0 blackbox_exporter 0.28.0 etcd 3.6.7 duckdb 1.4.3 pg_exporter 1.1.1 pgbackrest_exporter 0.22.0 minio 20251203 pig 0.9.0 uv 0.9.18 (new) opencode 1.0.223 (new) PostgreSQL Extensions\nNew: pg_textsearch 0.1.0, pg_clickhouse 0.1.0, pg_ai_query 0.1.1\nUpdated: IvorySQL 5.1, timescaledb 2.24.0, pg_search 0.20.4, pg_duckdb 1.1.1, pg_biscuit 2.0.1, pg_anon 2.5.1, pg_enigma 0.5.0, pg_session_jwt 0.4.0, pg_vectorize 0.26.0, vchord_bm25 0.3.0, wrappers 0.5.7\nPG18 Deb Fixes: pg_vectorize, pg_tiktoken, pg_tzf, pglite_fusion, pgsmcrypto, pgx_ulid, plprql, pg_summarize, supautils\nBreaking Changes\nObservability Stack\nBefore After Prometheus VictoriaMetrics Loki VictoriaLogs Promtail Vector Parameters\nRemoved Replacement node_disable_firewall node_firewall_mode (off/none/zone) node_disable_selinux node_selinux_mode (disabled/permissive/enforcing) pg_pwd_enc removed infra_pip infra_uv Defaults Changed\nParameter Before → After grafana_clean true → false effective_io_concurrency 1000 → 200 install.yml renamed to deploy.yml (symlink kept) Observability\nUsing the new VictoriaMetrics to replace Prometheus — achieving several times the performance with a fraction of the resources. Using the new log collection solution: VictoriaLogs + Vector, replacing Promtail + Loki. Unified log format adjustments for all components, PG logs use UTC timestamp (log_timezone) Adjusted PostgreSQL log rotation method, using weekly truncated log rotation mode Recording temporary file allocations over 1MB in PG logs, enabling PG 17/18 log new parameters in specific templates Added Nginx Access \u0026 Error / Syslog / PG CSV / Pgbackrest vector log parsing configurations Datasource registration now runs on all Infra nodes, Victoria datasources automatically registered in Grafana Added grafana_pgurl parameter allowing Grafana to use PG as backend metadata storage Added grafana_view_pgpass parameter to specify password used by Grafana Meta datasource pgbackrest_exporter default options now set a 120s internal cache interval (originally 600s) grafana_clean parameter default now changed from true to false, i.e., not cleaned by default. Added new metric collector pg_timeline, collecting more real-time timeline metrics pg_timeline_id pg_exporter updated to 1.1.1, fixing numerous historical issues. Interface Improvements\ninstall.yml playbook now renamed to deploy.yml for better semantics. pg_databases database provisioning improvements: Added database removal capability: use state field to specify create, absent, recreate states. Added clone capability: use strategy parameter in database definition to specify clone method Support newer version locale config parameters: locale_provider, icu_locale, icu_rules, builtin_locale Support is_template parameter to mark database as template database Added more type checks, avoiding character parameter injection Allow specifying state: absent in extension to remove extensions pg_users user provisioning improvements: added admin parameter, similar to roles, but with ADMIN OPTION permission for re-granting. Parameter Optimization\npg_io_method parameter, auto, sync, worker, io_uring four options available, default worker idle_replication_slot_timeout, default 7d, crit template 3d log_lock_failures, oltp, crit templates enabled track_cost_delay_timing, olap, crit templates enabled log_connections, oltp/olap enables authentication logs, crit enables all logs. maintenance_io_concurrency set to 100 if using SSD effective_io_concurrency reduced from 1000 to 200 file_copy_method parameter set to clone for PG18, providing instant database cloning capability For PG17+, if pg_checksums switch is off, explicitly disable checksums during patroni cluster initialization Fixed issue where duckdb.allow_community_extensions always took effect Allow specifying HBA trusted “intranet segments” via node_firewall_intranet pg_hba and pgbouncer_hba now support IPv6 localhost access Architecture Improvements\nOn Infra nodes, set fixed /infra symlink pointing to Infra data directory /data/infra. Infra data now defaults to /data/infra directory, making container usage more convenient. Local software repo now placed at /data/nginx/pigsty, /www now a symlink to /data/nginx for compatibility. DNS resolution records now placed under /infra/hosts directory, solving Ansible SELinux race condition issues pg_remove/pg_pitr etcd metadata removal tasks now run on etcd cluster instead of depending on admin_ip management node Simplify the 36-node simu template into the 20-node version. Adapted to upstream changes, removed PGDG sysupdate repo, removed all llvmjit related packages on EL systems Using full OS version numbers (major.minor) for EPEL 10 / PGDG 9/10 repos Allow specifying meta parameter in repo definitions to override yum repo definition metadata Added /pg/bin/pg-fork script for quickly creating CoW replica database instances Adjusted /pg/bin/pg-pitr script, now usable for instance-level PITR recovery Ensure vagrant libvirt templates default to 128GB disk, mounted at /data with xfs. Ensure pgbouncer no longer modifies 0.0.0.0 listen address to *. Multi-cloud Terraform templates: AWS, Azure, GCP, Hetzner, DigitalOcean, Linode, Vultr, TencentCloud Security Improvements\nconfigure now auto-generates random strong passwords, avoiding security risks from default passwords. Removed node_disable_firewall, added node_firewall_mode supporting off, none, zone three modes. Removed node_disable_selinux, added node_selinux_mode supporting disabled, permissive, enforcing three modes. Added nginx basic auth support, allowing optional HTTP Basic Auth for Nginx Servers. Fixed ownca certificate validity issues, ensuring Chrome can recognize self-signed certificates. Changed MinIO module default password to avoid conflict with well-known default passwords Enabled etcd RBAC, each cluster can now only manage its own PostgreSQL database cluster. etcd root password now placed in /etc/etcd/etcd.pass file, readable only by administrators Configured correct SELinux contexts for HAProxy, Nginx, DNSMasq, Redis and other components Revoked executable script ownership permissions from all non-root users Added admin_ip to Patroni API allowed access IP whitelist Always create admin system user group, patronictl config restricted to admin group users only Added node_admin_sudo parameter allowing specification/adjustment of database administrator sudo permission mode (all/nopass) Fixed several ansible copy content field empty error issues. Fixed some legacy issues in pg_pitr, ensuring no race conditions during patroni cluster recovery. Bug Fixes\nFixed ownca certificate validity for Chrome compatibility Fixed Vector 0.52 syslog_raw parsing issue Fixed pg_pitr multiple replica clonefrom timing issues Fixed Ansible SELinux race condition in dnsmasq Fixed EL9 aarch64 patroni \u0026 llvmjit issues Fixed Debian groupadd path issue Fixed empty sudoers file generation Fixed pgbouncer pid path (/run/postgresql) Fixed duckdb.allow_community_extensions always active Hidden pg_partman for EL8 due to upstream break Checksums\n4c38ca59e756f239448e7eb45d2236f0 pigsty-pkg-v4.0.0.d12.aarch64.tgz 020b0ded1af009d0e758de8a33393239 pigsty-pkg-v4.0.0.d12.x86_64.tgz 513c98a3ba911eebf10a1364fd70ce90 pigsty-pkg-v4.0.0.d13.aarch64.tgz 524ca6f1e8ef6ff821eff1f618f8683e pigsty-pkg-v4.0.0.d13.x86_64.tgz b5ad7a6b6dee0515e7a0dd33611b7aba pigsty-pkg-v4.0.0.el10.aarch64.tgz bb20de1730c9cce75f476f3dc444eab5 pigsty-pkg-v4.0.0.el10.x86_64.tgz fe2f27406d218216beba9b92d7da3080 pigsty-pkg-v4.0.0.el8.aarch64.tgz f2e12f9db85b280df5e4e6504bbf69af pigsty-pkg-v4.0.0.el8.x86_64.tgz 73d79ef99e5030cb0daf5ec1bd8afe2f pigsty-pkg-v4.0.0.el9.aarch64.tgz 27b59e5b4994dd0bb17d1b4f50eff96a pigsty-pkg-v4.0.0.el9.x86_64.tgz 9838065e0c43c67a3ff2274c9b48f354 pigsty-pkg-v4.0.0.u22.aarch64.tgz fec238e811b0f838770602ed1c93a5a1 pigsty-pkg-v4.0.0.u22.x86_64.tgz 0dc4140abd907c872c29db7b77aeb54a pigsty-pkg-v4.0.0.u24.aarch64.tgz 3aa158fb40555f34e45422a4177850b7 pigsty-pkg-v4.0.0.u24.x86_64.tgz 8eeb5d05edf865543aafcc7fcb935825 pigsty-v4.0.0.tgz v3.7.0 Highlights\nPostgreSQL 18 Deep Support: Now the default major PG version, with full extension readiness! Expanded OS Support: Added EL10 and Debian 13, bringing the total supported operating systems to 14. Extension Growth: The PostgreSQL extension library now includes 437 entries. Ansible 2.19 Compatibility: Full support for Ansible 2.19 following its breaking changes. Kernel Updates: Latest versions for Supabase, PolarDB, IvorySQL, and Percona kernels. Optimized Tuning: Refined logic for default PG parameters to maximize resource utilization. PGEXT.CLOUD: Dedicated extension website open-sourced under Apache-2.0 license Version Updates\nPostgreSQL 18.1, 17.7, 16.11, 15.15, 14.20, 13.23 Patroni 4.1.0 Pgbouncer 1.25.0 pg_exporter 1.0.3 pgbackrest 2.57.0 Supabase 2025-11 PolarDB 15.15.5.0 FerretDB 2.7.0 DuckDB 1.4.2 Etcd 3.6.6 pig 0.7.4 For detailed version changes, please refer to:\nINFRA Changelog RPM Changelog DEB Changelog API Changes\nImplemented a refined optimization strategy for parallel execution parameters. See Tuning Guide. The citus extension is no longer installed by default in rich and full templates (PG 18 support pending). Added duckdb extension stubs to PostgreSQL parameter templates. Capped min_wal_size, max_wal_size, and max_slot_wal_keep_size at 200 GB, 2000 GB, and 3000 GB, respectively. Capped temp_file_limit at 200 GB (2 TB for OLAP workloads). Increased the default connection count for the connection pool. Added prometheus_port (default: 9058) to avoid conflicts with the EL10 RHEL Web Console port. Changed alertmanager_port default to 9059 to avoid potential conflicts with Kafka SSL ports. Added a pg_pre subtask to pg_pkg: removes conflicting LLVM packages (bpftool, python3-perf) on EL9+ prior to PG installation. Added the llvm module to the default repository definition for Debian/Ubuntu. Fixed package removal logic in infra-rm.yml. Compatibility Fixes\nUbuntu/Debian CA Trust: Fixed incorrect warning return codes when trusting Certificate Authorities. Ansible 2.19 Support: Resolved numerous compatibility issues introduced by Ansible 2.19 to ensure stability across versions: Added explicit int type casting for sequence variables. Migrated with_items syntax to loop. Nested key exchange variables in lists to prevent character iteration on strings in newer versions. Explicitly cast range usage to list. Renamed reserved variables such as name and port. Replaced play_hosts with ansible_play_hosts. Added string casting for specific variables to prevent runtime errors. EL10 Adaptation: Fixed missing ansible-collection-community-crypto preventing key generation. Fixed missing ansible logic packages. Removed modulemd_tools, flamegraph, and timescaledb-tool. Replaced java-17-openjdk with java-21-openjdk. Resolved aarch64 YUM repository naming issues. Debian 13 Adaptation: Replaced dnsutils with bind9-dnsutils. Ubuntu 24 Fixes: Temporarily removed tcpdump due to upstream dependency crashes. Checksums\ne00d0c2ac45e9eff1cc77927f9cd09df pigsty-v3.7.0.tgz 987529769d85a3a01776caefefa93ecb pigsty-pkg-v3.7.0.d12.aarch64.tgz 2d8272493784ae35abeac84568950623 pigsty-pkg-v3.7.0.d12.x86_64.tgz 090cc2531dcc25db3302f35cb3076dfa pigsty-pkg-v3.7.0.d13.x86_64.tgz ddc54a9c4a585da323c60736b8560f55 pigsty-pkg-v3.7.0.el10.aarch64.tgz d376e75c490e8f326ea0f0fbb4a8fd9b pigsty-pkg-v3.7.0.el10.x86_64.tgz 8c2deeba1e1d09ef3d46d77a99494e71 pigsty-pkg-v3.7.0.el8.aarch64.tgz 9795e059bd884b9d1b2208011abe43cd pigsty-pkg-v3.7.0.el8.x86_64.tgz 08b860155d6764ae817ed25f2fcf9e5b pigsty-pkg-v3.7.0.el9.aarch64.tgz 1ac430768e488a449d350ce245975baa pigsty-pkg-v3.7.0.el9.x86_64.tgz e033aaf23690755848db255904ab3bcd pigsty-pkg-v3.7.0.u22.aarch64.tgz cc022ea89181d89d271a9aaabca04165 pigsty-pkg-v3.7.0.u22.x86_64.tgz 0e978598796db3ce96caebd76c76e960 pigsty-pkg-v3.7.0.u24.aarch64.tgz 48223898ace8812cc4ea79cf3178476a pigsty-pkg-v3.7.0.u24.x86_64.tgz v3.6.1 curl https://repo.pigsty.io/get | bash -s v3.6.1 Highlights\nPostgreSQL 17.6, 16.10, 15.14, 14.19, 13.22, and 18 Beta 3 Released! PGDG APT/YUM mirror for Mainland China Users New home website https://pgsty.com Add el10, debian 13 stub, add el10 terraform images Infra Package Updates\nGrafana 12.1.0 pg_exporter 1.0.2 pig 0.6.1 vector 0.49.0 redis_exporter 1.75.0 mongo_exporter 0.47.0 victoriametrics 1.123.0 victorialogs: 1.28.0 grafana-victoriametrics-ds 0.18.3 grafana-victorialogs-ds 0.19.3 grafana-infinity-ds 3.4.1 etcd 3.6.4 ferretdb 2.5.0 tigerbeetle 0.16.54 genai-toolbox 0.12.0 Extension Package Updates\npg_search 0.17.3 API Changes\nremove br_filter from default node_kernel_modules do not use OS minor version dir for pgdg yum repos Checksums\n045977aff647acbfa77f0df32d863739 pigsty-pkg-v3.6.1.d12.aarch64.tgz 636b15c2d87830f2353680732e1af9d2 pigsty-pkg-v3.6.1.d12.x86_64.tgz 700a9f6d0db9c686d371bf1c05b54221 pigsty-pkg-v3.6.1.el8.aarch64.tgz 2aff03f911dd7be363ba38a392b71a16 pigsty-pkg-v3.6.1.el8.x86_64.tgz ce07261b02b02b36a307dab83e460437 pigsty-pkg-v3.6.1.el9.aarch64.tgz d598d62a47bbba2e811059a53fe3b2b5 pigsty-pkg-v3.6.1.el9.x86_64.tgz 13fd68752e59f5fd2a9217e5bcad0acd pigsty-pkg-v3.6.1.u22.aarch64.tgz c25ccfb98840c01eb7a6e18803de55bb pigsty-pkg-v3.6.1.u22.x86_64.tgz 0d71e58feebe5299df75610607bf428c pigsty-pkg-v3.6.1.u24.aarch64.tgz 4fbbab1f8465166f494110c5ec448937 pigsty-pkg-v3.6.1.u24.x86_64.tgz 083d8680fa48e9fec3c3fcf481d25d2f pigsty-v3.6.1.tgz v3.6.0 curl https://repo.pigsty.io/get | bash -s v3.6.0 Highlights\nBrand-new documentation site: https://doc.pgsty.com Added pgsql-pitr playbook and backup/restore tutorial, improved PITR experience Added kernel support: Percona PG TDE (PG17) Optimized self-hosted Supabase experience, updated to the latest version, and fixed issues with the official template Simplified installation steps, online install by default, bootstrap now part of install script Improvements\nRefactored ETCD module with dedicated remove playbook and bin utils Refactored MinIO module with plain HTTP mode, better bucket provisioning options. Reorganized and streamlined all configuration templates for easier use Faster Docker Registry mirror for users in mainland China Optimized tuned OS parameter templates for modern hardware and NVMe disks Added extension pgactive for multi-master replication and sub-second failover Adjusted default values for pg_fs_main / pg_fs_backup, simplified file directory structure design Bug Fixes\nFixed pgbouncer configuration file error by @housei-zzy Fixed OrioleDB issues on Debian platform Fixed tuned shm configuration parameter issue Offline packages now use the PGDG source directly, avoiding out-of-sync mirror sites Fix ivorysql libxcrypt dependencies issues Fix Replace the slow and broken epel mirror Fix haproxy_enabled flag not working Infra Package Updates\nAdded Victoria Metrics / Victoria Logs related packages\ngenai-toolbox 0.9.0 (new) victoriametrics 1.120.0 -\u003e 1.121.0 (refactor) vmutils 1.121.0 (rename from victoria-metrics-utils) grafana-victoriametrics-ds 0.15.1 -\u003e 0.17.0 victorialogs 1.24.0 -\u003e 1.25.1 (refactor) vslogcli 1.24.0 -\u003e 1.25.1 vlagent 1.25.1 (new) grafana-victorialogs-ds 0.16.3 -\u003e 0.18.1 prometheus 3.4.1 -\u003e 3.5.0 grafana 12.0.0 -\u003e 12.0.2 vector 0.47.0 -\u003e 0.48.0 grafana-infinity-ds 3.2.1 -\u003e 3.3.0 keepalived_exporter 1.7.0 blackbox_exporter 0.26.0 -\u003e 0.27.0 redis_exporter 1.72.1 -\u003e 1.77.0 rclone 1.69.3 -\u003e 1.70.3 Database Package Updates\nPostgreSQL 18 Beta2 update pg_exporter 1.0.1, updated to latest dependencies and provides Docker image pig 0.6.0, updated extension and repository list, with pig install subcommand vip-manager 3.0.0 -\u003e 4.0.0 ferretdb 2.2.0 -\u003e 2.3.1 dblab 0.32.0 -\u003e 0.33.0 duckdb 1.3.1 -\u003e 1.3.2 etcd 3.6.1 -\u003e 3.6.3 ferretdb 2.2.0 -\u003e 2.4.0 juicefs 1.2.3 -\u003e 1.3.0 tigerbeetle 0.16.41 -\u003e 0.16.50 pev2 1.15.0 -\u003e 1.16.0 Extension Package Updates\nOrioleDB 1.5 beta12 OriolePG 17.11 plv8 3.2.3 -\u003e 3.2.4 postgresql_anonymizer 2.1.1 -\u003e 2.3.0 pgvectorscale 0.7.1 -\u003e 0.8.0 wrappers 0.5.0 -\u003e 0.5.3 supautils 2.9.1 -\u003e 2.10.0 citus 13.0.3 -\u003e 13.1.0 timescaledb 2.20.0 -\u003e 2.21.1 vchord 0.3.0 -\u003e 0.4.3 pgactive 2.1.5 (new) documentdb 0.103.0 -\u003e 0.105.0 pg_search 0.17.0 API Changes\npg_fs_backup: Renamed to pg_fs_backup, default value /data/backups. pg_rm_bkup: Renamed to pg_rm_backup, default value true. pg_fs_main: Default value adjusted to /data/postgres. nginx_cert_validity: New parameter to control Nginx self-signed certificate validity, default 397d. minio_buckets: Default value adjusted to create three buckets named pgsql, meta, data. minio_users: Removed dba user, added s3user_meta and s3user_data users for meta and data buckets respectively. minio_https: New parameter to allow MinIO to use HTTP mode. minio_provision: New parameter to allow skipping MinIO provisioning stage (skip bucket and user creation) minio_safeguard: New parameter, abort minio-rm.yml when enabled minio_rm_data: New parameter, whether to remove minio data directory during minio-rm.yml minio_rm_pkg: New parameter, whether to uninstall minio package during minio-rm.yml etcd_learner: New parameter to control whether to init etcd instance as learner etcd_rm_data: New parameter, whether to remove etcd data directory during etcd-rm.yml etcd_rm_pkg: New parameter, whether to uninstall etcd package during etcd-rm.yml Checksums\nab91bc05c54b88c455bf66533c1d8d43 pigsty-v3.6.0.tgz cea861e2b4ec7ff5318e1b3c30b470cb pigsty-pkg-v3.6.0.d12.aarch64.tgz 2f253af87e19550057c0e7fca876d37c pigsty-pkg-v3.6.0.d12.x86_64.tgz 0158145b9bbf0e4a120b8bfa8b44f857 pigsty-pkg-v3.6.0.el8.aarch64.tgz 07330d687d04d26e7d569c8755426c5a pigsty-pkg-v3.6.0.el8.x86_64.tgz 311df5a342b39e3288ebb8d14d81e0d1 pigsty-pkg-v3.6.0.el9.aarch64.tgz 92aad54cc1822b06d3e04a870ae14e29 pigsty-pkg-v3.6.0.el9.x86_64.tgz c4fadf1645c8bbe3e83d5a01497fa9ca pigsty-pkg-v3.6.0.u22.aarch64.tgz 5477ed6be96f156a43acd740df8a9b9b pigsty-pkg-v3.6.0.u22.x86_64.tgz 196169afc1be02f93fcc599d42d005ca pigsty-pkg-v3.6.0.u24.aarch64.tgz dbe5c1e8a242a62fe6f6e1f6e6b6c281 pigsty-pkg-v3.6.0.u24.x86_64.tgz v3.5.0 Highlights\nNew website: https://pgsty.com PostgreSQL 18 (Beta) support: monitoring via pg_exporter 1.0.0, installer alias via pig 0.4.2, and a pg18 template 421 bundled extensions, now including OrioleDB and OpenHalo kernels on all platforms pig do CLI replaces legacy bin/ scripts Hardening for self-hosted Supabase (replication lag, key distribution, etc.) Code \u0026 architecture refactor — slimmer tasks, cleaner defaults for Postgres \u0026 PgBouncer Monitoring stack refresh — Grafana 12, pg_exporter 1.0, new panels \u0026 plugins Run vagrant on Apple Silicon curl https://repo.pigsty.io/get | bash -s v3.5.0 Module Changes\nAdd PostgreSQL 18 support PG18 metrics support with pg_exporter 1.0.0+ PG18 install support with pig 0.4.1+ New config template pg18.yml Refactored pgsql module Split monitoring into a new pg_monitor role; removed clean logic Pruned duplicate tasks, dropped dir/utils block, renamed templates (no .j2) All extensions install in extensions schema (Supabase best-practice) Added SET search_path='' to every monitoring function Tuned PgBouncer defaults (larger pool, cleanup query); new pgbouncer_ignore_param New pg_key task to generate pgsodium master keys Enabled sync_replication_slots by default on PG 17 Retagged subtasks for clearer structure Refactored pg_remove module New flags pg_rm_data, pg_rm_bkup, pg_rm_pkg control what gets wiped Clearer role layout \u0026 tagging Added new pg_monitor module pgbouncer_exporter no longer shares configuration files with pg_exporter Added monitoring metrics for TimescaleDB and Citus Using pg_exporter 0.9.0 with updated replication slot metrics for PG16/17 Using more compact, newly designed collector configuration files Supabase Enhancement (thanks @lawso017 for the contribution) update supabase containers and schemas to the latest version Support pgsodium server key loading fix logflare lag issue with supa-kick crontab add set search_path clause for monitor functions Added new pig do command to CLI, allowing command-line tool to replace Shell scripts in bin/ Infra Package Updates\npig 0.4.2 duckdb 1.3.0 etcd 3.6.0 vector 0.47.0 minio 20250422221226 mcli 20250416181326 pev 1.5.0 rclone 1.69.3 mtail 3.0.8 (new) Observability Package Updates\ngrafana 12.0.0 grafana-victorialogs-ds 0.16.3 grafana-victoriametrics-ds 0.15.1 grafana-infinity-ds 3.2.1 grafana_plugins 12.0.0 prometheus 3.4.0 pushgateway 1.11.1 nginx_exporter 1.4.2 pg_exporter 1.0.0 pgbackrest_exporter 0.20.0 redis_exporter 1.72.1 keepalived_exporter 1.6.2 victoriametrics 1.117.1 victoria_logs 1.22.2 Database Package Updates\nPostgreSQL 17.5, 16.9, 15.13, 14.18, 13.21 PostgreSQL 18beta1 support pgbouncer 1.24.1 pgbackrest 2.55 pgbadger 13.1 Extension Package Updates\nspat 0.1.0a4 new extension pgsentinel 1.1.0 new extension pgdd 0.6.0 (pgrx 0.14.1) new extension add back convert 0.0.4 (pgrx 0.14.1) new extension pg_tokenizer.rs 0.1.0 (pgrx 0.13.1) pg_render 0.1.2 (pgrx 0.12.8) pgx_ulid 0.2.0 (pgrx 0.12.7) pg_idkit 0.3.0 (pgrx 0.14.1) pg_ivm 1.11.0 orioledb 1.4.0 beta11 rpm \u0026 add debian/ubuntu support openhalo 14.10 add debian/ubuntu support omnigres 20250507 (miss on d12/u22) citus 12.0.3 timescaledb 2.20.0 (DROP PG14 support) supautils 2.9.2 pg_envvar 1.0.1 pgcollection 1.0.0 aggs_for_vecs 1.4.0 pg_tracing 0.1.3 pgmq 1.5.1 tzf-pg 0.2.0 (pgrx 0.14.1) pg_search 0.15.18 (pgrx 0.14.1) anon 2.1.1 (pgrx 0.14.1) pg_parquet 0.4.0 (0.14.1) pg_cardano 1.0.5 (pgrx 0.12) -\u003e 0.14.1 pglite_fusion 0.0.5 (pgrx 0.12.8) -\u003e 14.1 vchord_bm25 0.2.1 (pgrx 0.13.1) vchord 0.3.0 (pgrx 0.13.1) pg_vectorize 0.22.1 (pgrx 0.13.1) wrappers 0.4.6 (pgrx 0.12.9) timescaledb-toolkit 1.21.0 (pgrx 0.12.9) pgvectorscale 0.7.1 (pgrx 0.12.9) pg_session_jwt 0.3.1 (pgrx 0.12.6) -\u003e 0.12.9 pg_timetable 5.13.0 ferretdb 2.2.0 documentdb 0.103.0 (+aarch64 support) pgml 2.10.0 (pgrx 0.12.9) sqlite_fdw 2.5.0 (fix pg17 deb) tzf 0.2.2 0.14.1 (rename src) pg_vectorize 0.22.2 (pgrx 0.13.1) wrappers 0.5.0 (pgrx 0.12.9) Checksums\nc7e5ce252ddf848e5f034173e0f29345 pigsty-v3.5.0.tgz ba31f311a16d615c1ee1083dc5a53566 pigsty-pkg-v3.5.0.d12.aarch64.tgz 3aa5c56c8f0de53303c7100f2b3934f4 pigsty-pkg-v3.5.0.d12.x86_64.tgz a098cb33822633357e6880eee51affd6 pigsty-pkg-v3.5.0.el8.x86_64.tgz 63723b0aeb4d6c02fff0da2c78e4de31 pigsty-pkg-v3.5.0.el9.aarch64.tgz eb91c8921d7b8a135d8330c77468bfe7 pigsty-pkg-v3.5.0.el9.x86_64.tgz 87ff25e14dfb9001fe02f1dfbe70ae9e pigsty-pkg-v3.5.0.u22.x86_64.tgz 18be503856f6b39a59efbd1d0a8556b6 pigsty-pkg-v3.5.0.u24.aarch64.tgz 2bbef6a18cfa99af9cd175ef0adf873c pigsty-pkg-v3.5.0.u24.x86_64.tgz v3.4.1 GitHub Release Page: v3.4.1\nAdded support for MySQL wire-compatible PostgreSQL kernel on EL systems: openHalo Added support for OLTP-enhanced PostgreSQL kernel on EL systems: orioledb Optimized pgAdmin 9.2 application template with automatic server list updates and pgpass password population Increased PG default max connections to 250, 500, 1000 Removed the mysql_fdw extension with dependency errors from EL8 Infra Updates\npig 0.3.4 etcd 3.5.21 restic 0.18.0 ferretdb 2.1.0 tigerbeetle 0.16.34 pg_exporter 0.8.1 node_exporter 1.9.1 grafana 11.6.0 zfs_exporter 3.8.1 mongodb_exporter 0.44.0 victoriametrics 1.114.0 minio 20250403145628 mcli 20250403170756 Extension Update\nBump pg_search to 0.15.13 Bump citus to 13.0.3 Bump timescaledb to 2.19.1 Bump pgcollection RPM to 1.0.0 Bump pg_vectorize RPM to 0.22.1 Bump pglite_fusion RPM to 0.0.4 Bump aggs_for_vecs RPM to 1.4.0 Bump pg_tracing RPM to 0.1.3 Bump pgmq RPM to 1.5.1 Checksums\n471c82e5f050510bd3cc04d61f098560 pigsty-v3.4.1.tgz 4ce17cc1b549cf8bd22686646b1c33d2 pigsty-pkg-v3.4.1.d12.aarch64.tgz c80391c6f93c9f4cad8079698e910972 pigsty-pkg-v3.4.1.d12.x86_64.tgz 811bf89d1087512a4f8801242ca8bed5 pigsty-pkg-v3.4.1.el9.x86_64.tgz 9fe2e6482b14a3e60863eeae64a78945 pigsty-pkg-v3.4.1.u22.x86_64.tgz v3.4.0 GitHub Release Page: v3.4.0\nIntroduction Blog: Pigsty v3.4 MySQL Compatibility and Overall Enhancements\nNew Features\nAdded new pgBackRest backup monitoring metrics and dashboards Enhanced Nginx server configuration options, with support for automated Certbot issuance Now prioritizing PostgreSQL’s built-in C/C.UTF-8 locale settings IvorySQL 4.4 is now fully supported across all platforms (RPM/DEB on x86/ARM) Added new software packages: Juicefs, Restic, TimescaleDB EventStreamer The Apache AGE graph database extension now fully supports PostgreSQL 13–17 on EL Improved the app.yml playbook: launch standard Docker app without extra config Bump Supabase, Dify, and Odoo app templates, bump to their latest versions Add electric app template, local-first PostgreSQL Sync Engine Infra Packages\n+restic 0.17.3 +juicefs 1.2.3 +timescaledb-event-streamer 0.12.0 Prometheus 3.2.1 AlertManager 0.28.1 blackbox_exporter 0.26.0 node_exporter 1.9.0 mysqld_exporter 0.17.2 kafka_exporter 1.9.0 redis_exporter 1.69.0 pgbackrest_exporter 0.19.0-2 DuckDB 1.2.1 etcd 3.5.20 FerretDB 2.0.0 tigerbeetle 0.16.31 vector 0.45.0 VictoriaMetrics 1.113.0 VictoriaLogs 1.17.0 rclone 1.69.1 pev2 1.14.0 grafana-victorialogs-ds 0.16.0 grafana-victoriametrics-ds 0.14.0 grafana-infinity-ds 3.0.0 PostgreSQL Related\nPatroni 4.0.5 PolarDB 15.12.3.0-e1e6d85b IvorySQL 4.4 pgbackrest 2.54.2 pev2 1.14 WiltonDB 13.17 PostgreSQL Extensions\npgspider_ext 1.3.0 (new extension) apache age 13–17 el rpm (1.5.0) timescaledb 2.18.2 → 2.19.0 citus 13.0.1 → 13.0.2 documentdb 1.101-0 → 1.102-0 pg_analytics 0.3.4 → 0.3.7 pg_search 0.15.2 → 0.15.8 pg_ivm 1.9 → 1.10 emaj 4.4.0 → 4.6.0 pgsql_tweaks 0.10.0 → 0.11.0 pgvectorscale 0.4.0 → 0.6.0 (pgrx 0.12.5) pg_session_jwt 0.1.2 → 0.2.0 (pgrx 0.12.6) wrappers 0.4.4 → 0.4.5 (pgrx 0.12.9) pg_parquet 0.2.0 → 0.3.1 (pgrx 0.13.1) vchord 0.2.1 → 0.2.2 (pgrx 0.13.1) pg_tle 1.2.0 → 1.5.0 supautils 2.5.0 → 2.6.0 sslutils 1.3 → 1.4 pg_profile 4.7 → 4.8 pg_snakeoil 1.3 → 1.4 pg_jsonschema 0.3.2 → 0.3.3 pg_incremental 1.1.1 → 1.2.0 pg_stat_monitor 2.1.0 → 2.1.1 ddl_historization 0.7 → 0.0.7 (bug fix) pg_sqlog 3.1.7 → 1.6 (bug fix) pg_random removed development suffix (bug fix) asn1oid 1.5 → 1.6 table_log 0.6.1 → 0.6.4 Interface Changes\nAdded new Docker parameters: docker_data and docker_storage_driver (#521 by @waitingsong) Added new Infra parameter: alertmanager_port, which lets you specify the AlertManager port Added new Infra parameter: certbot_sign, apply for cert during nginx init? (false by default) Added new Infra parameter: certbot_email, specifying the email used when requesting certificates via Certbot Added new Infra parameter: certbot_options, specifying additional parameters for Certbot Updated IvorySQL to place its default binary under /usr/ivory-4 starting in IvorySQL 4.4 Changed the default for pg_lc_ctype and other locale-related parameters from en_US.UTF-8 to C For PostgreSQL 17, if using UTF8 encoding with C or C.UTF-8 locales, PostgreSQL’s built-in localization rules now take priority configure automatically detects whether C.utf8 is supported by both the PG version and the environment, and adjusts locale-related options accordingly Set the default IvorySQL binary path to /usr/ivory-4 Updated the default value of pg_packages to pgsql-main patroni pgbouncer pgbackrest pg_exporter pgbadger vip-manager Updated the default value of repo_packages to [node-bootstrap, infra-package, infra-addons, node-package1, node-package2, pgsql-utility, extra-modules] Removed LANG and LC_ALL environment variable settings from /etc/profile.d/node.sh Now using bento/rockylinux-8 and bento/rockylinux-9 as the Vagrant box images for EL Added a new alias, extra_modules, which includes additional optional modules Updated PostgreSQL aliases: postgresql, pgsql-main, pgsql-core, pgsql-full GitLab repositories are now included among available modules The Docker module has been merged into the Infra module The node.yml playbook now includes a node_pip task to configure a pip mirror on each node The pgsql.yml playbook now includes a pgbackrest_exporter task for collecting backup metrics The Makefile now allows the use of META/PKG environment variables Added /pg/spool directory as temporary storage for pgBackRest Disabled pgBackRest’s link-all option by default Enabled block-level incremental backups for MinIO repositories by default Bug Fixes\nFixed the exit status code in pg-backup (#532 by @waitingsong) In pg-tune-hugepage, restricted PostgreSQL to use only large pages (#527 by @waitingsong) Fixed logic errors in the pg-role task Corrected type conversion for hugepage configuration parameters Fixed default value issues for node_repo_modules in the slim template Checksums\n768bea3bfc5d492f4c033cb019a81d3a pigsty-v3.4.0.tgz 7c3d47ef488a9c7961ca6579dc9543d6 pigsty-pkg-v3.4.0.d12.aarch64.tgz b5d76aefb1e1caa7890b3a37f6a14ea5 pigsty-pkg-v3.4.0.d12.x86_64.tgz 42dacf2f544ca9a02148aeea91f3153a pigsty-pkg-v3.4.0.el8.aarch64.tgz d0a694f6cd6a7f2111b0971a60c49ad0 pigsty-pkg-v3.4.0.el8.x86_64.tgz 7caa82254c1b0750e89f78a54bf065f8 pigsty-pkg-v3.4.0.el9.aarch64.tgz 8f817e5fad708b20ee217eb2e12b99cb pigsty-pkg-v3.4.0.el9.x86_64.tgz 8b2fcaa6ef6fd8d2726f6eafbb488aaf pigsty-pkg-v3.4.0.u22.aarch64.tgz 83291db7871557566ab6524beb792636 pigsty-pkg-v3.4.0.u22.x86_64.tgz c927238f0343cde82a4a9ab230ecd2ac pigsty-pkg-v3.4.0.u24.aarch64.tgz 14cbcb90693ed5de8116648a1f2c3e34 pigsty-pkg-v3.4.0.u24.x86_64.tgz v3.3.0 Total available extensions increased to 404! PostgreSQL February Minor Updates: 17.4, 16.8, 15.12, 14.17, 13.20 New Feature: app.yml script for auto-installing apps like Odoo, Supabase, Dify. New Feature: Further Nginx configuration customization in infra_portal. New Feature: Added Certbot support for quick free HTTPS certificate requests. New Feature: Pure-text extension list now supported in pg_default_extensions. New Feature: Default repositories now include mongo, redis, groonga, haproxy, etc. New Parameter: node_aliases to add command aliases for Nodes. Fix: Resolved default EPEL repo address issue in Bootstrap script. Improvement: Added Aliyun mirror for Debian Security repository. Improvement: pgBackRest backup support for IvorySQL kernel. Improvement: ARM64 and Debian/Ubuntu support for PolarDB. pg_exporter 0.8.0 now supports new metrics in pgbouncer 1.24. New Feature: Auto-completion for common commands like git, docker, systemctl #506 #507 by @waitingsong. Improvement: Refined ignore_startup_parameters in pgbouncer config template #488 by @waitingsong. New homepage design: Pigsty’s website now features a fresh new look. Extension Directory: Detailed information and download links for RPM/DEB binary packages. Extension Build: pig CLI now auto-sets PostgreSQL extension build environment. New Extensions\n12 new PostgreSQL extensions added, bringing the total to 404 available extensions.\ndocumentdb 0.101-0 VectorChord-bm25 (vchord_bm25) 0.1.0 pg_tracing 0.1.2 pg_curl 2.4 pgxicor 0.1.0 pgsparql 1.0 pgjq 0.1.0 hashtypes 0.1.5 db_migrator 1.0.0 pg_cooldown 0.1 pgcollection 0.9.1 pg_bzip 1.0.0 Bump Extension\ncitus 13.0.0 -\u003e 13.0.1 pg_duckdb 0.2.0 -\u003e 0.3.1 pg_mooncake 0.1.0 -\u003e 0.1.2 timescaledb 2.17.2 -\u003e 2.18.2 supautils 2.5.0 -\u003e 2.6.0 supabase_vault 0.3.1 (become C) VectorChord 0.1.0 -\u003e 0.2.1 pg_bulkload 3.1.22 (+pg17) pg_store_plan 1.8 (+pg17) pg_search 0.14 -\u003e 0.15.2 pg_analytics 0.3.0 -\u003e 0.3.4 pgroonga 3.2.5 -\u003e 4.0.0 zhparser 2.2 -\u003e 2.3 pg_vectorize 0.20.0 -\u003e 0.21.1 pg_net 0.14.0 pg_curl 2.4.2 table_version 1.10.3 -\u003e 1.11.0 pg_duration 1.0.2 pg_graphql 1.5.9 -\u003e 1.5.11 vchord 0.1.1 -\u003e 0.2.1 ((+13)) vchord_bm25 0.1.0 -\u003e 0.1.1 pg_mooncake 0.1.1 -\u003e 0.1.2 pgddl 0.29 pgsql_tweaks 0.11.0 Infra Updates\npig 0.1.3 -\u003e 0.3.0 pushgateway 1.10.0 -\u003e 1.11.0 alertmanager 0.27.0 -\u003e 0.28.0 nginx_exporter 1.4.0 -\u003e 1.4.1 pgbackrest_exporter 0.18.0 -\u003e 0.19.0 redis_exporter 1.66.0 -\u003e 1.67.0 mongodb_exporter 0.43.0 -\u003e 0.43.1 VictoriaMetrics 1.107.0 -\u003e 1.111.0 VictoriaLogs v1.3.2 -\u003e 1.9.1 DuckDB 1.1.3 -\u003e 1.2.0 Etcd 3.5.17 -\u003e 3.5.18 pg_timetable 5.10.0 -\u003e 5.11.0 FerretDB 1.24.0 -\u003e 2.0.0-rc tigerbeetle 0.16.13 -\u003e 0.16.27 grafana 11.4.0 -\u003e 11.5.2 vector 0.43.1 -\u003e 0.44.0 minio 20241218131544 -\u003e 20250218162555 mcli 20241121172154 -\u003e 20250215103616 rclone 1.68.2 -\u003e 1.69.0 vray 5.23 -\u003e 5.28 v3.2.2 New Extension(s): Omnigres 33 extensions, postgres as platform New Extension: pg_mooncake: duckdb in postgres New Extensions: pg_xxhash New Extension: timescaledb_toolkit New Extension: pg_xenophile New Extension: pg_drop_events New Extension: pg_incremental Bump citus to 13.0.0 with PostgreSQL 17 support. Bump pgml to 2.10.0 Bump pg_extra_time to 2.0.0 Bump pg_vectorize to 0.20.0 What’s Changed\nBump IvorySQL to 4.2 (PostgreSQL 17.2) Add Arm64 and Debian support for PolarDB kernel Add certbot and certbot-nginx to default infra_packages Increase pgbouncer max_prepared_statements to 256 remove pgxxx-citus package alias hide pgxxx-olap category in pg_extensions by default v3.2.1 Highlights\n351 PostgreSQL Extensions, including the powerful postgresql-anonymizer 2.0 IvorySQL 4.0 support for EL 8/9 Now use the Pigsty compiled Citus, TimescaleDB and pgroonga on all distros Add self-hosting Odoo template and support Bump software versions\npig CLI 0.1.2 self-updating capability prometheus 3.1.0 Add New Extension\nadd pg_anon 2.0.0 add omnisketch 1.0.2 add ddsketch 1.0.1 add pg_duration 1.0.1 add ddl_historization 0.0.7 add data_historization 1.1.0 add schedoc 0.0.1 add floatfile 1.3.1 add pg_upless 0.0.3 add pg_task 1.0.0 add pg_readme 0.7.0 add vasco 0.1.0 add pg_xxhash 0.0.1 Update Extension\nlower_quantile 1.0.3 quantile 1.1.8 sequential_uuids 1.0.3 pgmq 1.5.0 (subdir) floatvec 1.1.1 pg_parquet 0.2.0 wrappers 0.4.4 pg_later 0.3.0 topn fix for deb.arm64 add age 17 on debian powa + pg17, 5.0.1 h3 + pg17 ogr_fdw + pg17 age + pg17 1.5 on debian pgtap + pg17 1.3.3 repmgr topn + pg17 pg_partman 5.2.4 credcheck 3.0 ogr_fdw 1.1.5 ddlx 0.29 postgis 3.5.1 tdigest 1.4.3 pg_repack 1.5.2 v3.2.0 Highlights\nNew CLI: Introducing the pig command-line tool for managing extension plugins. ARM64 Support: 390 extensions are now available for ARM64 across five major distributions. Supabase Update: Latest Supabase Release Week updates are now supported for self-hosting on all distributions. Grafana v11.4: Upgraded Grafana to version 11.4, featuring a new Infinity datasource. Package Changes\nNew Extensions Added timescaledb, timescaledb-loader, timescaledb-toolkit, and timescaledb-tool to the PIGSTY repository. Added a custom-compiled pg_timescaledb for EL. Added pgroonga, custom-compiled for all EL variants. Added vchord 0.1.0. Added pg_bestmatch.rs 0.0.1. Added pglite_fusion 0.0.3. Added pgpdf 0.1.0. Updated Extensions pgvectorscale: 0.4.0 → 0.5.1 pg_parquet: 0.1.0 → 0.1.1 pg_polyline: 0.0.1 pg_cardano: 1.0.2 → 1.0.3 pg_vectorize: 0.20.0 pg_duckdb: 0.1.0 → 0.2.0 pg_search: 0.13.0 → 0.13.1 aggs_for_vecs: 1.3.1 → 1.3.2 Infrastructure Added promscale 0.17.0 Added grafana-plugins 11.4 Added grafana-infinity-plugins Added grafana-victoriametrics-ds Added grafana-victorialogs-ds vip-manager: 2.8.0 → 3.0.0 vector: 0.42.0 → 0.43.0 grafana: 11.3 → 11.4 prometheus: 3.0.0 → 3.0.1 (package name changed from prometheus2 to prometheus) nginx_exporter: 1.3.0 → 1.4.0 mongodb_exporter: 0.41.2 → 0.43.0 VictoriaMetrics: 1.106.1 → 1.107.0 VictoriaLogs: 1.0.0 → 1.3.2 pg_timetable: 5.9.0 → 5.10.0 tigerbeetle: 0.16.13 → 0.16.17 pg_export: 0.7.0 → 0.7.1 New Docker App Add mattermost the open-source Slack alternative self-hosting template Bug Fixes Added python3-cdiff for el8.aarch64 to fix missing Patroni dependency. Added timescaledb-tools for el9.aarch64 to fix missing package in official repo. Added pg_filedump for el9.aarch64 to fix missing package in official repo. Removed Extensions pg_mooncake: Removed due to conflicts with pg_duckdb. pg_top: Removed because of repeated version issues and quality concerns. hunspell_pt_pt: Removed because of conflict with official PG dictionary files. pgml: Disabled by default (no longer downloaded or installed). API Changes\nrepo_url_packages now defaults to an empty array; packages are installed via OS package managers. grafana_plugin_cache is deprecated; Grafana plugins are now installed via OS package managers. grafana_plugin_list is deprecated for the same reason. The 36-node “production” template has been renamed to simu. Auto-generated code under node_id/vars now includes aarch64 support. infra_packages now includes the pig CLI tool. The configure command now updates the version numbers of pgsql-xxx aliases in auto-generated config files. Update terraform templates with Makefile shortcuts and better provision experience Bug Fix\nFix pgbouncer dashboard selector issue #474 Add --arg value support for pg-pitr by @waitingsong Fix redis log message typo by @waitingsong Checksums\nc42da231067f25104b71a065b4a50e68 pigsty-pkg-v3.2.0.d12.aarch64.tgz ebb818f98f058f932b57d093d310f5c2 pigsty-pkg-v3.2.0.d12.x86_64.tgz d2b85676235c9b9f2f8a0ad96c5b15fd pigsty-pkg-v3.2.0.el9.aarch64.tgz 649f79e1d94ec1845931c73f663ae545 pigsty-pkg-v3.2.0.el9.x86_64.tgz 24c0be1d8436f3c64627c12f82665a17 pigsty-pkg-v3.2.0.u22.aarch64.tgz 0b9be0e137661e440cd4f171226d321d pigsty-pkg-v3.2.0.u22.x86_64.tgz 8fdc6a60820909b0a2464b0e2b90a3a6 pigsty-v3.2.0.tgz v3.1.0 2024-11-24 : ARM64 \u0026 Ubuntu24, PG17 by Default, Better Supabase \u0026 MinIO\nhttps://github.com/pgsty/pigsty/releases/tag/v3.1.0\nv3.0.4 2024-10-28 : PostgreSQL 17 Extensions, Better self-hosting Supabase\nhttps://github.com/pgsty/pigsty/releases/tag/v3.0.4\nv3.0.3 2024-09-27 : PostgreSQL 17, Etcd Enhancement, IvorySQL 3.4, PostGIS 3.5\nhttps://github.com/pgsty/pigsty/releases/tag/v3.0.3\nv3.0.2 2024-09-07 : Mini Install, PolarDB 15, Bloat View Update\nhttps://github.com/pgsty/pigsty/releases/tag/v3.0.2\nv3.0.1 2024-08-31 : Oracle Compatibility, Patroni 4.0, Routine Bug Fix\nhttps://github.com/pgsty/pigsty/releases/tag/v3.0.1\nv3.0.0 2024-08-30 : Extension Exploding \u0026 Pluggable Kernels (MSSQL, Oracle)\nhttps://github.com/pgsty/pigsty/releases/tag/v3.0.0\nv2.7.0 2024-05-16 : Extension Overwhelming, new docker apps\nhttps://github.com/pgsty/pigsty/releases/tag/v2.7.0\nv2.6.0 2024-02-29 : PG 16 as default version, ParadeDB \u0026 DuckDB\nhttps://github.com/pgsty/pigsty/releases/tag/v2.6.0\nv2.5.1 2023-12-01 : Routine update, pg16 major extensions\nhttps://github.com/pgsty/pigsty/releases/tag/v2.5.1\nv2.5.0 2023-10-24 : Ubuntu/Debian Support: bullseye, bookworm, jammy, focal\nhttps://github.com/pgsty/pigsty/releases/tag/v2.5.0\nv2.4.1 2023-09-24 : Supabase/PostgresML support, graphql, jwt, pg_net, vault\nhttps://github.com/pgsty/pigsty/releases/tag/v2.4.1\nv2.4.0 2023-09-14 : PG16, RDS Monitor, New Extensions\nhttps://github.com/pgsty/pigsty/releases/tag/v2.4.0\nv2.3.1 2023-09-01 : PGVector with HNSW, PG16 RC1, Chinese Docs, Bug Fix\nhttps://github.com/pgsty/pigsty/releases/tag/v2.3.1\nv2.3.0 2023-08-20 : PGSQL/REDIS Update, NODE VIP, Mongo/FerretDB, MYSQL Stub\nhttps://github.com/pgsty/pigsty/releases/tag/v2.3.0\nv2.2.0 2023-08-04 : Dashboard \u0026 Provision overhaul, UOS compatibility\nhttps://github.com/pgsty/pigsty/releases/tag/v2.2.0\nv2.1.0 2023-06-10 : PostgreSQL 12 ~ 16beta support\nhttps://github.com/pgsty/pigsty/releases/tag/v2.1.0\nv2.0.2 2023-03-31 : Add pgvector support and fix MinIO CVE\nhttps://github.com/pgsty/pigsty/releases/tag/v2.0.2\nv2.0.1 2023-03-21 : v2 Bug Fix, security enhance and bump grafana version\nhttps://github.com/pgsty/pigsty/releases/tag/v2.0.1\nv2.0.0 2023-02-28 : Compatibility Security Maintainability Enhancement\nhttps://github.com/pgsty/pigsty/releases/tag/v2.0.0\nv1.5.1 2022-06-18 : Grafana Security Hotfix\nhttps://github.com/pgsty/pigsty/releases/tag/v1.5.1\nv1.5.0 2022-05-31 : Docker Applications\nhttps://github.com/pgsty/pigsty/releases/tag/v1.5.0\nv1.4.1 2022-04-20 : Bug fix \u0026 Full translation of English documents.\nhttps://github.com/pgsty/pigsty/releases/tag/v1.4.1\nv1.4.0 2022-03-31 : MatrixDB Support, Separated INFRA, NODES, PGSQL, REDIS\nhttps://github.com/pgsty/pigsty/releases/tag/v1.4.0\nv1.3.0 2021-11-30 : PGCAT Overhaul \u0026 PGSQL Enhancement \u0026 Redis Support Beta\nhttps://github.com/pgsty/pigsty/releases/tag/v1.3.0\nv1.2.0 2021-11-03 : Upgrade default Postgres to 14, monitoring existing pg\nhttps://github.com/pgsty/pigsty/releases/tag/v1.2.0\nv1.1.0 2021-10-12 : HomePage, JupyterLab, PGWEB, Pev2 \u0026 Pgbadger\nhttps://github.com/pgsty/pigsty/releases/tag/v1.1.0\nv1.0.0 2021-07-26 : v1 GA, Monitoring System Overhaul\nhttps://github.com/pgsty/pigsty/releases/tag/v1.0.0\nv0.9.0 2021-04-04 : Pigsty GUI, CLI, Logging Integration\nhttps://github.com/pgsty/pigsty/releases/tag/v0.9.0\nv0.8.0 2021-03-28 : Service Provision\nhttps://github.com/pgsty/pigsty/releases/tag/v0.8.0\nv0.7.0 2021-03-01 : Monitor only deployment\nhttps://github.com/pgsty/pigsty/releases/tag/v0.7.0\nv0.6.0 2021-02-19 : Architecture Enhancement\nhttps://github.com/pgsty/pigsty/releases/tag/v0.6.0\nv0.5.0 2021-01-07 : Database Customize Template\nhttps://github.com/pgsty/pigsty/releases/tag/v0.5.0\nv0.4.0 2020-12-14 : PostgreSQL 13 Support, Official Documentation\nhttps://github.com/pgsty/pigsty/releases/tag/v0.4.0\nv0.3.0 2020-10-22 : Provisioning Solution GA\nhttps://github.com/pgsty/pigsty/releases/tag/v0.3.0\nv0.2.0 2020-07-10 : PGSQL Monitoring v6 GA\nhttps://github.com/pgsty/pigsty/commit/385e33a62a19817e8ba19997260e6b77d99fe2ba\nv0.1.0 2020-06-20 : Validation on Testing Environment\nhttps://github.com/pgsty/pigsty/commit/1cf2ea5ee91db071de00ec805032928ff582453b\nv0.0.5 2020-08-19 : Offline Installation Mode\nhttps://github.com/pgsty/pigsty/commit/0fe9e829b298fe5e56307de3f78c95071de28245\nv0.0.4 2020-07-27 : Refactor playbooks into ansible roles\nhttps://github.com/pgsty/pigsty/commit/90b44259818d2c71e37df5250fe8ed1078a883d0\nv0.0.3 2020-06-22 : Interface enhancement\nhttps://github.com/pgsty/pigsty/commit/4c5c68ccd57bc32a9e9c98aa3f264aa19f45c7ee\nv0.0.2 2020-04-30 : First Commit\nhttps://github.com/pgsty/pigsty/commit/dd646775624ddb33aef7884f4f030682bdc371f8\nv0.0.1 2019-05-15 : POC\nhttps://github.com/Vonng/pg/commit/fa2ade31f8e81093eeba9d966c20120054f0646b\n","categories":["Reference"],"description":"Pigsty historical version release notes","excerpt":"Pigsty historical version release notes","ref":"/docs/about/release/","tags":"","title":"Release Note"},{"body":"A node is an abstraction of hardware resources and operating systems. It can be a physical machine, bare metal, virtual machine, or container/pod.\nAny machine running a Linux OS (with systemd daemon) and standard CPU/memory/disk/network resources can be treated as a node.\nNodes can have modules installed. Pigsty has several node types, distinguished by which modules are deployed:\nType Description Regular Node A node managed by Pigsty ADMIN Node The node that runs Ansible to issue management commands INFRA Node Nodes with the INFRA module installed ETCD Node Nodes with the ETCD module for DCS MINIO Node Nodes with the MINIO module for object storage PGSQL Node Nodes with the PGSQL module installed … Nodes with other modules… In a singleton Pigsty deployment, multiple roles converge on one node: it serves as the regular node, admin node, infra node, ETCD node, and database node simultaneously.\nRegular Node Nodes managed by Pigsty can have modules installed. The node.yml playbook configures nodes to the desired state. A regular node may run the following services:\nComponent Port Description Status node_exporter 9100 Host metrics exporter Enabled haproxy 9101 HAProxy load balancer (admin port) Enabled vector 9598 Log collection agent Enabled docker 9323 Container runtime support Optional keepalived n/a L2 VIP for node cluster Optional keepalived_exporter 9650 Keepalived status monitor Optional Here, node_exporter exposes host metrics, vector sends logs to the collection system, and haproxy provides load balancing. These three are enabled by default. Docker, keepalived, and keepalived_exporter are optional and can be enabled as needed.\nADMIN Node A Pigsty deployment has exactly one admin node—the node that runs Ansible playbooks and issues control/deployment commands.\nThis node has ssh/sudo access to all other nodes. Admin node security is critical; ensure access is strictly controlled.\nDuring single-node installation and configuration, the current node becomes the admin node. However, alternatives exist. For example, if your laptop can SSH to all managed nodes and has Ansible installed, it can serve as the admin node—though this isn’t recommended for production.\nFor instance, you might use your laptop to manage a Pigsty VM in the cloud. In this case, your laptop is the admin node.\nIn serious production environments, the admin node is typically 1-2 dedicated DBA machines. In resource-constrained setups, INFRA nodes often double as admin nodes since all INFRA nodes have Ansible installed by default.\nINFRA Node A Pigsty deployment may have 1 or more INFRA nodes; large production environments typically have 2-3.\nThe infra group in the inventory defines which nodes are INFRA nodes. These nodes run the INFRA module with these components:\nComponent Port Description nginx 80/443 Web UI, local software repository grafana 3000 Visualization platform victoriaMetrics 8428 Time-series database (metrics) victoriaLogs 9428 Log collection server victoriaTraces 10428 Trace collection server vmalert 8880 Alerting and derived metrics alertmanager 9059 Alert aggregation and routing blackbox_exporter 9115 Blackbox probing (ping nodes/VIPs) dnsmasq 53 Internal DNS resolution chronyd 123 NTP time server ansible - Playbook execution Nginx serves as the module’s entry point, providing the web UI and local software repository. With multiple INFRA nodes, services on each are independent, but you can access all monitoring data sources from any INFRA node’s Grafana.\nNote: The INFRA module is licensed under AGPLv3 due to Grafana. As an exception, if you only use Nginx/Victoria components without Grafana, you’re effectively under Apache-2.0.\nETCD Node The ETCD module provides Distributed Consensus Service (DCS) for PostgreSQL high availability.\nThe etcd group in the inventory defines ETCD nodes. These nodes run etcd servers on two ports:\nComponent Port Description etcd 2379 ETCD key-value store (client port) etcd 2380 ETCD cluster peer communication MINIO Node The MINIO module provides optional backup storage for PostgreSQL.\nThe minio group in the inventory defines MinIO nodes. These nodes run MinIO servers on:\nComponent Port Description minio 9000 MinIO S3 API endpoint minio 9001 MinIO admin console PGSQL Node Nodes with the PGSQL module are called PGSQL nodes. Node and PostgreSQL instance have a 1:1 deployment—one PG instance per node.\nPGSQL nodes can borrow identity from their PostgreSQL instance—controlled by node_id_from_pg, defaulting to true, meaning the node name is set to the PG instance name.\nPGSQL nodes run these additional components beyond regular node services:\nComponent Port Description Status postgres 5432 PostgreSQL database server Enabled pgbouncer 6432 PgBouncer connection pool Enabled patroni 8008 Patroni HA management Enabled pg_exporter 9630 PostgreSQL metrics exporter Enabled pgbouncer_exporter 9631 PgBouncer metrics exporter Enabled pgbackrest_exporter 9854 pgBackRest metrics exporter Enabled vip-manager n/a Binds L2 VIP to cluster primary Optional {{ pg_cluster }}-primary 5433 HAProxy service: pooled read/write Enabled {{ pg_cluster }}-replica 5434 HAProxy service: pooled read-only Enabled {{ pg_cluster }}-default 5436 HAProxy service: primary direct connection Enabled {{ pg_cluster }}-offline 5438 HAProxy service: offline read Enabled {{ pg_cluster }}-\u003cservice\u003e 543x HAProxy service: custom PostgreSQL services Custom The vip-manager is only enabled when users configure a PG VIP. Additional custom services can be defined in pg_services, exposed via haproxy using additional service ports.\nNode Relationships Regular nodes typically reference an INFRA node via the admin_ip parameter as their infrastructure provider. For example, with global admin_ip = 10.10.10.10, all nodes use infrastructure services at this IP.\nParameters that reference ${admin_ip}:\nParameter Module Default Value Description repo_endpoint INFRA http://${admin_ip}:80 Software repo URL repo_upstream.baseurl INFRA http://${admin_ip}/pigsty Local repo baseurl infra_portal.endpoint INFRA ${admin_ip}:\u003cport\u003e Nginx proxy backend dns_records INFRA [\"${admin_ip} i.pigsty\", ...] DNS records node_default_etc_hosts NODE [\"${admin_ip} i.pigsty\"] Default static DNS node_etc_hosts NODE - Custom static DNS node_dns_servers NODE [\"${admin_ip}\"] Dynamic DNS servers node_ntp_servers NODE - NTP servers (optional) Typically the admin node and INFRA node coincide. With multiple INFRA nodes, the admin node is usually the first one; others serve as backups.\nIn large-scale production deployments, you might separate the Ansible admin node from INFRA module nodes. For example, use 1-2 small dedicated hosts under the DBA team as the control hub (ADMIN nodes), and 2-3 high-spec physical machines as monitoring infrastructure (INFRA nodes).\nTypical node counts by deployment scale:\nScale ADMIN INFRA ETCD MINIO PGSQL Single-node 1 1 1 0 1 3-node 1 3 3 0 3 Small prod 1 2 3 0 N Large prod 2 3 5 4+ N ","categories":["Concept"],"description":"A node is an abstraction of hardware/OS resources—physical machines, bare metal, VMs, or containers/pods.","excerpt":"A node is an abstraction of hardware/OS resources—physical machines, …","ref":"/docs/concept/arch/node/","tags":"","title":"Nodes"},{"body":" Comparison with RDS Pigsty is a local-first RDS alternative released under AGPLv3, deployable on your own physical/virtual machines or cloud servers.\nWe’ve chosen Amazon AWS RDS for PostgreSQL (the global market leader) and Alibaba Cloud RDS for PostgreSQL (China’s market leader) as benchmarks for comparison.\nBoth Aliyun RDS and AWS RDS are closed-source cloud database services, available only through rental models on public clouds. The following comparison is based on the latest PostgreSQL 16 as of February 2024.\nFeature Comparison Feature Pigsty Aliyun RDS AWS RDS Major Version Support 13 - 18 13 - 18 13 - 18 Read Replicas Supports unlimited read replicas Standby instances not exposed to users Standby instances not exposed to users Read/Write Splitting Port-based traffic separation Separate paid component Separate paid component Fast/Slow Separation Supports offline ETL instances Not available Not available Cross-Region DR Supports standby clusters Multi-AZ deployment supported Multi-AZ deployment supported Delayed Replicas Supports delayed instances Not available Not available Load Balancing HAProxy / LVS Separate paid component Separate paid component Connection Pool Pgbouncer Separate paid component: RDS Separate paid component: RDS Proxy High Availability Patroni / etcd Requires HA edition Requires HA edition Point-in-Time Recovery pgBackRest / MinIO Backup supported Backup supported Metrics Monitoring Prometheus / Exporter Free basic / Paid advanced Free basic / Paid advanced Log Collection Loki / Promtail Basic support Basic support Visualization Grafana / Echarts Basic monitoring Basic monitoring Alert Aggregation AlertManager Basic support Basic support Key Extensions Here are some important extensions compared based on PostgreSQL 16, as of 2024-02-28\nPigsty Extension List AWS RDS Extension List: Aliyun RDS Extension List Extension Pigsty RDS / PGDG Official Repo Aliyun RDS AWS RDS Install Extensions Free to install Not allowed Not allowed Geospatial PostGIS 3.4.2 PostGIS 3.3.4 / Ganos 6.1 PostGIS 3.4.1 Point Cloud PG PointCloud 1.2.5 Ganos PointCloud 6.1 Vector Embedding PGVector 0.6.1 / Svector 0.5.6 pase 0.0.1 PGVector 0.6 Machine Learning PostgresML 2.8.1 Time Series TimescaleDB 2.14.2 Horizontal Scaling Citus 12.1 Columnar Storage Hydra 1.1.1 Full Text Search pg_bm25 0.5.6\nGraph Database Apache AGE 1.5.0 GraphQL PG GraphQL 1.5.0 OLAP pg_analytics 0.5.6 Message Queue pgq 3.5.0 DuckDB duckdb_fdw 1.1 Fuzzy Tokenization zhparser 1.1 / pg_bigm 1.2 zhparser 1.0 / pg_jieba pg_bigm 1.2 CDC Extraction wal2json 2.5.3 wal2json 2.5 Bloat Management pg_repack 1.5.0 pg_repack 1.4.8 pg_repack 1.5.0 AWS RDS PG Available Extensions AWS RDS for PostgreSQL 16 available extensions (excluding PG built-in extensions)\nname pg16 pg15 pg14 pg13 pg12 pg11 pg10 amcheck 1.3 1.3 1.3 1.2 1.2 yes 1 auto_explain yes yes yes yes yes yes yes autoinc 1 1 1 1 null null null bloom 1 1 1 1 1 1 1 bool_plperl 1 1 1 1 null null null btree_gin 1.3 1.3 1.3 1.3 1.3 1.3 1.2 btree_gist 1.7 1.7 1.6 1.5 1.5 1.5 1.5 citext 1.6 1.6 1.6 1.6 1.6 1.5 1.4 cube 1.5 1.5 1.5 1.4 1.4 1.4 1.2 dblink 1.2 1.2 1.2 1.2 1.2 1.2 1.2 dict_int 1 1 1 1 1 1 1 dict_xsyn 1 1 1 1 1 1 1 earthdistance 1.1 1.1 1.1 1.1 1.1 1.1 1.1 fuzzystrmatch 1.2 1.1 1.1 1.1 1.1 1.1 1.1 hstore 1.8 1.8 1.8 1.7 1.6 1.5 1.4 hstore_plperl 1 1 1 1 1 1 1 insert_username 1 1 1 1 null null null intagg 1.1 1.1 1.1 1.1 1.1 1.1 1.1 intarray 1.5 1.5 1.5 1.3 1.2 1.2 1.2 isn 1.2 1.2 1.2 1.2 1.2 1.2 1.1 jsonb_plperl 1 1 1 1 1 null null lo 1.1 1.1 1.1 1.1 1.1 1.1 1.1 ltree 1.2 1.2 1.2 1.2 1.1 1.1 1.1 moddatetime 1 1 1 1 null null null old_snapshot 1 1 1 null null null null pageinspect 1.12 1.11 1.9 1.8 1.7 1.7 1.6 pg_buffercache 1.4 1.3 1.3 1.3 1.3 1.3 1.3 pg_freespacemap 1.2 1.2 1.2 1.2 1.2 1.2 1.2 pg_prewarm 1.2 1.2 1.2 1.2 1.2 1.2 1.1 pg_stat_statements 1.1 1.1 1.9 1.8 1.7 1.6 1.6 pg_trgm 1.6 1.6 1.6 1.5 1.4 1.4 1.3 pg_visibility 1.2 1.2 1.2 1.2 1.2 1.2 1.2 pg_walinspect 1.1 1 null null null null null pgcrypto 1.3 1.3 1.3 1.3 1.3 1.3 1.3 pgrowlocks 1.2 1.2 1.2 1.2 1.2 1.2 1.2 pgstattuple 1.5 1.5 1.5 1.5 1.5 1.5 1.5 plperl 1 1 1 1 1 1 1 plpgsql 1 1 1 1 1 1 1 pltcl 1 1 1 1 1 1 1 postgres_fdw 1.1 1.1 1.1 1 1 1 1 refint 1 1 1 1 null null null seg 1.4 1.4 1.4 1.3 1.3 1.3 1.1 sslinfo 1.2 1.2 1.2 1.2 1.2 1.2 1.2 tablefunc 1 1 1 1 1 1 1 tcn 1 1 1 1 1 1 1 tsm_system_rows 1 1 1 1 1 1 1.1 tsm_system_time 1 1 1 1 1 1 1.1 unaccent 1.1 1.1 1.1 1.1 1.1 1.1 1.1 uuid-ossp 1.1 1.1 1.1 1.1 1.1 1.1 1.1 Aliyun RDS PG Available Extensions Aliyun RDS for PostgreSQL 16 available extensions (excluding PG built-in extensions)\nname pg16 pg15 pg14 pg13 pg12 pg11 pg10 description bloom 1 1 1 1 1 1 1 Provides a bloom filter-based index access method. btree_gin 1.3 1.3 1.3 1.3 1.3 1.3 1.2 Provides GIN operator class examples that implement B-tree equivalent behavior for multiple data types and all enum types. btree_gist 1.7 1.7 1.6 1.5 1.5 1.5 1.5 Provides GiST operator class examples that implement B-tree equivalent behavior for multiple data types and all enum types. citext 1.6 1.6 1.6 1.6 1.6 1.5 1.4 Provides a case-insensitive string type. cube 1.5 1.5 1.5 1.4 1.4 1.4 1.2 Provides a data type for representing multi-dimensional cubes. dblink 1.2 1.2 1.2 1.2 1.2 1.2 1.2 Cross-database table operations. dict_int 1 1 1 1 1 1 1 Additional full-text search dictionary template example. earthdistance 1.1 1.1 1.1 1.1 1.1 1.1 1.1 Provides two different methods to calculate great circle distances on the Earth’s surface. fuzzystrmatch 1.2 1.1 1.1 1.1 1.1 1.1 1.1 Determines similarities and distances between strings. hstore 1.8 1.8 1.8 1.7 1.6 1.5 1.4 Stores key-value pairs in a single PostgreSQL value. intagg 1.1 1.1 1.1 1.1 1.1 1.1 1.1 Provides an integer aggregator and an enumerator. intarray 1.5 1.5 1.5 1.3 1.2 1.2 1.2 Provides some useful functions and operators for manipulating null-free integer arrays. isn 1.2 1.2 1.2 1.2 1.2 1.2 1.1 Validates input according to a hard-coded prefix list, also used for concatenating numbers during output. ltree 1.2 1.2 1.2 1.2 1.1 1.1 1.1 For representing labels of data stored in a hierarchical tree structure. pg_buffercache 1.4 1.3 1.3 1.3 1.3 1.3 1.3 Provides a way to examine the shared buffer cache in real time. pg_freespacemap 1.2 1.2 1.2 1.2 1.2 1.2 1.2 Examines the free space map (FSM). pg_prewarm 1.2 1.2 1.2 1.2 1.2 1.2 1.1 Provides a convenient way to load data into the OS buffer or PostgreSQL buffer. pg_stat_statements 1.1 1.1 1.9 1.8 1.7 1.6 1.6 Provides a means of tracking execution statistics of all SQL statements executed by a server. pg_trgm 1.6 1.6 1.6 1.5 1.4 1.4 1.3 Provides functions and operators for alphanumeric text similarity, and index operator classes that support fast searching of similar strings. pgcrypto 1.3 1.3 1.3 1.3 1.3 1.3 1.3 Provides cryptographic functions for PostgreSQL. pgrowlocks 1.2 1.2 1.2 1.2 1.2 1.2 1.2 Provides a function to show row locking information for a specified table. pgstattuple 1.5 1.5 1.5 1.5 1.5 1.5 1.5 Provides multiple functions to obtain tuple-level statistics. plperl 1 1 1 1 1 1 1 Provides Perl procedural language. plpgsql 1 1 1 1 1 1 1 Provides SQL procedural language. pltcl 1 1 1 1 1 1 1 Provides Tcl procedural language. postgres_fdw 1.1 1.1 1.1 1 1 1 1 Cross-database table operations. sslinfo 1.2 1.2 1.2 1.2 1.2 1.2 1.2 Provides information about the SSL certificate provided by the current client. tablefunc 1 1 1 1 1 1 1 Contains multiple table-returning functions. tsm_system_rows 1 1 1 1 1 1 1 Provides the table sampling method SYSTEM_ROWS. tsm_system_time 1 1 1 1 1 1 1 Provides the table sampling method SYSTEM_TIME. unaccent 1.1 1.1 1.1 1.1 1.1 1.1 1.1 A text search dictionary that can remove accents (diacritics) from lexemes. uuid-ossp 1.1 1.1 1.1 1.1 1.1 1.1 1.1 Provides functions to generate universally unique identifiers (UUIDs) using several standard algorithms. xml2 1.1 1.1 1.1 1.1 1.1 1.1 1.1 Provides XPath queries and XSLT functionality. Performance Comparison Metric Pigsty Aliyun RDS AWS RDS Peak Performance PGTPC on NVME SSD Benchmark sysbench oltp_rw RDS PG Performance Whitepaper sysbench oltp scenario QPS 4000 ~ 8000 per core Storage Spec: Max Capacity 32TB / NVME SSD 32 TB / ESSD PL3 64 TB / io2 EBS Block Express Storage Spec: Max IOPS 4K Random Read: Max 3M, Random Write 2000~350K 4K Random Read: Max 1M 16K Random IOPS: 256K Storage Spec: Max Latency 4K Random Read: 75µs, Random Write: 15µs 4K Random Read: 200µs 500µs / Inferred as 16K random IO Storage Spec: Max Reliability UBER \u003c 1e-18, equivalent to 18 nines MTBF: 2M hours 5DWPD, 3 years continuous Reliability 9 nines, equivalent to UBER 1e-9 Storage and Data Reliability Durability: 99.999%, 5 nines (0.001% annual failure rate) io2 specification Storage Spec: Max Cost ¥31.5/TB·month (5-year warranty amortized / 3.2T / Enterprise-grade / MLC) ¥3200/TB·month (original ¥6400, monthly ¥4000) 50% off with 3-year prepaid ¥1900/TB·month using max spec 65536GB / 256K IOPS best discount Observability Pigsty provides nearly 3000 monitoring metrics and 50+ monitoring dashboards, covering database monitoring, host monitoring, connection pool monitoring, load balancer monitoring, and more, providing users with an unparalleled observability experience.\nPigsty provides 638 PostgreSQL-related monitoring metrics, while AWS RDS only has 99, and Aliyun RDS has only single-digit metrics:\nAdditionally, some projects provide PostgreSQL monitoring capabilities, but are relatively simple:\npgwatch: 123 metric types pgmonitor: 156 metric types datadog: 69 metric types pgDash ClusterControl pganalyze Aliyun RDS: 8 metric types AWS RDS: 99 metric types Azure RDS Maintainability Metric Pigsty Aliyun RDS AWS RDS System Usability Simple Simple Simple Configuration Management Config files / CMDB based on Ansible Inventory Can use Terraform Can use Terraform Change Method Idempotent Playbooks based on Ansible Playbook Console click operations Console click operations Parameter Tuning Auto-adapts to node specs, Four preset templates: OLTP, OLAP, TINY, CRIT Infra as Code Natively supported Can use Terraform Can use Terraform Customizable Parameters Pigsty Parameters 283 parameters Service \u0026 Support Commercial subscription support available After-sales ticket support After-sales ticket support Air-gapped Deployment Offline installation supported N/A N/A Database Migration Playbooks for zero-downtime migration from existing v10+ PG instances to Pigsty managed instances via logical replication Cloud migration assistance Aliyun RDS Data Sync Cost Based on experience, RDS unit cost is 5-15 times that of self-hosted for software and hardware resources, with a rent-to-own ratio typically around one month. For details, see Cost Analysis.\nFactor Metric Pigsty Aliyun RDS AWS RDS Cost Software License/Service Fee Free, hardware ~¥20-40/core·month ¥200-400/core·month ¥400-1300/core·month Support Service Fee Service ~¥100/core·month Included in RDS cost Other On-Premises Database Management Software Some software and vendors providing PostgreSQL management capabilities:\nAiven: Closed-source commercial cloud-hosted solution Percona: Commercial consulting, simple PG distribution ClusterControl: Commercial database management software Other Kubernetes Operators Pigsty refuses to use Kubernetes for managing databases in production, so there are ecological differences with these solutions.\nPGO StackGres CloudNativePG TemboOperator PostgresOperator PerconaOperator Kubegres KubeDB KubeBlocks For more information, see:\nIs Putting Databases in K8S a Good Idea? Is Putting Databases in Containers a Good Idea? ","categories":["Reference"],"description":"This article compares Pigsty with similar products and projects, highlighting feature differences.","excerpt":"This article compares Pigsty with similar products and projects, …","ref":"/docs/about/compare/","tags":["Cost"],"title":"Comparison"},{"body":"The largest entity concept in Pigsty is a Deployment. The main entities and relationships (E-R diagram) in a deployment are shown below:\nA deployment can also be understood as an Environment. For example, Production (Prod), User Acceptance Testing (UAT), Staging, Testing, Development (Devbox), etc. Each environment corresponds to a Pigsty inventory that describes all entities and attributes in that environment.\nTypically, an environment includes shared infrastructure (INFRA), which broadly includes ETCD (HA DCS) and MINIO (centralized backup repository), serving multiple PostgreSQL database clusters (and other database module components). (Exception: there are also deployments without infrastructure)\nIn Pigsty, almost all database modules are organized as “Clusters”. Each cluster is an Ansible group containing several node resources. For example, PostgreSQL HA database clusters, Redis, Etcd/MinIO all exist as clusters. An environment can contain multiple clusters.\nPostgreSQL Cluster ETCD Cluster MinIO Cluster Redis Cluster INFRA Nodes ","categories":["Concept"],"description":"How Pigsty abstracts different functionality into modules, and the E-R diagrams for these modules.","excerpt":"How Pigsty abstracts different functionality into modules, and the E-R …","ref":"/docs/concept/model/","tags":"","title":"ER Model"},{"body":" Overview EC2 Core·Month RDS Core·Month DHH Self-Hosted Core-Month Price (192C 384G) 25.32 Junior Open Source DB DBA Reference Salary ¥15K/person·month IDC Self-Hosted (Dedicated Physical: 64C384G) 19.53 Mid-Level Open Source DB DBA Reference Salary ¥30K/person·month IDC Self-Hosted (Container, 500% Oversold) 7 Senior Open Source DB DBA Reference Salary ¥60K/person·month UCloud Elastic VM (8C16G, Oversold) 25 ORACLE Database License 10000 Aliyun ECS 2x Memory (Dedicated, No Oversold) 107 Aliyun RDS PG 2x Memory (Dedicated) 260 Aliyun ECS 4x Memory (Dedicated, No Oversold) 138 Aliyun RDS PG 4x Memory (Dedicated) 320 Aliyun ECS 8x Memory (Dedicated, No Oversold) 180 Aliyun RDS PG 8x Memory (Dedicated) 410 AWS C5D.METAL 96C 200G (Monthly No Prepaid) 100 AWS RDS PostgreSQL db.T2 (2x) 440 AWS C5D.METAL 96C 200G (3-Year Prepaid) 80 AWS RDS PostgreSQL db.M5 (4x) 611 AWS C7A.METAL 192C 384G (3-Year Prepaid) 104.8 AWS RDS PostgreSQL db.R6G (8x) 786 RDS Cost Reference Payment Model Price Annualized (¥10K) IDC Self-Hosted (Single Physical Machine) ¥75K / 5 years 1.5 IDC Self-Hosted (2-3 Machines for HA) ¥150K / 5 years 3.0 ~ 4.5 Aliyun RDS On-Demand ¥87.36/hour 76.5 Aliyun RDS Monthly (Baseline) ¥42K / month 50 Aliyun RDS Annual (85% off) ¥425,095 / year 42.5 Aliyun RDS 3-Year Prepaid (50% off) ¥750,168 / 3 years 25 AWS On-Demand $25,817 / month 217 AWS 1-Year No Prepaid $22,827 / month 191.7 AWS 3-Year Full Prepaid $120K + $17.5K/month 175 AWS China/Ningxia On-Demand ¥197,489 / month 237 AWS China/Ningxia 1-Year No Prepaid ¥143,176 / month 171 AWS China/Ningxia 3-Year Full Prepaid ¥647K + ¥116K/month 160.6 Here’s a comparison of self-hosted vs cloud database costs:\nMethod Annualized (¥10K) IDC Hosted Server 64C / 384G / 3.2TB NVME SSD 660K IOPS (2-3 Machines) 3.0 ~ 4.5 Aliyun RDS PG HA Edition pg.x4m.8xlarge.2c, 64C / 256GB / 3.2TB ESSD PL3 25 ~ 50 AWS RDS PG HA Edition db.m5.16xlarge, 64C / 256GB / 3.2TB io1 x 80k IOPS 160 ~ 217 ECS Cost Reference Pure Compute Price Comparison (Excluding NVMe SSD / ESSD PL3) Using Aliyun as an example, the monthly pure compute price is 5-7x the self-hosted baseline, while 5-year prepaid is 2x self-hosted\nPayment Model Unit Price (¥/Core·Month) Relative to Standard Self-Hosted Premium Multiple On-Demand (1.5x) ¥ 202 160 % 9.2 ~ 11.2 Monthly (Standard) ¥ 126 100 % 5.7 ~ 7.0 1-Year Prepaid (65% off) ¥ 83.7 66 % 3.8 ~ 4.7 2-Year Prepaid (55% off) ¥ 70.6 56 % 3.2 ~ 3.9 3-Year Prepaid (44% off) ¥ 55.1 44 % 2.5 ~ 3.1 4-Year Prepaid (35% off) ¥ 45 35 % 2.0 ~ 2.5 5-Year Prepaid (30% off) ¥ 38.5 30 % 1.8 ~ 2.1 DHH @ 2023 ¥ 22.0 Tantan IDC Self-Hosted ¥ 18.0 Equivalent Price Comparison Including NVMe SSD / ESSD PL3 Including common NVMe SSD specs, the monthly pure compute price is 11-14x the self-hosted baseline, while 5-year prepaid is about 9x.\nPayment Model Unit Price (¥/Core·Month) + 40GB ESSD PL3 Self-Hosted Premium Multiple On-Demand (1.5x) ¥ 202 ¥ 362 14.3 ~ 18.6 Monthly (Standard) ¥ 126 ¥ 286 11.3 ~ 14.7 1-Year Prepaid (65% off) ¥ 83.7 ¥ 244 9.6 ~ 12.5 2-Year Prepaid (55% off) ¥ 70.6 ¥ 230 9.1 ~ 11.8 3-Year Prepaid (44% off) ¥ 55.1 ¥ 215 8.5 ~ 11.0 4-Year Prepaid (35% off) ¥ 45 ¥ 205 8.1 ~ 10.5 5-Year Prepaid (30% off) ¥ 38.5 ¥ 199 7.9 ~ 10.2 DHH @ 2023 ¥ 25.3 Tantan IDC Self-Hosted ¥ 19.5 DHH Case: 192 cores with 12.8TB Gen4 SSD (1c:66); Tantan Case: 64 cores with 3.2T Gen3 MLC SSD (1c:50).\nCloud prices calculated at 40GB ESSD PL3 per core (1 core:4x RAM:40x disk).\nEBS Cost Reference Evaluation Factor Local PCI-E NVME SSD Aliyun ESSD PL3 AWS io2 Block Express Capacity 32TB 32 TB 64 TB IOPS 4K Random Read: 600K ~ 1.1M, 4K Random Write: 200K ~ 350K 4K Random Read: Max 1M 16K Random IOPS: 256K Latency 4K Random Read: 75µs, 4K Random Write: 15µs 4K Random Read: 200µs Random IO: ~500µs (contextually inferred as 16K) Reliability UBER \u003c 1e-18, equivalent to 18 nines, MTBF: 2M hours, 5DWPD for 3 years Data Reliability 9 nines Storage and Data Reliability Durability: 99.999%, 5 nines (0.001% annual failure rate) io2 Specification Cost ¥16/TB·month (5-year amortized / 3.2T MLC), 5-year warranty, ¥3000 retail ¥3200/TB·month (original ¥6400, monthly ¥4000), 50% off with 3-year full prepaid ¥1900/TB·month using max spec 65536GB 256K IOPS best discount SLA 5-year warranty, replacement on failure Aliyun RDS SLA Availability 99.99%: 15% monthly fee, 99%: 30% monthly fee, 95%: 100% monthly fee Amazon RDS SLA Availability 99.95%: 15% monthly fee, 99%: 25% monthly fee, 95%: 100% monthly fee S3 Cost Reference Date $/GB·Month ¥/TB·5Years HDD ¥/TB SSD ¥/TB 2006.03 0.150 63000 2800 2010.11 0.140 58800 1680 2012.12 0.095 39900 420 15400 2014.04 0.030 12600 371 9051 2016.12 0.023 9660 245 3766 2023.12 0.023 9660 105 280 Other References High-Perf Storage Top-Tier Discounted vs Purchased NVMe SSD Price Ref S3 Express 0.160 67200 DHH 12T 1400 EBS io2 0.125 + IOPS 114000 Shannon 3.2T 900 Cloud Exit Collection There was a time when “moving to the cloud” was almost politically correct in tech circles, and an entire generation of app developers had their vision obscured by the cloud. Let’s use real data analysis and firsthand experience to explain the value and pitfalls of the public cloud rental model — for your reference in this era of cost reduction and efficiency improvement — please see “Cloud Computing Mudslide: Collection”\nCloud Infrastructure Basics\nReclaiming the Dividends of Computer Hardware\nExposing Object Storage: From Cost Reduction to Price Gouging\nIs Cloud Disk a Scam?\nIs Cloud Database an IQ Tax?\nGarbage Tencent Cloud CDN: From Getting Started to Giving Up\nCloud Business Model\nThe End of FinOps is Leaving the Cloud\nWhy Is Cloud Computing Still Not as Profitable as Mining Sand?\nIs Cloud SLA Just a Placebo?\nHave the Price-Gouging Platforms Really Cut Prices?\nParadigm Shift: From Cloud to Local-First\nCloud Exit Odyssey\nThe Secret to High Availability Off-Cloud: Reject Intellectual Masturbation\nSave Millions in Half a Year: DHH Cloud Exit FAQ\nIs It Time to Abandon Cloud Computing?\nCloud Exit Odyssey\nCloud Failure Post-Mortems\nFrom Cost Reduction Comedy to Real Cost Reduction\nWhat We Can Learn from Alibaba Cloud’s Epic Failure\nAlibaba Cloud Weekly Explosion: Cloud Database Control Plane Down Again\nAlibaba Cloud Computing Epic Failure Incoming\nRDS Failures\nA Better Open Source RDS Alternative: Pigsty\nRebuttal: Why You Shouldn’t Hire DBAs\nCloud RDS: From DROP DATABASE to Run Away\nShould Databases Go in K8S?\nIs Putting Databases in Docker a Good Idea?\nCloud Vendor Profiles\nInternet Tech Master Crash Course [Repost]\nHow State-Owned Enterprises Inside the Door View Cloud Vendors Outside [Repost]\nAlibaba Cloud Stuck at State Enterprise Customers’ Door [Repost]\nThe Amateur Troupes Behind Internet Failures [Repost]\nCloud Vendors View of Customers: Poor, Idle, and Starved for Love [Repost]\n","categories":["Reference"],"description":"This article provides cost data to help you evaluate self-hosted Pigsty, cloud RDS costs, and typical DBA salaries.","excerpt":"This article provides cost data to help you evaluate self-hosted …","ref":"/docs/about/compare/cost/","tags":["Cost"],"title":"Cost Reference"},{"body":"Pigsty follows the IaC and GitOPS philosophy: use a declarative config inventory to describe the entire environment, and materialize it through idempotent playbooks.\nUsers describe their desired state declaratively through parameters, and playbooks idempotently adjust target nodes to reach that state. This is similar to Kubernetes CRDs \u0026 Operators, but Pigsty implements this functionality on bare metal and virtual machines through Ansible.\nPigsty was born to solve the operational management problem of ultra-large-scale PostgreSQL clusters. The idea behind it is simple — we need the ability to replicate the entire infrastructure (100+ database clusters + PG/Redis + observability) on ready servers within ten minutes. No GUI + ClickOps can complete such a complex task in such a short time, making CLI + IaC the only choice — it provides precise, efficient control.\nThe config inventory pigsty.yml file describes the state of the entire deployment. Whether it’s production (prod), staging, test, or development (devbox) environments, the difference between infrastructures lies only in the config inventory, while the deployment delivery logic is exactly the same.\nYou can use git for version control and auditing of this deployment “seed/gene”, and Pigsty even supports storing the config inventory as database tables in PostgreSQL CMDB, further achieving Infra as Data capability. Seamlessly integrate with your existing workflows.\nIaC is designed for professional users and enterprise scenarios but is also deeply optimized for individual developers and SMBs. Even if you’re not a professional DBA, you don’t need to understand these hundreds of adjustment knobs and switches. All parameters come with well-performing default values. You can get an out-of-the-box single-node database with zero configuration; Simply add two more IP addresses to get an enterprise-grade high-availability PostgreSQL cluster.\nDeclare Modules Take the following default config snippet as an example. This config describes a node 10.10.10.10 with INFRA, NODE, ETCD, and PGSQL modules installed.\n# monitoring, alerting, DNS, NTP and other infrastructure cluster... infra: { hosts: { 10.10.10.10: { infra_seq: 1 } } } # minio cluster, s3 compatible object storage minio: { hosts: { 10.10.10.10: { minio_seq: 1 } }, vars: { minio_cluster: minio } } # etcd cluster, used as DCS for PostgreSQL high availability etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 } }, vars: { etcd_cluster: etcd } } # PGSQL example cluster: pg-meta pg-meta: { hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary }, vars: { pg_cluster: pg-meta } } To actually install these modules, execute the following playbooks:\n./infra.yml -l 10.10.10.10 # Initialize infra module on node 10.10.10.10 ./etcd.yml -l 10.10.10.10 # Initialize etcd module on node 10.10.10.10 ./minio.yml -l 10.10.10.10 # Initialize minio module on node 10.10.10.10 ./pgsql.yml -l 10.10.10.10 # Initialize pgsql module on node 10.10.10.10 Declare Clusters You can declare PostgreSQL database clusters by installing the PGSQL module on multiple nodes, making them a service unit:\nFor example, to deploy a three-node high-availability PostgreSQL cluster using streaming replication on the following three Pigsty-managed nodes, you can add the following definition to the all.children section of the config file pigsty.yml:\npg-test: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } 10.10.10.12: { pg_seq: 2, pg_role: replica } 10.10.10.13: { pg_seq: 3, pg_role: offline } vars: { pg_cluster: pg-test } After defining, you can use playbooks to create the cluster:\nbin/pgsql-add pg-test # Create the pg-test cluster You can use different instance roles such as primary, replica, offline, delayed, sync standby; as well as different clusters: such as standby clusters, Citus clusters, and even Redis / MinIO / Etcd clusters\nCustomize Cluster Content Not only can you define clusters declaratively, but you can also define databases, users, services, and HBA rules within the cluster. For example, the following config file deeply customizes the content of the default pg-meta single-node database cluster:\nIncluding: declaring six business databases and seven business users, adding an extra standby service (synchronous standby, providing read capability with no replication delay), defining some additional pg_hba rules, an L2 VIP address pointing to the cluster primary, and a customized backup strategy.\npg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary , pg_offline_query: true } } vars: pg_cluster: pg-meta pg_databases: # define business databases on this cluster, array of database definition - name: meta # REQUIRED, `name` is the only mandatory field of a database definition baseline: cmdb.sql # optional, database sql baseline path, (relative path among ansible search path, e.g files/) pgbouncer: true # optional, add this database to pgbouncer database list? true by default schemas: [pigsty] # optional, additional schemas to be created, array of schema names extensions: # optional, additional extensions to be installed: array of `{name[,schema]}` - { name: postgis , schema: public } - { name: timescaledb } comment: pigsty meta database # optional, comment string for this database owner: postgres # optional, database owner, postgres by default template: template1 # optional, which template to use, template1 by default encoding: UTF8 # optional, database encoding, UTF8 by default. (MUST same as template database) locale: C # optional, database locale, C by default. (MUST same as template database) lc_collate: C # optional, database collate, C by default. (MUST same as template database) lc_ctype: C # optional, database ctype, C by default. (MUST same as template database) tablespace: pg_default # optional, default tablespace, 'pg_default' by default. allowconn: true # optional, allow connection, true by default. false will disable connect at all revokeconn: false # optional, revoke public connection privilege. false by default. (leave connect with grant option to owner) register_datasource: true # optional, register this database to grafana datasources? true by default connlimit: -1 # optional, database connection limit, default -1 disable limit pool_auth_user: dbuser_meta # optional, all connection to this pgbouncer database will be authenticated by this user pool_mode: transaction # optional, pgbouncer pool mode at database level, default transaction pool_size: 64 # optional, pgbouncer pool size at database level, default 64 pool_size_reserve: 32 # optional, pgbouncer pool size reserve at database level, default 32 pool_size_min: 0 # optional, pgbouncer pool size min at database level, default 0 pool_max_db_conn: 100 # optional, max database connections at database level, default 100 - { name: grafana ,owner: dbuser_grafana ,revokeconn: true ,comment: grafana primary database } - { name: bytebase ,owner: dbuser_bytebase ,revokeconn: true ,comment: bytebase primary database } - { name: kong ,owner: dbuser_kong ,revokeconn: true ,comment: kong the api gateway database } - { name: gitea ,owner: dbuser_gitea ,revokeconn: true ,comment: gitea meta database } - { name: wiki ,owner: dbuser_wiki ,revokeconn: true ,comment: wiki meta database } pg_users: # define business users/roles on this cluster, array of user definition - name: dbuser_meta # REQUIRED, `name` is the only mandatory field of a user definition password: DBUser.Meta # optional, password, can be a scram-sha-256 hash string or plain text login: true # optional, can log in, true by default (new biz ROLE should be false) superuser: false # optional, is superuser? false by default createdb: false # optional, can create database? false by default createrole: false # optional, can create role? false by default inherit: true # optional, can this role use inherited privileges? true by default replication: false # optional, can this role do replication? false by default bypassrls: false # optional, can this role bypass row level security? false by default pgbouncer: true # optional, add this user to pgbouncer user-list? false by default (production user should be true explicitly) connlimit: -1 # optional, user connection limit, default -1 disable limit expire_in: 3650 # optional, now + n days when this role is expired (OVERWRITE expire_at) expire_at: '2030-12-31' # optional, YYYY-MM-DD 'timestamp' when this role is expired (OVERWRITTEN by expire_in) comment: pigsty admin user # optional, comment string for this user/role roles: [dbrole_admin] # optional, belonged roles. default roles are: dbrole_{admin,readonly,readwrite,offline} parameters: {} # optional, role level parameters with `ALTER ROLE SET` pool_mode: transaction # optional, pgbouncer pool mode at user level, transaction by default pool_connlimit: -1 # optional, max database connections at user level, default -1 disable limit - {name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [dbrole_readonly], comment: read-only viewer for meta database} - {name: dbuser_grafana ,password: DBUser.Grafana ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for grafana database } - {name: dbuser_bytebase ,password: DBUser.Bytebase ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for bytebase database } - {name: dbuser_kong ,password: DBUser.Kong ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for kong api gateway } - {name: dbuser_gitea ,password: DBUser.Gitea ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for gitea service } - {name: dbuser_wiki ,password: DBUser.Wiki ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for wiki.js service } pg_services: # extra services in addition to pg_default_services, array of service definition # standby service will route {ip|name}:5435 to sync replica's pgbouncer (5435-\u003e6432 standby) - name: standby # required, service name, the actual svc name will be prefixed with `pg_cluster`, e.g: pg-meta-standby port: 5435 # required, service exposed port (work as kubernetes service node port mode) ip: \"*\" # optional, service bind ip address, `*` for all ip by default selector: \"[]\" # required, service member selector, use JMESPath to filter inventory dest: default # optional, destination port, default|postgres|pgbouncer|\u003cport_number\u003e, 'default' by default check: /sync # optional, health check url path, / by default backup: \"[? pg_role == `primary`]\" # backup server selector maxconn: 3000 # optional, max allowed front-end connection balance: roundrobin # optional, haproxy load balance algorithm (roundrobin by default, other: leastconn) options: 'inter 3s fastinter 1s downinter 5s rise 3 fall 3 on-marked-down shutdown-sessions slowstart 30s maxconn 3000 maxqueue 128 weight 100' pg_hba_rules: - {user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes'} pg_vip_enabled: true pg_vip_address: 10.10.10.2/24 pg_vip_interface: eth1 node_crontab: # make a full backup 1 am everyday - '00 01 * * * postgres /pg/bin/pg-backup full' Declare Access Control You can also deeply customize Pigsty’s access control capabilities through declarative configuration. For example, the following config file provides deep security customization for the pg-meta cluster:\nUses the three-node core cluster template: crit.yml, to ensure data consistency is prioritized with zero data loss during failover. Enables L2 VIP and restricts database and connection pool listening addresses to local loopback IP + internal network IP + VIP three specific addresses. The template enforces Patroni’s SSL API and Pgbouncer’s SSL, and in HBA rules, enforces SSL usage for accessing the database cluster. Also enables the $libdir/passwordcheck extension in pg_libs to enforce password strength security policy.\nFinally, a separate pg-meta-delay cluster is declared as pg-meta’s delayed replica from one hour ago, for emergency data deletion recovery.\npg-meta: # 3 instance postgres cluster `pg-meta` hosts: 10.10.10.10: { pg_seq: 1, pg_role: primary } 10.10.10.11: { pg_seq: 2, pg_role: replica } 10.10.10.12: { pg_seq: 3, pg_role: replica , pg_offline_query: true } vars: pg_cluster: pg-meta pg_conf: crit.yml pg_users: - { name: dbuser_meta , password: DBUser.Meta , pgbouncer: true , roles: [ dbrole_admin ] , comment: pigsty admin user } - { name: dbuser_view , password: DBUser.Viewer , pgbouncer: true , roles: [ dbrole_readonly ] , comment: read-only viewer for meta database } pg_databases: - {name: meta ,baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [pigsty] ,extensions: [{name: postgis, schema: public}, {name: timescaledb}]} pg_default_service_dest: postgres pg_services: - { name: standby ,src_ip: \"*\" ,port: 5435 , dest: default ,selector: \"[]\" , backup: \"[? pg_role == `primary`]\" } pg_vip_enabled: true pg_vip_address: 10.10.10.2/24 pg_vip_interface: eth1 pg_listen: '${ip},${vip},${lo}' patroni_ssl_enabled: true pgbouncer_sslmode: require pgbackrest_method: minio pg_libs: 'timescaledb, $libdir/passwordcheck, pg_stat_statements, auto_explain' # add passwordcheck extension to enforce strong password pg_default_roles: # default roles and users in postgres cluster - { name: dbrole_readonly ,login: false ,comment: role for global read-only access } - { name: dbrole_offline ,login: false ,comment: role for restricted read-only access } - { name: dbrole_readwrite ,login: false ,roles: [dbrole_readonly] ,comment: role for global read-write access } - { name: dbrole_admin ,login: false ,roles: [pg_monitor, dbrole_readwrite] ,comment: role for object creation } - { name: postgres ,superuser: true ,expire_in: 7300 ,comment: system superuser } - { name: replicator ,replication: true ,expire_in: 7300 ,roles: [pg_monitor, dbrole_readonly] ,comment: system replicator } - { name: dbuser_dba ,superuser: true ,expire_in: 7300 ,roles: [dbrole_admin] ,pgbouncer: true ,pool_mode: session, pool_connlimit: 16 , comment: pgsql admin user } - { name: dbuser_monitor ,roles: [pg_monitor] ,expire_in: 7300 ,pgbouncer: true ,parameters: {log_min_duration_statement: 1000 } ,pool_mode: session ,pool_connlimit: 8 ,comment: pgsql monitor user } pg_default_hba_rules: # postgres host-based auth rules by default - {user: '${dbsu}' ,db: all ,addr: local ,auth: ident ,title: 'dbsu access via local os user ident' } - {user: '${dbsu}' ,db: replication ,addr: local ,auth: ident ,title: 'dbsu replication from local os ident' } - {user: '${repl}' ,db: replication ,addr: localhost ,auth: ssl ,title: 'replicator replication from localhost'} - {user: '${repl}' ,db: replication ,addr: intra ,auth: ssl ,title: 'replicator replication from intranet' } - {user: '${repl}' ,db: postgres ,addr: intra ,auth: ssl ,title: 'replicator postgres db from intranet' } - {user: '${monitor}' ,db: all ,addr: localhost ,auth: pwd ,title: 'monitor from localhost with password' } - {user: '${monitor}' ,db: all ,addr: infra ,auth: ssl ,title: 'monitor from infra host with password'} - {user: '${admin}' ,db: all ,addr: infra ,auth: ssl ,title: 'admin @ infra nodes with pwd \u0026 ssl' } - {user: '${admin}' ,db: all ,addr: world ,auth: cert ,title: 'admin @ everywhere with ssl \u0026 cert' } - {user: '+dbrole_readonly',db: all ,addr: localhost ,auth: ssl ,title: 'pgbouncer read/write via local socket'} - {user: '+dbrole_readonly',db: all ,addr: intra ,auth: ssl ,title: 'read/write biz user via password' } - {user: '+dbrole_offline' ,db: all ,addr: intra ,auth: ssl ,title: 'allow etl offline tasks from intranet'} pgb_default_hba_rules: # pgbouncer host-based authentication rules - {user: '${dbsu}' ,db: pgbouncer ,addr: local ,auth: peer ,title: 'dbsu local admin access with os ident'} - {user: 'all' ,db: all ,addr: localhost ,auth: pwd ,title: 'allow all user local access with pwd' } - {user: '${monitor}' ,db: pgbouncer ,addr: intra ,auth: ssl ,title: 'monitor access via intranet with pwd' } - {user: '${monitor}' ,db: all ,addr: world ,auth: deny ,title: 'reject all other monitor access addr' } - {user: '${admin}' ,db: all ,addr: intra ,auth: ssl ,title: 'admin access via intranet with pwd' } - {user: '${admin}' ,db: all ,addr: world ,auth: deny ,title: 'reject all other admin access addr' } - {user: 'all' ,db: all ,addr: intra ,auth: ssl ,title: 'allow all user intra access with pwd' } # OPTIONAL delayed cluster for pg-meta pg-meta-delay: # delayed instance for pg-meta (1 hour ago) hosts: { 10.10.10.13: { pg_seq: 1, pg_role: primary, pg_upstream: 10.10.10.10, pg_delay: 1h } } vars: { pg_cluster: pg-meta-delay } Citus Distributed Cluster Below is a declarative configuration for a four-node Citus distributed cluster:\nall: children: pg-citus0: # citus coordinator, pg_group = 0 hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: { pg_cluster: pg-citus0 , pg_group: 0 } pg-citus1: # citus data node 1 hosts: { 10.10.10.11: { pg_seq: 1, pg_role: primary } } vars: { pg_cluster: pg-citus1 , pg_group: 1 } pg-citus2: # citus data node 2 hosts: { 10.10.10.12: { pg_seq: 1, pg_role: primary } } vars: { pg_cluster: pg-citus2 , pg_group: 2 } pg-citus3: # citus data node 3, with an extra replica hosts: 10.10.10.13: { pg_seq: 1, pg_role: primary } 10.10.10.14: { pg_seq: 2, pg_role: replica } vars: { pg_cluster: pg-citus3 , pg_group: 3 } vars: # global parameters for all citus clusters pg_mode: citus # pgsql cluster mode: citus pg_shard: pg-citus # citus shard name: pg-citus patroni_citus_db: meta # citus distributed database name pg_dbsu_password: DBUser.Postgres # all dbsu password access for citus cluster pg_users: [ { name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [ dbrole_admin ] } ] pg_databases: [ { name: meta ,extensions: [ { name: citus }, { name: postgis }, { name: timescaledb } ] } ] pg_hba_rules: - { user: 'all' ,db: all ,addr: 127.0.0.1/32 ,auth: ssl ,title: 'all user ssl access from localhost' } - { user: 'all' ,db: all ,addr: intra ,auth: ssl ,title: 'all user ssl access from intranet' } Redis Clusters Below are declarative configuration examples for Redis primary-replica cluster, sentinel cluster, and Redis Cluster:\nredis-ms: # redis classic primary \u0026 replica hosts: { 10.10.10.10: { redis_node: 1 , redis_instances: { 6379: { }, 6380: { replica_of: '10.10.10.10 6379' } } } } vars: { redis_cluster: redis-ms ,redis_password: 'redis.ms' ,redis_max_memory: 64MB } redis-meta: # redis sentinel x 3 hosts: { 10.10.10.11: { redis_node: 1 , redis_instances: { 26379: { } ,26380: { } ,26381: { } } } } vars: redis_cluster: redis-meta redis_password: 'redis.meta' redis_mode: sentinel redis_max_memory: 16MB redis_sentinel_monitor: # primary list for redis sentinel, use cls as name, primary ip:port - { name: redis-ms, host: 10.10.10.10, port: 6379 ,password: redis.ms, quorum: 2 } redis-test: # redis native cluster: 3m x 3s hosts: 10.10.10.12: { redis_node: 1 ,redis_instances: { 6379: { } ,6380: { } ,6381: { } } } 10.10.10.13: { redis_node: 2 ,redis_instances: { 6379: { } ,6380: { } ,6381: { } } } vars: { redis_cluster: redis-test ,redis_password: 'redis.test' ,redis_mode: cluster, redis_max_memory: 32MB } ETCD Cluster Below is a declarative configuration example for a three-node Etcd cluster:\netcd: # dcs service for postgres/patroni ha consensus hosts: # 1 node for testing, 3 or 5 for production 10.10.10.10: { etcd_seq: 1 } # etcd_seq required 10.10.10.11: { etcd_seq: 2 } # assign from 1 ~ n 10.10.10.12: { etcd_seq: 3 } # odd number please vars: # cluster level parameter override roles/etcd etcd_cluster: etcd # mark etcd cluster name etcd etcd_safeguard: false # safeguard against purging etcd_clean: true # purge etcd during init process MinIO Cluster Below is a declarative configuration example for a three-node MinIO cluster:\nminio: hosts: 10.10.10.10: { minio_seq: 1 } 10.10.10.11: { minio_seq: 2 } 10.10.10.12: { minio_seq: 3 } vars: minio_cluster: minio minio_data: '/data{1...2}' # use two disks per node minio_node: '${minio_cluster}-${minio_seq}.pigsty' # node name pattern haproxy_services: - name: minio # [required] service name, must be unique port: 9002 # [required] service port, must be unique options: - option httpchk - option http-keep-alive - http-check send meth OPTIONS uri /minio/health/live - http-check expect status 200 servers: - { name: minio-1 ,ip: 10.10.10.10 , port: 9000 , options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-2 ,ip: 10.10.10.11 , port: 9000 , options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-3 ,ip: 10.10.10.12 , port: 9000 , options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } ","categories":["Concept"],"description":"Pigsty uses Infrastructure as Code (IaC) philosophy to manage all components, providing declarative management for large-scale clusters.","excerpt":"Pigsty uses Infrastructure as Code (IaC) philosophy to manage all …","ref":"/docs/concept/iac/","tags":"","title":"Infra as Code"},{"body":" Overview Pigsty’s PostgreSQL clusters come with out-of-the-box high availability, powered by Patroni, Etcd, and HAProxy.\nWhen your PostgreSQL cluster has two or more instances, you automatically have self-healing database high availability without any additional configuration — as long as any instance in the cluster survives, the cluster can provide complete service. Clients only need to connect to any node in the cluster to get full service without worrying about primary-replica topology changes.\nWith default configuration, the primary failure Recovery Time Objective (RTO) ≈ 45s, and Recovery Point Objective (RPO) \u003c 1MB; for replica failures, RPO = 0 and RTO ≈ 0 (brief interruption). In consistency-first mode, failover can guarantee zero data loss: RPO = 0. All these metrics can be configured as needed based on your actual hardware conditions and reliability requirements.\nPigsty includes built-in HAProxy load balancers for automatic traffic switching, providing DNS/VIP/LVS and other access methods for clients. Failover and switchover are almost transparent to the business side except for brief interruptions - applications don’t need to modify connection strings or restart. The minimal maintenance window requirements bring great flexibility and convenience: you can perform rolling maintenance and upgrades on the entire cluster without application coordination. The feature that hardware failures can wait until the next day to handle lets developers, operations, and DBAs sleep well during incidents.\nMany large organizations and core institutions have been using Pigsty in production for extended periods. The largest deployment has 25K CPU cores and 220+ PostgreSQL ultra-large instances (64c / 512g / 3TB NVMe SSD). In this deployment case, dozens of hardware failures and various incidents occurred over five years, yet overall availability of over 99.999% was maintained.\nWhat problems does High Availability solve?\nElevates data security C/IA availability to a new level: RPO ≈ 0, RTO \u003c 45s. Gains seamless rolling maintenance capability, minimizing maintenance window requirements and bringing great convenience. Hardware failures can self-heal immediately without human intervention, allowing operations and DBAs to sleep well. Replicas can handle read-only requests, offloading primary load and fully utilizing resources. What are the costs of High Availability?\nInfrastructure dependency: HA requires DCS (etcd/zk/consul) for consensus. Higher starting threshold: A meaningful HA deployment requires at least three nodes. Extra resource consumption: Each new replica consumes additional resources, though this is usually not a major concern. Significantly increased complexity: Backup costs increase significantly, requiring tools to manage complexity. Limitations of High Availability\nSince replication happens in real-time, all changes are immediately applied to replicas. Therefore, streaming replication-based HA solutions cannot handle data deletion or modification caused by human errors and software defects. (e.g., DROP TABLE or DELETE data) Such failures require using delayed clusters or performing point-in-time recovery using previous base backups and WAL archives.\nConfiguration Strategy RTO RPO Standalone + Nothing Data permanently lost, unrecoverable All data lost Standalone + Base Backup Depends on backup size and bandwidth (hours) Lose data since last backup (hours to days) Standalone + Base Backup + WAL Archive Depends on backup size and bandwidth (hours) Lose unarchived data (tens of MB) Primary-Replica + Manual Failover ~10 minutes Lose data in replication lag (~100KB) Primary-Replica + Auto Failover Within 1 minute Lose data in replication lag (~100KB) Primary-Replica + Auto Failover + Sync Commit Within 1 minute No data loss How It Works In Pigsty, the high availability architecture works as follows:\nPostgreSQL uses standard streaming replication to build physical replicas; replicas take over when the primary fails. Patroni manages PostgreSQL server processes and handles high availability matters. Etcd provides distributed configuration storage (DCS) capability and is used for leader election after failures. Patroni relies on Etcd to reach cluster leader consensus and provides health check interfaces externally. HAProxy exposes cluster services externally and uses Patroni health check interfaces to automatically distribute traffic to healthy nodes. vip-manager provides an optional Layer 2 VIP, retrieves leader information from Etcd, and binds the VIP to the node where the cluster primary resides. When the primary fails, a new round of leader election is triggered. The healthiest replica in the cluster (highest LSN position, minimum data loss) wins and is promoted to the new primary. After the winning replica is promoted, read-write traffic is immediately routed to the new primary. The impact of primary failure is brief write service unavailability: write requests will be blocked or fail directly from primary failure until new primary promotion, with unavailability typically lasting 15 to 30 seconds, usually not exceeding 1 minute.\nWhen a replica fails, read-only traffic is routed to other replicas. Only when all replicas fail will read-only traffic ultimately be handled by the primary. The impact of replica failure is partial read-only query interruption: queries currently running on that replica will abort due to connection reset and be immediately taken over by other available replicas.\nFailure detection is performed jointly by Patroni and Etcd. The cluster leader holds a lease; if the cluster leader fails to renew the lease in time (10s) due to failure, the lease is released, triggering a Failover and new cluster election.\nEven without any failures, you can proactively change the cluster primary through Switchover. In this case, write queries on the primary will experience a brief interruption and be immediately routed to the new primary. This operation is typically used for rolling maintenance/upgrades of database servers.\nTradeoffs Recovery Time Objective (RTO) and Recovery Point Objective (RPO) are two parameters that require careful tradeoffs when designing high availability clusters.\nThe default RTO and RPO values used by Pigsty meet reliability requirements for most scenarios. You can adjust them based on your hardware level, network quality, and business requirements.\nRTO and RPO are NOT always better when smaller! Too small an RTO increases false positive rates; too small an RPO reduces the probability of successful automatic failover.\nThe upper limit of unavailability during failover is controlled by the pg_rto parameter. RTO defaults to 45s. Increasing it will result in longer primary failure write unavailability, while decreasing it will increase the rate of false positive failovers (e.g., repeated switching due to brief network jitter).\nThe upper limit of potential data loss is controlled by the pg_rpo parameter, defaulting to 1MB. Reducing this value can lower the data loss ceiling during failover but also increases the probability of refusing automatic failover when replicas are not healthy enough (lagging too far behind).\nPigsty uses availability-first mode by default, meaning it will failover as quickly as possible when the primary fails, and data not yet replicated to replicas may be lost (under typical 10GbE networks, replication lag is usually a few KB to 100KB).\nIf you need to ensure zero data loss during failover, you can use the crit.yml template to ensure no data loss during failover, but this sacrifices some performance as a tradeoff.\nRelated Parameters pg_rto Parameter name: pg_rto, Type: int, Level: C\nRecovery Time Objective (RTO) in seconds. This is used to calculate Patroni’s TTL value, defaulting to 45 seconds.\nIf the primary instance is missing for this long, a new leader election will be triggered. This value is not always better when lower; it involves tradeoffs: Reducing this value can decrease unavailability during cluster failover (inability to write), but makes the cluster more sensitive to short-term network jitter, increasing the probability of false positive failover triggers. You need to configure this value based on network conditions and business constraints, making a tradeoff between failure probability and failure impact.\npg_rpo Parameter name: pg_rpo, Type: int, Level: C\nRecovery Point Objective (RPO) in bytes, default: 1048576.\nDefaults to 1MiB, meaning up to 1MiB of data loss can be tolerated during failover.\nWhen the primary goes down and all replicas are lagging, you must make a difficult choice: Either promote a replica to become the new primary immediately, accepting acceptable data loss (e.g., less than 1MB), and restore service as quickly as possible. Or wait for the primary to come back online (which may never happen) to avoid any data loss, or abandon automatic failover and wait for human intervention to make the final decision. You need to configure this value based on business preference, making a tradeoff between availability and consistency.\nAdditionally, you can always ensure RPO = 0 by enabling synchronous commit (e.g., using the crit.yml template), sacrificing some cluster latency/throughput performance to guarantee data consistency.\n","categories":["Concept"],"description":"Pigsty uses Patroni to implement PostgreSQL high availability, ensuring automatic failover when the primary becomes unavailable.","excerpt":"Pigsty uses Patroni to implement PostgreSQL high availability, …","ref":"/docs/concept/ha/","tags":"","title":"High Availability"},{"body":"Running production-grade, highly available PostgreSQL clusters typically requires a comprehensive set of infrastructure services (foundation) for support, such as monitoring and alerting, log collection, time synchronization, DNS resolution, and local software repositories. Pigsty provides the INFRA module to address this—it’s an optional module, but we strongly recommend enabling it.\nOverview The diagram below shows the architecture of a single-node deployment. The right half represents the components included in the INFRA module:\nComponent Type Description Nginx Web Server Unified entry for WebUI, local repo, reverse proxy for internal services Repo Software Repo APT/DNF repository with all RPM/DEB packages needed for deployment Grafana Visualization Displays metrics, logs, and traces; hosts dashboards, reports, and custom data apps VictoriaMetrics Time Series DB Scrapes all metrics, Prometheus API compatible, provides VMUI query interface VictoriaLogs Log Platform Centralized log storage; all nodes run Vector by default, pushing logs here VictoriaTraces Tracing Collects slow SQL, service traces, and other tracing data VMAlert Alert Engine Evaluates alerting rules, pushes events to Alertmanager AlertManager Alert Manager Aggregates alerts, dispatches notifications via email, Webhook, etc. BlackboxExporter Blackbox Probe Probes reachability of IPs/VIPs/URLs DNSMASQ DNS Service Provides DNS resolution for domains used within Pigsty [Optional] Chronyd Time Sync Provides NTP time synchronization to ensure consistent time across nodes [Optional] CA Certificate Issues encryption certificates within the environment Ansible Orchestration Batch, declarative, agentless tool for managing large numbers of servers Nginx Nginx is the access entry point for all WebUI services in Pigsty, using ports 80 / 443 for HTTP/HTTPS by default. Live Demo\nIP Access (replace) Domain (HTTP) Domain (HTTPS) Public Demo http://10.10.10.10 http://i.pigsty https://i.pigsty https://demo.pigsty.io Infrastructure components with WebUIs can be exposed uniformly through Nginx, such as Grafana, VictoriaMetrics (VMUI), AlertManager, and HAProxy console. Additionally, the local software repository and other static resources are served via Nginx.\nNginx configures local web servers or reverse proxy servers based on definitions in infra_portal.\ninfra_portal: home : { domain: i.pigsty } By default, it exposes Pigsty’s admin homepage: i.pigsty. Different endpoints on this page proxy different components:\nEndpoint Component Native Port Notes Public Demo / Nginx 80/443 Homepage, local repo, file server demo.pigsty.io /ui/ Grafana 3000 Grafana dashboard entry demo.pigsty.io/ui/ /vmetrics/ VictoriaMetrics 8428 Time series DB Web UI demo.pigsty.io/vmetrics/ /vlogs/ VictoriaLogs 9428 Log DB Web UI demo.pigsty.io/vlogs/ /vtraces/ VictoriaTraces 10428 Tracing Web UI demo.pigsty.io/vtraces/ /vmalert/ VMAlert 8880 Alert rule management demo.pigsty.io/vmalert/ /alertmgr/ AlertManager 9059 Alert management Web UI demo.pigsty.io/alertmgr/ /blackbox/ Blackbox 9115 Blackbox probe Pigsty allows rich customization of Nginx as a local file server or reverse proxy, with self-signed or real HTTPS certificates.\nFor more information, see: Tutorial: Nginx—Expose Web Services via Proxy and Tutorial: Certbot—Request and Renew HTTPS Certificates\nRepo Pigsty creates a local software repository on the Infra node during installation to accelerate subsequent software installations. Live Demo\nThis repository defaults to the /www/pigsty directory, served by Nginx and mounted at the /pigsty path:\nIP Access (replace) Domain (HTTP) Domain (HTTPS) Public Demo http://10.10.10.10/pigsty http://i.pigsty/pigsty https://i.pigsty/pigsty https://demo.pigsty.io/pigsty Pigsty supports offline installation, which essentially pre-copies a prepared local software repository to the target environment. When Pigsty performs production deployment and needs to create a local software repository, if it finds the /www/pigsty/repo_complete marker file already exists locally, it skips downloading packages from upstream and uses existing packages directly, avoiding internet downloads.\nFor more information, see: Config: INFRA - REPO\nGrafana Grafana is the core component of Pigsty’s monitoring system, used for visualizing metrics, logs, and various information. Live Demo\nGrafana listens on port 3000 by default and is proxied via Nginx at the /ui path:\nIP Access (replace) Domain (HTTP) Domain (HTTPS) Public Demo http://10.10.10.10/ui http://i.pigsty/ui https://i.pigsty/ui https://demo.pigsty.io/ui Pigsty provides pre-built dashboards based on VictoriaMetrics / Logs / Traces, with one-click drill-down and roll-up via URL jumps for rapid troubleshooting.\nGrafana can also serve as a low-code visualization platform, so ECharts, victoriametrics-datasource, victorialogs-datasource plugins are installed by default, with Vector / Victoria datasources registered uniformly as vmetrics-*, vlogs-*, vtraces-* for easy custom dashboard extension.\nFor more information, see: Config: INFRA - GRAFANA.\nVictoriaMetrics VictoriaMetrics is Pigsty’s time series database, responsible for scraping and storing all monitoring metrics. Live Demo\nIt listens on port 8428 by default, mounted at Nginx /vmetrics path, and also accessible via the p.pigsty domain:\nIP Access (replace) Domain (HTTP) Domain (HTTPS) Public Demo http://10.10.10.10/vmetrics http://p.pigsty https://i.pigsty/vmetrics https://demo.pigsty.io/vmetrics VictoriaMetrics is fully compatible with the Prometheus API, supporting PromQL queries, remote read/write protocols, and the Alertmanager API. The built-in VMUI provides an ad-hoc query interface for exploring metrics data directly, and also serves as a Grafana datasource.\nFor more information, see: Config: INFRA - VMETRICS\nVictoriaLogs VictoriaLogs is Pigsty’s log platform, centrally storing structured logs from all nodes. Live Demo\nIt listens on port 9428 by default, mounted at Nginx /vlogs path:\nIP Access (replace) Domain (HTTP) Domain (HTTPS) Public Demo http://10.10.10.10/vlogs http://i.pigsty/vlogs https://i.pigsty/vlogs https://demo.pigsty.io/vlogs All managed nodes run Vector Agent by default, collecting system logs, PostgreSQL logs, Patroni logs, Pgbouncer logs, etc., processing them into structured format and pushing to VictoriaLogs. The built-in Web UI supports log search and filtering, and can be integrated with Grafana’s victorialogs-datasource plugin for visual analysis.\nFor more information, see: Config: INFRA - VLOGS\nVictoriaTraces VictoriaTraces is used for collecting trace data and slow SQL records. Live Demo\nIt listens on port 10428 by default, mounted at Nginx /vtraces path:\nIP Access (replace) Domain (HTTP) Domain (HTTPS) Public Demo http://10.10.10.10/vtraces http://i.pigsty/vtraces https://i.pigsty/vtraces https://demo.pigsty.io/vtraces VictoriaTraces provides a Jaeger-compatible interface for analyzing service call chains and database slow queries. Combined with Grafana dashboards, it enables rapid identification of performance bottlenecks and root cause tracing.\nFor more information, see: Config: INFRA - VTRACES\nVMAlert VMAlert is the alerting rule computation engine, responsible for evaluating alert rules and pushing triggered events to Alertmanager. Live Demo\nIt listens on port 8880 by default, mounted at Nginx /vmalert path:\nIP Access (replace) Domain (HTTP) Domain (HTTPS) Public Demo http://10.10.10.10/vmalert http://i.pigsty/vmalert https://i.pigsty/vmalert https://demo.pigsty.io/vmalert VMAlert reads metrics data from VictoriaMetrics and periodically evaluates alerting rules. Pigsty provides pre-built alerting rules for PGSQL, NODE, REDIS, and other modules, covering common failure scenarios out of the box.\nFor more information, see: Config: INFRA - VMALERT\nAlertManager AlertManager handles alert event aggregation, deduplication, grouping, and dispatch. Live Demo\nIt listens on port 9059 by default, mounted at Nginx /alertmgr path, and also accessible via the a.pigsty domain:\nIP Access (replace) Domain (HTTP) Domain (HTTPS) Public Demo http://10.10.10.10/alertmgr http://a.pigsty https://i.pigsty/alertmgr https://demo.pigsty.io/alertmgr AlertManager supports multiple notification channels: email, Webhook, Slack, PagerDuty, WeChat Work, etc. Through alert routing rules, differentiated dispatch based on severity level and module type is possible, with support for silencing, inhibition, and other advanced features.\nFor more information, see: Config: INFRA - AlertManager\nBlackboxExporter Blackbox Exporter is used for active probing of target reachability, enabling blackbox monitoring.\nIt listens on port 9115 by default, mounted at Nginx /blackbox path:\nIP Access (replace) Domain (HTTP) Domain (HTTPS) Public Demo http://10.10.10.10/blackbox http://i.pigsty/blackbox https://i.pigsty/blackbox https://demo.pigsty.io/blackbox It supports multiple probe methods including ICMP Ping, TCP ports, and HTTP/HTTPS endpoints. Useful for monitoring VIP reachability, service port availability, external dependency health, etc.—an important tool for assessing failure impact scope.\nFor more information, see: Config: INFRA - BLACKBOX\nAnsible Ansible is Pigsty’s core orchestration tool; all deployment, configuration, and management operations are performed through Ansible Playbooks.\nPigsty automatically installs Ansible on the admin node (Infra node) during installation. It adopts a declarative configuration style and idempotent playbook design: the same playbook can be run repeatedly, and the system automatically converges to the desired state without side effects.\nAnsible’s core advantages:\nAgentless: Executes remotely via SSH, no additional software needed on target nodes. Declarative: Describes the desired state rather than execution steps; configuration is documentation. Idempotent: Multiple executions produce consistent results; supports retry after partial failures. For more information, see: Playbooks: Pigsty Playbook\nDNSMASQ DNSMASQ provides DNS resolution on INFRA nodes, resolving domain names to their corresponding IP addresses.\nDNSMASQ listens on port 53 (UDP/TCP) by default, providing DNS resolution for all nodes. Records are stored in the /infra/hosts directory.\nOther modules automatically register their domain names with DNSMASQ during deployment, which you can use as needed. DNS is completely optional—Pigsty works normally without it. Client nodes can configure INFRA nodes as their DNS servers, allowing access to services via domain names without remembering IP addresses.\ndns_records: Default DNS records written to INFRA nodes node_dns_servers: Configure DNS servers for nodes, defaults to INFRA node via admin_ip (can also be disabled) For more information, see: Config: INFRA - DNS and Tutorial: DNS—Configure Domain Resolution\nChronyd Chronyd provides NTP time synchronization, ensuring consistent clocks across all nodes. It listens on port 123 (UDP) by default as the time source.\nTime synchronization is critical for distributed systems: log analysis requires aligned timestamps, certificate validation depends on accurate clocks, and PostgreSQL streaming replication is sensitive to clock drift. In isolated network environments, the INFRA node can serve as an internal NTP server with other nodes synchronizing to it.\nIn Pigsty, all nodes run chronyd by default for time sync. The default upstream is pool.ntp.org public NTP servers. Chronyd is essentially managed by the Node module, but in isolated networks, you can use admin_ip to point to the INFRA node’s Chronyd service as the internal time source. In this case, the Chronyd service on the INFRA node serves as the internal time synchronization infrastructure.\nFor more information, see: Config: NODE - TIME\nINFRA Node vs Regular Node In Pigsty, the relationship between nodes and infrastructure is a weak circular dependency: node_monitor → infra → node\nThe NODE module itself doesn’t depend on the INFRA module, but the monitoring functionality (node_monitor) requires the monitoring platform and services provided by the infrastructure module.\nTherefore, in the infra.yml and deploy playbooks, an “interleaved deployment” technique is used:\nFirst, initialize the NODE module on all regular nodes, but skip monitoring config since infrastructure isn’t deployed yet. Then, initialize the INFRA module on the INFRA node—monitoring is now available. Finally, reconfigure monitoring on all regular nodes, connecting to the now-deployed monitoring platform. If you don’t need “one-shot” deployment of all nodes, you can use phased deployment: initialize INFRA nodes first, then regular nodes.\nHow Are Nodes Coupled to Infrastructure? Regular nodes reference an INFRA node via the admin_ip parameter as their infrastructure provider.\nFor example, when you configure global admin_ip = 10.10.10.10, all nodes will typically use infrastructure services at this IP.\nThis design allows quick, batch switching of infrastructure providers. Parameters that may reference ${admin_ip}:\nParameter Module Default Value Description repo_endpoint INFRA http://${admin_ip}:80 Software repo URL repo_upstream.baseurl INFRA http://${admin_ip}/pigsty Local repo baseurl infra_portal.endpoint INFRA ${admin_ip}:\u003cport\u003e Nginx proxy backend dns_records INFRA [\"${admin_ip} i.pigsty\", ...] DNS records node_default_etc_hosts NODE [\"${admin_ip} i.pigsty\"] Default static DNS node_etc_hosts NODE [] Custom static DNS node_dns_servers NODE [\"${admin_ip}\"] Dynamic DNS servers node_ntp_servers NODE [\"pool pool.ntp.org iburst\"] NTP servers (optional) For example, when a node installs software, the local repo points to the Nginx local software repository at admin_ip:80/pigsty. The DNS server also points to DNSMASQ at admin_ip:53. However, this isn’t mandatory—nodes can ignore the local repo and install directly from upstream internet sources (most single-node config templates); DNS servers can also remain unconfigured, as Pigsty has no DNS dependency.\nINFRA Node vs ADMIN Node The management-initiating ADMIN node typically coincides with the INFRA node. In single-node deployment, this is exactly the case. In multi-node deployment with multiple INFRA nodes, the admin node is usually the first in the infra group; others serve as backups. However, exceptions exist. You might separate them for various reasons:\nFor example, in large-scale production deployments, a classic pattern uses 1-2 dedicated management hosts (tiny VMs suffice) belonging to the DBA team as the control hub, with 2-3 high-spec physical machines (or more!) as monitoring infrastructure. Here, admin nodes are separate from infrastructure nodes. In this case, the admin_ip in your config should point to an INFRA node’s IP, not the current ADMIN node’s IP. This is for historical reasons: initially ADMIN and INFRA nodes were tightly coupled concepts, with separation capabilities evolving later, so the parameter name wasn’t changed.\nAnother common scenario is managing cloud nodes locally. For example, you can install Ansible on your laptop and specify cloud nodes as “managed targets.” In this case, your laptop acts as the ADMIN node, while cloud servers act as INFRA nodes.\nall: children: infra: { hosts: { 10.10.10.10: { infra_seq: 1 , ansible_host: your_ssh_alias } } } # \u003c--- Use ansible_host to point to cloud node (fill in ssh alias) etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 } }, vars: { etcd_cluster: etcd } } # SSH connection will use: ssh your_ssh_alias pg-meta: { hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } }, vars: { pg_cluster: pg-meta } } vars: version: v4.0.0 admin_ip: 10.10.10.10 region: default Multiple INFRA Nodes By default, Pigsty only needs one INFRA node for most requirements. Even if the INFRA module goes down, it won’t affect database services on other nodes.\nHowever, in production environments with high monitoring and alerting requirements, you may want multiple INFRA nodes to improve infrastructure availability. A common deployment uses two Infra nodes for redundancy, monitoring each other… or more nodes to deploy a distributed Victoria cluster for unlimited horizontal scaling.\nEach Infra node is independent—Nginx points to services on the local machine. VictoriaMetrics independently scrapes metrics from all services in the environment, and logs are pushed to all VictoriaLogs collection endpoints by default. The only exception is Grafana: every Grafana instance registers all VictoriaMetrics / Logs / Traces / PostgreSQL instances as datasources. Therefore, each Grafana instance can see complete monitoring data.\nIf you modify Grafana—such as adding new dashboards or changing datasource configs—these changes only affect the Grafana instance on that node. To keep Grafana consistent across all nodes, use a PostgreSQL database as shared storage. See Tutorial: Configure Grafana High Availability for details.\n","categories":["Concept"],"description":"Infrastructure module architecture, components, and functionality in Pigsty.","excerpt":"Infrastructure module architecture, components, and functionality in …","ref":"/docs/concept/arch/infra/","tags":"","title":"Infrastructure"},{"body":"The PGSQL module organizes PostgreSQL in production as clusters—logical entities composed of a group of database instances associated by primary-replica relationships.\nOverview The PGSQL module includes the following components, working together to provide production-grade PostgreSQL HA cluster services:\nComponent Type Description postgres Database The world’s most advanced open-source relational database, PGSQL core patroni HA Manages PostgreSQL, coordinates failover, leader election, config changes pgbouncer Pool Lightweight connection pooling middleware, reduces overhead, adds flexibility pgbackrest Backup Full/incremental backup and WAL archiving, supports local and object storage pg_exporter Metrics Exports PostgreSQL monitoring metrics for Prometheus scraping pgbouncer_exporter Metrics Exports Pgbouncer connection pool metrics pgbackrest_exporter Metrics Exports backup status metrics vip-manager VIP Binds L2 VIP to current primary node for transparent failover [Optional] The vip-manager is an on-demand component. Additionally, PGSQL uses components from other modules:\nComponent Module Type Description haproxy NODE LB Exposes service ports, routes traffic to primary or replicas vector NODE Logging Collects PostgreSQL, Patroni, Pgbouncer logs and ships to center etcd ETCD DCS Distributed consistent store for cluster metadata and leader info By analogy, the PostgreSQL database kernel is the CPU, while the PGSQL module packages it as a complete computer. Patroni and Etcd form the HA subsystem, pgBackRest and MinIO form the backup subsystem. HAProxy, Pgbouncer, and vip-manager form the access subsystem. Various Exporters and Vector build the observability subsystem; finally, you can swap different kernel CPUs and extension cards.\nSubsystem Components Function HA Subsystem Patroni + etcd Failure detection, auto-failover, config management Access Subsystem HAProxy + Pgbouncer + vip-manager Service exposure, load balancing, pooling, VIP Backup Subsystem pgBackRest (+ MinIO) Full/incremental backup, WAL archiving, PITR Observability Subsystem pg_exporter / pgbouncer_exporter / pgbackrest_exporter + Vector Metrics collection, log aggregation Component Interaction Cluster DNS is resolved by DNSMASQ on infra nodes Cluster VIP is managed by vip-manager, which binds pg_vip_address to the cluster primary node. vip-manager gets cluster leader info written by patroni from the etcd cluster Cluster services are exposed by HAProxy on nodes, different services distinguished by node ports (543x). HAProxy port 9101: Monitoring metrics \u0026 statistics \u0026 admin page HAProxy port 5433: Routes to primary pgbouncer: read-write service HAProxy port 5434: Routes to replica pgbouncer: read-only service HAProxy port 5436: Routes to primary postgres: default service HAProxy port 5438: Routes to offline postgres: offline service HAProxy routes traffic based on health check info from patroni. Pgbouncer is connection pooling middleware, listening on port 6432 by default, buffering connections, exposing additional metrics, and providing extra flexibility. Pgbouncer is stateless and deployed 1:1 with Postgres via local Unix socket. Production traffic (primary/replica) goes through pgbouncer by default (can specify bypass via pg_default_service_dest) Default/offline services always bypass pgbouncer and connect directly to target Postgres. PostgreSQL listens on port 5432, providing relational database services Installing PGSQL module on multiple nodes with the same cluster name automatically forms an HA cluster via streaming replication PostgreSQL process is managed by patroni by default. Patroni listens on port 8008 by default, supervising PostgreSQL server processes Patroni starts Postgres server as child process Patroni uses etcd as DCS: stores config, failure detection, and leader election. Patroni provides Postgres info (e.g., primary/replica) via health checks, HAProxy uses this to distribute traffic pg_exporter exposes postgres monitoring metrics on port 9630 pgbouncer_exporter exposes pgbouncer metrics on port 9631 pgBackRest uses local backup repository by default (pgbackrest_method = local) If using local (default), pgBackRest creates local repository under pg_fs_bkup on primary node If using minio, pgBackRest creates backup repository on dedicated MinIO cluster Vector collects Postgres-related logs (postgres, pgbouncer, patroni, pgbackrest) vector listens on port 9598, also exposes its own metrics to VictoriaMetrics on infra nodes vector sends logs to VictoriaLogs on infra nodes HA Subsystem The HA subsystem consists of Patroni and etcd, responsible for PostgreSQL cluster failure detection, automatic failover, and configuration management.\nHow it works: Patroni runs on each node, managing the local PostgreSQL process and writing cluster state (leader, members, config) to etcd. When the primary fails, Patroni coordinates election via etcd, promoting the healthiest replica to new primary. The entire process is automatic, with RTO typically under 45 seconds.\nKey Interactions:\nPostgreSQL: Starts, stops, reloads PG as parent process, controls its lifecycle etcd: External dependency, writes/watches leader key for distributed consensus and failure detection HAProxy: Provides health checks via REST API (:8008), reporting instance role vip-manager: Watches leader key in etcd, auto-migrates VIP For more information, see: High Availability and Config: PGSQL - PG_BOOTSTRAP\nAccess Subsystem The access subsystem consists of HAProxy, Pgbouncer, and vip-manager, responsible for service exposure, traffic routing, and connection pooling.\nThere are multiple access methods. A typical traffic path is: Client → DNS/VIP → HAProxy (543x) → Pgbouncer (6432) → PostgreSQL (5432)\nLayer Component Port Role L2 VIP vip-manager - Binds L2 VIP to primary (optional) L4 Load Bal HAProxy 543x Service exposure, load balancing, health checks L7 Pool Pgbouncer 6432 Connection reuse, session management, transaction pooling Service Ports:\n5433 primary: Read-write service, routes to primary Pgbouncer 5434 replica: Read-only service, routes to replica Pgbouncer 5436 default: Default service, direct to primary (bypasses pool) 5438 offline: Offline service, direct to offline replica (ETL/analytics) Key Features:\nHAProxy uses Patroni REST API to determine instance role, auto-routes traffic Pgbouncer uses transaction-level pooling, absorbs connection spikes, reduces PG connection overhead vip-manager watches etcd leader key, auto-migrates VIP during failover For more information, see: Service Access and Config: PGSQL - PG_ACCESS\nBackup Subsystem The backup subsystem consists of pgBackRest (optionally with MinIO as remote repository), responsible for data backup and point-in-time recovery (PITR).\nBackup Types:\nFull backup: Complete database copy Incremental/differential backup: Only backs up changed data blocks WAL archiving: Continuous transaction log archiving, enables any point-in-time recovery Storage Backends:\nlocal (default): Local disk, backups stored at pg_fs_bkup mount point minio: S3-compatible object storage, supports centralized backup management and off-site DR Key Interactions:\npgBackRest → PostgreSQL: Executes backup commands, manages WAL archiving pgBackRest → Patroni: Recovery can bootstrap replicas as new primary or standby pgbackrest_exporter → Prometheus: Exports backup status metrics, monitors backup health For more information, see: PITR, Backup \u0026 Recovery, and Config: PGSQL - PG_BACKUP\nObservability Subsystem The observability subsystem consists of three Exporters and Vector, responsible for metrics collection and log aggregation.\nComponent Port Target Key Metrics pg_exporter 9630 PostgreSQL Sessions, transactions, replication lag, buffer hits pgbouncer_exporter 9631 Pgbouncer Pool utilization, wait queue, hit rate pgbackrest_exporter 9854 pgBackRest Latest backup time, size, type vector 9598 postgres/patroni/pgbouncer logs Structured log stream Data Flow:\nMetrics: Exporter → VictoriaMetrics (INFRA) → Grafana dashboards Logs: Vector → VictoriaLogs (INFRA) → Grafana log queries pg_exporter / pgbouncer_exporter connect to target services via local Unix socket, decoupled from HA topology. In slim install mode, these components can be disabled.\nFor more information, see: Config: PGSQL - PG_MONITOR\nPostgreSQL PostgreSQL is the PGSQL module core, listening on port 5432 by default for relational database services, deployed 1:1 with nodes.\nPigsty currently supports PostgreSQL 14-18 (lifecycle major versions), installed via binary packages from the PGDG official repo. Pigsty also allows you to use other PG kernel forks to replace the default PostgreSQL kernel, and install up to 440 extension plugins on top of the PG kernel.\nPostgreSQL processes are managed by default by the HA agent—Patroni. When a cluster has only one node, that instance is the primary; when the cluster has multiple nodes, other instances automatically join as replicas: through physical replication, syncing data changes from the primary in real-time. Replicas can handle read-only requests and automatically take over when the primary fails.\nYou can access PostgreSQL directly, or through HAProxy and Pgbouncer connection pool.\nFor more information, see: Config: PGSQL - PG_BOOTSTRAP\nPatroni Patroni is the PostgreSQL HA control component, listening on port 8008 by default.\nPatroni takes over PostgreSQL startup, shutdown, configuration, and health status, writing leader and member information to etcd. It handles automatic failover, maintains replication factor, coordinates parameter changes, and provides a REST API for HAProxy, monitoring, and administrators.\nHAProxy uses Patroni health check endpoints to determine instance roles and route traffic to the correct primary or replica. vip-manager monitors the leader key in etcd and automatically migrates the VIP when the primary changes.\nFor more information, see: Config: PGSQL - PG_BOOTSTRAP\nPgbouncer Pgbouncer is a lightweight connection pooling middleware, listening on port 6432 by default, deployed 1:1 with PostgreSQL database and node.\nPgbouncer runs statelessly on each instance, connecting to PostgreSQL via local Unix socket, using Transaction Pooling by default for pool management, absorbing burst client connections, stabilizing database sessions, reducing lock contention, and significantly improving performance under high concurrency.\nPigsty routes production traffic (read-write service 5433 / read-only service 5434) through Pgbouncer by default, while only the default service (5436) and offline service (5438) bypass the pool for direct PostgreSQL connections.\nPool mode is controlled by pgbouncer_poolmode, defaulting to transaction (transaction-level pooling). Connection pooling can be disabled via pgbouncer_enabled.\nFor more information, see: Config: PGSQL - PG_ACCESS\npgBackRest pgBackRest is a professional PostgreSQL backup/recovery tool, one of the strongest in the PG ecosystem, supporting full/incremental/differential backup and WAL archiving.\nPigsty uses pgBackRest for PostgreSQL PITR capability, allowing you to roll back clusters to any point within the backup retention window.\npgBackRest works with PostgreSQL to create backup repositories on the primary, executing backup and archive tasks. By default, it uses local backup repository (pgbackrest_method = local), but can be configured for MinIO or other object storage for centralized backup management.\nAfter initialization, pgbackrest_init_backup can automatically trigger the first full backup. Recovery integrates with Patroni, supporting bootstrapping replicas as new primaries or standbys.\nFor more information, see: Backup \u0026 Recovery and Config: PGSQL - PG_BACKUP\nHAProxy HAProxy is the service entry point and load balancer, exposing multiple database service ports.\nPort Service Target Description 9101 Admin - HAProxy statistics and admin page 5433 primary Primary Pgbouncer Read-write service, routes to primary pool 5434 replica Replica Pgbouncer Read-only service, routes to replica pool 5436 default Primary Postgres Default service, direct to primary (bypasses pool) 5438 offline Offline Postgres Offline service, direct to offline replica (ETL/analytics) HAProxy uses Patroni REST API health checks to determine instance roles and route traffic to the appropriate primary or replica. Service definitions are composed from pg_default_services and pg_services.\nA dedicated HAProxy node group can be specified via pg_service_provider to handle higher traffic; by default, HAProxy on local nodes publishes services.\nFor more information, see: Service Access and Config: PGSQL - PG_ACCESS\nvip-manager vip-manager binds L2 VIP to the current primary node. This is an optional component; enable it if your network supports L2 VIP.\nvip-manager runs on each PG node, monitoring the leader key written by Patroni in etcd, and binds pg_vip_address to the current primary node’s network interface. When cluster failover occurs, vip-manager immediately releases the VIP from the old primary and rebinds it on the new primary, switching traffic to the new primary.\nThis component is optional, enabled via pg_vip_enabled. When enabled, ensure all nodes are in the same VLAN; otherwise, VIP migration will fail. Public cloud networks typically don’t support L2 VIP; it’s recommended only for on-premises and private cloud environments.\nFor more information, see: Tutorial: VIP Configuration and Config: PGSQL - PG_ACCESS\npg_exporter pg_exporter exports PostgreSQL monitoring metrics, listening on port 9630 by default.\npg_exporter runs on each PG node, connecting to PostgreSQL via local Unix socket, exporting rich metrics covering sessions, buffer hits, replication lag, transaction rates, etc., scraped by VictoriaMetrics on INFRA nodes.\nCollection configuration is specified by pg_exporter_config, with support for automatic database discovery (pg_exporter_auto_discovery), and tiered cache strategies via pg_exporter_cache_ttls.\nYou can disable this component via parameters; in slim install, this component is not enabled.\nFor more information, see: Config: PGSQL - PG_MONITOR\npgbouncer_exporter pgbouncer_exporter exports Pgbouncer connection pool metrics, listening on port 9631 by default.\npgbouncer_exporter uses the same pg_exporter binary but with a dedicated metrics config file, supporting pgbouncer 1.8-1.25+. pgbouncer_exporter reads Pgbouncer statistics views, providing pool utilization, wait queue, and hit rate metrics.\nIf Pgbouncer is disabled, this component is also disabled. In slim install, this component is not enabled.\nFor more information, see: Config: PGSQL - PG_MONITOR\npgbackrest_exporter pgbackrest_exporter exports backup status metrics, listening on port 9854 by default.\npgbackrest_exporter parses pgBackRest status, generating metrics for most recent backup time, size, type, etc. Combined with alerting policies, it quickly detects expired or failed backups, ensuring data safety. Note that when there are many backups or using large network repositories, collection overhead can be significant, so pgbackrest_exporter has a default 2-minute collection interval. In the worst case, you may see the latest backup status in the monitoring system 2 minutes after a backup completes.\nFor more information, see: Config: PGSQL - PG_MONITOR\netcd etcd is a distributed consistent store (DCS), providing cluster metadata storage and leader election capability for Patroni.\netcd is deployed and managed by the independent ETCD module, not part of the PGSQL module itself, but critical for PostgreSQL HA. Patroni writes cluster state, leader info, and config parameters to etcd; all nodes reach consensus through etcd. vip-manager also reads the leader key from etcd to enable automatic VIP migration.\nFor more information, see: ETCD Module\nvector Vector is a high-performance log collection component, deployed by the NODE module, responsible for collecting PostgreSQL-related logs.\nVector runs on nodes, tracking PostgreSQL, Pgbouncer, Patroni, and pgBackRest log directories, sending structured logs to VictoriaLogs on INFRA nodes for centralized storage and querying.\nFor more information, see: NODE Module\n","categories":["Concept"],"description":"PostgreSQL module component interactions and data flow.","excerpt":"PostgreSQL module component interactions and data flow.","ref":"/docs/concept/arch/pgsql/","tags":"","title":"PGSQL Arch"},{"body":" Overview You can restore and roll back your cluster to any point in the past, avoiding data loss caused by software defects and human errors.\nPigsty’s PostgreSQL clusters come with auto-configured Point-in-Time Recovery (PITR) capability, powered by the backup component pgBackRest and optional object storage repository MinIO.\nHigh availability solutions can address hardware failures but are powerless against data deletion/overwriting/database drops caused by software defects and human errors. For such situations, Pigsty provides out-of-the-box Point-in-Time Recovery (PITR) capability, enabled by default without additional configuration.\nPigsty provides default configurations for base backups and WAL archiving. You can use local directories and disks, or dedicated MinIO clusters or S3 object storage services to store backups and achieve geo-redundant disaster recovery. When using local disks, the default capability to recover to any point within the past day is retained. When using MinIO or S3, the default capability to recover to any point within the past week is retained. As long as storage space permits, you can retain any arbitrarily long recoverable time window, as your budget allows.\nWhat problems does PITR solve?\nEnhanced disaster recovery: RPO drops from ∞ to tens of MB, RTO drops from ∞ to hours/minutes. Ensures data security: Data integrity in C/I/A: avoids data consistency issues caused by accidental deletion. Ensures data security: Data availability in C/I/A: provides fallback for “permanently unavailable” disaster scenarios Standalone Configuration Strategy Event RTO RPO Nothing Crash Permanently lost All lost Base Backup Crash Depends on backup size and bandwidth (hours) Lose data since last backup (hours to days) Base Backup + WAL Archive Crash Depends on backup size and bandwidth (hours) Lose unarchived data (tens of MB) What are the costs of PITR?\nReduces C in data security: Confidentiality, creates additional leak points, requires additional backup protection. Extra resource consumption: Local storage or network traffic/bandwidth overhead, usually not a concern. Increased complexity: Users need to pay backup management costs. Limitations of PITR\nIf only PITR is used for failure recovery, RTO and RPO metrics are inferior compared to high availability solutions, and typically both should be used together.\nRTO: With only standalone + PITR, recovery time depends on backup size and network/disk bandwidth, ranging from tens of minutes to hours or days. RPO: With only standalone + PITR, some data may be lost during crashes - one or several WAL segment files may not yet be archived, losing 16 MB to tens of MB of data. Besides PITR, you can also use delayed clusters in Pigsty to address data deletion/modification caused by human errors or software defects.\nHow It Works Point-in-time recovery allows you to restore and roll back your cluster to “any point” in the past, avoiding data loss caused by software defects and human errors. To achieve this, two preparations are needed: Base Backup and WAL Archiving. Having a base backup allows users to restore the database to its state at backup time, while having WAL archives starting from a base backup allows users to restore the database to any point after the base backup time.\nFor specific operations, refer to PGSQL Admin: Backup and Recovery.\nBase Backup Pigsty uses pgBackRest to manage PostgreSQL backups. pgBackRest initializes empty repositories on all cluster instances but only actually uses the repository on the cluster primary.\npgBackRest supports three backup modes: full backup, incremental backup, and differential backup, with the first two being most commonly used. Full backup takes a complete physical snapshot of the database cluster at the current moment; incremental backup records the differences between the current database cluster and the previous full backup.\nPigsty provides a wrapper command for backups: /pg/bin/pg-backup [full|incr]. You can schedule regular base backups as needed through Crontab or any other task scheduling system.\nWAL Archiving Pigsty enables WAL archiving on the cluster primary by default and uses the pgbackrest command-line tool to continuously push WAL segment files to the backup repository.\npgBackRest automatically manages required WAL files and timely cleans up expired backups and their corresponding WAL archive files based on the backup retention policy.\nIf you don’t need PITR functionality, you can disable WAL archiving by configuring the cluster: archive_mode: off and remove node_crontab to stop scheduled backup tasks.\nImplementation By default, Pigsty provides two preset backup strategies: The default uses local filesystem backup repository, performing one full backup daily to ensure users can roll back to any point within the past day. The alternative strategy uses dedicated MinIO clusters or S3 storage for backups, with weekly full backups, daily incremental backups, and two weeks of backup and WAL archive retention by default.\nPigsty uses pgBackRest to manage backups, receive WAL archives, and perform PITR. Backup repositories can be flexibly configured (pgbackrest_repo): defaults to primary’s local filesystem (local), but can also use other disk paths, or the included optional MinIO service (minio) and cloud S3 services.\npgbackrest_enabled: true # enable pgBackRest on pgsql host? pgbackrest_clean: true # remove pg backup data during init? pgbackrest_log_dir: /pg/log/pgbackrest # pgbackrest log dir, `/pg/log/pgbackrest` by default pgbackrest_method: local # pgbackrest repo method: local, minio, [user-defined...] pgbackrest_repo: # pgbackrest repo: https://pgbackrest.org/configuration.html#section-repository local: # default pgbackrest repo with local posix fs path: /pg/backup # local backup directory, `/pg/backup` by default retention_full_type: count # retention full backup by count retention_full: 2 # keep at most 3 full backup, at least 2, when using local fs repo minio: # optional minio repo for pgbackrest type: s3 # minio is s3-compatible, so use s3 s3_endpoint: sss.pigsty # minio endpoint domain name, `sss.pigsty` by default s3_region: us-east-1 # minio region, us-east-1 by default, not used for minio s3_bucket: pgsql # minio bucket name, `pgsql` by default s3_key: pgbackrest # minio user access key for pgbackrest s3_key_secret: S3User.Backup # minio user secret key for pgbackrest s3_uri_style: path # use path style uri for minio rather than host style path: /pgbackrest # minio backup path, `/pgbackrest` by default storage_port: 9000 # minio port, 9000 by default storage_ca_file: /etc/pki/ca.crt # minio ca file path, `/etc/pki/ca.crt` by default bundle: y # bundle small files into a single file cipher_type: aes-256-cbc # enable AES encryption for remote backup repo cipher_pass: pgBackRest # AES encryption password, default is 'pgBackRest' retention_full_type: time # retention full backup by time on minio repo retention_full: 14 # keep full backup for last 14 days # You can also add other optional backup repos, such as S3, for geo-redundant disaster recovery Pigsty parameter pgbackrest_repo target repositories are converted to repository definitions in the /etc/pgbackrest/pgbackrest.conf configuration file. For example, if you define a US West S3 repository for storing cold backups, you can use the following reference configuration.\ns3: # ------\u003e /etc/pgbackrest/pgbackrest.conf repo1-type: s3 # ----\u003e repo1-type=s3 repo1-s3-region: us-west-1 # ----\u003e repo1-s3-region=us-west-1 repo1-s3-endpoint: s3-us-west-1.amazonaws.com # ----\u003e repo1-s3-endpoint=s3-us-west-1.amazonaws.com repo1-s3-key: '\u003cyour_access_key\u003e' # ----\u003e repo1-s3-key=\u003cyour_access_key\u003e repo1-s3-key-secret: '\u003cyour_secret_key\u003e' # ----\u003e repo1-s3-key-secret=\u003cyour_secret_key\u003e repo1-s3-bucket: pgsql # ----\u003e repo1-s3-bucket=pgsql repo1-s3-uri-style: host # ----\u003e repo1-s3-uri-style=host repo1-path: /pgbackrest # ----\u003e repo1-path=/pgbackrest repo1-bundle: y # ----\u003e repo1-bundle=y repo1-cipher-type: aes-256-cbc # ----\u003e repo1-cipher-type=aes-256-cbc repo1-cipher-pass: pgBackRest # ----\u003e repo1-cipher-pass=pgBackRest repo1-retention-full-type: time # ----\u003e repo1-retention-full-type=time repo1-retention-full: 90 # ----\u003e repo1-retention-full=90 Recovery You can directly use the following wrapper commands for PostgreSQL database cluster point-in-time recovery.\nPigsty uses incremental differential parallel recovery by default, allowing you to recover to a specified point in time at maximum speed.\npg-pitr # Restore to the end of WAL archive stream (e.g., for entire datacenter failure) pg-pitr -i # Restore to the most recent backup completion time (rarely used) pg-pitr --time=\"2022-12-30 14:44:44+08\" # Restore to a specified point in time (for database or table drops) pg-pitr --name=\"my-restore-point\" # Restore to a named restore point created with pg_create_restore_point pg-pitr --lsn=\"0/7C82CB8\" -X # Restore to immediately before the LSN pg-pitr --xid=\"1234567\" -X -P # Restore to immediately before the specified transaction ID, then promote cluster to primary pg-pitr --backup=latest # Restore to the latest backup set pg-pitr --backup=20221108-105325 # Restore to a specific backup set, backup sets can be listed with pgbackrest info pg-pitr # pgbackrest --stanza=pg-meta restore pg-pitr -i # pgbackrest --stanza=pg-meta --type=immediate restore pg-pitr -t \"2022-12-30 14:44:44+08\" # pgbackrest --stanza=pg-meta --type=time --target=\"2022-12-30 14:44:44+08\" restore pg-pitr -n \"my-restore-point\" # pgbackrest --stanza=pg-meta --type=name --target=my-restore-point restore pg-pitr -b 20221108-105325F # pgbackrest --stanza=pg-meta --type=name --set=20221230-120101F restore pg-pitr -l \"0/7C82CB8\" -X # pgbackrest --stanza=pg-meta --type=lsn --target=\"0/7C82CB8\" --target-exclusive restore pg-pitr -x 1234567 -X -P # pgbackrest --stanza=pg-meta --type=xid --target=\"0/7C82CB8\" --target-exclusive --target-action=promote restore When performing PITR, you can use Pigsty’s monitoring system to observe the cluster LSN position status and determine whether recovery to the specified point in time, transaction point, LSN position, or other point was successful.\n","categories":["Task","Concept"],"description":"Pigsty uses pgBackRest to implement PostgreSQL point-in-time recovery, allowing users to roll back to any point in time within the backup policy window.","excerpt":"Pigsty uses pgBackRest to implement PostgreSQL point-in-time recovery, …","ref":"/docs/concept/pitr/","tags":["PITR"],"title":"Point-in-Time Recovery"},{"body":"Every Pigsty deployment corresponds to an Inventory that describes key properties of the infrastructure and database clusters.\nConfiguration File Pigsty uses Ansible YAML configuration format by default, with a single YAML configuration file pigsty.yml as the inventory.\n~/pigsty ^---- pigsty.yml # \u003c---- Default configuration file You can directly edit this configuration file to customize your deployment, or use the configure wizard script provided by Pigsty to automatically generate an appropriate configuration file.\nConfiguration Structure The inventory uses standard Ansible YAML configuration format, consisting of two parts: global parameters (all.vars) and multiple groups (all.children).\nYou can define new clusters in all.children and describe the infrastructure using global variables: all.vars, which looks like this:\nall: # Top-level object: all vars: {...} # Global parameters children: # Group definitions infra: # Group definition: 'infra' hosts: {...} # Group members: 'infra' vars: {...} # Group parameters: 'infra' etcd: {...} # Group definition: 'etcd' pg-meta: {...} # Group definition: 'pg-meta' pg-test: {...} # Group definition: 'pg-test' redis-test: {...} # Group definition: 'redis-test' # ... Cluster Definition Each Ansible group may represent a cluster, which can be a node cluster, PostgreSQL cluster, Redis cluster, Etcd cluster, MinIO cluster, etc.\nA cluster definition consists of two parts: cluster members (hosts) and cluster parameters (vars). You can define cluster members in \u003ccls\u003e.hosts and describe the cluster using configuration parameters in \u003ccls\u003e.vars. Here’s an example of a 3-node high-availability PostgreSQL cluster definition:\nall: children: # Ansible group list pg-test: # Ansible group name hosts: # Ansible group instances (cluster members) 10.10.10.11: { pg_seq: 1, pg_role: primary } # Host 1 10.10.10.12: { pg_seq: 2, pg_role: replica } # Host 2 10.10.10.13: { pg_seq: 3, pg_role: offline } # Host 3 vars: # Ansible group variables (cluster parameters) pg_cluster: pg-test Cluster-level vars (cluster parameters) override global parameters, and instance-level vars override both cluster parameters and global parameters.\nSplitting Configuration If your deployment is large or you want to better organize configuration files, you can split the inventory into multiple files for easier management and maintenance.\ninventory/ ├── hosts.yml # Host and cluster definitions ├── group_vars/ │ ├── all.yml # Global default variables (corresponds to all.vars) │ ├── infra.yml # infra group variables │ ├── etcd.yml # etcd group variables │ └── pg-meta.yml # pg-meta cluster variables └── host_vars/ ├── 10.10.10.10.yml # Specific host variables └── 10.10.10.11.yml You can place cluster member definitions in the hosts.yml file and put cluster-level configuration parameters in corresponding files under the group_vars directory.\nSwitching Configuration You can temporarily specify a different inventory file when running playbooks using the -i parameter.\n./pgsql.yml -i another_config.yml ./infra.yml -i nginx_config.yml Additionally, Ansible supports multiple configuration methods. You can use local yaml|ini configuration files, or use CMDB and any dynamic configuration scripts as configuration sources.\nIn Pigsty, we specify pigsty.yml in the same directory as the default inventory through ansible.cfg in the Pigsty home directory. You can modify it as needed.\n[defaults] inventory = pigsty.yml Additionally, Pigsty supports using a CMDB metabase to store the inventory, facilitating integration with existing systems.\n","categories":["Tutorial"],"description":"Describe your infrastructure and clusters using declarative configuration files","excerpt":"Describe your infrastructure and clusters using declarative …","ref":"/docs/concept/iac/inventory/","tags":"","title":"Inventory"},{"body":"Pigsty provides a configure script as a configuration wizard that automatically generates an appropriate pigsty.yml configuration file based on your current environment.\nThis is an optional script: if you already understand how to configure Pigsty, you can directly edit the pigsty.yml configuration file and skip the wizard.\nQuick Start Enter the pigsty source home directory and run ./configure to automatically start the configuration wizard. Without any arguments, it defaults to the meta single-node configuration template:\ncd ~/pigsty ./configure # Interactive configuration wizard, auto-detect environment and generate config This command will use the selected template as a base, detect the current node’s IP address and region, and generate a pigsty.yml configuration file suitable for the current environment.\nFeatures The configure script performs the following adjustments based on environment and input, generating a pigsty.yml configuration file in the current directory.\nDetects the current node IP address; if multiple IPs exist, prompts the user to input a primary IP address as the node’s identity Uses the IP address to replace the placeholder 10.10.10.10 in the configuration template and sets it as the admin_ip parameter value Detects the current region, setting region to default (global default repos) or china (using Chinese mirror repos) For micro instances (vCPU \u003c 4), uses the tiny parameter template for node_tune and pg_conf to optimize resource usage If -v PG major version is specified, sets pg_version and all PG alias parameters to the corresponding major version If -g is specified, replaces all default passwords with randomly generated strong passwords for enhanced security (strongly recommended) When PG major version ≥ 17, prioritizes the built-in C.UTF-8 locale, or the OS-supported C.UTF-8 Checks if the core dependency ansible for deployment is available in the current environment Also checks if the deployment target node is SSH-reachable and can execute commands with sudo (-s to skip) Usage Examples # Basic usage ./configure # Interactive configuration wizard ./configure -i 10.10.10.10 # Specify primary IP address # Specify configuration template ./configure -c meta # Use default single-node template (default) ./configure -c rich # Use feature-rich single-node template ./configure -c slim # Use minimal template (PGSQL + ETCD only) ./configure -c ha/full # Use 4-node HA sandbox template ./configure -c ha/trio # Use 3-node HA template ./configure -c app/supa # Use Supabase self-hosted template # Specify PostgreSQL version ./configure -v 17 # Use PostgreSQL 17 ./configure -v 16 # Use PostgreSQL 16 ./configure -c rich -v 16 # rich template + PG 16 # Region and proxy ./configure -r china # Use Chinese mirrors ./configure -r europe # Use European mirrors ./configure -x # Import current proxy environment variables # Skip and automation ./configure -s # Skip IP detection, keep placeholder ./configure -n -i 10.10.10.10 # Non-interactive mode with specified IP ./configure -c ha/full -s # 4-node template, skip IP replacement # Security enhancement ./configure -g # Generate random passwords ./configure -c meta -g -i 10.10.10.10 # Complete production configuration # Specify output and SSH port ./configure -o prod.yml # Output to prod.yml ./configure -p 2222 # Use SSH port 2222 Command Arguments ./configure [-c|--conf \u003ctemplate\u003e] # Configuration template name (meta|rich|slim|ha/full|...) [-i|--ip \u003cipaddr\u003e] # Specify primary IP address [-v|--version \u003cpgver\u003e] # PostgreSQL major version (13|14|15|16|17|18) [-r|--region \u003cregion\u003e] # Upstream software repo region (default|china|europe) [-o|--output \u003cfile\u003e] # Output configuration file path (default: pigsty.yml) [-s|--skip] # Skip IP address detection and replacement [-x|--proxy] # Import proxy settings from environment variables [-n|--non-interactive] # Non-interactive mode (don't ask any questions) [-p|--port \u003cport\u003e] # Specify SSH port [-g|--generate] # Generate random passwords [-h|--help] # Display help information Argument Details Argument Description -c, --conf Generate config from conf/\u003ctemplate\u003e.yml, supports subdirectories like ha/full -i, --ip Replace placeholder 10.10.10.10 in config template with specified IP -v, --version Specify PostgreSQL major version (13-18), keeps template default if not specified -r, --region Set software repo mirror region: default, china (Chinese mirrors), europe (European) -o, --output Specify output file path, defaults to pigsty.yml -s, --skip Skip IP address detection and replacement, keep 10.10.10.10 placeholder in template -x, --proxy Write current environment proxy variables (HTTP_PROXY, HTTPS_PROXY, ALL_PROXY, NO_PROXY) to config -n, --non-interactive Non-interactive mode, don’t ask any questions (requires -i to specify IP) -p, --port Specify SSH port (when using non-default port 22) -g, --generate Generate random values for passwords in config file, improving security (strongly recommended) Execution Flow The configure script executes detection and configuration in the following order:\n┌─────────────────────────────────────────────────────────────┐ │ configure Execution Flow │ ├─────────────────────────────────────────────────────────────┤ │ │ │ 1. check_region Detect network region (GFW check) │ │ ↓ │ │ 2. check_version Validate PostgreSQL version │ │ ↓ │ │ 3. check_kernel Detect OS kernel (Linux/Darwin) │ │ ↓ │ │ 4. check_machine Detect CPU arch (x86_64/aarch64) │ │ ↓ │ │ 5. check_package_manager Detect package manager (dnf/yum/apt) │ │ ↓ │ │ 6. check_vendor_version Detect OS distro and version │ │ ↓ │ │ 7. check_sudo Detect passwordless sudo │ │ ↓ │ │ 8. check_ssh Detect passwordless SSH to self │ │ ↓ │ │ 9. check_proxy Handle proxy environment vars │ │ ↓ │ │ 10. check_ipaddr Detect/input primary IP address │ │ ↓ │ │ 11. check_admin Validate admin SSH + Sudo access │ │ ↓ │ │ 12. check_conf Select configuration template │ │ ↓ │ │ 13. check_config Generate configuration file │ │ ↓ │ │ 14. check_utils Check if Ansible etc. installed │ │ ↓ │ │ ✓ Configuration complete, output pigsty.yml │ │ │ └─────────────────────────────────────────────────────────────┘ Automatic Behaviors Region Detection The script automatically detects the network environment to determine if you’re in mainland China (behind GFW):\n# Check network environment by accessing Google curl -I -s --connect-timeout 1 www.google.com If Google is inaccessible, automatically sets region: china to use domestic mirrors If accessible, uses region: default default mirrors Can manually specify region via -r argument IP Address Handling The script determines the primary IP address in the following priority:\nCommand line argument: If IP is specified via -i, use it directly Single IP detection: If the current node has only one IP, use it automatically Demo IP detection: If 10.10.10.10 is detected, select it automatically (for sandbox environments) Interactive input: When multiple IPs exist, prompt user to choose or input [WARN] Multiple IP address candidates found: (1) 192.168.1.100 inet 192.168.1.100/24 scope global eth0 (2) 10.10.10.10 inet 10.10.10.10/24 scope global eth1 [ IN ] INPUT primary_ip address (of current meta node, e.g 10.10.10.10): =\u003e 10.10.10.10 Low-End Hardware Optimization When CPU core count ≤ 4 is detected, the script automatically adjusts configuration:\n[WARN] replace oltp template with tiny due to cpu \u003c 4 Changes pg_conf from oltp.yml to tiny.yml Changes node_tune from oltp to tiny This ensures smooth operation on low-spec virtual machines.\nLocale Settings The script automatically enables C.UTF-8 as the default locale when:\nPostgreSQL version ≥ 17 (built-in Locale Provider support) Or the current system supports C.UTF-8 / C.utf8 locale pg_locale: C.UTF-8 pg_lc_collate: C.UTF-8 pg_lc_ctype: C.UTF-8 China Region Special Handling When region is set to china, the script automatically:\nEnables docker_registry_mirrors Docker mirror acceleration Enables PIP_MIRROR_URL Python mirror acceleration Password Generation When using the -g argument, the script generates 24-character random strings for the following passwords:\nPassword Parameter Description grafana_admin_password Grafana admin password pg_admin_password PostgreSQL admin password pg_monitor_password PostgreSQL monitor user password pg_replication_password PostgreSQL replication user password patroni_password Patroni API password haproxy_admin_password HAProxy admin password minio_secret_key MinIO Secret Key etcd_root_password ETCD Root password It also replaces the following placeholder passwords:\nDBUser.Meta → random password DBUser.Viewer → random password S3User.Backup → random password S3User.Meta → random password S3User.Data → random password $ ./configure -g [INFO] generating random passwords... grafana_admin_password : xK9mL2nP4qR7sT1vW3yZ5bD8 pg_admin_password : aB3cD5eF7gH9iJ1kL2mN4oP6 ... [INFO] random passwords generated, check and save them Configuration Templates The script reads configuration templates from the conf/ directory, supporting the following templates:\nCore Templates Template Description meta Default template: Single-node installation with INFRA + NODE + ETCD + PGSQL rich Feature-rich version: Includes almost all extensions, MinIO, local repo slim Minimal version: PostgreSQL + ETCD only, no monitoring infrastructure fat Complete version: rich base with more extensions installed pgsql Pure PostgreSQL template infra Pure infrastructure template HA Templates (ha/) Template Description ha/dual 2-node HA cluster ha/trio 3-node HA cluster ha/full 4-node complete sandbox environment ha/safe Security-hardened HA configuration ha/simu 42-node large-scale simulation environment Application Templates (app/) Template Description supabase Supabase self-hosted configuration app/dify Dify AI platform configuration app/odoo Odoo ERP configuration app/teable Teable table database configuration app/registry Docker Registry configuration Special Kernel Templates Template Description ivory IvorySQL: Oracle-compatible PostgreSQL mssql Babelfish: SQL Server-compatible PostgreSQL polar PolarDB: Alibaba Cloud open-source distributed PostgreSQL citus Citus: Distributed PostgreSQL oriole OrioleDB: Next-generation storage engine Demo Templates (demo/) Template Description demo/demo Demo environment configuration demo/redis Redis cluster demo demo/minio MinIO cluster demo Output Example $ ./configure configure pigsty v4.0.0 begin [ OK ] region = china [ OK ] kernel = Linux [ OK ] machine = x86_64 [ OK ] package = rpm,dnf [ OK ] vendor = rocky (Rocky Linux) [ OK ] version = 9 (9.5) [ OK ] sudo = vagrant ok [ OK ] ssh = vagrant@127.0.0.1 ok [WARN] Multiple IP address candidates found: (1) 192.168.121.193\tinet 192.168.121.193/24 brd 192.168.121.255 scope global dynamic noprefixroute eth0 (2) 10.10.10.10\tinet 10.10.10.10/24 brd 10.10.10.255 scope global noprefixroute eth1 [ OK ] primary_ip = 10.10.10.10 (from demo) [ OK ] admin = vagrant@10.10.10.10 ok [ OK ] mode = meta (el9) [ OK ] locale = C.UTF-8 [ OK ] ansible = ready [ OK ] pigsty configured [WARN] don't forget to check it and change passwords! proceed with ./deploy.yml Environment Variables The script supports the following environment variables:\nEnvironment Variable Description Default PIGSTY_HOME Pigsty installation directory ~/pigsty METADB_URL Metabase connection URL service=meta HTTP_PROXY HTTP proxy - HTTPS_PROXY HTTPS proxy - ALL_PROXY Universal proxy - NO_PROXY Proxy whitelist Built-in default Notes Passwordless access: Before running configure, ensure the current user has passwordless sudo privileges and passwordless SSH to localhost. This can be automatically configured via the bootstrap script.\nIP address selection: Choose an internal IP as the primary IP address, not a public IP or 127.0.0.1.\nPassword security: In production environments, always modify default passwords in the configuration file, or use the -g argument to generate random passwords.\nConfiguration review: After the script completes, it’s recommended to review the generated pigsty.yml file to confirm the configuration meets expectations.\nMultiple executions: You can run configure multiple times to regenerate configuration; each run will overwrite the existing pigsty.yml.\nmacOS limitations: When running on macOS, the script skips some Linux-specific checks and uses placeholder IP 10.10.10.10. macOS can only serve as an admin node.\nFAQ How to use a custom configuration template? Place your configuration file in the conf/ directory, then specify it with the -c argument:\ncp my-config.yml ~/pigsty/conf/myconf.yml ./configure -c myconf How to generate different configurations for multiple clusters? Use the -o argument to specify different output files:\n./configure -c ha/full -o cluster-a.yml ./configure -c ha/trio -o cluster-b.yml Then specify the configuration file when running playbooks:\n./deploy.yml -i cluster-a.yml How to handle multiple IPs in non-interactive mode? You must explicitly specify the IP address using the -i argument:\n./configure -n -i 10.10.10.10 How to keep the placeholder IP in the template? Use the -s argument to skip IP replacement:\n./configure -c ha/full -s # Keep 10.10.10.10 placeholder Related Documentation Inventory: Understand the Ansible inventory structure Parameters: Understand Pigsty parameter hierarchy and priority Templates: View all available configuration templates Installation: Understand the complete installation process Metabase: Use PostgreSQL as a dynamic configuration source ","categories":["Concept"],"description":"Use the configure script to automatically generate recommended configuration files based on your environment.","excerpt":"Use the configure script to automatically generate recommended …","ref":"/docs/concept/iac/configure/","tags":["Configuration","Wizard","Installation"],"title":"Configure"},{"body":"In the inventory, you can use various parameters to fine-tune Pigsty customization. These parameters cover everything from infrastructure settings to database configuration.\nParameter List Pigsty provides approximately 380+ configuration parameters distributed across 8 default modules for fine-grained control of various system aspects. See Reference - Parameter List for the complete list.\nModule Groups Params Description PGSQL 9 123 Core configuration for PostgreSQL database clusters INFRA 10 82 Infrastructure: repos, Nginx, DNS, monitoring, Grafana, etc. NODE 11 83 Host node tuning: identity, DNS, packages, tuning, security, admin, time, VIP, etc. ETCD 2 13 Distributed configuration store and service discovery REDIS 1 21 Redis cache and data structure server MINIO 2 21 S3-compatible object storage service FERRET 1 9 MongoDB-compatible database FerretDB DOCKER 1 8 Docker container engine Parameter Form Parameters are key-value pairs that describe entities. The Key is a string, and the Value can be one of five types: boolean, string, number, array, or object.\nall: # \u003c------- Top-level object: all vars: admin_ip: 10.10.10.10 # \u003c------- Global configuration parameter children: pg-meta: # \u003c------- pg-meta group vars: pg_cluster: pg-meta # \u003c------- Cluster-level parameter hosts: 10.10.10.10: # \u003c------- Host node IP pg_seq: 1 pg_role: primary # \u003c------- Instance-level parameter Parameter Priority Parameters can be set at different levels with the following priority:\nLevel Location Description Priority CLI -e command line argument Passed via command line Highest (5) Host/Instance \u003cgroup\u003e.hosts.\u003chost\u003e Parameters specific to a single host Higher (4) Group/Cluster \u003cgroup\u003e.vars Parameters shared by hosts in group/cluster Medium (3) Global all.vars Parameters shared by all hosts Lower (2) Default \u003croles\u003e/default/main.yml Role implementation defaults Lowest (1) Here are some examples of parameter priority:\nUse command line parameter -e grafana_clean=true when running playbooks to wipe Grafana data Use instance-level parameter pg_role on host variables to override pg instance role Use cluster-level parameter pg_cluster on group variables to override pg cluster name Use global parameter node_ntp_servers on global variables to specify global NTP servers If pg_version is not set, Pigsty will use the default value from the pgsql role implementation (default is 18) Except for identity parameters, every parameter has an appropriate default value, so explicit setting is not required.\nIdentity Parameters Identity parameters are special parameters that serve as entity ID identifiers, therefore they have no default values and must be explicitly set.\nModule Identity Parameters PGSQL pg_cluster, pg_seq, pg_role, … NODE nodename, node_cluster ETCD etcd_cluster, etcd_seq MINIO minio_cluster, minio_seq REDIS redis_cluster, redis_node, redis_instances INFRA infra_seq Exceptions are etcd_cluster and minio_cluster which have default values. This assumes each deployment has only one etcd cluster for DCS and one optional MinIO cluster for centralized backup storage, so they are assigned default cluster names etcd and minio. However, you can still deploy multiple etcd or MinIO clusters using different names.\n","categories":["Concept"],"description":"Fine-tune Pigsty customization using configuration parameters","excerpt":"Fine-tune Pigsty customization using configuration parameters","ref":"/docs/concept/iac/parameter/","tags":"","title":"Parameters"},{"body":"In Pigsty, deployment blueprint details are defined by the inventory, which is the pigsty.yml configuration file. You can customize it through declarative configuration.\nHowever, writing configuration files directly can be daunting for new users. To address this, we provide some ready-to-use configuration templates covering common usage scenarios.\nEach template is a predefined pigsty.yml configuration file containing reasonable defaults suitable for specific scenarios.\nYou can choose a template as your customization starting point, then modify it as needed to meet your specific requirements.\nUsing Templates Pigsty provides the configure script as an optional configuration wizard that generates an inventory with good defaults based on your environment and input.\nUse ./configure -c \u003cconf\u003e to specify a configuration template, where \u003cconf\u003e is the path relative to the conf directory (the .yml suffix can be omitted).\n./configure # Default to meta.yml configuration template ./configure -c meta # Explicitly specify meta.yml single-node template ./configure -c rich # Use feature-rich template with all extensions and MinIO ./configure -c slim # Use minimal single-node template # Use different database kernels ./configure -c pgsql # Native PostgreSQL kernel, basic features (13~18) ./configure -c citus # Citus distributed HA PostgreSQL (14~17) ./configure -c mssql # Babelfish kernel, SQL Server protocol compatible (15) ./configure -c polar # PolarDB PG kernel, Aurora/RAC style (15) ./configure -c ivory # IvorySQL kernel, Oracle syntax compatible (18) ./configure -c mysql # OpenHalo kernel, MySQL compatible (14) ./configure -c pgtde # Percona PostgreSQL Server transparent encryption (18) ./configure -c oriole # OrioleDB kernel, OLTP enhanced (17) ./configure -c supabase # Supabase self-hosted configuration (15~18) # Use multi-node HA templates ./configure -c ha/dual # Use 2-node HA template ./configure -c ha/trio # Use 3-node HA template ./configure -c ha/full # Use 4-node HA template If no template is specified, Pigsty defaults to the meta.yml single-node configuration template.\nTemplate List Main Templates The following are single-node configuration templates for installing Pigsty on a single server:\nTemplate Description meta.yml Default template, single-node PostgreSQL online installation rich.yml Feature-rich template with local repo, MinIO, and more examples slim.yml Minimal template, PostgreSQL only without monitoring and infrastructure Database Kernel Templates Templates for various database management systems and kernels:\nTemplate Description pgsql.yml Native PostgreSQL kernel, basic features (13~18) citus.yml Citus distributed HA PostgreSQL (14~17) mssql.yml Babelfish kernel, SQL Server protocol compatible (15) polar.yml PolarDB PG kernel, Aurora/RAC style (15) ivory.yml IvorySQL kernel, Oracle syntax compatible (17) mysql.yml OpenHalo kernel, MySQL compatible (14) pgtde.yml Percona PostgreSQL Server transparent encryption (17) oriole.yml OrioleDB kernel, OLTP enhanced (17, Debian pkg pending) supabase.yml Supabase self-hosted configuration (15~17) You can add more nodes later or use HA templates to plan your cluster from the start.\nHA Templates You can configure Pigsty to run on multiple nodes, forming a high-availability (HA) cluster:\nTemplate Description dual.yml 2-node semi-HA deployment trio.yml 3-node standard HA deployment full.yml 4-node standard deployment safe.yml 4-node security-enhanced deployment with delayed replica simu.yml 20-node production environment simulation Application Templates You can use the following templates to run Docker applications/software:\nTemplate Description supa.yml Start single-node Supabase odoo.yml Start Odoo ERP system dify.yml Start Dify AI workflow system electric.yml Start Electric sync engine Demo Templates Besides main templates, Pigsty provides a set of demo templates for different scenarios:\nTemplate Description el.yml Full-parameter config file for EL 8/9 systems debian.yml Full-parameter config file for Debian/Ubuntu systems remote.yml Example config for monitoring remote PostgreSQL clusters or RDS redis.yml Redis cluster example configuration minio.yml 3-node MinIO cluster example configuration demo.yml Configuration file for Pigsty public demo site Build Templates The following configuration templates are for development and testing purposes:\nTemplate Description build.yml Open source build config for EL 9/10, Debian 12/13, Ubuntu 22.04/24.04 ","categories":["Concept"],"description":"Use pre-made configuration templates to quickly generate configuration files adapted to your environment","excerpt":"Use pre-made configuration templates to quickly generate configuration …","ref":"/docs/concept/iac/template/","tags":"","title":"Conf Templates"},{"body":"Pigsty allows you to use a PostgreSQL metabase as a dynamic configuration source, replacing static YAML configuration files for more powerful configuration management capabilities.\nOverview CMDB (Configuration Management Database) is a method of storing configuration information in a database for management.\nIn Pigsty, the default configuration source is a static YAML file pigsty.yml, which serves as Ansible’s inventory.\nThis approach is simple and direct, but when infrastructure scales and requires complex, fine-grained management and external integration, a single static file becomes insufficient.\nFeature Static YAML File CMDB Metabase Querying Manual search/grep SQL queries with any conditions, aggregation analysis Versioning Depends on Git or manual backup Database transactions, audit logs, time-travel snapshots Access Control File system permissions, coarse-grained PostgreSQL fine-grained access control Concurrent Editing Requires file locking or merge conflicts Database transactions naturally support concurrency External Integration Requires YAML parsing Standard SQL interface, easy integration with any language Scalability Difficult to maintain when file becomes too large Scales to physical limits Dynamic Generation Static file, changes require manual application Immediate effect, real-time configuration changes Pigsty provides the CMDB database schema in the sample database pg-meta.meta schema baseline definition.\nHow It Works The core idea of CMDB is to replace the static configuration file with a dynamic script. Ansible supports using executable scripts as inventory, as long as the script outputs inventory data in JSON format. When you enable CMDB, Pigsty creates a dynamic inventory script named inventory.sh:\n#!/bin/bash psql ${METADB_URL} -AXtwc 'SELECT text FROM pigsty.inventory;' This script’s function is simple: every time Ansible needs to read the inventory, it queries configuration data from the PostgreSQL database’s pigsty.inventory view and returns it in JSON format.\nThe overall architecture is as follows:\nflowchart LR conf[\"bin/inventory_conf\"] tocmdb[\"bin/inventory_cmdb\"] load[\"bin/inventory_load\"] ansible[\"🚀 Ansible\"] subgraph static[\"📄 Static Config Mode\"] yml[(\"pigsty.yml\")] end subgraph dynamic[\"🗄️ CMDB Dynamic Mode\"] sh[\"inventory.sh\"] cmdb[(\"PostgreSQL CMDB\")] end conf --\u003e|\"switch\"| yml yml --\u003e|\"load config\"| load load --\u003e|\"write\"| cmdb tocmdb --\u003e|\"switch\"| sh sh --\u003e cmdb yml --\u003e ansible cmdb --\u003e ansible Data Model The CMDB database schema is defined in files/cmdb.sql, with all objects in the pigsty schema.\nCore Tables Table Description Primary Key pigsty.group Cluster/group definitions, corresponds to Ansible groups cls pigsty.host Host definitions, belongs to a group (cls, ip) pigsty.global_var Global variables, corresponds to all.vars key pigsty.group_var Group variables, corresponds to all.children.\u003ccls\u003e.vars (cls, key) pigsty.host_var Host variables, host-level variables (cls, ip, key) pigsty.default_var Default variable definitions, stores parameter metadata key pigsty.job Job records table, records executed tasks id Table Structure Details Cluster Table pigsty.group\nCREATE TABLE pigsty.group ( cls TEXT PRIMARY KEY, -- Cluster name, primary key ctime TIMESTAMPTZ DEFAULT now(), -- Creation time mtime TIMESTAMPTZ DEFAULT now() -- Modification time ); Host Table pigsty.host\nCREATE TABLE pigsty.host ( cls TEXT NOT NULL REFERENCES pigsty.group(cls), -- Parent cluster ip INET NOT NULL, -- Host IP address ctime TIMESTAMPTZ DEFAULT now(), mtime TIMESTAMPTZ DEFAULT now(), PRIMARY KEY (cls, ip) ); Global Variables Table pigsty.global_var\nCREATE TABLE pigsty.global_var ( key TEXT PRIMARY KEY, -- Variable name value JSONB NULL, -- Variable value (JSON format) mtime TIMESTAMPTZ DEFAULT now() -- Modification time ); Group Variables Table pigsty.group_var\nCREATE TABLE pigsty.group_var ( cls TEXT NOT NULL REFERENCES pigsty.group(cls), key TEXT NOT NULL, value JSONB NULL, mtime TIMESTAMPTZ DEFAULT now(), PRIMARY KEY (cls, key) ); Host Variables Table pigsty.host_var\nCREATE TABLE pigsty.host_var ( cls TEXT NOT NULL, ip INET NOT NULL, key TEXT NOT NULL, value JSONB NULL, mtime TIMESTAMPTZ DEFAULT now(), PRIMARY KEY (cls, ip, key), FOREIGN KEY (cls, ip) REFERENCES pigsty.host(cls, ip) ); Core Views CMDB provides a series of views for querying and displaying configuration data:\nView Description pigsty.inventory Core view: Generates Ansible dynamic inventory JSON pigsty.raw_config Raw configuration in JSON format pigsty.global_config Global config view, merges defaults and global vars pigsty.group_config Group config view, includes host list and group vars pigsty.host_config Host config view, merges group and host-level vars pigsty.pg_cluster PostgreSQL cluster view pigsty.pg_instance PostgreSQL instance view pigsty.pg_database PostgreSQL database definition view pigsty.pg_users PostgreSQL user definition view pigsty.pg_service PostgreSQL service definition view pigsty.pg_hba PostgreSQL HBA rules view pigsty.pg_remote Remote PostgreSQL instance view pigsty.inventory is the core view that converts database configuration data to the JSON format required by Ansible:\nSELECT text FROM pigsty.inventory; Utility Scripts Pigsty provides three convenience scripts for managing CMDB:\nScript Function bin/inventory_load Load YAML configuration file into PostgreSQL database bin/inventory_cmdb Switch configuration source to CMDB (dynamic inventory script) bin/inventory_conf Switch configuration source to static config file pigsty.yml inventory_load Parse and import YAML configuration file into CMDB:\nbin/inventory_load # Load default pigsty.yml to default CMDB bin/inventory_load -p /path/to/conf.yml # Specify configuration file path bin/inventory_load -d \"postgres://...\" # Specify database connection URL bin/inventory_load -n myconfig # Specify configuration name The script performs the following operations:\nClears existing data in the pigsty schema Parses the YAML configuration file Writes global variables to the global_var table Writes cluster definitions to the group table Writes cluster variables to the group_var table Writes host definitions to the host table Writes host variables to the host_var table Environment Variables\nPIGSTY_HOME: Pigsty installation directory, defaults to ~/pigsty METADB_URL: Database connection URL, defaults to service=meta inventory_cmdb Switch Ansible to use CMDB as the configuration source:\nbin/inventory_cmdb The script performs the following operations:\nCreates dynamic inventory script ${PIGSTY_HOME}/inventory.sh Modifies ansible.cfg to set inventory to inventory.sh The generated inventory.sh contents:\n#!/bin/bash psql ${METADB_URL} -AXtwc 'SELECT text FROM pigsty.inventory;' inventory_conf Switch back to using static YAML configuration file:\nbin/inventory_conf The script modifies ansible.cfg to set inventory back to pigsty.yml.\nUsage Workflow First-time CMDB Setup Initialize CMDB schema (usually done automatically during Pigsty installation): psql -f ~/pigsty/files/cmdb.sql Load configuration to database: bin/inventory_load Switch to CMDB mode: bin/inventory_cmdb Verify configuration: ansible all --list-hosts # List all hosts ansible-inventory --list # View complete inventory Query Configuration After enabling CMDB, you can flexibly query configuration using SQL:\n-- View all clusters SELECT cls FROM pigsty.group; -- View all hosts in a cluster SELECT ip FROM pigsty.host WHERE cls = 'pg-meta'; -- View global variables SELECT key, value FROM pigsty.global_var; -- View cluster variables SELECT key, value FROM pigsty.group_var WHERE cls = 'pg-meta'; -- View all PostgreSQL clusters SELECT cls, name, pg_databases, pg_users FROM pigsty.pg_cluster; -- View all PostgreSQL instances SELECT cls, ins, ip, seq, role FROM pigsty.pg_instance; -- View all database definitions SELECT cls, datname, owner, encoding FROM pigsty.pg_database; -- View all user definitions SELECT cls, name, login, superuser FROM pigsty.pg_users; Modify Configuration You can modify configuration directly via SQL:\n-- Add new cluster INSERT INTO pigsty.group (cls) VALUES ('pg-new'); -- Add cluster variable INSERT INTO pigsty.group_var (cls, key, value) VALUES ('pg-new', 'pg_cluster', '\"pg-new\"'); -- Add host INSERT INTO pigsty.host (cls, ip) VALUES ('pg-new', '10.10.10.20'); -- Add host variables INSERT INTO pigsty.host_var (cls, ip, key, value) VALUES ('pg-new', '10.10.10.20', 'pg_seq', '1'), ('pg-new', '10.10.10.20', 'pg_role', '\"primary\"'); -- Modify global variable UPDATE pigsty.global_var SET value = '\"new-value\"' WHERE key = 'some_param'; -- Delete cluster (cascades to hosts and variables) DELETE FROM pigsty.group WHERE cls = 'pg-old'; Changes take effect immediately without reloading or restarting any service.\nSwitch Back to Static Configuration To switch back to static configuration file mode:\nbin/inventory_conf Advanced Usage Export Configuration Export CMDB configuration to YAML format:\npsql service=meta -AXtwc \"SELECT jsonb_pretty(jsonb_build_object('all', jsonb_build_object('children', children, 'vars', vars))) FROM pigsty.raw_config;\" Or use the ansible-inventory command:\nansible-inventory --list --yaml \u003e exported_config.yml Configuration Auditing Track configuration changes using the mtime field:\n-- View recently modified global variables SELECT key, value, mtime FROM pigsty.global_var ORDER BY mtime DESC LIMIT 10; -- View changes after a specific time SELECT * FROM pigsty.group_var WHERE mtime \u003e '2024-01-01'::timestamptz; Integration with External Systems CMDB uses standard PostgreSQL, making it easy to integrate with other systems:\nWeb Management Interface: Expose configuration data through REST API (e.g., PostgREST) CI/CD Pipelines: Read/write database directly in deployment scripts Monitoring \u0026 Alerting: Generate monitoring rules based on configuration data ITSM Systems: Sync with enterprise CMDB systems Considerations Data Consistency: After modifying configuration, you need to re-run the corresponding Ansible playbooks to apply changes to the actual environment\nBackup: Configuration data in CMDB is critical, ensure regular backups\nPermissions: Configure appropriate database access permissions for CMDB to avoid accidental modifications\nTransactions: When making batch configuration changes, perform them within a transaction for rollback on errors\nConnection Pooling: The inventory.sh script creates a new connection on each execution; if Ansible runs frequently, consider using connection pooling\nSummary CMDB is Pigsty’s advanced configuration management solution, suitable for scenarios requiring large-scale cluster management, complex queries, external integration, or fine-grained access control. By storing configuration data in PostgreSQL, you can fully leverage the database’s powerful capabilities to manage infrastructure configuration.\nFeature Description Storage PostgreSQL pigsty schema Dynamic Inventory inventory.sh script Config Load bin/inventory_load Switch to CMDB bin/inventory_cmdb Switch to YAML bin/inventory_conf Core View pigsty.inventory ","categories":["Concept"],"description":"Use PostgreSQL as a CMDB metabase to store Ansible inventory.","excerpt":"Use PostgreSQL as a CMDB metabase to store Ansible inventory.","ref":"/docs/concept/iac/cmdb/","tags":["CMDB","Configuration Management","IaC"],"title":"Use CMDB as Config Inventory"},{"body":"","categories":["Concept"],"description":"How Pigsty's monitoring system is architected and how monitored targets are automatically managed.","excerpt":"How Pigsty's monitoring system is architected and how monitored …","ref":"/docs/concept/monitor/","tags":"","title":"Monitoring System"},{"body":" Split read and write operations, route traffic correctly, and deliver PostgreSQL cluster capabilities reliably.\nService is an abstraction: it represents the form in which database clusters expose their capabilities externally, encapsulating underlying cluster details.\nServices are crucial for stable access in production environments, showing their value during automatic failover in high availability clusters. Personal users typically don’t need to worry about this concept.\nPersonal Users The concept of “service” is for production environments. Personal users with single-node clusters can skip the complexity and directly use instance names or IP addresses to access the database.\nFor example, Pigsty’s default single-node pg-meta.meta database can be connected directly using three different users:\npsql postgres://dbuser_dba:DBUser.DBA@10.10.10.10/meta # Connect directly with DBA superuser psql postgres://dbuser_meta:DBUser.Meta@10.10.10.10/meta # Connect with default business admin user psql postgres://dbuser_view:DBUser.View@pg-meta/meta # Connect with default read-only user via instance domain name Service Overview In real-world production environments, we use primary-replica database clusters based on replication. Within a cluster, one and only one instance serves as the leader (primary) that can accept writes. Other instances (replicas) continuously fetch change logs from the cluster leader to stay synchronized. Replicas can also handle read-only requests, significantly offloading the primary in read-heavy, write-light scenarios. Therefore, distinguishing write requests from read-only requests is a common practice.\nAdditionally, for production environments with high-frequency, short-lived connections, we pool requests through connection pool middleware (Pgbouncer) to reduce connection and backend process creation overhead. However, for scenarios like ETL and change execution, we need to bypass the connection pool and directly access the database. Meanwhile, high-availability clusters may undergo failover during failures, causing cluster leadership changes. Therefore, high-availability database solutions require write traffic to automatically adapt to cluster leadership changes. These varying access needs (read-write separation, pooled vs. direct connections, failover auto-adaptation) ultimately lead to the abstraction of the Service concept.\nTypically, database clusters must provide this most basic service:\nRead-write service (primary): Can read from and write to the database For production database clusters, at least these two services should be provided:\nRead-write service (primary): Write data: Can only be served by the primary. Read-only service (replica): Read data: Can be served by replicas; falls back to primary when no replicas are available Additionally, depending on specific business scenarios, there may be other services, such as:\nDefault direct service (default): Allows (admin) users to bypass the connection pool and directly access the database Offline replica service (offline): Dedicated replica not serving online read traffic, used for ETL and analytical queries Sync replica service (standby): Read-only service with no replication delay, handled by synchronous standby/primary for read queries Delayed replica service (delayed): Access data from the same cluster as it was some time ago, handled by delayed replicas Access Services Pigsty’s service delivery boundary stops at the cluster’s HAProxy. Users can access these load balancers through various means.\nThe typical approach is to use DNS or VIP access, binding them to all or any number of load balancers in the cluster.\nYou can use different host \u0026 port combinations, which provide PostgreSQL service in different ways.\nHost\nType Sample Description Cluster Domain Name pg-test Access via cluster domain name (resolved by dnsmasq @ infra nodes) Cluster VIP Address 10.10.10.3 Access via L2 VIP address managed by vip-manager, bound to primary node Instance Hostname pg-test-1 Access via any instance hostname (resolved by dnsmasq @ infra nodes) Instance IP Address 10.10.10.11 Access any instance’s IP address Port\nPigsty uses different ports to distinguish pg services\nPort Service Type Description 5432 postgres Database Direct access to postgres server 6432 pgbouncer Middleware Access postgres through connection pool middleware 5433 primary Service Access primary pgbouncer (or postgres) 5434 replica Service Access replica pgbouncer (or postgres) 5436 default Service Access primary postgres 5438 offline Service Access offline postgres Combinations\n# Access via cluster domain postgres://test@pg-test:5432/test # DNS -\u003e L2 VIP -\u003e primary direct connection postgres://test@pg-test:6432/test # DNS -\u003e L2 VIP -\u003e primary connection pool -\u003e primary postgres://test@pg-test:5433/test # DNS -\u003e L2 VIP -\u003e HAProxy -\u003e primary connection pool -\u003e primary postgres://test@pg-test:5434/test # DNS -\u003e L2 VIP -\u003e HAProxy -\u003e replica connection pool -\u003e replica postgres://dbuser_dba@pg-test:5436/test # DNS -\u003e L2 VIP -\u003e HAProxy -\u003e primary direct connection (for admin) postgres://dbuser_stats@pg-test:5438/test # DNS -\u003e L2 VIP -\u003e HAProxy -\u003e offline direct connection (for ETL/personal queries) # Access via cluster VIP directly postgres://test@10.10.10.3:5432/test # L2 VIP -\u003e primary direct access postgres://test@10.10.10.3:6432/test # L2 VIP -\u003e primary connection pool -\u003e primary postgres://test@10.10.10.3:5433/test # L2 VIP -\u003e HAProxy -\u003e primary connection pool -\u003e primary postgres://test@10.10.10.3:5434/test # L2 VIP -\u003e HAProxy -\u003e replica connection pool -\u003e replica postgres://dbuser_dba@10.10.10.3:5436/test # L2 VIP -\u003e HAProxy -\u003e primary direct connection (for admin) postgres://dbuser_stats@10.10.10.3::5438/test # L2 VIP -\u003e HAProxy -\u003e offline direct connection (for ETL/personal queries) # Directly specify any cluster instance name postgres://test@pg-test-1:5432/test # DNS -\u003e database instance direct connection (singleton access) postgres://test@pg-test-1:6432/test # DNS -\u003e connection pool -\u003e database postgres://test@pg-test-1:5433/test # DNS -\u003e HAProxy -\u003e connection pool -\u003e database read/write postgres://test@pg-test-1:5434/test # DNS -\u003e HAProxy -\u003e connection pool -\u003e database read-only postgres://dbuser_dba@pg-test-1:5436/test # DNS -\u003e HAProxy -\u003e database direct connection postgres://dbuser_stats@pg-test-1:5438/test # DNS -\u003e HAProxy -\u003e database offline read/write # Directly specify any cluster instance IP access postgres://test@10.10.10.11:5432/test # Database instance direct connection (directly specify instance, no automatic traffic distribution) postgres://test@10.10.10.11:6432/test # Connection pool -\u003e database postgres://test@10.10.10.11:5433/test # HAProxy -\u003e connection pool -\u003e database read/write postgres://test@10.10.10.11:5434/test # HAProxy -\u003e connection pool -\u003e database read-only postgres://dbuser_dba@10.10.10.11:5436/test # HAProxy -\u003e database direct connection postgres://dbuser_stats@10.10.10.11:5438/test # HAProxy -\u003e database offline read-write # Smart client: read/write separation via URL postgres://test@10.10.10.11:6432,10.10.10.12:6432,10.10.10.13:6432/test?target_session_attrs=primary postgres://test@10.10.10.11:6432,10.10.10.12:6432,10.10.10.13:6432/test?target_session_attrs=prefer-standby ","categories":["Concept"],"description":"Pigsty uses HAProxy to provide service access, with optional pgBouncer for connection pooling, and optional L2 VIP and DNS access.","excerpt":"Pigsty uses HAProxy to provide service access, with optional pgBouncer …","ref":"/docs/concept/ha/svc/","tags":["Service"],"title":"Service Access"},{"body":"Pigsty’s Security Philosophy Secure by Default: Out-of-the-box security configuration—basic protection without additional setup.\nProgressive Configuration: Enterprise users can gradually enhance security measures through configuration.\nDefense in Depth: Multiple security layers—even if one layer is breached, others remain protective.\nLeast Privilege: Grant users only the minimum permissions needed to complete tasks, reducing risk.\nDefault Security Configuration Pigsty enables these security features by default:\nFeature Default Config Description Password Encryption scram-sha-256 PostgreSQL’s most secure password hash algorithm SSL Support Enabled Clients can optionally use SSL encrypted connections Local CA Auto-generated Self-signed CA issues server certificates HBA Layering Source-based control Different auth strength for different sources Role System Four-tier permissions Read-only/Read-write/Admin/Offline Data Checksums Enabled Detects storage-layer data corruption Audit Logs Enabled Records connections and slow queries Enhanced Configuration Additional configuration enables higher security levels:\nFeature Configuration Method Security Level Password strength check Enable passwordcheck extension Enterprise Enforce SSL HBA uses hostssl Enterprise Client certificates HBA uses cert auth Financial-grade Backup encryption Configure cipher_type Compliance Firewall Configure node_firewall_mode Infrastructure If you only have one minute, remember this diagram:\nflowchart TB subgraph L1[\"Layer 1: Network Security\"] L1A[\"Firewall + SSL/TLS Encryption + HAProxy Proxy\"] L1B[\"Who can connect? Is the connection encrypted?\"] end subgraph L2[\"Layer 2: Authentication\"] L2A[\"HBA Rules + SCRAM-SHA-256 Passwords + Certificate Auth\"] L2B[\"Who are you? How do you prove it?\"] end subgraph L3[\"Layer 3: Access Control\"] L3A[\"Role System + Object Permissions + Database Isolation\"] L3B[\"What can you do? What data can you access?\"] end subgraph L4[\"Layer 4: Data Security\"] L4A[\"Data Checksums + Backup Encryption + Audit Logs\"] L4B[\"Is data intact? Are operations logged?\"] end L1 --\u003e L2 --\u003e L3 --\u003e L4 Core Value: Enterprise-grade security configuration out of the box, best practices enabled by default, additional configuration achieves SOC 2 compliance.\nContents Section Description Core Question Security Overview Security capability overview and checklist What’s the overall security architecture? Authentication HBA rules, password policies, certificate auth How to verify user identity? Access Control Role system, permission model, database isolation How to control user permissions? Encrypted Communication SSL/TLS, local CA, certificate management How to protect data in transit? Compliance Checklist Detailed SOC2 mapping How to meet compliance requirements? Why Security Matters The Cost of Data Breaches flowchart LR Breach[\"Data Breach\"] subgraph Direct[\"Direct Losses\"] D1[\"Regulatory Fines\u003cbr/\u003eGDPR up to 4% global revenue\"] D2[\"Legal Costs\"] D3[\"Customer Compensation\"] end subgraph Indirect[\"Indirect Losses\"] I1[\"Brand Reputation Damage\"] I2[\"Customer Trust Loss\"] I3[\"Business Disruption\"] end subgraph Compliance[\"Compliance Risk\"] C1[\"Liability\"] C2[\"SOC 2: Certification Revocation\"] C3[\"Industry Access: Banned from Operating\"] end Breach --\u003e Direct Breach --\u003e Indirect Breach --\u003e Compliance Default Users and Passwords Pigsty creates these system users by default:\nUser Purpose Default Password Post-Deploy Action postgres System superuser No password (local only) Keep passwordless dbuser_dba Admin user DBUser.DBA Must change dbuser_monitor Monitor user DBUser.Monitor Must change replicator Replication user DBUser.Replicator Must change # pigsty.yml - Change default passwords pg_admin_password: 'YourSecurePassword123!' pg_monitor_password: 'AnotherSecurePass456!' pg_replication_password: 'ReplicationPass789!' Important: After production deployment, immediately change these default passwords!\nRole and Permission System Pigsty provides a four-tier role system out of the box:\nflowchart TB subgraph Admin[\"dbrole_admin (Admin)\"] A1[\"Inherits dbrole_readwrite\"] A2[\"Can CREATE/DROP/ALTER objects (DDL)\"] A3[\"For: Business admins, apps needing table creation\"] end subgraph RW[\"dbrole_readwrite (Read-Write)\"] RW1[\"Inherits dbrole_readonly\"] RW2[\"Can INSERT/UPDATE/DELETE\"] RW3[\"For: Production business accounts\"] end subgraph RO[\"dbrole_readonly (Read-Only)\"] RO1[\"Can SELECT all tables\"] RO2[\"For: Reporting, data analysis\"] end subgraph Offline[\"dbrole_offline (Offline)\"] OFF1[\"Can only access offline instances\"] OFF2[\"For: ETL, personal analysis, slow queries\"] end Admin --\u003e |inherits| RW RW --\u003e |inherits| RO Creating Business Users pg_users: # Read-only user - for reporting - name: dbuser_report password: ReportUser123 roles: [dbrole_readonly] pgbouncer: true # Read-write user - for production - name: dbuser_app password: AppUser456 roles: [dbrole_readwrite] pgbouncer: true # Admin user - for DDL operations - name: dbuser_admin password: AdminUser789 roles: [dbrole_admin] pgbouncer: true HBA Access Control HBA (Host-Based Authentication) controls “who can connect from where”:\nflowchart LR subgraph Sources[\"Connection Sources\"] S1[\"Local Socket\"] S2[\"localhost\"] S3[\"Intranet CIDR\"] S4[\"Admin Nodes\"] S5[\"External\"] end subgraph Auth[\"Auth Methods\"] A1[\"ident/peer\u003cbr/\u003eOS user mapping, most secure\"] A2[\"scram-sha-256\u003cbr/\u003ePassword auth\"] A3[\"scram-sha-256 + SSL\u003cbr/\u003eEnforce SSL\"] end S1 --\u003e A1 S2 --\u003e A2 S3 --\u003e A2 S4 --\u003e A3 S5 --\u003e A3 Note[\"Rules matched in order\u003cbr/\u003eFirst matching rule applies\"] Custom HBA Rules pg_hba_rules: # Allow app servers from intranet - {user: dbuser_app, db: mydb, addr: '10.10.10.0/24', auth: scram-sha-256} # Force SSL for certain users - {user: admin, db: all, addr: world, auth: ssl} # Require certificate auth (highest security) - {user: secure_user, db: all, addr: world, auth: cert} Encrypted Communication SSL/TLS Architecture sequenceDiagram participant Client as Client participant Server as PostgreSQL Client-\u003e\u003eServer: 1. ClientHello Server-\u003e\u003eClient: 2. ServerHello Server-\u003e\u003eClient: 3. Server Certificate Client-\u003e\u003eServer: 4. Client Key Client-\u003e\u003eServer: 5. Encrypted Channel Established Server-\u003e\u003eClient: 5. Encrypted Channel Established rect rgb(200, 255, 200) Note over Client,Server: Encrypted Data Transfer Client-\u003e\u003eServer: 6. Application Data (encrypted) Server-\u003e\u003eClient: 6. Application Data (encrypted) end Note over Client,Server: Prevents eavesdropping, tampering, verifies server identity Local CA Pigsty automatically generates a local CA and issues certificates:\n/etc/pki/ ├── ca.crt # CA certificate (public) ├── ca.key # CA private key (keep secret!) └── server.crt/key # Server certificate/key Important: Securely back up ca.key—if lost, all certificates must be reissued!\nCompliance Mapping SOC 2 Type II Control Point Pigsty Support Description CC6.1 Logical Access Control Yes HBA + Role System CC6.6 Transmission Encryption Yes SSL/TLS CC7.2 System Monitoring Yes Prometheus + Grafana CC9.1 Business Continuity Yes HA + PITR A1.2 Data Recovery Yes pgBackRest Backup Legend: Yes = Default satisfaction · Partial = Needs additional config\nSecurity Checklist Before Deployment Prepare strong passwords (use password manager) Plan network partitions (intranet/external CIDRs) Decide SSL strategy (self-signed/external CA) After Deployment (Required) Change all default passwords Verify HBA rules match expectations Test SSL connections work Configure auth failure alerts Securely back up CA private key Regular Maintenance Audit user permissions Check for expired accounts Update certificates (if needed) Review audit logs Quick Config Examples Production Security Configuration # pigsty.yml - Production security config example all: vars: # Change default passwords (required!) pg_admin_password: 'SecureDBAPassword2024!' pg_monitor_password: 'SecureMonitorPass2024!' pg_replication_password: 'SecureReplPass2024!' # Enable password strength check pg_libs: 'passwordcheck, pg_stat_statements, auto_explain' # Custom HBA rules pg_hba_rules: # App servers - {user: app, db: appdb, addr: '10.10.10.0/24', auth: scram-sha-256} # Admin enforce SSL - {user: dbuser_dba, db: all, addr: world, auth: ssl} Financial-Grade Security Configuration # Financial-grade config - enable certificate auth pg_hba_rules: # Trading system uses certificate auth - {user: trade_user, db: trade, addr: world, auth: cert} # Other systems use SSL + password - {user: all, db: all, addr: world, auth: ssl} # Enable backup encryption pgbackrest_repo: minio: cipher_type: aes-256-cbc cipher_pass: 'YourBackupEncryptionKey' Next Steps Deep dive into security configuration details:\nSecurity Overview: Overall security architecture and checklist Authentication: HBA rules and password policies Access Control: Role system and permission model Encrypted Communication: SSL/TLS and certificate management Compliance Checklist: Detailed SOC2 mapping Related topics:\nHigh Availability: Business continuity assurance Backup \u0026 Recovery: Data recovery capabilities Observability: Security event monitoring ","categories":["Concept"],"description":"Authentication, access control, encrypted communication, audit logs—meeting SOC2 compliance requirements.","excerpt":"Authentication, access control, encrypted communication, audit …","ref":"/docs/concept/sec/","tags":"","title":"Security and Compliance"},{"body":"Pigsty enables security best practices by default: using SSL to encrypt network traffic and HTTPS for web interfaces.\nTo achieve this, Pigsty includes a local self-signed CA for issuing SSL certificates and encrypting network communications.\nBy default, SSL and HTTPS are enabled but not enforced. For environments with higher security requirements, you can enforce SSL and HTTPS usage.\nLocal CA During initialization, Pigsty generates a self-signed CA in the Pigsty source directory (~/pigsty) on the ADMIN node. This CA can be used for SSL, HTTPS, digital signatures, issuing database client certificates, and advanced security features.\nEach Pigsty deployment uses a unique CA—CAs from different Pigsty deployments are not mutually trusted.\nThe local CA consists of two files, located in the files/pki/ca directory by default:\nca.crt: Self-signed CA root certificate, distributed to all managed nodes for certificate verification. ca.key: CA private key for issuing certificates and verifying CA identity—keep this file secure and prevent leakage! Protect the CA Private Key Keep the CA private key file safe—don’t lose it or leak it. We recommend encrypting and backing up this file after completing Pigsty installation.\nUsing an Existing CA If you already have your own CA PKI infrastructure, Pigsty can be configured to use your existing CA.\nSimply place your CA public key and private key files in the files/pki/ca directory:\nfiles/pki/ca/ca.key # Core CA private key file, must exist; if missing, a new one is randomly generated files/pki/ca/ca.crt # If certificate file is missing, Pigsty auto-generates a new root certificate from the CA private key When Pigsty executes the deploy.yml or infra.yml playbooks, if a ca.key private key file exists in files/pki/ca, the existing CA will be used. Since ca.crt can be generated from the ca.key private key, Pigsty will automatically regenerate the root certificate file if it’s missing.\nNote When Using Existing CA You can set the ca_method parameter to copy to ensure Pigsty errors out and stops if it can’t find a local CA, rather than auto-generating a new self-signed CA.\nTrusting the CA During Pigsty installation, ca.crt is distributed to all nodes at /etc/pki/ca.crt during the node_ca task in the node.yml playbook.\nEL-family and Debian-family operating systems have different default trusted CA certificate paths, so the distribution path and update methods differ:\nTrust CA Certificate EL Debian / Ubuntu rm -rf /etc/pki/ca-trust/source/anchors/ca.crt ln -s /etc/pki/ca.crt /etc/pki/ca-trust/source/anchors/ca.crt /bin/update-ca-trust rm -rf /usr/local/share/ca-certificates/ca.crt ln -s /etc/pki/ca.crt /usr/local/share/ca-certificates/ca.crt /usr/sbin/update-ca-certificates Pigsty issues HTTPS certificates for domain names used by web systems on infrastructure nodes by default, allowing HTTPS access to Pigsty’s web interfaces.\nIf you want to avoid “untrusted CA certificate” warnings in client browsers, distribute ca.crt to the trusted certificate directory on client machines.\nYou can double-click the ca.crt file to add it to your system keychain. For example, on MacOS, open “Keychain Access,” search for pigsty-ca, and set it to “trust” this root certificate.\nViewing Certificate Contents Use the following command to view the Pigsty CA certificate contents:\nopenssl x509 -text -in /etc/pki/ca.crt Local CA Root Certificate Content Example Certificate: Data: Version: 3 (0x2) Serial Number: 50:29:e3:60:96:93:f4:85:14:fe:44:81:73:b5:e1:09:2a:a8:5c:0a Signature Algorithm: sha256WithRSAEncryption Issuer: O=pigsty, OU=ca, CN=pigsty-ca Validity Not Before: Feb 7 00:56:27 2023 GMT Not After : Jan 14 00:56:27 2123 GMT Subject: O=pigsty, OU=ca, CN=pigsty-ca Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (4096 bit) Modulus: 00:c1:41:74:4f:28:c3:3c:2b:13:a2:37:05:87:31: .... e6:bd:69:a5:5b:e3:b4:c0:65:09:6e:84:14:e9:eb: 90:f7:61 Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Subject Alternative Name: DNS:pigsty-ca X509v3 Key Usage: Digital Signature, Certificate Sign, CRL Sign X509v3 Basic Constraints: critical CA:TRUE, pathlen:1 X509v3 Subject Key Identifier: C5:F6:23:CE:BA:F3:96:F6:4B:48:A5:B1:CD:D4:FA:2B:BD:6F:A6:9C Signature Algorithm: sha256WithRSAEncryption Signature Value: 89:9d:21:35:59:6b:2c:9b:c7:6d:26:5b:a9:49:80:93:81:18: .... 9e:dd:87:88:0d:c4:29:9e -----BEGIN CERTIFICATE----- ... cXyWAYcvfPae3YeIDcQpng== -----END CERTIFICATE----- Issuing Certificates If you want to use client certificate authentication, you can use the local CA and the cert.yml playbook to manually issue PostgreSQL client certificates.\nSet the certificate’s CN field to the database username:\n./cert.yml -e cn=dbuser_dba ./cert.yml -e cn=dbuser_monitor Issued certificates are generated in files/pki/misc/\u003ccn\u003e.{key,crt} by default.\n","categories":["Concept"],"description":"Pigsty includes a self-signed CA PKI infrastructure for issuing SSL certificates and encrypting network traffic.","excerpt":"Pigsty includes a self-signed CA PKI infrastructure for issuing SSL …","ref":"/docs/concept/sec/ca/","tags":"","title":"Local CA"},{"body":"Security is not a wall, but a fortress. Pigsty adopts a defense-in-depth strategy, building multiple protections across seven layers. Even if one layer is breached, other layers still provide protection.\nOverview Physical Security If physical access is compromised, other layers are rendered useless.\nPhysical security is the most basic layer, involving data center access control, surveillance, environmental control, equipment theft prevention, power protection, etc. As a software solution, Pigsty provides the following protection mechanisms at the physical layer:\nData Checksums Enabling data checksums can detect silent data corruption at the storage layer (e.g., bad disk blocks, memory errors, firmware bugs):\npg_checksum: true # Default enabled since v3.5+ Principle: PostgreSQL calculates checksums when writing each data page and validates when reading. Reports error instead of returning corrupt data when corruption is detected.\nFallback mechanism: Replica provides bad block fallback; damaged data pages on primary can be recovered from replica.\nTransparent Data Encryption For compliance-required scenarios, use PGTDE (PostgreSQL Transparent Data Encryption) extension:\npg_extensions: - pg_tde # Transparent data encryption extension Effect: Data is stored encrypted on disk; data cannot be read even if physical media is stolen.\nNetwork Security Control access and filtering at the packet layer.\nFirewall Pigsty supports node-level firewall configuration, controlling which ports are exposed:\nnode_firewall_mode: zone # off | none | zone node_firewall_intranet: # Intranet CIDR (trust zone) - 10.0.0.0/8 - 172.16.0.0/12 - 192.168.0.0/16 node_firewall_public_port: # Public exposed ports - 22 # SSH - 80 # HTTP - 443 # HTTPS Three Modes:\nMode Description Use Case off Don’t configure firewall (default) Dev environment, existing security groups none Disable firewalld Using external firewall zone Zone mode: trust intranet, restrict public Production recommended SSL/TLS Encryption Pigsty provides SSL/TLS encryption at multiple layers:\nComponent Parameter Default Description PostgreSQL HBA auth pwd Supports ssl (enforced), cert (certificate) Pgbouncer pgbouncer_sslmode disable Optional require / verify-full Patroni patroni_ssl_enabled false REST API encryption Nginx nginx_sslmode enable Optional enforce (force HTTPS) MinIO Enabled by default enabled Uses local CA certificate etcd Enabled by default enabled TLS encrypted communication Security Hardening Configuration:\npatroni_ssl_enabled: true # Enable Patroni SSL pgbouncer_sslmode: require # Force Pgbouncer SSL nginx_sslmode: enforce # Force HTTPS Local CA Certificate Infrastructure Pigsty automatically generates a local CA and issues certificates, no commercial certificates needed:\nfiles/pki/ca/ ├── ca.crt # CA certificate (public, distributed to all nodes) └── ca.key # CA private key (⚠️ Keep secret! Backup securely!) /etc/pki/ # Certificate directory on nodes ├── ca.crt # CA certificate ├── server.crt # Server certificate └── server.key # Server private key ⚠️ Important: Please securely backup ca.key. If lost, all certificates need to be re-issued!\nPerimeter Security Handle security policies at internal/external network boundaries.\nHAProxy Security HAProxy serves as the unified entry point for database traffic, providing these security features:\nhaproxy_admin_password: 'StrongPassword123' # Admin interface password Security Features:\nHealth checks and traffic control to prevent split-brain Connection limits and rate limiting Admin interface password protection Nginx Security Nginx serves as the unified gateway for web services, providing:\nnginx_sslmode: enforce # Force HTTPS infra_portal: # Configure component domains grafana: { domain: g.pigsty.io } alertmanager: { domain: a.pigsty.io } Security Features:\nUnified HTTPS entry, easy to audit Reverse proxy protects backend services Can integrate external authentication (OAuth, LDAP) Host Security OS hardening, patch management, minimal installation.\nSELinux Configuration Pigsty correctly configures SELinux policies to ensure PostgreSQL and other services run properly:\nnode_selinux_mode: permissive # disabled | permissive | enforcing Mode Description Use Case disabled Completely disabled Dev environment permissive Permissive mode (log but don’t block) Production recommended enforcing Enforcing mode High security environments OS Hardening Pigsty follows the principle of least privilege in design:\nFile permissions: Sensitive files (e.g., CA private key) have strictly controlled permissions User groups: PostgreSQL, etcd, etc. run with dedicated users Admin configuration: node_admin_username: dba # Admin username node_admin_sudo: nopasswd # sudo policy System Updates Keep critical security components updated:\nopenssh: SSH service ca-certificates: System root certificates openssl: Encryption library Application Security Database configuration, authentication/authorization, input validation.\nPassword Policy Password Encryption Algorithm pg_pwd_enc: scram-sha-256 # Most secure password hash algorithm Algorithm Security Compatibility Description scram-sha-256 ⭐⭐⭐ PostgreSQL 10+ Recommended, default md5 ⭐ All versions Only for legacy clients Password Strength Check Enable passwordcheck extension to enforce password complexity:\npg_libs: '$libdir/passwordcheck, pg_stat_statements, auto_explain' pg_extensions: - passwordcheck # Enforce password complexity - credcheck # Additional password checks Password Expiration pg_users: - { name: dbuser_app, password: 'SecurePass123', expire_in: 365 } # Expires in 1 year HBA Rules HBA (Host-Based Authentication) controls “who can connect from where, using what authentication method”:\npg_default_hba_rules: - {user: '${dbsu}' ,db: all ,addr: local ,auth: ident ,title: 'dbsu local via ident'} - {user: '${dbsu}' ,db: replication ,addr: local ,auth: ident ,title: 'dbsu repl via ident'} - {user: '${repl}' ,db: replication ,addr: localhost ,auth: pwd ,title: 'repl via localhost'} - {user: '${repl}' ,db: replication ,addr: intra ,auth: pwd ,title: 'repl from intranet'} - {user: '${repl}' ,db: postgres ,addr: intra ,auth: pwd ,title: 'repl from intranet'} - {user: '${monitor}' ,db: all ,addr: localhost ,auth: pwd ,title: 'monitor via localhost'} - {user: '${monitor}' ,db: all ,addr: infra ,auth: pwd ,title: 'monitor from infra'} - {user: '${admin}' ,db: all ,addr: infra ,auth: ssl ,title: 'admin from infra'} - {user: '${admin}' ,db: all ,addr: world ,auth: ssl ,title: 'admin from world'} - {user: '+dbrole_readonly',db: all ,addr: localhost ,auth: pwd ,title: 'read from localhost'} - {user: '+dbrole_readonly',db: all ,addr: intra ,auth: pwd ,title: 'read from intranet'} - {user: '+dbrole_offline' ,db: all ,addr: intra ,auth: pwd ,title: 'offline from intranet'} Authentication Methods:\nAlias Description Security Level ident/peer OS user mapping ⭐⭐⭐ Local only pwd Password auth (scram-sha-256) ⭐⭐ ssl Forced SSL + password ⭐⭐⭐ cert Client certificate auth ⭐⭐⭐⭐ Highest deny Deny access - Listen Address Limit network interfaces PostgreSQL listens on:\npg_listen: '${ip},${vip},${lo}' # Only listen on specific IPs, not 0.0.0.0 Data Security Encryption, backup, audit, integrity protection.\nBackup Encryption pgBackRest supports AES-256 encrypted backups:\npgbackrest_repo: minio: cipher_type: aes-256-cbc # AES-256-CBC encryption cipher_pass: 'pgBR.${pg_cluster}' # Use cluster name as part of password Effect: Backup files are stored encrypted; data cannot be read even if storage is compromised.\nAudit Logs PostgreSQL Audit Extensions pg_extensions: - pgaudit # SQL audit logs - pgauditlogtofile # Audit logs to file - pg_auth_mon # Authentication monitoring - pg_auditor # Audit helper Connection Logs # Configure in pg_parameters log_connections: on # Log connection establishment log_disconnections: on # Log connection termination Slow Query Logs log_min_duration_statement: 1000 # Log queries \u003e1s Point-in-Time Recovery (PITR) Pigsty configures pgBackRest for PITR by default:\npgbackrest_enabled: true pgbackrest_repo: local: # Local backup path: /pg/backup retention_full: 2 minio: # Remote backup path: /pgbackrest retention_full_type: time retention_full: 14 # Retain 14 days User Security Identity authentication, permission management, behavior audit.\nFour-Role Model Pigsty provides out-of-box four-tier permission roles:\nflowchart TB subgraph Admin[\"🔴 dbrole_admin (Administrator)\"] A1[\"Inherits dbrole_readwrite\"] A2[\"Can CREATE/DROP/ALTER (DDL)\"] A3[\"Use case: Business admins, schema-creating apps\"] end subgraph RW[\"🟡 dbrole_readwrite (Read-Write)\"] RW1[\"Inherits dbrole_readonly\"] RW2[\"Can INSERT/UPDATE/DELETE\"] RW3[\"Use case: Production business accounts\"] end subgraph RO[\"🟢 dbrole_readonly (Read-Only)\"] RO1[\"Can SELECT all tables\"] RO2[\"Use case: Reporting, data analysis\"] end subgraph Offline[\"🔵 dbrole_offline (Offline)\"] OFF1[\"Can only access offline instances\"] OFF2[\"Use case: ETL, slow queries, personal analysis\"] end Admin --\u003e |inherits| RW RW --\u003e |inherits| RO Creating Business Users:\npg_users: - { name: dbuser_report, password: 'ReportPass123', roles: [dbrole_readonly] } - { name: dbuser_app, password: 'AppPass456', roles: [dbrole_readwrite] } - { name: dbuser_admin, password: 'AdminPass789', roles: [dbrole_admin] } Default Users and Passwords User Default Password Purpose Post-Deploy Action postgres No password (local only) System superuser Keep without password dbuser_dba DBUser.DBA Admin user Must change dbuser_monitor DBUser.Monitor Monitor user Must change replicator DBUser.Replicator Replication user Must change Auto-Generate Strong Passwords:\n./configure -g # Automatically generate random strong passwords Certificate Authentication Highest security level, requires clients to provide valid certificates:\npg_hba_rules: - {user: admin, db: all, addr: world, auth: cert} # Admin uses cert auth ETCD and MinIO Security Auxiliary components also use RBAC model and TLS encryption:\n# ETCD etcd_root_password: 'Etcd.Root.Strong' # Must change # MinIO minio_access_key: minioadmin minio_secret_key: 'S3User.MinIO.Strong' # Must change minio_users: - { access_key: pgbackrest, secret_key: 'Min10.bAckup', policy: readwrite } - { access_key: dba, secret_key: 'S3User.DBA.Strong', policy: consoleAdmin } Watchdog Protection Prevents split-brain, ensures primary forced shutdown during failover:\npatroni_watchdog_mode: required # off | automatic | required Effect: When Patroni process is abnormal, watchdog forces node restart to avoid dual-primary split-brain.\nCompliance Mapping China MLPS Level 3 (GB/T 22239-2019) Requirement Default Configurable Implementation Unique Identity ✅ ✅ Unique username identifier Password Complexity ⚠️ ✅ passwordcheck extension Password Rotation ⚠️ ✅ expire_in attribute Two-Factor Auth ⚠️ ✅ Certificate + password (auth: cert) Access Control ✅ ✅ HBA + Four-role model Least Privilege ✅ ✅ dbrole_readonly/readwrite/admin Encrypted Communication ✅ ✅ SSL/TLS Audit Logs ✅ ✅ pgaudit + connection logs Data Integrity ✅ ✅ pg_checksum: true Backup \u0026 Recovery ✅ ✅ pgBackRest + PITR SOC 2 Type II Control Point Pigsty Support Implementation CC6.1 Logical Access Control ✅ HBA + RBAC CC6.6 Transmission Encryption ✅ SSL/TLS (can enforce) CC7.2 System Monitoring ✅ Prometheus + Grafana CC9.1 Business Continuity ✅ HA + PITR A1.2 Data Recovery ✅ pgBackRest backup Legend: ✅ Default satisfied · ⚠️ Requires additional configuration\nSecurity Checklist Pre-Deployment Prepare strong passwords (use ./configure -g to auto-generate) Plan network partitioning (intranet/public CIDR) Determine SSL policy (self-signed CA or external CA) Determine whether to enable firewall Post-Deployment (Required) Change all default passwords Verify HBA rules meet expectations Test SSL connections work properly Configure authentication failure alerts Securely backup CA private key Security Enhancement (Optional) Enable passwordcheck extension Force SSL (pgbouncer_sslmode: require) Enable certificate authentication (auth: cert) Enable pgaudit audit logs Enable backup encryption Enable Patroni SSL Enable watchdog Configure firewall rules Enable SELinux enforcing mode What’s Next Deep dive into security configuration details:\n👤 Access Control: Role system and permission model 🔐 Encrypted Communication: SSL/TLS and certificate management Related topics:\n♾️ High Availability: Business continuity assurance ⏰ Backup \u0026 Recovery: Data recovery capabilities ","categories":["Concept"],"description":"How Pigsty provides defense-in-depth across seven security layers, from physical security to user security.","excerpt":"How Pigsty provides defense-in-depth across seven security layers, …","ref":"/docs/concept/sec/level/","tags":"","title":"Seven-Layer Security Model"},{"body":" Pigsty provides an out-of-the-box access control model based on the Role System and Permission System.\nAccess control is important, but many users struggle to implement it properly. Pigsty provides a streamlined access control model that serves as a security baseline for your cluster.\nRole System Pigsty’s default role system includes four default roles and four default users:\nRole Name Attributes Member Of Description dbrole_readonly NOLOGIN Role: Global read-only dbrole_readwrite NOLOGIN dbrole_readonly Role: Global read-write dbrole_admin NOLOGIN pg_monitor,dbrole_readwrite Role: Admin/Object creation dbrole_offline NOLOGIN Role: Restricted read-only postgres SUPERUSER System superuser replicator REPLICATION pg_monitor,dbrole_readonly System replication user dbuser_dba SUPERUSER dbrole_admin PostgreSQL admin user dbuser_monitor pg_monitor PostgreSQL monitor user These roles and users are defined as follows:\npg_default_roles: # Global default roles and system users - { name: dbrole_readonly ,login: false ,comment: role for global read-only access } - { name: dbrole_offline ,login: false ,comment: role for restricted read-only access } - { name: dbrole_readwrite ,login: false ,roles: [dbrole_readonly] ,comment: role for global read-write access } - { name: dbrole_admin ,login: false ,roles: [pg_monitor, dbrole_readwrite] ,comment: role for object creation } - { name: postgres ,superuser: true ,comment: system superuser } - { name: replicator ,replication: true ,roles: [pg_monitor, dbrole_readonly] ,comment: system replicator } - { name: dbuser_dba ,superuser: true ,roles: [dbrole_admin] ,pgbouncer: true ,pool_mode: session, pool_connlimit: 16 ,comment: pgsql admin user } - { name: dbuser_monitor ,roles: [pg_monitor] ,pgbouncer: true ,parameters: {log_min_duration_statement: 1000 } ,pool_mode: session ,pool_connlimit: 8 ,comment: pgsql monitor user } Default Roles Pigsty has four default roles:\nBusiness Read-Only (dbrole_readonly): Role for global read-only access. Use this if other services need read-only access to this database. Business Read-Write (dbrole_readwrite): Role for global read-write access. Production accounts for primary business should have database read-write permissions. Business Admin (dbrole_admin): Role with DDL permissions. Typically used for business administrators or scenarios requiring table creation in applications. Offline Read-Only (dbrole_offline): Restricted read-only access role (can only access offline instances). Usually for personal users or ETL tool accounts. Default roles are defined in pg_default_roles. Unless you know what you’re doing, don’t change the default role names.\n- { name: dbrole_readonly , login: false , comment: role for global read-only access } - { name: dbrole_offline , login: false , comment: role for restricted read-only access (offline instance) } - { name: dbrole_readwrite , login: false , roles: [dbrole_readonly], comment: role for global read-write access } - { name: dbrole_admin , login: false , roles: [pg_monitor, dbrole_readwrite] , comment: role for object creation } Default Users Pigsty also has four default users (system users):\nSuperuser (postgres): Cluster owner and creator, same name as OS dbsu. Replication User (replicator): System user for primary-replica replication. Monitor User (dbuser_monitor): User for monitoring database and connection pool metrics. Admin User (dbuser_dba): Administrator for daily operations and database changes. These 4 default users’ username/password are defined by 4 pairs of dedicated parameters and referenced in many places:\npg_dbsu: OS dbsu name, defaults to postgres. Best not to change it. pg_dbsu_password: dbsu password, default empty means no password. Best not to set it. pg_replication_username: Replication username, defaults to replicator pg_replication_password: Replication password, defaults to DBUser.Replicator pg_admin_username: Admin username, defaults to dbuser_dba pg_admin_password: Admin password plaintext, defaults to DBUser.DBA pg_monitor_username: Monitor username, defaults to dbuser_monitor pg_monitor_password: Monitor password, defaults to DBUser.Monitor Remember to change these passwords in production deployments—don’t use defaults!\npg_dbsu: postgres # Database superuser name, recommended not to change pg_dbsu_password: '' # Database superuser password, recommended to leave empty! pg_replication_username: replicator # System replication username pg_replication_password: DBUser.Replicator # System replication password, must change! pg_monitor_username: dbuser_monitor # System monitor username pg_monitor_password: DBUser.Monitor # System monitor password, must change! pg_admin_username: dbuser_dba # System admin username pg_admin_password: DBUser.DBA # System admin password, must change! Permission System Pigsty has an out-of-the-box permission model that works with default roles.\nAll users can access all schemas. Read-only users (dbrole_readonly) can read from all tables. (SELECT, EXECUTE) Read-write users (dbrole_readwrite) can write to all tables and run DML. (INSERT, UPDATE, DELETE) Admin users (dbrole_admin) can create objects and run DDL. (CREATE, USAGE, TRUNCATE, REFERENCES, TRIGGER) Offline users (dbrole_offline) are similar to read-only but with restricted access—only offline instances (pg_role = 'offline' or pg_offline_query = true) Objects created by admin users will have correct permissions. Default privileges are configured on all databases, including template databases. Database connection permissions are managed by database definition. CREATE privilege on databases and public schema is revoked from PUBLIC by default. Object Privileges Default privileges for newly created objects are controlled by pg_default_privileges:\n- GRANT USAGE ON SCHEMAS TO dbrole_readonly - GRANT SELECT ON TABLES TO dbrole_readonly - GRANT SELECT ON SEQUENCES TO dbrole_readonly - GRANT EXECUTE ON FUNCTIONS TO dbrole_readonly - GRANT USAGE ON SCHEMAS TO dbrole_offline - GRANT SELECT ON TABLES TO dbrole_offline - GRANT SELECT ON SEQUENCES TO dbrole_offline - GRANT EXECUTE ON FUNCTIONS TO dbrole_offline - GRANT INSERT ON TABLES TO dbrole_readwrite - GRANT UPDATE ON TABLES TO dbrole_readwrite - GRANT DELETE ON TABLES TO dbrole_readwrite - GRANT USAGE ON SEQUENCES TO dbrole_readwrite - GRANT UPDATE ON SEQUENCES TO dbrole_readwrite - GRANT TRUNCATE ON TABLES TO dbrole_admin - GRANT REFERENCES ON TABLES TO dbrole_admin - GRANT TRIGGER ON TABLES TO dbrole_admin - GRANT CREATE ON SCHEMAS TO dbrole_admin Objects newly created by admins will have the above privileges by default. Use \\ddp+ to view these default privileges:\nType Access Privileges Function =X dbrole_readonly=X dbrole_offline=X dbrole_admin=X Schema dbrole_readonly=U dbrole_offline=U dbrole_admin=UC Sequence dbrole_readonly=r dbrole_offline=r dbrole_readwrite=wU dbrole_admin=rwU Table dbrole_readonly=r dbrole_offline=r dbrole_readwrite=awd dbrole_admin=arwdDxt Default Privileges The SQL statement ALTER DEFAULT PRIVILEGES lets you set privileges for future objects. It doesn’t affect existing objects or objects created by non-admin users.\nIn Pigsty, default privileges are defined for three roles:\n{% for priv in pg_default_privileges %} ALTER DEFAULT PRIVILEGES FOR ROLE {{ pg_dbsu }} {{ priv }}; {% endfor %} {% for priv in pg_default_privileges %} ALTER DEFAULT PRIVILEGES FOR ROLE {{ pg_admin_username }} {{ priv }}; {% endfor %} -- For other business admins, they should SET ROLE dbrole_admin before executing DDL {% for priv in pg_default_privileges %} ALTER DEFAULT PRIVILEGES FOR ROLE \"dbrole_admin\" {{ priv }}; {% endfor %} To maintain correct object permissions, you must execute DDL with admin users:\n{{ pg_dbsu }}, defaults to postgres {{ pg_admin_username }}, defaults to dbuser_dba Business admin users granted dbrole_admin role (using SET ROLE to switch to dbrole_admin) Using postgres as global object owner is wise. If creating objects as business admin, use SET ROLE dbrole_admin before creation to maintain correct permissions.\nDatabase Privileges In Pigsty, database-level privileges are covered in database definition.\nDatabases have three privilege levels: CONNECT, CREATE, TEMP, and a special ‘privilege’: OWNERSHIP.\n- name: meta # Required, `name` is the only required field owner: postgres # Optional, database owner, defaults to postgres allowconn: true # Optional, allow connections, default true revokeconn: false # Optional, revoke public connect privilege, default false If owner parameter exists, it becomes database owner instead of {{ pg_dbsu }} If revokeconn is false, all users have database CONNECT privilege (default behavior) If revokeconn is explicitly true: Database CONNECT privilege is revoked from PUBLIC CONNECT privilege is explicitly granted to {{ pg_replication_username }}, {{ pg_monitor_username }}, {{ pg_admin_username }} CONNECT privilege with GRANT OPTION is granted to database owner revokeconn can isolate cross-database access within the same cluster CREATE Privilege For security, Pigsty revokes CREATE privilege on databases from PUBLIC by default. This is also default behavior since PostgreSQL 15.\nDatabase owners can always adjust CREATE privileges based on actual needs.\n","categories":["Concept"],"description":"Pigsty provides standard security practices with an out-of-the-box role and permission model.","excerpt":"Pigsty provides standard security practices with an out-of-the-box …","ref":"/docs/concept/sec/ac/","tags":"","title":"Access Control"},{"body":"Pigsty uses a scalable architecture design, suitable for both large-scale production environments and single-node development/demo environments. This guide focuses on the latter.\nIf you intend to learn about Pigsty, you can start with the Quick Start single-node deployment. A Linux virtual machine with 1C/2G is sufficient to run Pigsty.\nYou can use a Linux MiniPC, free/discounted virtual machines provided by cloud providers, Windows WSL, or create a virtual machine on your own laptop for Pigsty deployment. Pigsty provides out-of-the-box Vagrant templates and Terraform templates to help you provision Linux VMs with one click locally or in the cloud.\nThe single-node version of Pigsty includes all core features: 440+ PG extensions, self-contained Grafana/Victoria monitoring, IaC provisioning capabilities, and local PITR point-in-time recovery. If you have external object storage (for PostgreSQL PITR backup), then for scenarios like demos, personal websites, and small services, even a single-node environment can provide a certain degree of data persistence guarantee. However, single-node cannot achieve High Availability—automatic failover requires at least 3 nodes.\nIf you want to install Pigsty in an environment without internet connection, please refer to the Offline Install mode. If you only need the PostgreSQL database itself, please refer to the Slim Install mode. If you are ready to start serious multi-node production deployment, please refer to the Deployment Guide.\nQuick Start Prepare a node with compatible Linux system, and execute as an admin user with passwordless ssh and sudo privileges:\ncurl -fsSL https://repo.pigsty.io/get | bash # Install Pigsty and dependencies cd ~/pigsty; ./configure -g # Generate config (with 1-node template, -g generates random passwords) ./deploy.yml # Execute deployment playbook Yes, it’s that simple. You can use pre-configured templates to bring up Pigsty with one click without understanding any details.\nNext, you can explore the Graphical User Interface, access PostgreSQL database services; or perform configuration customization and execute playbooks to deploy more clusters.\n","categories":["Tutorial"],"description":"Deploy Pigsty single-node version on your laptop/cloud server, access DB and Web UI","excerpt":"Deploy Pigsty single-node version on your laptop/cloud server, access …","ref":"/docs/setup/","tags":"","title":"Get Started"},{"body":"This is the Pigsty single-node install guide. For multi-node HA prod deployment, refer to the Deployment docs.\nPigsty single-node installation consists of three steps: Install, Configure, and Deploy.\nSummary Prepare a node with compatible OS, and run as an admin user with nopass ssh and sudo:\npigsty.io (Global) pigsty.cc (Mirror) curl -fsSL https://repo.pigsty.io/get | bash; curl -fsSL https://repo.pigsty.cc/get | bash; This command runs the install script, downloads and extracts Pigsty source to your home directory and installs dependencies. Then complete Configure and Deploy:\ncd ~/pigsty # Enter Pigsty directory ./configure -g # Generate config file (optional, skip if you know how to configure) ./deploy.yml # Execute deployment playbook based on generated config After installation, access the Web UI via IP/domain + port 80/443 through Nginx, and access the default PostgreSQL service via port 5432.\nThe complete process takes 3–10 minutes depending on server specs/network. Offline installation speeds this up significantly; for monitoring-free setups, use Slim Install for even faster deployment.\nVideo Example: Online Single-Node Installation (Debian 13, x86_64)\nPrepare Installing Pigsty involves some preparation work. Here’s a checklist.\nFor single-node installations, many constraints can be relaxed—typically you only need to know your IP address. If you don’t have a static IP, use 127.0.0.1.\nItem Requirement Item Requirement Node 1-node, at least 1C2G, no upper limit Disk /data mount point, xfs recommended OS Linux x86_64 / aarch64, EL/Debian/Ubuntu Network Static IPv4; single-node without fixed IP can use 127.0.0.1 SSH nopass SSH login via public key SUDO sudo privilege, preferably with nopass option Typically, you only need to focus on your local IP address—as an exception, for single-node deployment, use 127.0.0.1 if no static IP available.\nInstall Use the following commands to auto-install Pigsty source to ~/pigsty (recommended). Deployment dependencies (Ansible) are installed automatically.\npigsty.io (Global) pigsty.cc (Mirror) curl -fsSL https://repo.pigsty.io/get | bash # Install latest stable version curl -fsSL https://repo.pigsty.io/get | bash -s v4.0.0 # Install specific version curl -fsSL https://repo.pigsty.cc/get | bash # Install latest stable version curl -fsSL https://repo.pigsty.cc/get | bash -s v4.0.0 # Install specific version If you prefer not to run a remote script, you can manually download or clone the source. When using git, always checkout a specific version before use.\ngit clone https://github.com/pgsty/pigsty; cd pigsty; git checkout v4.0.0-b4; # Always checkout a specific version when using git For manual download/clone installations, run the bootstrap script to install Ansible and other dependencies. You can also install them yourself.\n./bootstrap # Install ansible for subsequent deployment Configure In Pigsty, deployment blueprints are defined by the inventory, the pigsty.yml configuration file. You can customize through declarative configuration.\nPigsty provides the configure script as an optional configuration wizard, which generates an inventory with good defaults based on your environment and input:\n./configure -g # Use config wizard to generate config with random passwords The generated config file is at ~/pigsty/pigsty.yml by default. Review and customize as needed before installation.\nMany configuration templates are available for reference. You can skip the wizard and directly edit pigsty.yml:\n./configure # Default template, install PG 18 with essential extensions ./configure -v 17 # Use PG 17 instead of default PG 18 ./configure -c rich # Create local repo, download all extensions, install major ones ./configure -c slim # Minimal install template, use with ./slim.yml playbook ./configure -c app/supa # Use app/supa self-hosted Supabase template ./configure -c ivory # Use IvorySQL kernel instead of native PG ./configure -i 10.11.12.13 # Explicitly specify primary IP address ./configure -r china # Use China mirrors instead of default repos ./configure -c ha/full -s # Use 4-node sandbox template, skip IP replacement/detection Example configure output $ ./configure configure pigsty v4.0.0 begin [ OK ] region = default [ OK ] kernel = Linux [ OK ] machine = x86_64 [ OK ] package = rpm,dnf [ OK ] vendor = rocky (Rocky Linux) [ OK ] version = 9 (9.6) [ OK ] sudo = vagrant ok [ OK ] ssh = vagrant@127.0.0.1 ok [WARN] Multiple IP address candidates found: (1) 192.168.121.24\tinet 192.168.121.24/24 brd 192.168.121.255 scope global dynamic noprefixroute eth0 (2) 10.10.10.12\tinet 10.10.10.12/24 brd 10.10.10.255 scope global noprefixroute eth1 [ IN ] INPUT primary_ip address (of current meta node, e.g 10.10.10.10): =\u003e 10.10.10.12 # \u003c------- INPUT YOUR PRIMARY IPV4 ADDRESS HERE! [ OK ] primary_ip = 10.10.10.12 (from input) [ OK ] admin = vagrant@10.10.10.12 ok [ OK ] mode = meta (el9) [ OK ] locale = C.UTF-8 [ OK ] configure pigsty done proceed with ./deploy.yml Common configure arguments:\nArgument Description -i|--ip Primary internal IP of current host, replaces placeholder 10.10.10.10 -c|--conf Config template name relative to conf/, without .yml suffix -v|--version PostgreSQL major version: 13, 14, 15, 16, 17, 18 -r|--region Upstream repo region for faster downloads: (default|china|europe) -n|--non-interactive Use command-line args for primary IP, skip interactive wizard -x|--proxy Use current env vars to configure proxy_env If your machine has multiple IPs bound, use -i|--ip \u003cipaddr\u003e to explicitly specify the primary IP, or provide it in the interactive prompt. The script replaces the placeholder 10.10.10.10 with your node’s primary IPv4 address. Choose a static IP; do not use public IPs.\nChange default passwords! We strongly recommend modifying default passwords and credentials in the config file before installation. See Security Recommendations for details.\nDeploy Pigsty’s deploy.yml playbook applies the blueprint from Configure to target nodes.\n./deploy.yml # Deploy all defined modules on current node at once Example deployment output ...... TASK [pgsql : pgsql init done] ************************************************* ok: [10.10.10.11] =\u003e { \"msg\": \"postgres://10.10.10.11/postgres | meta | dbuser_meta dbuser_view \" } ...... TASK [pg_monitor : load grafana datasource meta] ******************************* changed: [10.10.10.11] PLAY RECAP ********************************************************************* 10.10.10.11 : ok=302 changed=232 unreachable=0 failed=0 skipped=65 rescued=0 ignored=1 localhost : ok=6 changed=3 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 When you see pgsql init done, PLAY RECAP and similar output at the end, installation is complete!\nUpstream repo changes may cause online installation failures! Upstream repos used by Pigsty (like Linux/PGDG repos) can sometimes enter a broken state due to improper updates, causing deployment failures (this has happened multiple times)! You can wait for upstream fixes or use pre-made offline packages to solve this.\nAvoid re-running the deployment playbook! Warning: Running deploy.yml again on an existing deployment may restart services and overwrite configurations!\nInterface After single-node installation, you typically have four modules installed on the current node: PGSQL, INFRA, NODE, and ETCD.\nID NODE PGSQL INFRA ETCD 1 10.10.10.10 pg-meta-1 infra-1 etcd-1 The INFRA module provides a graphical management interface, accessible via Nginx on ports 80/443.\nThe PGSQL module provides a PostgreSQL database server, listening on 5432, also accessible via Pgbouncer/HAProxy proxies.\nMore Use the current node as a base to deploy and monitor more clusters: add cluster definitions to the inventory and run:\nbin/node-add pg-test # Add the 3 nodes of cluster pg-test to Pigsty management bin/pgsql-add pg-test # Initialize a 3-node pg-test HA PG cluster bin/redis-add redis-ms # Initialize Redis cluster: redis-ms Most modules require the NODE module installed first. See available modules for details:\nPGSQL, INFRA, NODE, ETCD, MINIO, REDIS, FERRET, DOCKER……\n","categories":["Tutorial"],"description":"Get started with Pigsty—complete single-node install on a fresh Linux host!","excerpt":"Get started with Pigsty—complete single-node install on a fresh Linux …","ref":"/docs/setup/install/","tags":"","title":"Single-Node Installation"},{"body":"After single-node installation, you’ll have the INFRA module installed on the current node, which includes an out-of-the-box Nginx web server.\nThe default server configuration provides a WebUI graphical interface for displaying monitoring dashboards and unified proxy access to other component web interfaces.\nAccess You can access this graphical interface by entering the deployment node’s IP address in your browser. By default, Nginx serves on standard ports 80/443.\nDirect IP Access Domain (HTTP) Domain (HTTPS) Demo http://10.10.10.10 http://i.pigsty https://i.pigsty https://demo.pigsty.io Monitoring To access Pigsty’s monitoring system dashboards (Grafana), visit the /ui endpoint on the server.\nDirect IP Access Domain (HTTP) Domain (HTTPS) Demo http://10.10.10.10/ui http://i.pigsty/ui https://i.pigsty/ui https://demo.pigsty.io/ui If your service is exposed to Internet or office network, we recommend accessing via domain names and enabling HTTPS encryption—only minimal configuration is needed.\nEndpoints By default, Nginx exposes the following endpoints via different paths on the default server at ports 80/443:\nEndpoint Component Native Port Description Public Demo / Nginx 80/443 Homepage, local repo, file service demo.pigsty.io /ui/ Grafana 3000 Grafana dashboard portal demo.pigsty.io/ui/ /vmetrics/ VictoriaMetrics 8428 Time series database Web UI demo.pigsty.io/vmetrics/ /vlogs/ VictoriaLogs 9428 Log database Web UI demo.pigsty.io/vlogs/ /vtraces/ VictoriaTraces 10428 Distributed tracing Web UI demo.pigsty.io/vtraces/ /vmalert/ VMAlert 8880 Alert rule management demo.pigsty.io/vmalert/ /alertmgr/ AlertManager 9059 Alert management Web UI demo.pigsty.io/alertmgr/ /blackbox/ Blackbox 9115 Blackbox exporter /haproxy/* HAProxy 9101 Load balancer admin Web UI /pev PEV2 80 PostgreSQL execution plan visualizer demo.pigsty.io/pev /nginx Nginx 80 Nginx status page (for metrics) Domain Access If you have your own domain name, you can point it to Pigsty server’s IP address to access various services via domain.\nIf you want to enable HTTPS, you should modify the home server configuration in the infra_portal parameter:\nall: vars: infra_portal: home : { domain: i.pigsty } # Replace i.pigsty with your domain all: vars: infra_portal: # domain specifies the domain name # certbot parameter specifies certificate name home : { domain: demo.pigsty.io ,certbot: mycert } You can run make cert command after deployment to apply for a free Let’s Encrypt certificate for the domain. If you don’t define the certbot field, Pigsty will use the local CA to issue a self-signed HTTPS certificate by default. In this case, you must first trust Pigsty’s self-signed CA to access normally in your browser.\nYou can also mount local directories and other upstream services to Nginx. For more management details, refer to INFRA Management - Nginx.\n","categories":["Tutorial"],"description":"Explore Pigsty's Web graphical management interface, Grafana dashboards, and how to access them via domain names and HTTPS.","excerpt":"Explore Pigsty's Web graphical management interface, Grafana …","ref":"/docs/setup/webui/","tags":"","title":"Web Interface"},{"body":"PostgreSQL (abbreviated as PG) is the world’s most advanced and popular open-source relational database. Use it to store and retrieve multi-modal data.\nThis guide is for developers with basic Linux CLI experience but not very familiar with PostgreSQL, helping you quickly get started with PG in Pigsty.\nWe assume you’re a personal user deploying in the default single-node mode. For prod multi-node HA cluster access, refer to Prod Service Access.\nBasics In the default single-node installation template, you’ll create a PostgreSQL database cluster named pg-meta on the current node, with only one primary instance.\nPostgreSQL listens on port 5432, and the cluster has a preset database meta available for use.\nAfter installation, exit the current admin user ssh session and re-login to refresh environment variables. Then simply type p and press Enter to access the database cluster via the psql CLI tool:\nvagrant@pg-meta-1:~$ p psql (18.1 (Ubuntu 18.1-1.pgdg24.04+2)) Type \"help\" for help. postgres=# You can also switch to the postgres OS user and execute psql directly to connect to the default postgres admin database.\nConnecting to Database To access a PostgreSQL database, use a CLI tool or graphical client and fill in the PostgreSQL connection string:\npostgres://username:password@host:port/dbname Some drivers and tools may require you to fill in these parameters separately. The following five are typically required:\nParameter Description Example Value Notes host Database server address 10.10.10.10 Replace with your node IP or domain; can omit for localhost port Port number 5432 PG default port, can be omitted username Username dbuser_dba Pigsty default database admin password Password DBUser.DBA Pigsty default admin password (change this!) dbname Database name meta Default template database name For personal use, you can directly use the Pigsty default database superuser dbuser_dba for connection and management. The dbuser_dba has full database privileges. By default, if you specified the configure -g parameter when configuring Pigsty, the password will be randomly generated and saved in ~/pigsty/pigsty.yml:\ncat ~/pigsty/pigsty.yml | grep pg_admin_password Default Accounts Pigsty’s default single-node template presets the following database users, ready to use out of the box:\nUsername Password Role Purpose dbuser_dba DBUser.DBA Superuser Database admin (change this!) dbuser_meta DBUser.Meta Business admin App R/W (change this!) dbuser_view DBUser.Viewer Read-only user Data viewing (change this!) For example, you can connect to the meta database in the pg-meta cluster using three different connection strings with three different users:\npostgres://dbuser_dba:DBUser.DBA@10.10.10.10:5432/meta postgres://dbuser_meta:DBUser.Meta@10.10.10.10:5432/meta postgres://dbuser_view:DBUser.View@10.10.10.10:5432/meta Note: These default passwords are automatically replaced with random strong passwords when using configure -g. Remember to replace the IP address and password with actual values.\nUsing CLI Tools psql is the official PostgreSQL CLI client tool, powerful and the first choice for DBAs and developers.\nOn a server with Pigsty deployed, you can directly use psql to connect to the local database:\n# Simplest way: use postgres system user for local connection (no password needed) sudo -u postgres psql # Use connection string (recommended, most universal) psql 'postgres://dbuser_dba:DBUser.DBA@10.10.10.10:5432/meta' # Use parameter form psql -h 10.10.10.10 -p 5432 -U dbuser_dba -d meta # Use env vars to avoid password appearing in command line export PGPASSWORD='DBUser.DBA' psql -h 10.10.10.10 -p 5432 -U dbuser_dba -d meta After successful connection, you’ll see a prompt like this:\npsql (18.1) Type \"help\" for help. meta=# Common psql Commands\nAfter entering psql, you can execute SQL statements or use meta-commands starting with \\:\nCommand Description Command Description Ctrl+C Interrupt query Ctrl+D Exit psql \\? Show all meta commands \\h Show SQL command help \\l List all databases \\c dbname Switch to database \\d table View table structure \\d+ table View table details \\du List all users/roles \\dx List installed extensions \\dn List all schemas \\dt List all tables Executing SQL\nIn psql, directly enter SQL statements ending with semicolon ;:\n-- Check PostgreSQL version SELECT version(); -- Check current time SELECT now(); -- Create a test table CREATE TABLE test (id SERIAL PRIMARY KEY, name TEXT, created_at TIMESTAMPTZ DEFAULT now()); -- Insert data INSERT INTO test (name) VALUES ('hello'), ('world'); -- Query data SELECT * FROM test; -- Drop test table DROP TABLE test; Using Graphical Clients If you prefer graphical interfaces, here are some popular PostgreSQL clients:\nGrafana\nPigsty’s INFRA module includes Grafana with a pre-configured PostgreSQL data source (Meta). You can directly query the database using SQL from the Grafana Explore panel through the browser graphical interface, no additional client tools needed.\nGrafana’s default username is admin, and the password can be found in the grafana_admin_password field in the inventory (default pigsty).\nDataGrip\nDataGrip is a professional database IDE from JetBrains, with powerful features. IntelliJ IDEA’s built-in Database Console can also connect to PostgreSQL in a similar way.\nDBeaver\nDBeaver is a free open-source universal database tool supporting almost all major databases. It’s a cross-platform desktop client.\npgAdmin\npgAdmin is the official PostgreSQL-specific GUI tool from PGDG, available through browser or as a desktop client.\nPigsty provides a configuration template for one-click pgAdmin service deployment using Docker in Software Template: pgAdmin.\nViewing Monitoring Dashboards Pigsty provides many PostgreSQL monitoring dashboards, covering everything from cluster overview to single-table analysis.\nWe recommend starting with PGSQL Overview. Many elements in the dashboards are clickable, allowing you to drill down layer by layer to view details of each cluster, instance, database, and even internal database objects like tables, indexes, and functions.\nTrying Extensions One of PostgreSQL’s most powerful features is its extension ecosystem. Extensions can add new data types, functions, index methods, and more to the database.\nPigsty provides an unparalleled 440+ extensions in the PG ecosystem, covering 16 major categories including time-series, geographic, vector, and full-text search—install with one click. Start with three powerful and commonly used extensions that are automatically installed in Pigsty’s default template. You can also install more extensions as needed.\npostgis: Geographic information system for processing maps and location data pgvector: Vector database supporting AI embedding vector similarity search timescaledb: Time-series database for efficient storage and querying of time-series data \\dx -- psql meta command, list installed extensions TABLE pg_available_extensions; -- Query installed, available extensions CREATE EXTENSION postgis; -- Enable postgis extension Next Steps Congratulations on completing the PostgreSQL basics! Next, you can start configuring and customizing your database.\n","categories":["Tutorial"],"description":"Get started with PostgreSQL—connect using CLI and graphical clients","excerpt":"Get started with PostgreSQL—connect using CLI and graphical clients","ref":"/docs/setup/pgsql/","tags":"","title":"Getting Started with PostgreSQL"},{"body":"Besides using the configuration wizard to auto-generate configs, you can write Pigsty config files from scratch. This tutorial guides you through building a complex inventory step by step.\nIf you define everything in the inventory upfront, a single deploy.yml playbook run completes all deployment—but it hides the details.\nThis doc breaks down all modules and playbooks, showing how to incrementally build from a simple config to a complete deployment.\nMinimal Configuration The simplest valid config only defines the admin_ip variable—the IP address of the node where Pigsty is installed (admin node):\nMinimal Mirror all: { vars: { admin_ip: 10.10.10.10 } } # Set region: china to use mirrors all: { vars: { admin_ip: 10.10.10.10, region: china } } This config deploys nothing, but running ./deploy.yml generates a self-signed CA in files/pki/ca for issuing certificates.\nFor convenience, you can also set region to specify which region’s software mirrors to use (default, china, europe).\nAdd Nodes Pigsty’s NODE module manages cluster nodes. Any IP address in the inventory will be managed by Pigsty with the NODE module installed.\nMinimal Mirror all: # Remember to replace 10.10.10.10 with your actual IP children: { nodes: { hosts: { 10.10.10.10: {} } } } vars: admin_ip: 10.10.10.10 # Current node IP region: default # Default repos node_repo_modules: node,pgsql,infra # Add node, pgsql, infra repos all: # Remember to replace 10.10.10.10 with your actual IP children: { nodes: { hosts: { 10.10.10.10: {} } } } vars: admin_ip: 10.10.10.10 # Current node IP region: china # Use mirrors node_repo_modules: node,pgsql,infra # Add node, pgsql, infra repos We added two global parameters: node_repo_modules specifies repos to add; region specifies which region’s mirrors to use.\nThese parameters enable the node to use correct repositories and install required packages. The NODE module offers many customization options: node names, DNS, repos, packages, NTP, kernel params, tuning templates, monitoring, log collection, etc. Even without changes, the defaults are sufficient.\nRun deploy.yml or more precisely node.yml to bring the defined node under Pigsty management.\nID NODE INFRA ETCD PGSQL Description 1 10.10.10.10 - - - Add node Add Infrastructure A full-featured RDS cloud database service needs infrastructure support: monitoring (metrics/log collection, alerting, visualization), NTP, DNS, and other foundational services.\nDefine a special group infra to deploy the INFRA module:\nMinimal Mirror all: # Simply changed group name from nodes -\u003e infra and added infra_seq children: { infra: { hosts: { 10.10.10.10: { infra_seq: 1 } } } } vars: admin_ip: 10.10.10.10 region: default node_repo_modules: node,pgsql,infra all: # Simply changed group name from nodes -\u003e infra and added infra_seq children: { infra: { hosts: { 10.10.10.10: { infra_seq: 1 } } } } vars: admin_ip: 10.10.10.10 region: china node_repo_modules: node,pgsql,infra We also assigned an identity parameter: infra_seq to distinguish nodes in multi-node HA INFRA deployments.\nRun infra.yml to install INFRA**](/docs/infra/) and [**NODE modules on 10.10.10.10:\n./infra.yml # Install INFRA module on infra group (includes NODE module) NODE module is implicitly defined as long as an IP exists. NODE is idempotent—re-running has no side effects.\nAfter completion, you’ll have complete observability infrastructure and node monitoring, but PostgreSQL database service is not yet deployed.\nIf your goal is just to set up this monitoring system (Grafana + Victoria), you’re done! The infra template is designed for this. Everything in Pigsty is modular: you can deploy only monitoring infra without databases; or vice versa—run HA PostgreSQL clusters without infra—Slim Install.\nID NODE INFRA ETCD PGSQL Description 1 10.10.10.10 infra-1 - - Add infrastructure Deploy Database Cluster To provide PostgreSQL service, install the PGSQL` module and its dependency ETCD—just two lines of config:\nMinimal Mirror all: children: infra: { hosts: { 10.10.10.10: { infra_seq: 1 } } } etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 } } } # Add etcd cluster pg-meta: { hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } }, vars: { pg_cluster: pg-meta } } # Add pg cluster vars: { admin_ip: 10.10.10.10, region: default, node_repo_modules: node,pgsql,infra } all: children: infra: { hosts: { 10.10.10.10: { infra_seq: 1 } } } etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 } } } # Add etcd cluster pg-meta: { hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } }, vars: { pg_cluster: pg-meta } } # Add pg cluster vars: { admin_ip: 10.10.10.10, region: china, node_repo_modules: node,pgsql,infra } We added two new groups: etcd and pg-meta, defining a single-node etcd cluster and a single-node PostgreSQL cluster.\nUse ./deploy.yml to redeploy everything, or incrementally deploy:\n./etcd.yml -l etcd # Install ETCD module on etcd group ./pgsql.yml -l pg-meta # Install PGSQL module on pg-meta group PGSQL depends on ETCD for HA consensus, so install ETCD first. After completion, you have a working PostgreSQL service!\nID NODE INFRA ETCD PGSQL Description 1 10.10.10.10 infra-1 etcd-1 pg-meta-1 Add etcd and PostgreSQL cluster We used node.yml, infra.yml, etcd.yml, and pgsql.yml to deploy all four core modules on a single machine.\nDefine Databases and Users In Pigsty, you can customize PostgreSQL cluster internals like databases and users through the inventory:\nall: children: # Other groups and variables hidden for brevity pg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta pg_users: # Define database users - { name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user } pg_databases: # Define business databases - { name: meta ,baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [pigsty] ,extensions: [vector] } pg_users: Defines a new user dbuser_meta with password DBUser.Meta pg_databases: Defines a new database meta with Pigsty CMDB schema (optional) and vector extension Pigsty offers rich customization parameters covering all aspects of databases and users. If you define these parameters upfront, they’re automatically created during ./pgsql.yml execution. For existing clusters, you can incrementally create or modify users and databases:\nbin/pgsql-user pg-meta dbuser_meta # Ensure user dbuser_meta exists in pg-meta bin/pgsql-db pg-meta meta # Ensure database meta exists in pg-meta Configure PG Version and Extensions You can install different major versions of PostgreSQL, and up to 440 extensions. Let’s remove the current default PG 18 and install PG 17:\n./pgsql-rm.yml -l pg-meta # Remove old pg-meta cluster (it's PG 18) We can customize parameters to install and enable common extensions by default: timescaledb, postgis, and pgvector:\npg_extensions: Install timescaledb, postgis, pgvector extensions. pg_libs: Configure loading timescaledb, pg_stat_statements, auto_explain dynamic libraries. pg_databases: Create and enable vector, postgis, timescaledb extensions for the meta database. all: children: infra: { hosts: { 10.10.10.10: { infra_seq: 1 } } } etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 } } } # Add etcd cluster pg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta pg_version: 17 # Specify PG version as 17 pg_extensions: [ timescaledb, postgis, pgvector ] # Install these extensions pg_libs: 'timescaledb,pg_stat_statements,auto_explain' # Preload these extension libraries pg_databases: { { name: meta ,baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [pigsty] ,extensions: [vector, postgis, timescaledb ] } } pg_users: { { name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user } } vars: admin_ip: 10.10.10.10 region: default node_repo_modules: node,pgsql,infra ./pgsql.yml -l pg-meta # Install PG17 and extensions, recreate pg-meta cluster Add More Nodes Add more nodes to the deployment, bring them under Pigsty management, deploy monitoring, configure repos, install software…\n# Add entire cluster at once, or add nodes individually bin/node-add pg-test bin/node-add 10.10.10.11 bin/node-add 10.10.10.12 bin/node-add 10.10.10.13 Deploy HA PostgreSQL Cluster Now deploy a new database cluster pg-test on the three newly added nodes, using a three-node HA architecture:\nall: children: infra: { hosts: { 10.10.10.10: { infra_seq: 1 } } } etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 } } }, vars: { etcd_cluster: etcd } } pg-meta: { hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } }, vars: { pg_cluster: pg-meta } } pg-test: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } 10.10.10.12: { pg_seq: 2, pg_role: replica } 10.10.10.13: { pg_seq: 3, pg_role: replica } vars: { pg_cluster: pg-test } Deploy Redis Cluster Pigsty provides optional Redis support as a caching service in front of PostgreSQL:\nbin/redis-add redis-ms bin/redis-add redis-meta bin/redis-add redis-test Redis HA requires cluster mode or sentinel mode. See Redis Configuration.\nDeploy MinIO Cluster Pigsty provides optional open-source object storage, S3 alternative—MinIO support, as backup repository for PostgreSQL.\n./minio.yml -l minio Serious prod MinIO deployments typically require at least 4 nodes with 4 disks each (4N/16D).\nDeploy Docker Module If you want to use containers to run tools for managing PG or software using PostgreSQL, install the DOCKER module:\n./docker.yml -l infra Use pre-made application templates to launch common software tools with one click, such as the GUI tool for PG management: Pgadmin:\n./app.yml -l infra -e app=pgadmin You can even self-host enterprise-grade Supabase with Pigsty, using external HA PostgreSQL clusters as the foundation and running stateless components in containers.\n","categories":["Tutorial"],"description":"Express your infra and clusters with declarative config files","excerpt":"Express your infra and clusters with declarative config files","ref":"/docs/setup/config/","tags":"","title":"Customize Pigsty with Configuration"},{"body":"Pigsty uses Ansible to manage clusters, a very popular large-scale/batch/automation ops tool in the SRE community.\nAnsible can use declarative approach for server configuration management. All module deployments are implemented through a series of idempotent Ansible playbooks.\nFor example, in single-node deployment, you’ll use the deploy.yml playbook. Pigsty has more built-in playbooks, you can choose to use as needed.\nUnderstanding Ansible basics helps with better use of Pigsty, but this is not required, especially for single-node deployment.\nDeploy Playbook Pigsty provides a “one-stop” deploy playbook deploy.yml, installing all modules on the current env in one go (if defined in config):\nPlaybook Command Group infra [nodes] etcd minio [pgsql] infra.yml ./infra.yml -l infra ✓ ✓ node.yml ./node.yml ✓ ✓ ✓ ✓ etcd.yml ./etcd.yml -l etcd ✓ minio.yml ./minio.yml -l minio ✓ pgsql.yml ./pgsql.yml ✓ This is the simplest deployment method. You can also follow instructions in Customization Guide to incrementally complete deployment of all modules and nodes step by step.\nInstall Ansible When using the Pigsty installation script, or the bootstrap phase of offline installation, Pigsty will automatically install ansible and its dependencies for you.\nIf you want to manually install Ansible, refer to the following instructions. The minimum supported Ansible version is 2.9.\nDebian / Ubuntu EL MacOS sudo apt install -y ansible python3-jmespath sudo dnf install -y ansible python-jmespath # EL 10 sudo dnf install -y ansible python3.12-jmespath # EL 9/8 brew install ansible pip3 install jmespath Change default passwords! Please note that EL10 EPEL repo doesn’t yet provide a complete Ansible package. Pigsty PGSQL EL10 repo supplements this.\nAnsible is also available on macOS. You can use Homebrew to install Ansible on Mac, and use it as an admin node to manage remote cloud servers. This is convenient for single-node Pigsty deployment on cloud VPS, but not recommended in prod envs.\nExecute Playbook Ansible playbooks are executable YAML files containing a series of task definitions to execute. Running playbooks requires the ansible-playbook executable in your environment variable PATH. Running ./node.yml playbook is essentially executing the ansible-playbook node.yml command.\nYou can use some parameters to fine-tune playbook execution. The following 4 parameters are essential for effective Ansible use:\nPurpose Parameter Description Target -l|--limit \u003cpattern\u003e Limit execution to specific groups/hosts/patterns Tasks -t|--tags \u003ctags\u003e Only run tasks with specific tags Params -e|--extra-vars \u003cvars\u003e Extra command-line parameters Config -i|--inventory \u003cpath\u003e Use a specific inventory file ./node.yml # Run node playbook on all hosts ./pgsql.yml -l pg-test # Run pgsql playbook on pg-test cluster ./infra.yml -t repo_build # Run infra.yml subtask repo_build ./pgsql-rm.yml -e pg_rm_pkg=false # Remove pgsql, but keep packages (don't uninstall software) ./infra.yml -i conf/mynginx.yml # Use another location's config file Limit Hosts Playbook execution targets can be limited with -l|--limit \u003cselector\u003e. This is convenient when running playbooks on specific hosts/nodes or groups/clusters. Here are some host limit examples:\n./pgsql.yml # Run on all hosts (dangerous!) ./pgsql.yml -l pg-test # Run on pg-test cluster ./pgsql.yml -l 10.10.10.10 # Run on single host 10.10.10.10 ./pgsql.yml -l pg-* # Run on hosts/groups matching glob `pg-*` ./pgsql.yml -l '10.10.10.11,\u0026pg-test' # Run on 10.10.10.11 in pg-test group ./pgsql-rm.yml -l 'pg-test,!10.10.10.11' # Run on pg-test, except 10.10.10.11 See all details in Ansible documentation: Patterns: targeting hosts and groups\nUse caution when running playbooks without host limits! Missing this value can be dangerous—most playbooks execute on all hosts. Use with caution.\nLimit Tasks Execution tasks can be controlled with -t|--tags \u003ctags\u003e. If specified, only tasks with the given tags will execute instead of the entire playbook.\n./infra.yml -t repo # Create repo ./node.yml -t node_pkg # Install node packages ./pgsql.yml -t pg_install # Install PG packages and extensions ./etcd.yml -t etcd_purge # Destroy ETCD cluster ./minio.yml -t minio_alias # Write MinIO CLI config To run multiple tasks, specify multiple tags separated by commas -t tag1,tag2:\n./node.yml -t node_repo,node_pkg # Add repos, then install packages ./pgsql.yml -t pg_hba,pg_reload # Configure, then reload pg hba rules Extra Vars You can override config parameters at runtime using CLI arguments, which have highest priority.\nExtra command-line parameters are passed via -e|--extra-vars KEY=VALUE, usable multiple times:\n# Create admin using another admin user ./node.yml -e ansible_user=admin -k -K -t node_admin # Initialize a specific Redis instance: 10.10.10.11:6379 ./redis.yml -l 10.10.10.10 -e redis_port=6379 -t redis # Remove PostgreSQL but keep packages and data ./pgsql-rm.yml -e pg_rm_pkg=false -e pg_rm_data=false For complex parameters, use JSON strings to pass multiple complex parameters at once:\n# Add repo and install packages ./node.yml -t node_install -e '{\"node_repo_modules\":\"infra\",\"node_packages\":[\"duckdb\"]}' Specify Inventory The default config file is pigsty.yml in the Pigsty home directory.\nYou can use -i \u003cpath\u003e to specify a different inventory file path.\n./pgsql.yml -i conf/rich.yml # Initialize single node with all extensions per rich config ./pgsql.yml -i conf/ha/full.yml # Initialize 4-node cluster per full config ./pgsql.yml -i conf/app/supa.yml # Initialize 1-node Supabase deployment per supa.yml Changing the default inventory file To permanently change the default config file, modify the inventory parameter in ansible.cfg.\nConvenience Scripts Pigsty provides a series of convenience scripts to simplify common operations. These scripts are in the bin/ directory:\nbin/node-add \u003ccls\u003e # Add nodes to Pigsty management: ./node.yml -l \u003ccls\u003e bin/node-rm \u003ccls\u003e # Remove nodes from Pigsty: ./node-rm.yml -l \u003ccls\u003e bin/pgsql-add \u003ccls\u003e # Initialize PG cluster: ./pgsql.yml -l \u003ccls\u003e bin/pgsql-rm \u003ccls\u003e # Remove PG cluster: ./pgsql-rm.yml -l \u003ccls\u003e bin/pgsql-user \u003ccls\u003e \u003cusername\u003e # Add business user: ./pgsql-user.yml -l \u003ccls\u003e -e username=\u003cuser\u003e bin/pgsql-db \u003ccls\u003e \u003cdbname\u003e # Add business database: ./pgsql-db.yml -l \u003ccls\u003e -e dbname=\u003cdb\u003e bin/redis-add \u003ccls\u003e # Initialize Redis cluster: ./redis.yml -l \u003ccls\u003e bin/redis-rm \u003ccls\u003e # Remove Redis cluster: ./redis-rm.yml -l \u003ccls\u003e These scripts are simple wrappers around Ansible playbooks, making common operations more convenient.\nPlaybook List Below are the built-in playbooks in Pigsty. You can also easily add your own playbooks, or customize and modify playbook implementation logic as needed.\nModule Playbook Function INFRA deploy.yml One-click deploy Pigsty on current node INFRA infra.yml Initialize Pigsty infrastructure on infra nodes INFRA infra-rm.yml Remove infrastructure components from infra nodes INFRA cache.yml Create offline packages from target node INFRA cert.yml Issue certificates using Pigsty self-signed CA NODE node.yml Initialize node, adjust to desired state NODE node-rm.yml Remove node from Pigsty PGSQL pgsql.yml Initialize HA PostgreSQL cluster or add replica PGSQL pgsql-rm.yml Remove PostgreSQL cluster or replica PGSQL pgsql-db.yml Add new business database to existing cluster PGSQL pgsql-user.yml Add new business user to existing cluster PGSQL pgsql-pitr.yml Perform point-in-time recovery on cluster PGSQL pgsql-monitor.yml Monitor remote PostgreSQL with local exporter PGSQL pgsql-migration.yml Generate migration manual and scripts PGSQL slim.yml Install Pigsty with minimal components REDIS redis.yml Initialize Redis cluster/node/instance REDIS redis-rm.yml Remove Redis cluster/node/instance ETCD etcd.yml Initialize ETCD cluster or add new member ETCD etcd-rm.yml Remove ETCD cluster/data or shrink member MINIO minio.yml Initialize MinIO cluster (optional pgBackRest repo) MINIO minio-rm.yml Remove MinIO cluster and data DOCKER docker.yml Install Docker on nodes DOCKER app.yml Install applications using Docker Compose FERRET mongo.yml Install Mongo/FerretDB on nodes ","categories":["Tutorial"],"description":"Use Ansible playbooks to deploy and manage Pigsty clusters","excerpt":"Use Ansible playbooks to deploy and manage Pigsty clusters","ref":"/docs/setup/playbook/","tags":"","title":"Run Playbooks with Ansible"},{"body":"Pigsty installs from Internet upstream by default, but some envs are isolated from the Internet. To address this, Pigsty supports offline installation using offline packages. Think of them as Linux-native Docker images.\nOverview Offline packages bundle all required RPM/DEB packages and dependencies; they are snapshots of the local APT/YUM repo after a normal installation.\nIn serious prod deployments, we strongly recommend using offline packages. They ensure all future nodes have consistent software versions with the existing env, and avoid online installation failures caused by upstream changes (quite common!), guaranteeing you can run it independently forever.\nAdvantages of offline packages Easy delivery in Internet-isolated envs. Pre-download all packages in one pass to speed up installation. No need to worry about upstream dependency breakage causing install failures. If you have multiple nodes, all packages only need to be downloaded once, saving bandwidth. Use local repo to ensure all nodes have consistent software versions for unified version management. Disadvantages of offline packages Offline packages are made for specific OS minor versions, typically cannot be used across versions. It’s a snapshot at the time of creation, may not include the latest updates and OS security patches. Offline packages are typically about 1GB, while online installation downloads on-demand, saving space. Offline Packages We typically release offline packages for the following Linux distros, using the latest OS minor version.\nLinux Distribution System Code Minor Version Package RockyLinux 8 x86_64 el8.x86_64 8.10 pigsty-pkg-v4.0.0.el8.x86_64.tgz RockyLinux 8 aarch64 el8.aarch64 8.10 pigsty-pkg-v4.0.0.el8.aarch64.tgz RockyLinux 9 x86_64 el9.x86_64 9.6 pigsty-pkg-v4.0.0.el9.x86_64.tgz RockyLinux 9 aarch64 el9.aarch64 9.6 pigsty-pkg-v4.0.0.el9.aarch64.tgz RockyLinux 10 x86_64 el10.x86_64 10.0 pigsty-pkg-v4.0.0.el10.x86_64.tgz RockyLinux 10 aarch64 el10.aarch64 10.0 pigsty-pkg-v4.0.0.el10.aarch64.tgz Debian 12 x86_64 d12.x86_64 12.11 pigsty-pkg-v4.0.0.d12.x86_64.tgz Debian 12 aarch64 d12.aarch64 12.11 pigsty-pkg-v4.0.0.d12.aarch64.tgz Debian 13 x86_64 d13.x86_64 13.2 pigsty-pkg-v4.0.0.d13.x86_64.tgz Debian 13 aarch64 d13.aarch64 13.2 pigsty-pkg-v4.0.0.d13.aarch64.tgz Ubuntu 24.04 x86_64 u24.x86_64 24.04.2 pigsty-pkg-v4.0.0.u24.x86_64.tgz Ubuntu 24.04 aarch64 u24.aarch64 24.04.2 pigsty-pkg-v4.0.0.u24.aarch64.tgz Ubuntu 22.04 x86_64 u22.x86_64 22.04.5 pigsty-pkg-v4.0.0.u22.x86_64.tgz Ubuntu 22.04 aarch64 u22.aarch64 22.04.5 pigsty-pkg-v4.0.0.u22.aarch64.tgz If you use an OS from the list above (exact minor version match), we recommend using offline packages. Pigsty provides ready-to-use pre-made offline packages for these systems, freely downloadable from GitHub.\nYou can find these packages on the GitHub release page:\n6a26fa44f90a16c7571d2aaf0e997d07 pigsty-v4.0.0.tgz 537839201c536a1211f0b794482d733b pigsty-pkg-v4.0.0.el9.x86_64.tgz 85687cb56517acc2dce14245452fdc05 pigsty-pkg-v4.0.0.el9.aarch64.tgz a333e8eb34bf93f475c85a9652605139 pigsty-pkg-v4.0.0.el10.x86_64.tgz 4b98b463e2ebc104c35ddc94097e5265 pigsty-pkg-v4.0.0.el10.aarch64.tgz 4f62851c9d79a490d403f59deb4823f4 pigsty-pkg-v4.0.0.el8.x86_64.tgz 66e283c9f6bfa80654f7ed3ffb9b53e5 pigsty-pkg-v4.0.0.el8.aarch64.tgz f7971d9d6aab1f8f307556c2f64b701c pigsty-pkg-v4.0.0.d12.x86_64.tgz c4d870e5ef61ed05724c15fbccd1220b pigsty-pkg-v4.0.0.d12.aarch64.tgz 408991c5ff028b5c0a86fac804d64b93 pigsty-pkg-v4.0.0.d13.x86_64.tgz 8d7c9404b97a11066c00eb7fc1330181 pigsty-pkg-v4.0.0.d13.aarch64.tgz 2a25eff283332d9006854f36af6602b2 pigsty-pkg-v4.0.0.u24.x86_64.tgz a4fb30148a2d363bbfd3bec0daa14ab6 pigsty-pkg-v4.0.0.u24.aarch64.tgz 87bb91ef703293b6ec5b77ae3bb33d54 pigsty-pkg-v4.0.0.u22.x86_64.tgz 5c81bdaa560dad4751840dec736fe404 pigsty-pkg-v4.0.0.u22.aarch64.tgz Offline packages are made for specific Linux OS minor versions When OS minor versions don’t match, it may work or may fail—we don’t recommend taking the risk.\nPlease note that Pigsty’s EL9/EL10 packages are built on 9.6/10.0 and currently cannot be used for 9.7/10.1 minor versions (due to OpenSSL version changes). You need to perform an online installation on a matching OS version and create your own offline package, or contact us for custom offline packages.\nUsing Offline Packages Offline installation steps:\nDownload Pigsty offline package, place it at /tmp/pkg.tgz Download Pigsty source package, extract and enter directory (assume extracted to home: cd ~/pigsty) ./bootstrap, it will extract the package and configure using local repo (and install ansible from it offline) ./configure -g -c rich, you can directly use the rich template configured for offline installation, or configure yourself Run ./deploy.yml as usual—it will install everything from the local repo If you want to use the already extracted and configured offline package in your own config, modify and ensure these settings:\nrepo_enabled: Set to true, will build local software repo (explicitly disabled in most templates) node_repo_modules: Set to local, then all nodes in the env will install from the local software repo In most templates, this is explicitly set to: node,infra,pgsql, i.e., install directly from these upstream repos. Setting it to local will use the local software repo to install all packages, fastest, no interference from other repos. If you want to use both local and upstream repos, you can add other repo module names too, e.g., local,node,infra,pgsql The first parameter, if enabled, Pigsty will create a local software repo. The second parameter, if contains local, then all nodes in the env will use this local software repo. If it only contains local, then it becomes the sole repo for all nodes. If you still want to install other packages from other upstream repos, you can add other repo module names too, e.g., local,node,infra,pgsql.\nHybrid Installation Mode\nIf your env has Internet access, there’s a hybrid approach combining advantages of offline and online installation. You can use the offline package as a base, and supplement missing packages online.\nFor example, if you’re using RockyLinux 9.5 but the official offline package is for RockyLinux 9.6. You can use the el9 offline package (though made for 9.6), then execute make repo-build before formal installation to re-download missing packages for 9.5. Pigsty will download the required increments from upstream repos.\nMaking Offline Packages If your OS isn’t in the default list, you can make your own offline package with the built-in cache.yml playbook:\nFind a node running the exact same OS version with Internet access Use rich config template to perform online installation (configure -c rich) cd ~/pigsty; ./cache.yml: make and fetch the offline package to ~/pigsty/dist/${version}/ Copy the offline package to the env without Internet access (ftp, scp, usb, etc.), extract and use via bootstrap We offer paid services providing tested, pre-made offline packages for specific Linux major.minor versions (¥200).\nBootstrap Pigsty relies on ansible to execute playbooks; this script is responsible for ensuring ansible is correctly installed in various ways.\n./bootstrap # Ensure ansible is correctly installed (if offline package exists, use offline installation and extract first) Usually, you need to run this script in two cases:\nYou didn’t install Pigsty via the installation script, but by downloading or git clone of the source package, so ansible isn’t installed. You’re preparing to install Pigsty via offline packages and need to use this script to install ansible from the offline package. The bootstrap script will automatically detect if the offline package exists (-p to specify, default is /tmp/pkg.tgz). If it exists, it will extract and use it, then install ansible from it. If the offline package doesn’t exist, it will try to install ansible from the Internet. If that still fails, you’re on your own!\nWhere are my yum/apt repo files? The bootloader will by default move away existing repo configurations to ensure only required repos are enabled. You can find them in /etc/yum.repos.d/backup (EL) or /etc/apt/backup (Debian / Ubuntu).\nIf you want to keep existing repo configurations during bootstrap, use the -k|--keep parameter.\n./bootstrap -k # or --keep ","categories":["Tutorial"],"description":"Install Pigsty in air-gapped env using offline packages","excerpt":"Install Pigsty in air-gapped env using offline packages","ref":"/docs/setup/offline/","tags":"","title":"Offline Installation"},{"body":"If you only want HA PostgreSQL database cluster itself without monitoring, infra, etc., consider Slim Installation.\nSlim installation has no INFRA module, no monitoring, no local repo—just ETCD and PGSQL and partial NODE functionality.\nSlim installation is suitable for: Only needing PostgreSQL database itself, no observability infra required. Extremely resource-constrained envs unwilling to bear infra overhead (~0.2 vCPU / 500MB on single node). Already having external monitoring system, wanting to use your own unified monitoring framework. Not wanting to introduce the AGPLv3-licensed Grafana visualization dashboard component. Limitations of slim installation: No INFRA module, cannot use WebUI and local software repo features. Offline Install is limited to single-node mode; multi-node slim install can only be done online. Overview To use slim installation, you need to:\nUse the slim.yml slim install config template (configure -c slim) Run the slim.yml playbook instead of the default deploy.yml curl https://repo.pigsty.io/get | bash ./configure -g -c slim ./slim.yml Description Slim installation only installs/configures these components:\nComponent Required Description patroni ⚠️ Required Bootstrap HA PostgreSQL cluster etcd ⚠️ Required Meta database dependency (DCS) for Patroni pgbouncer ✔️ Optional PostgreSQL connection pooler vip-manager ✔️ Optional L2 VIP binding to PostgreSQL cluster primary haproxy ✔️ Optional Auto-routing services via Patroni health checks chronyd ✔️ Optional Time synchronization with NTP server tuned ✔️ Optional Node tuning template and kernel parameter management You can disable all optional components via configuration, keeping only the required patroni and etcd.\nBecause there’s no INFRA module’s Nginx providing local repo service, offline installation only works in single-node mode.\nConfiguration Slim installation config file example: conf/slim.yml:\nID NODE PGSQL INFRA ETCD 1 10.10.10.10 pg-meta-1 No INFRA module etcd-1 --- #==============================================================# # File : slim.yml # Desc : Pigsty slim installation config template # Ctime : 2020-05-22 # Mtime : 2025-12-28 # Docs : https://doc.pgsty.com/config # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# # This is the config template for slim / minimal installation # No monitoring \u0026 infra will be installed, just raw postgresql # # Usage: # curl https://repo.pigsty.io/get | bash # ./configure -c slim # ./slim.yml all: children: etcd: # dcs service for postgres/patroni ha consensus hosts: # 1 node for testing, 3 or 5 for production 10.10.10.10: { etcd_seq: 1 } # etcd_seq required #10.10.10.11: { etcd_seq: 2 } # assign from 1 ~ n #10.10.10.12: { etcd_seq: 3 } # odd number please vars: # cluster level parameter override roles/etcd etcd_cluster: etcd # mark etcd cluster name etcd #----------------------------------------------# # PostgreSQL Cluster #----------------------------------------------# pg-meta: hosts: 10.10.10.10: { pg_seq: 1, pg_role: primary } #10.10.10.11: { pg_seq: 2, pg_role: replica } # you can add more! #10.10.10.12: { pg_seq: 3, pg_role: replica, pg_offline_query: true } vars: pg_cluster: pg-meta pg_users: - { name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [dbrole_admin ] ,comment: pigsty admin user } - { name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [dbrole_readonly] ,comment: read-only viewer } pg_databases: - { name: meta, baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [pigsty] ,extensions: [ vector ]} node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] # make a full backup every 1am vars: version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default,china,europe nodename_overwrite: false # do not overwrite node hostname on single node mode node_repo_modules: node,infra,pgsql # add these repos directly to the singleton node node_tune: oltp # node tuning specs: oltp,olap,tiny,crit pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml pg_version: 18 # Default PostgreSQL Major Version is 18 pg_packages: [ pgsql-main, pgsql-common ] # pg kernel and common utils #pg_extensions: [ pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-olap ,pg18-feat ,pg18-lang ,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl] #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root ... Deployment Slim installation uses the slim.yml playbook instead of deploy.yml:\n./slim.yml HA Cluster Slim installation can also deploy HA clusters—just add more nodes to the etcd and pg-meta groups. A three-node deployment example:\nID NODE PGSQL INFRA ETCD 1 10.10.10.10 pg-meta-1 No INFRA module etcd-1 2 10.10.10.11 pg-meta-2 No INFRA module etcd-2 3 10.10.10.12 pg-meta-3 No INFRA module etcd-3 all: children: etcd: hosts: 10.10.10.10: { etcd_seq: 1 } 10.10.10.11: { etcd_seq: 2 } # \u003c-- New 10.10.10.12: { etcd_seq: 3 } # \u003c-- New pg-meta: hosts: 10.10.10.10: { pg_seq: 1, pg_role: primary } 10.10.10.11: { pg_seq: 2, pg_role: replica } # \u003c-- New 10.10.10.12: { pg_seq: 3, pg_role: replica } # \u003c-- New vars: pg_cluster: pg-meta pg_users: - { name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [dbrole_admin ] ,comment: pigsty admin user } - { name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [dbrole_readonly] ,comment: read-only viewer } pg_databases: - { name: meta, baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [pigsty] ,extensions: [ vector ]} node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] # make a full backup every 1am vars: # omitted …… ","categories":["Tutorial"],"description":"Install only HA PostgreSQL clusters with minimal dependencies","excerpt":"Install only HA PostgreSQL clusters with minimal dependencies","ref":"/docs/setup/slim/","tags":"","title":"Slim Installation"},{"body":"For Demo/Dev single-node deployments, Pigsty’s default config is secure enough as long as you change default passwords.\nIf your deployment is exposed to Internet or office network, consider adding firewall rules to restrict port access and source IPs for enhanced security.\nAdditionally, we recommend protecting Pigsty’s critical files (config files and CA private key) from unauthorized access and backing them up regularly.\nFor enterprise prod envs with strict security requirements, refer to the Deployment - Security Hardening documentation for advanced configuration.\nPasswords Pigsty is an open-source project with well-known default passwords. If your deployment is exposed to Internet or office network, you must change all default passwords!\nModule Parameter Default Value INFRA grafana_admin_password pigsty INFRA grafana_view_password DBUser.Viewer PGSQL pg_admin_password DBUser.DBA PGSQL pg_monitor_password DBUser.Monitor PGSQL pg_replication_password DBUser.Replicator PGSQL patroni_password Patroni.API NODE haproxy_admin_password pigsty MINIO minio_secret_key S3User.MinIO ETCD etcd_root_password Etcd.Root To avoid manually modifying passwords, Pigsty’s configuration wizard provides automatic random strong password generation using the -g argument with configure.\n$ ./configure -g configure pigsty v4.0.0 begin [ OK ] region = china [WARN] kernel = Darwin, can be used as admin node only [ OK ] machine = arm64 [ OK ] package = brew (macOS) [WARN] primary_ip = default placeholder 10.10.10.10 (macOS) [ OK ] mode = meta (unknown distro) [ OK ] locale = C.UTF-8 [ OK ] generating random passwords... grafana_admin_password : CdG0bDcfm3HFT9H2cvFuv9w7 pg_admin_password : 86WqSGdokjol7WAU9fUxY8IG pg_monitor_password : 0X7PtgMmLxuCd2FveaaqBuX9 pg_replication_password : 4iAjjXgEY32hbRGVUMeFH460 patroni_password : DsD38QLTSq36xejzEbKwEqBK haproxy_admin_password : uhdWhepXrQBrFeAhK9sCSUDo minio_secret_key : z6zrYUN1SbdApQTmfRZlyWMT etcd_root_password : Bmny8op1li1wKlzcaAmvPiWc DBUser.Meta : U5v3CmeXICcMdhMNzP9JN3KY DBUser.Viewer : 9cGQF1QMNCtV3KlDn44AEzpw S3User.Backup : 2gjgSCFYNmDs5tOAiviCqM2X S3User.Meta : XfqkAKY6lBtuDMJ2GZezA15T S3User.Data : OygorcpCbV7DpDmqKe3G6UOj [ OK ] random passwords generated, check and save them [ OK ] ansible = ready [ OK ] pigsty configured [WARN] don't forget to check it and change passwords! proceed with ./deploy.yml Firewall For deployments exposed to Internet or office networks, we strongly recommend configuring firewall rules to limit access IP ranges and ports.\nYou can use your cloud provider’s security group features, or Linux distribution firewall services (like firewalld, ufw, iptables, etc.) to implement this.\nDirection Protocol Port Service Description Inbound TCP 22 SSH Allow SSH login access Inbound TCP 80 Nginx Allow Nginx HTTP access Inbound TCP 443 Nginx Allow Nginx HTTPS access Inbound TCP 5432 PostgreSQL Remote database access, enable as needed Pigsty supports configuring firewall rules to allow 22/80/443/5432 from external networks, but this is not enabled by default.\nFiles In Pigsty, you need to protect the following files:\npigsty.yml: Pigsty main config file, contains access information and passwords for all nodes files/pki/ca/ca.key: Pigsty self-signed CA private key, used to issue all SSL certificates in the deployment (auto-generated during deployment) We recommend strictly controlling access permissions for these two files, regularly backing them up, and storing them in a secure location.\n","categories":["Tutorial"],"description":"Three security hardening tips for single-node quick-start deployment","excerpt":"Three security hardening tips for single-node quick-start deployment","ref":"/docs/setup/security/","tags":"","title":"Security Tips"},{"body":"Unlike Getting Started, production Pigsty deployments require more Architecture Planning and Preparation.\nThis chapter helps you understand the complete deployment process and provides best practices for production environments.\nBefore deploying to production, we recommend testing in Pigsty’s Sandbox to fully understand the workflow. Use Vagrant to create a local 4-node sandbox, or leverage Terraform to provision larger simulation environments in the cloud.\nFor production, you typically need at least three nodes for high availability. You should understand Pigsty’s core Concepts and common administration procedures, including Configuration, Ansible Playbooks, and Security Hardening for enterprise compliance.\n","categories":["Tutorial"],"description":"Multi-node, high-availability Pigsty deployment for serious production environments.","excerpt":"Multi-node, high-availability Pigsty deployment for serious production …","ref":"/docs/deploy/","tags":"","title":"Deployment"},{"body":"This is the Pigsty production multi-node deployment guide. For single-node Demo/Dev setups, see Getting Started.\nSummary Prepare nodes with SSH access following your architecture plan, install a compatible Linux OS, then execute with an admin user having passwordless ssh and sudo:\ncurl -fsSL https://repo.pigsty.io/get | bash; # International curl -fsSL https://repo.pigsty.cc/get | bash; # Backup Mirror This runs the install script, downloading and extracting Pigsty source to your home directory with dependencies installed. Complete configuration and deployment to finish.\nBefore running deploy.yml for deployment, review and edit the configuration inventory: pigsty.yml.\ncd ~/pigsty # Enter Pigsty directory ./configure -g # Generate config file (optional, skip if you know how to configure) ./deploy.yml # Execute deployment playbook based on generated config After installation, access the WebUI via IP/domain + ports 80/443, and PostgreSQL service via port 5432.\nFull installation takes 3-10 minutes depending on specs/network. Offline installation significantly speeds this up; slim installation further accelerates when monitoring isn’t needed.\nVideo Example: 20-node Production Simulation (Ubuntu 24.04 x86_64)\nPrepare Production Pigsty deployment involves preparation work. Here’s the complete checklist:\nItem Requirement Item Requirement Node At least 1C2G, no upper limit Plan Multiple homogeneous nodes: 2/3/4 or more Disk /data as default mount point FS xfs recommended; ext4/zfs as needed VIP L2 VIP, optional (unavailable in cloud) Network Static IPv4, single-node can use 127.0.0.1 CA Self-signed CA or specify existing certs Domain Local/public domain, optional, default h.pigsty Kernel Linux x86_64 / aarch64 Linux el8, el9, el10, d12, d13, u22, u24 Locale C.UTF-8 or C Firewall Ports: 80/443/22/5432 (optional) User Avoid root and postgres Sudo sudo privilege, preferably with nopass SSH Passwordless SSH via public key Accessible ssh \u003cip|alias\u003e sudo ls no error Install Use the following to automatically install the Pigsty source package to ~/pigsty (recommended). Deployment dependencies (Ansible) are auto-installed.\ncurl -fsSL https://repo.pigsty.io/get | bash # Install latest stable version curl -fsSL https://repo.pigsty.cc/get | bash # Backup mirror curl -fsSL https://repo.pigsty.io/get | bash -s v4.0.0 # Install specific version If you prefer not to run remote scripts, manually download or clone the source. When using git, always checkout a specific version before use:\ngit clone https://github.com/pgsty/pigsty; cd pigsty; git checkout v4.0.0-b4; # Always checkout a specific version when using git For manual download/clone, additionally run bootstrap to manually install Ansible and other dependencies, or install them yourself:\n./bootstrap # Install ansible for subsequent deployment Configure In Pigsty, deployment details are defined by the configuration inventory—the pigsty.yml config file. Customize through declarative configuration.\nPigsty provides configure as an optional configuration wizard, generating a configuration inventory with good defaults based on your environment:\n./configure -g # Use wizard to generate config with random passwords The generated config defaults to ~/pigsty/pigsty.yml. Review and customize before installation.\nMany configuration templates are available for reference. You can skip the wizard and directly edit pigsty.yml:\n./configure -c ha/full -g # Use 4-node sandbox template ./configure -c ha/trio -g # Use 3-node minimal HA template ./configure -c ha/dual -g -v 17 # Use 2-node semi-HA template with PG 17 ./configure -c ha/simu -s # Use 20-node production simulation, skip IP check, no random passwords Example configure output vagrant@meta:~/pigsty$ ./configure configure pigsty v4.0.0 begin [ OK ] region = china [ OK ] kernel = Linux [ OK ] machine = x86_64 [ OK ] package = deb,apt [ OK ] vendor = ubuntu (Ubuntu) [ OK ] version = 22 (22.04) [ OK ] sudo = vagrant ok [ OK ] ssh = vagrant@127.0.0.1 ok [WARN] Multiple IP address candidates found: (1) 192.168.121.38\tinet 192.168.121.38/24 metric 100 brd 192.168.121.255 scope global dynamic eth0 (2) 10.10.10.10\tinet 10.10.10.10/24 brd 10.10.10.255 scope global eth1 [ OK ] primary_ip = 10.10.10.10 (from demo) [ OK ] admin = vagrant@10.10.10.10 ok [ OK ] mode = meta (ubuntu22.04) [ OK ] locale = C.UTF-8 [ OK ] ansible = ready [ OK ] pigsty configured [WARN] don't forget to check it and change passwords! proceed with ./deploy.yml The wizard only replaces the current node’s IP (use -s to skip replacement). For multi-node deployments, replace other node IPs manually. Also customize the config as needed—modify default passwords, add nodes, etc.\nCommon configure parameters:\nParameter Description -c|--conf Specify config template relative to conf/, without .yml suffix -v|--version PostgreSQL major version: 13, 14, 15, 16, 17, 18 -r|--region Upstream repo region for faster downloads: default|china|europe -n|--non-interactive Use CLI params for primary IP, skip interactive wizard -x|--proxy Configure proxy_env from current environment variables If your machine has multiple IPs, explicitly specify one with -i|--ip \u003cipaddr\u003e or provide it interactively. The script replaces IP placeholder 10.10.10.10 with the current node’s primary IPv4. Use a static IP; never use public IPs.\nGenerated config is at ~/pigsty/pigsty.yml. Review and modify before installation.\nChange default passwords! We strongly recommend modifying default passwords and credentials before installation. See Security Hardening.\nDeploy Pigsty’s deploy.yml playbook applies the configuration blueprint to all target nodes.\n./deploy.yml # Deploy everything on all nodes at once Example deployment output ...... TASK [pgsql : pgsql init done] ************************************************* ok: [10.10.10.11] =\u003e { \"msg\": \"postgres://10.10.10.11/postgres | meta | dbuser_meta dbuser_view \" } ...... TASK [pg_monitor : load grafana datasource meta] ******************************* changed: [10.10.10.11] PLAY RECAP ********************************************************************* 10.10.10.11 : ok=302 changed=232 unreachable=0 failed=0 skipped=65 rescued=0 ignored=1 localhost : ok=6 changed=3 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 When output ends with pgsql init done, PLAY RECAP, etc., installation is complete!\nUpstream repo changes may cause online installation failures! Upstream repos (Linux/PGDG) may break due to improper updates, causing deployment failures (quite common)! For serious production deployments, we strongly recommend using verified offline packages for offline installation.\nAvoid running deploy playbook repeatedly! Warning: Running deploy.yml again on an initialized environment may restart services and overwrite configs. Be careful!\nInterface Assuming the 4-node deployment template, your Pigsty environment should have a structure like:\nID NODE PGSQL INFRA ETCD 1 10.10.10.10 pg-meta-1 infra-1 etcd-1 2 10.10.10.11 pg-test-1 - - 3 10.10.10.12 pg-test-2 - - 4 10.10.10.13 pg-test-3 - - The INFRA module provides a graphical management interface via browser, accessible through Nginx’s 80/443 ports.\nThe PGSQL module provides a PostgreSQL database server on port 5432, also accessible via Pgbouncer/HAProxy proxies.\nFor production multi-node HA PostgreSQL clusters, use service access for automatic traffic routing.\nMore After installation, explore the WebUI and access PostgreSQL service via port 5432.\nDeploy and monitor more clusters—add definitions to the configuration inventory and run:\nbin/node-add pg-test # Add pg-test cluster's 3 nodes to Pigsty management bin/pgsql-add pg-test # Initialize a 3-node pg-test HA PG cluster bin/redis-add redis-ms # Initialize Redis cluster: redis-ms Most modules require the NODE module first. See available modules:\nPGSQL, INFRA, NODE, ETCD, MINIO, REDIS, FERRET, DOCKER…\n","categories":["Tutorial"],"description":"How to install Pigsty on Linux hosts for production?","excerpt":"How to install Pigsty on Linux hosts for production?","ref":"/docs/deploy/install/","tags":"","title":"Install Pigsty for Production"},{"body":"Pigsty runs on nodes (physical machines or VMs). This document covers the planning and preparation required for deployment.\nNode Pigsty currently runs on Linux kernel with x86_64 / aarch64 architecture. A “node” refers to an SSH accessible resource that provides a bare Linux OS environment. It can be a physical machine, virtual machine, or a systemd-enabled container equipped with systemd, sudo, and sshd.\nDeploying Pigsty requires at least 1 node. You can prepare more and deploy everything in one pass via playbooks, or add nodes later. The minimum spec requirement is 1C1G, but at least 1C2G is recommended. Higher is better—no upper limit. Parameters are auto-tuned based on available resources.\nThe number of nodes you need depends on your requirements. See Architecture Planning for details. Although a single-node deployment with external backup provides reasonable recovery guarantees, we recommend multiple nodes for production. A functioning HA setup requires at least 3 nodes; 2 nodes provide Semi-HA.\nDisk Pigsty uses /data as the default data directory. If you have a dedicated data disk, mount it there. Use /data1, /data2, /dataN for additional disk drives.\nTo use a different data directory, configure these parameters:\nName Description Default node_data Node main data directory /data pg_fs_main PG main data directory /data/postgres pg_fs_backup PG backup directory /data/backups etcd_data ETCD data directory /data/etcd infra_data Infra data directory /data/infra nginx_data Nginx data directory /data/nginx minio_data MinIO data directory /data/minio redis_fs_main Redis data directory /data/redis Filesystem You can use any supported Linux filesystem for data disks. For production, we recommend xfs.\nxfs is a Linux standard with excellent performance and CoW capabilities for instant large database cluster cloning. MinIO requires xfs. ext4 is another viable option with a richer data recovery tool ecosystem, but lacks CoW. zfs provides RAID and snapshot features but with significant performance overhead and requires separate installation.\nChoose among these three based on your needs. Avoid NFS for database services.\nPigsty assumes /data is owned by root:root with 755 permissions. Admins can assign ownership for first-level directories; each application runs with a dedicated user in its subdirectory. See FHS for the directory structure reference.\nNetwork Pigsty defaults to online installation mode, requiring outbound Internet access. Offline installation eliminates the Internet requirement.\nInternally, Pigsty requires a static network. Assign a fixed IPv4 address to each node.\nThe IP address serves as the node’s unique identifier—the primary IP bound to the main network interface for internal communications.\nFor single-node deployment without a fixed IP, use the loopback address 127.0.0.1 as a workaround.\nNever use Public IP as identifier Using public IP addresses as node identifiers can cause security and connectivity issues. Always use internal IP addresses.\nVIP Pigsty supports optional L2 VIP for NODE clusters (keepalived) and PGSQL clusters (vip-manager).\nTo use L2 VIP, you must explicitly assign an L2 VIP address for each node/database cluster. This is straightforward on your own hardware but may be challenging in public cloud environments.\nL2 VIP requires L2 Networking To use optional Node VIP and PG VIP features, ensure all nodes are on the same L2 network.\nCA Pigsty generates a self-signed CA infrastructure for each deployment, issuing all encryption certificates.\nIf you have an existing enterprise CA or self-signed CA, you can use it to issue the certificates Pigsty requires.\nDomain Pigsty uses a local static domain i.pigsty by default for WebUI access. This is optional—IP addresses work too.\nFor production, domain names are recommended to enable HTTPS and encrypted data transmission. Domains also allow multiple services on the same port, differentiated by domain name.\nFor Internet-facing deployments, use public DNS providers (Cloudflare, AWS Route53, etc.) to manage resolution. Point your domain to the Pigsty node’s public IP address. For LAN/office network deployments, use internal DNS servers with the node’s internal IP address.\nFor local-only access, add the following to /etc/hosts on machines accessing the Pigsty WebUI:\n10.10.10.10 i.pigsty # Replace with your domain and Pigsty node IP Linux Pigsty runs on Linux. It supports 14 mainstream distributions: Compatible OS List\nWe recommend RockyLinux 10.0, Debian 13.2, or Ubuntu 24.04.2 as default options.\nOn macOS and Windows, use VM software or Docker systemd images to run Pigsty.\nWe strongly recommend a fresh OS installation. If your server already runs Nginx, PostgreSQL, or similar services, consider deploying on new nodes.\nUse the same OS version on all nodes For multi-node deployments, ensure all nodes use the same Linux distribution, architecture, and version. Heterogeneous deployments may work but are unsupported and may cause unpredictable issues.\nLocale We recommend setting en_US as the primary OS language, or at minimum ensuring this locale is available, so PostgreSQL logs are in English.\nSome distributions (e.g., Debian) may not provide the en_US locale by default. Enable it with:\nlocaledef -i en_US -f UTF-8 en_US.UTF-8 localectl set-locale LANG=en_US.UTF-8 For PostgreSQL, we strongly recommend using the built-in C.UTF-8 collation (PG 17+) as the default.\nThe configuration wizard automatically sets C.UTF-8 as the collation when PG version and OS support are detected.\nAnsible Pigsty uses Ansible to control all managed nodes from the admin node. See Installing Ansible for details.\nPigsty installs Ansible on Infra nodes by default, making them usable as admin nodes (or backup admin nodes). For single-node deployment, the installation node serves as both the admin node running Ansible and the INFRA node hosting infrastructure.\nPigsty You can install the latest stable Pigsty source with:\ncurl -fsSL https://repo.pigsty.io/get | bash; # International curl -fsSL https://repo.pigsty.cc/get | bash; # Backup Mirror To install a specific version, use the -s \u003cversion\u003e parameter:\ncurl -fsSL https://repo.pigsty.io/get | bash -s v4.0.0 curl -fsSL https://repo.pigsty.cc/get | bash -s v4.0.0 To install the latest beta version:\ncurl -fsSL https://repo.pigsty.io/beta | bash; curl -fsSL https://repo.pigsty.cc/beta | bash; For developers or the latest development version, clone the repository directly:\ngit clone https://github.com/pgsty/pigsty.git; cd pigsty; git checkout v4.0.0 If your environment lacks Internet access, download the source tarball from GitHub Releases or the Pigsty repository:\nwget https://repo.pigsty.io/src/pigsty-v4.0.0.tgz wget https://repo.pigsty.cc/src/pigsty-v4.0.0.tgz ","categories":["Tutorial"],"description":"Production deployment preparation including hardware, nodes, disks, network, VIP, domain, software, and filesystem requirements.","excerpt":"Production deployment preparation including hardware, nodes, disks, …","ref":"/docs/deploy/prepare/","tags":"","title":"Prepare Resources for Serious Deployment"},{"body":"Pigsty uses a modular architecture. You can combine modules like building blocks and express your intent through declarative configuration.\nCommon Patterns Here are common deployment patterns for reference. Customize based on your requirements:\nPattern INFRA ETCD PGSQL MINIO Description Single-node (meta) 1 1 1 Single-node deployment default Slim deploy (slim) 1 1 Database only, no monitoring infra Infra-only (infra) 1 Monitoring infrastructure only Rich deploy (rich) 1 1 1 1 Single-node + object storage + local repo with all extensions Multi-node Pattern INFRA ETCD PGSQL MINIO Description Two-node (dual) 1 1 2 Semi-HA, tolerates specific node failure Three-node (trio) 3 3 3 Standard HA, tolerates any one failure Four-node (full) 1 1 1+3 Demo setup, single INFRA/ETCD Production (simu) 2 3 n n 2 INFRA, 3 ETCD Large-scale (custom) 3 5 n n 3 INFRA, 5 ETCD Your architecture choice depends on reliability requirements and available resources. Serious production deployments require at least 3 nodes for HA configuration. With only 2 nodes, use Semi-HA configuration.\nExpert Consulting: Architecture Planning We offer Architecture Consulting Services to help plan your Pigsty configuration.\nTrade-offs Pigsty monitoring requires at least 1 INFRA node. Production typically uses 2; large-scale deployments use 3. PostgreSQL HA requires at least 1 ETCD node. Production typically uses 3; large-scale uses 5. Must be odd numbers. Object storage (MinIO) requires at least 1 MINIO node. Production typically uses 4+ nodes in MNMD clusters. Production PG clusters typically use at least two-node primary-replica configuration; serious deployments use 3 nodes; high read loads can have dozens of replicas. For PostgreSQL, you can also use advanced configurations: offline instances, sync instances, standby clusters, delayed clusters, etc. Single-Node Setup The simplest configuration with everything on a single node. Installs four essential modules by default. Typically used for demos, devbox, or testing.\nID NODE PGSQL INFRA ETCD 1 node-1 pg-meta-1 infra-1 etcd-1 With an external S3/MinIO backup repository providing RTO/RPO guarantees, this configuration works for standard production environments.\nSingle-node variants:\nRich (rich): Production single-node template with local MinIO object storage, local software repo, and all PG extensions. Slim (slim): Installs only PGSQL and ETCD, no monitoring infra. Slim installation can expand to multi-node HA deployment. Infra-only (infra): Opposite of slim—installs only INFRA monitoring infrastructure, no database services, for monitoring other instances. Alternative kernels: Replace vanilla PG with derivatives: pgsql, citus, mssql, polar, ivory, mysql, pgtde, oriole, supabase. Two-Node Setup Two-node configuration enables database replication and Semi-HA capability with better data redundancy and limited failover support:\nID NODE PGSQL INFRA ETCD 1 node-1 pg-meta-1 (replica) infra-1 etcd-1 2 node-2 pg-meta-2 (primary) Two-node HA auto-failover has limitations. This “Semi-HA” setup only auto-recovers from specific node failures:\nIf node-1 fails: No automatic failover—requires manual promotion of node-2 If node-2 fails: Automatic failover works—node-1 auto-promoted Three-Node Setup Three-node template provides true baseline HA configuration, tolerating any single node failure with automatic recovery.\nID NODE PGSQL INFRA ETCD 1 node-1 pg-meta-1 infra-1 etcd-1 2 node-2 pg-meta-2 infra-2 etcd-2 3 node-3 pg-meta-3 infra-3 etcd-3 Four-Node Setup Pigsty Sandbox uses the standard four-node configuration.\nID NODE PGSQL INFRA ETCD 1 node-1 pg-meta-1 infra-1 etcd-1 2 node-2 pg-test-1 3 node-3 pg-test-2 4 node-4 pg-test-3 For demo purposes, INFRA / ETCD modules aren’t configured for HA. You can adjust further:\nID NODE PGSQL INFRA ETCD MINIO 1 node-1 pg-meta-1 infra-1 etcd-1 minio-1 2 node-2 pg-test-1 infra-2 etcd-2 3 node-3 pg-test-2 etcd-3 4 node-4 pg-test-3 More Nodes With proper virtualization infrastructure or abundant resources, you can use more nodes for dedicated deployment of each module, achieving optimal reliability, observability, and performance.\nID NODE INFRA ETCD MINIO PGSQL 1 10.10.10.10 infra-1 pg-meta-1 2 10.10.10.11 infra-2 pg-meta-2 3 10.10.10.21 etcd-1 4 10.10.10.22 etcd-2 5 10.10.10.23 etcd-3 6 10.10.10.31 minio-1 7 10.10.10.32 minio-2 8 10.10.10.33 minio-3 9 10.10.10.34 minio-4 10 10.10.10.40 pg-src-1 11 10.10.10.41 pg-src-2 12 10.10.10.42 pg-src-3 13 10.10.10.50 pg-test-1 14 10.10.10.51 pg-test-2 15 10.10.10.52 pg-test-3 16 …… ","categories":["Tutorial"],"description":"How many nodes? Which modules need HA? How to plan based on available resources and requirements?","excerpt":"How many nodes? Which modules need HA? How to plan based on available …","ref":"/docs/deploy/planning/","tags":"","title":"Planning Architecture and Nodes"},{"body":"Pigsty requires an OS admin user with passwordless SSH and Sudo privileges on all managed nodes.\nThis user must be able to SSH to all managed nodes and execute sudo commands on them.\nUser Typically use names like dba or admin, avoiding root and postgres:\nUsing root for deployment is possible but not a production best practice. Using postgres (pg_dbsu) as admin user is strictly prohibited. Passwordless The passwordless requirement is optional if you can accept entering a password for every ssh and sudo command.\nUse -k|--ask-pass when running playbooks to prompt for SSH password, and -K|--ask-become-pass to prompt for sudo password.\n./deploy.yml -k -K Some enterprise security policies may prohibit passwordless ssh or sudo. In such cases, use the options above, or consider configuring a sudoers rule with a longer password cache time to reduce password prompts.\nCreate Admin User Typically, your server/VM provider creates an initial admin user.\nIf unsatisfied with that user, Pigsty’s deployment playbook can create a new admin user for you.\nAssuming you have root access or an existing admin user on the node, create an admin user with Pigsty itself:\n./node.yml -k -K -t node_admin -e ansible_user=[your_existing_admin] This leverages the existing admin to create a new one—a dedicated dba (uid=88) user described by these parameters, with sudo/ssh properly configured:\nName Description Default node_admin_enabled Enable node admin user true node_admin_uid Node admin user UID 88 node_admin_username Node admin username dba Sudo All admin users should have sudo privileges on all managed nodes, preferably with passwordless execution.\nTo configure an admin user with passwordless sudo from scratch, edit/create a sudoers file (assuming username vagrant):\necho '%vagrant ALL=(ALL) NOPASSWD: ALL' | sudo tee /etc/sudoers.d/vagrant For admin user dba, the /etc/sudoers.d/dba content should be:\n%dba ALL=(ALL) NOPASSWD: ALL If your security policy prohibits passwordless sudo, remove the NOPASSWD: part:\n%dba ALL=(ALL) ALL Ansible relies on sudo to execute commands with root privileges on managed nodes. In environments where sudo is unavailable (e.g., inside Docker containers), install sudo first.\nSSH Your current user should have passwordless SSH access to all managed nodes as the corresponding admin user.\nYour current user can be the admin user itself, but this isn’t required—as long as you can SSH as the admin user.\nSSH configuration is Linux 101, but here are the basics:\nGenerate SSH Key If you don’t have an SSH key pair, generate one:\nssh-keygen -t rsa -b 2048 -N '' -f ~/.ssh/id_rsa -q Pigsty will do this for you during the bootstrap stage if you lack a key pair.\nCopy SSH Key Distribute your generated public key to remote (and local) servers, placing it in the admin user’s ~/.ssh/authorized_keys file on all nodes. Use the ssh-copy-id utility:\nssh-copy-id \u003cip\u003e # Interactive password entry sshpass -p \u003cpassword\u003e ssh-copy-id \u003cip\u003e # Non-interactive (use with caution) Using Alias When direct SSH access is unavailable (jumpserver, non-standard port, different credentials), configure SSH aliases in ~/.ssh/config:\nHost meta HostName 10.10.10.10 User dba # Different user on remote IdentityFile /etc/dba/id_rsa # Non-standard key Port 24 # Non-standard port Reference the alias in the inventory using ansible_host for the real SSH alias:\nnodes: hosts: # If node `10.10.10.10` requires SSH alias `meta` 10.10.10.10: { ansible_host: meta } # Access via `ssh meta` SSH parameters work directly in Ansible. See Ansible Inventory Guide for details. This technique enables accessing nodes in private networks via jumpservers, or using different ports and credentials, or using your local laptop as an admin node.\nCheck Accessibility You should be able to passwordlessly ssh from the admin node to all managed nodes as your current user. The remote user (admin user) should have privileges to run passwordless sudo commands.\nTo verify passwordless ssh/sudo works, run this command on the admin node for all managed nodes:\nssh \u003cip|alias\u003e 'sudo ls' If there’s no password prompt or error, passwordless ssh/sudo is working as expected.\nFirewall Production deployments typically require firewall configuration to block unauthorized port access.\nBy default, block inbound access from office/Internet networks except:\nSSH port 22 for node access HTTP (80) / HTTPS (443) for WebUI services PostgreSQL port 5432 for database access If accessing PostgreSQL via other ports, allow them accordingly. See used ports for the complete port list.\n5432: PostgreSQL database 6432: Pgbouncer connection pooler 5433: PG primary service 5434: PG replica service 5436: PG default service 5438: PG offline service ","categories":["Tutorial"],"description":"Admin user, sudo, SSH, accessibility verification, and firewall configuration","excerpt":"Admin user, sudo, SSH, accessibility verification, and firewall …","ref":"/docs/deploy/admin/","tags":"","title":"Setup Admin User and Privileges"},{"body":"Pigsty provides a standard 4-node sandbox environment for learning, testing, and feature demonstration.\nThe sandbox uses fixed IP addresses and predefined identity identifiers, making it easy to reproduce various demo use cases.\nDescription The default sandbox environment consists of 4 nodes, using the ha/full.yml configuration template.\nID IP Address Node PostgreSQL INFRA ETCD MINIO 1 10.10.10.10 meta pg-meta-1 infra-1 etcd-1 minio-1 2 10.10.10.11 node-1 pg-test-1 3 10.10.10.12 node-2 pg-test-2 4 10.10.10.13 node-3 pg-test-3 The sandbox configuration can be summarized as the following config:\nall: children: infra: { hosts: { 10.10.10.10: { infra_seq: 1 } } } etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 } }, vars: { etcd_cluster: etcd } } minio: { hosts: { 10.10.10.10: { minio_seq: 1 } }, vars: { minio_cluster: minio } } pg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: { pg_cluster: pg-meta } pg-test: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } 10.10.10.12: { pg_seq: 2, pg_role: replica } 10.10.10.13: { pg_seq: 3, pg_role: replica } vars: { pg_cluster: pg-test } vars: version: v4.0.0 admin_ip: 10.10.10.10 region: default pg_version: 18 PostgreSQL Clusters The sandbox comes with a single-instance PostgreSQL cluster pg-meta on the meta node:\n10.10.10.10 meta pg-meta-1 10.10.10.2 pg-meta # Optional L2 VIP There’s also a 3-instance PostgreSQL HA cluster pg-test deployed on the other three nodes:\n10.10.10.11 node-1 pg-test-1 10.10.10.12 node-2 pg-test-2 10.10.10.13 node-3 pg-test-3 10.10.10.3 pg-test # Optional L2 VIP Two optional L2 VIPs are bound to the primary instances of pg-meta and pg-test clusters respectively.\nInfrastructure The meta node also hosts:\nETCD cluster: Single-node etcd cluster providing DCS service for PostgreSQL HA MinIO cluster: Single-node minio cluster providing S3-compatible object storage 10.10.10.10 etcd-1 10.10.10.10 minio-1 Creating Sandbox Pigsty provides out-of-the-box templates. You can use Vagrant to create a local sandbox, or use Terraform to create a cloud sandbox.\nLocal Sandbox (Vagrant) Local sandbox uses VirtualBox/libvirt to create local virtual machines, running free on your Mac / PC.\nTo run the full 4-node sandbox, your machine should have at least 4 CPU cores and 8GB memory.\ncd ~/pigsty make full # Create 4-node sandbox with default RockyLinux 9 image make full9 # Create 4-node sandbox with RockyLinux 9 make full12 # Create 4-node sandbox with Debian 12 make full24 # Create 4-node sandbox with Ubuntu 24.04 For more details, please refer to Vagrant documentation.\nCloud Sandbox (Terraform) Cloud sandbox uses public cloud API to create virtual machines. Easy to create and destroy, pay-as-you-go, ideal for quick testing.\nUse spec/aliyun-full.tf template to create a 4-node sandbox on Alibaba Cloud:\ncd ~/pigsty/terraform cp spec/aliyun-full.tf terraform.tf terraform init terraform apply For more details, please refer to Terraform documentation.\nOther Specs Besides the standard 4-node sandbox, Pigsty also provides other environment specs:\nSingle Node Devbox (meta) The simplest 1-node environment for quick start, development, and testing:\nmake meta # Create single-node devbox Two Node Environment (dual) 2-node environment for testing primary-replica replication:\nmake dual # Create 2-node environment Three Node Environment (trio) 3-node environment for testing basic high availability:\nmake trio # Create 3-node environment Production Simulation (simu) 20-node large simulation environment for full production environment testing:\nmake simu # Create 20-node production simulation environment This environment includes:\n3 infrastructure nodes (meta1, meta2, meta3) 2 HAProxy proxy nodes 4 MinIO nodes 5 ETCD nodes 6 PostgreSQL nodes (2 clusters, 3 nodes each) ","categories":["Tutorial"],"description":"4-node sandbox environment for learning, testing, and demonstration","excerpt":"4-node sandbox environment for learning, testing, and demonstration","ref":"/docs/deploy/sandbox/","tags":"","title":"Sandbox"},{"body":"Vagrant is a popular local virtualization tool that creates local virtual machines in a declarative manner.\nPigsty requires a Linux environment to run. You can use Vagrant to easily create Linux virtual machines locally for testing.\nQuick Start Install Dependencies First, ensure you have Vagrant and a virtual machine provider (such as VirtualBox or libvirt) installed on your system.\nOn macOS, you can use Homebrew for one-click installation:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" brew install vagrant virtualbox ansible VirtualBox requires reboot after installation After installing VirtualBox, you need to restart your system and allow its kernel extensions in System Preferences.\nOn Linux, you can use VirtualBox or vagrant-libvirt as the VM provider.\nCreate Virtual Machines Use the Pigsty-provided make shortcuts to create virtual machines:\ncd ~/pigsty make meta # 1 node devbox for quick start, development, and testing make full # 4 node sandbox for HA testing and feature demonstration make simu # 20 node simubox for production environment simulation # Other less common specs make dual # 2 node environment make trio # 3 node environment make deci # 10 node environment You can use variant aliases to specify different operating system images:\nmake meta9 # Create single node with RockyLinux 9 make full12 # Create 4-node sandbox with Debian 12 make simu24 # Create 20-node simubox with Ubuntu 24.04 Available OS suffixes: 7 (EL7), 8 (EL8), 9 (EL9), 10 (EL10), 11 (Debian 11), 12 (Debian 12), 13 (Debian 13), 20 (Ubuntu 20.04), 22 (Ubuntu 22.04), 24 (Ubuntu 24.04)\nBuild Environment You can also use the following aliases to create Pigsty build environments. These templates won’t replace the base image:\nmake oss # 3 node OSS build environment make pro # 5 node PRO build environment make rpm # 3 node EL7/8/9 build environment make deb # 5 node Debian11/12 Ubuntu20/22/24 build environment make all # 7 node full build environment Spec Templates Pigsty provides multiple predefined VM specs in the vagrant/spec/ directory:\nTemplate Nodes Spec Description Alias meta.rb 1 node 2c4g x 1 Single-node devbox Devbox dual.rb 2 nodes 1c2g x 2 Two-node environment trio.rb 3 nodes 1c2g x 3 Three-node environment full.rb 4 nodes 2c4g + 1c2g x 3 4-node full sandbox Sandbox deci.rb 10 nodes Mixed 10-node environment simu.rb 20 nodes Mixed 20-node production simubox Simubox minio.rb 4 nodes 1c2g x 4 + disk MinIO test environment oss.rb 3 nodes 1c2g x 3 3-node OSS build environment pro.rb 5 nodes 1c2g x 5 5-node PRO build environment rpm.rb 3 nodes 1c2g x 3 3-node EL build environment deb.rb 5 nodes 1c2g x 5 5-node Deb build environment all.rb 7 nodes 1c2g x 7 7-node full build environment Each spec file contains a Specs variable describing the VM nodes. For example, full.rb contains the 4-node sandbox definition:\n# full: pigsty full-featured 4-node sandbox for HA-testing \u0026 tutorial \u0026 practices Specs = [ { \"name\" =\u003e \"meta\" , \"ip\" =\u003e \"10.10.10.10\" , \"cpu\" =\u003e \"2\" , \"mem\" =\u003e \"4096\" , \"image\" =\u003e \"bento/rockylinux-9\" }, { \"name\" =\u003e \"node-1\" , \"ip\" =\u003e \"10.10.10.11\" , \"cpu\" =\u003e \"1\" , \"mem\" =\u003e \"2048\" , \"image\" =\u003e \"bento/rockylinux-9\" }, { \"name\" =\u003e \"node-2\" , \"ip\" =\u003e \"10.10.10.12\" , \"cpu\" =\u003e \"1\" , \"mem\" =\u003e \"2048\" , \"image\" =\u003e \"bento/rockylinux-9\" }, { \"name\" =\u003e \"node-3\" , \"ip\" =\u003e \"10.10.10.13\" , \"cpu\" =\u003e \"1\" , \"mem\" =\u003e \"2048\" , \"image\" =\u003e \"bento/rockylinux-9\" }, ] simu Spec Details simu.rb provides a 20-node production environment simulation configuration:\n3 x infra nodes (meta1-3): 4c16g 2 x haproxy nodes (proxy1-2): 1c2g 4 x minio nodes (minio1-4): 1c2g 5 x etcd nodes (etcd1-5): 1c2g 6 x pgsql nodes (pg-src-1-3, pg-dst-1-3): 2c4g Config Script Use the vagrant/config script to generate the final Vagrantfile based on spec and options:\ncd ~/pigsty vagrant/config [spec] [image] [scale] [provider] # Examples vagrant/config meta # Use 1-node spec with default EL9 image vagrant/config dual el9 # Use 2-node spec with EL9 image vagrant/config trio d12 2 # Use 3-node spec with Debian 12, double resources vagrant/config full u22 4 # Use 4-node spec with Ubuntu 22, 4x resources vagrant/config simu u24 1 libvirt # Use 20-node spec with Ubuntu 24, libvirt provider Image Aliases The config script supports various image aliases:\nDistro Alias Vagrant Box CentOS 7 el7, 7, centos generic/centos7 Rocky 8 el8, 8, rocky8 bento/rockylinux-9 Rocky 9 el9, 9, rocky9, el bento/rockylinux-9 Rocky 10 el10, rocky10 rockylinux/10 Debian 11 d11, 11, debian11 generic/debian11 Debian 12 d12, 12, debian12 generic/debian12 Debian 13 d13, 13, debian13 cloud-image/debian-13 Ubuntu 20.04 u20, 20, ubuntu20 generic/ubuntu2004 Ubuntu 22.04 u22, 22, ubuntu22, ubuntu generic/ubuntu2204 Ubuntu 24.04 u24, 24, ubuntu24 bento/ubuntu-24.04 Resource Scaling You can use the VM_SCALE environment variable to adjust the resource multiplier (default is 1):\nVM_SCALE=2 vagrant/config meta # Double the CPU/memory resources for meta spec For example, using VM_SCALE=4 with the meta spec will adjust the default 2c4g to 8c16g:\nSpecs = [ { \"name\" =\u003e \"meta\" , \"ip\" =\u003e \"10.10.10.10\", \"cpu\" =\u003e \"8\" , \"mem\" =\u003e \"16384\" , \"image\" =\u003e \"bento/rockylinux-9\" }, ] simu spec doesn't support scaling The simu spec doesn’t support resource scaling. The scale parameter will be automatically ignored because its resource configuration is already optimized for simulation scenarios.\nVM Management Pigsty provides a set of Makefile shortcuts for managing virtual machines:\nmake # Equivalent to make start make new # Destroy existing VMs and create new ones make ssh # Write VM SSH config to ~/.ssh/ (must run after creation) make dns # Write VM DNS records to /etc/hosts (optional) make start # Start VMs and configure SSH (up + ssh) make up # Start VMs with vagrant up make halt # Shutdown VMs (alias: down, dw) make clean # Destroy VMs (alias: del, destroy) make status # Show VM status (alias: st) make pause # Pause VMs (alias: suspend) make resume # Resume VMs make nuke # Destroy all VMs and volumes with virsh (libvirt only) make info # Show libvirt info (VMs, networks, storage volumes) SSH Keys Pigsty Vagrant templates use your ~/.ssh/id_rsa[.pub] as the SSH key for VMs by default.\nBefore starting, ensure you have a valid SSH key pair. If not, generate one with:\nssh-keygen -t rsa -b 2048 -N '' -f ~/.ssh/id_rsa -q Supported Images Pigsty currently uses the following Vagrant Boxes for testing:\n# x86_64 / amd64 el8 : bento/rockylinux-8 (libvirt, 202502.21.0, (amd64)) el9 : bento/rockylinux-9 (libvirt, 202502.21.0, (amd64)) el10: rockylinux/10 (libvirt) d11 : generic/debian11 (libvirt, 4.3.12, (amd64)) d12 : generic/debian12 (libvirt, 4.3.12, (amd64)) d13 : cloud-image/debian-13 (libvirt) u20 : generic/ubuntu2004 (libvirt, 4.3.12, (amd64)) u22 : generic/ubuntu2204 (libvirt, 4.3.12, (amd64)) u24 : bento/ubuntu-24.04 (libvirt, 20250316.0.0, (amd64)) For Apple Silicon (aarch64) architecture, fewer images are available:\n# aarch64 / arm64 bento/rockylinux-9 (virtualbox, 202502.21.0, (arm64)) bento/ubuntu-24.04 (virtualbox, 202502.21.0, (arm64)) You can find more available Box images on Vagrant Cloud.\nEnvironment Variables You can use the following environment variables to control Vagrant behavior:\nexport VM_SPEC='meta' # Spec name export VM_IMAGE='bento/rockylinux-9' # Image name export VM_SCALE='1' # Resource scaling multiplier export VM_PROVIDER='virtualbox' # Virtualization provider export VAGRANT_EXPERIMENTAL=disks # Enable experimental disk features Notes VirtualBox Network Configuration When using older versions of VirtualBox as Vagrant provider, additional configuration is required to use 10.x.x.x CIDR as Host-Only network:\necho \"* 10.0.0.0/8\" | sudo tee -a /etc/vbox/networks.conf First-time image download is slow The first time you use Vagrant to start a specific operating system, it will download the corresponding Box image file (typically 1-2 GB). After download, the image is cached and reused for subsequent VM creation.\nlibvirt Provider If you’re using libvirt as the provider, you can use make info to view VMs, networks, and storage volume information, and make nuke to forcefully destroy all related resources.\n","categories":["Tutorial"],"description":"Create local virtual machine environment with Vagrant","excerpt":"Create local virtual machine environment with Vagrant","ref":"/docs/deploy/vagrant/","tags":"","title":"Vagrant"},{"body":"Terraform is a popular “Infrastructure as Code” tool that you can use to create virtual machines on public clouds with one click.\nPigsty provides Terraform templates for Alibaba Cloud, AWS, and Tencent Cloud as examples.\nQuick Start Install Terraform On macOS, you can use Homebrew to install Terraform:\nbrew install terraform For other platforms, refer to the Terraform Official Installation Guide.\nInitialize and Apply Enter the Terraform directory, select a template, initialize provider plugins, and apply the configuration:\ncd ~/pigsty/terraform cp spec/aliyun-meta.tf terraform.tf # Select template terraform init # Install cloud provider plugins (first use) terraform apply # Generate execution plan and create resources After running the apply command, type yes to confirm when prompted. Terraform will create VMs and related cloud resources for you.\nGet IP Address After creation, print the public IP address of the admin node:\nterraform output | grep -Eo '[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}' Configure SSH Access Use the ssh script to automatically configure SSH aliases and distribute keys:\n./ssh # Write SSH config to ~/.ssh/pigsty_config and copy keys This script writes the IP addresses from Terraform output to ~/.ssh/pigsty_config and automatically distributes SSH keys using the default password PigstyDemo4.\nAfter configuration, you can login directly using hostnames:\nssh meta # Login using hostname instead of IP Using SSH Config File If you want to use the configuration in ~/.ssh/pigsty_config, ensure your ~/.ssh/config includes:\nInclude ~/.ssh/pigsty_config Destroy Resources After testing, you can destroy all created cloud resources with one click:\nterraform destroy Template Specs Pigsty provides multiple predefined cloud resource templates in the terraform/spec/ directory:\nTemplate File Cloud Provider Description aliyun-meta.tf Alibaba Cloud Single-node meta template, supports all distros and AMD/ARM (default) aliyun-meta-s3.tf Alibaba Cloud Single-node template + OSS bucket for PITR backup aliyun-full.tf Alibaba Cloud 4-node sandbox template, supports all distros and AMD/ARM aliyun-oss.tf Alibaba Cloud 5-node build template, supports all distros and AMD/ARM aliyun-pro.tf Alibaba Cloud Multi-distro test template for cross-OS testing aws-cn.tf AWS AWS China region 4-node environment tencentcloud.tf Tencent Cloud Tencent Cloud 4-node environment When using a template, copy the template file to terraform.tf:\ncd ~/pigsty/terraform cp spec/aliyun-full.tf terraform.tf # Use Alibaba Cloud 4-node sandbox template terraform init \u0026\u0026 terraform apply Variable Configuration Pigsty’s Terraform templates use variables to control architecture, OS distribution, and resource configuration:\nArchitecture and Distribution variable \"architecture\" { description = \"Architecture type (amd64 or arm64)\" type = string default = \"amd64\" # Comment this line to use arm64 #default = \"arm64\" # Uncomment to use arm64 } variable \"distro\" { description = \"Distribution code (el8,el9,el10,u22,u24,d12,d13)\" type = string default = \"el9\" # Default uses Rocky Linux 9 } Resource Configuration The following resource parameters can be configured in the locals block:\nlocals { bandwidth = 100 # Public bandwidth (Mbps) disk_size = 40 # System disk size (GB) spot_policy = \"SpotWithPriceLimit\" # Spot policy: NoSpot, SpotWithPriceLimit, SpotAsPriceGo spot_price_limit = 5 # Max spot price (only effective with SpotWithPriceLimit) } Alibaba Cloud Configuration Credential Setup Add your Alibaba Cloud credentials to environment variables, for example in ~/.bash_profile or ~/.zshrc:\nexport ALICLOUD_ACCESS_KEY=\"\u003cyour_access_key\u003e\" export ALICLOUD_SECRET_KEY=\"\u003cyour_secret_key\u003e\" export ALICLOUD_REGION=\"cn-shanghai\" Supported Images The following are commonly used ECS Public OS Image prefixes in Alibaba Cloud:\nDistro Code x86_64 Image Prefix aarch64 Image Prefix CentOS 7.9 el7 centos_7_9_x64 - Rocky 8.10 el8 rockylinux_8_10_x64 rockylinux_8_10_arm64 Rocky 9.6 el9 rockylinux_9_6_x64 rockylinux_9_6_arm64 Rocky 10.0 el10 rockylinux_10_0_x64 rockylinux_10_0_arm64 Debian 11.11 d11 debian_11_11_x64 - Debian 12.11 d12 debian_12_11_x64 debian_12_11_arm64 Debian 13.2 d13 debian_13_2_x64 debian_13_2_arm64 Ubuntu 20.04 u20 ubuntu_20_04_x64 - Ubuntu 22.04 u22 ubuntu_22_04_x64 ubuntu_22_04_arm64 Ubuntu 24.04 u24 ubuntu_24_04_x64 ubuntu_24_04_arm64 Anolis 8.9 an8 anolisos_8_9_x64 - Alibaba Cloud Linux 3 al3 aliyun_3_0_x64 - OSS Storage Configuration The aliyun-meta-s3.tf template additionally creates an OSS bucket and related permissions for PostgreSQL PITR backup:\nOSS Bucket: Creates a private bucket named pigsty-oss RAM User: Creates a dedicated pigsty-oss-user user Access Key: Generates AccessKey and saves to ~/pigsty.sk IAM Policy: Grants full access to the bucket AWS Configuration Credential Setup Set up AWS configuration and credential files:\n# ~/.aws/config [default] region = cn-northwest-1 # ~/.aws/credentials [default] aws_access_key_id = \u003cYOUR_AWS_ACCESS_KEY\u003e aws_secret_access_key = \u003cAWS_ACCESS_SECRET\u003e If you need to use SSH keys, place the key files at:\n~/.aws/pigsty-key ~/.aws/pigsty-key.pub AWS templates may need adjustments AWS templates are community-contributed examples and may need adjustments based on your specific requirements.\nTencent Cloud Configuration Credential Setup Add Tencent Cloud credentials to environment variables:\nexport TENCENTCLOUD_SECRET_ID=\"\u003cyour_secret_id\u003e\" export TENCENTCLOUD_SECRET_KEY=\"\u003cyour_secret_key\u003e\" export TENCENTCLOUD_REGION=\"ap-beijing\" Tencent Cloud templates may need adjustments Tencent Cloud templates are community-contributed examples and may need adjustments based on your specific requirements.\nShortcut Commands Pigsty provides some Makefile shortcuts for Terraform operations:\ncd ~/pigsty/terraform make u # terraform apply -auto-approve + configure SSH make d # terraform destroy -auto-approve make apply # terraform apply (interactive confirmation) make destroy # terraform destroy (interactive confirmation) make out # terraform output make ssh # Run ssh script to configure SSH access make r # Reset terraform.tf to repository state Notes Cloud Resource Costs Cloud resources created with Terraform incur costs. After testing, promptly use terraform destroy to destroy resources to avoid unnecessary expenses.\nIt’s recommended to use pay-as-you-go instance types for testing. Templates default to using Spot Instances to reduce costs.\nDefault Password The default root password for VMs in all templates is PigstyDemo4. In production environments, be sure to change this password or use SSH key authentication.\nSecurity Group Configuration Terraform templates automatically create security groups and open necessary ports (all TCP ports open by default). In production environments, adjust security group rules according to actual needs, following the principle of least privilege.\nSSH Access After creation, SSH login to the admin node using:\nssh root@\u003cpublic_ip\u003e You can also use ./ssh or make ssh to write SSH aliases to the config file, then login using ssh pg-meta.\n","categories":["Tutorial"],"description":"Create virtual machine environment on public cloud with Terraform","excerpt":"Create virtual machine environment on public cloud with Terraform","ref":"/docs/deploy/terraform/","tags":"","title":"Terraform"},{"body":"Pigsty’s default configuration is sufficient to cover the security needs of most scenarios.\nPigsty already provides out-of-the-box authentication and access control models that are secure enough for most scenarios.\nIf you want to further harden system security, here are some recommendations:\nConfidentiality Important Files Protect your pigsty.yml configuration file or CMDB\nThe pigsty.yml configuration file usually contains highly sensitive confidential information. You should ensure its security. Strictly control access permissions to admin nodes, limiting access to DBAs or Infra administrators only. Strictly control access permissions to the pigsty.yml configuration file repository (if you manage it with git) Protect your CA private key and other certificates, these files are very important.\nRelated files are generated by default in the files/pki directory under the Pigsty source directory on the admin node. You should regularly back them up to a secure location. Passwords You MUST change these passwords when deploying to production, don’t use defaults!\ngrafana_admin_password : pigsty pg_admin_password : DBUser.DBA pg_monitor_password : DBUser.Monitor pg_replication_password : DBUser.Replicator patroni_password : Patroni.API haproxy_admin_password : pigsty minio_access_key : minioadmin minio_secret_key : minioadmin If using MinIO, change the default MinIO user passwords and references in pgbackrest\nModify MinIO regular user password: minio_users.[pgbackrest].secret_key` Modify the backup user password used by pgbackrest for MinIO: pgbackrest_repo.minio.s3_key_secret` If using remote backup repositories, enable backup encryption and set encryption passwords\nSet [pgbackrest_repo.*.cipher_type](/docs/pgsql/param#pgbackrest_repo) to aes-256-cbc` You can use ${pg_cluster} as part of the password to avoid all clusters using the same password Use secure and reliable password encryption algorithms for PostgreSQL\nUse pg_pwd_enc default value scram-sha-256 instead of legacy md5 This is the default behavior. Unless there’s a special reason (supporting legacy old clients), don’t change it back to md5 Use passwordcheck extension to enforce strong passwords\nAdd $lib/passwordcheck to pg_libs to enforce password policies. Encrypt remote backups with encryption algorithms\nUse repo_cipher_type in pgbackrest_repo backup repository definitions to enable encryption Configure automatic password expiration for business users\nYou should set an automatic password expiration time for each business user to meet compliance requirements.\nAfter configuring auto-expiration, don’t forget to regularly update these passwords during maintenance.\n- { name: dbuser_meta , password: Pleas3-ChangeThisPwd ,expire_in: 7300 ,pgbouncer: true ,roles: [ dbrole_admin ] ,comment: pigsty admin user } - { name: dbuser_view , password: Make.3ure-Compl1ance ,expire_in: 7300 ,pgbouncer: true ,roles: [ dbrole_readonly ] ,comment: read-only viewer for meta database } - { name: postgres ,superuser: true ,expire_in: 7300 ,comment: system superuser } - { name: replicator ,replication: true ,expire_in: 7300 ,roles: [pg_monitor, dbrole_readonly] ,comment: system replicator } - { name: dbuser_dba ,superuser: true ,expire_in: 7300 ,roles: [dbrole_admin] ,pgbouncer: true ,pool_mode: session, pool_connlimit: 16 , comment: pgsql admin user } - { name: dbuser_monitor ,roles: [pg_monitor] ,expire_in: 7300 ,pgbouncer: true ,parameters: {log_min_duration_statement: 1000 } ,pool_mode: session ,pool_connlimit: 8 ,comment: pgsql monitor user } Don’t log password change statements to postgres logs or other logs\nSET log_statement TO 'none'; ALTER USER \"{{ user.name }}\" PASSWORD '{{ user.password }}'; SET log_statement TO DEFAULT; IP Addresses Bind specified IP addresses for postgres/pgbouncer/patroni, not all addresses.\nThe default pg_listen address is 0.0.0.0, meaning all IPv4 addresses. Consider using pg_listen: '${ip},${vip},${lo}' to bind to specific IP address(es) for enhanced security. Don’t expose any ports directly to public IP, except infrastructure egress Nginx ports (default 80/443)\nFor convenience, components like Prometheus/Grafana listen on all IP addresses by default and can be accessed directly via public IP ports You can modify their configurations to listen only on internal IP addresses, restricting access through the Nginx portal via domain names only. You can also use security groups or firewall rules to implement these security restrictions. For convenience, Redis servers listen on all IP addresses by default. You can modify redis_bind_address to listen only on internal IP addresses. Use HBA to restrict postgres client access\nThere’s a security-enhanced configuration template: security.yml Restrict patroni management access: only infra/admin nodes can call control APIs\nBy default, this is restricted via restapi.allowlist. Network Traffic Use SSL and domain names to access infrastructure components through Nginx\nNginx SSL is controlled by nginx_sslmode, default is enable. Nginx domain names are specified by infra_portal..domain. Use SSL to protect Patroni REST API\npatroni_ssl_enabled is disabled by default. Because it affects health checks and API calls. Note this is a global option; you must decide before deployment. Use SSL to protect Pgbouncer client traffic\npgbouncer_sslmode defaults to disable It has significant performance impact on Pgbouncer, so it’s disabled by default. Integrity Configure consistency-first mode for critical PostgreSQL database clusters (e.g., finance-related databases)\npg_conf database tuning template, using crit.yml will trade some availability for best data consistency. Use crit node tuning template for better consistency.\nnode_tune host tuning template using crit can reduce dirty page ratio and lower data consistency risks. Enable data checksums to detect silent data corruption.\npg_checksum defaults to off, but is recommended to enable. When pg_conf = crit.yml is enabled, checksums are mandatory. Log connection establishment/termination\nThis is disabled by default, but enabled by default in the crit.yml config template. You can manually configure the cluster to enable log_connections and log_disconnections parameters. Enable watchdog if you want to completely eliminate the possibility of split-brain during PG cluster failover\nIf your traffic goes through the recommended default HAProxy distribution, you won’t encounter split-brain even without watchdog. If your machine hangs and Patroni is killed with kill -9, watchdog can serve as a fallback: automatic shutdown on timeout. It’s best not to enable watchdog on infrastructure nodes. Availability Use sufficient nodes/instances for critical PostgreSQL database clusters\nYou need at least three nodes (able to tolerate one node failure) for production-grade high availability. If you only have two nodes, you can tolerate specific standby node failures. If you only have one node, use external S3/MinIO for cold backup and WAL archive storage. For PostgreSQL, make trade-offs between availability and consistency\npg_rpo : Trade-off between availability and consistency pg_rto : Trade-off between failure probability and impact Don’t access databases directly via fixed IP addresses; use VIP, DNS, HAProxy, or combinations\nUse HAProxy for service access In case of failover/switchover, HAProxy will handle client traffic switching. Use multiple infrastructure nodes in important production deployments (e.g., 1~3)\nSmall deployments or lenient scenarios can use a single infrastructure/admin node. Large production deployments should have at least two infrastructure nodes as mutual backup. Use sufficient etcd server instances, and use an odd number of instances (1,3,5,7)\nSee ETCD Administration for details. ","categories":["Tutorial"],"description":"Security considerations for production Pigsty deployment","excerpt":"Security considerations for production Pigsty deployment","ref":"/docs/deploy/security/","tags":"","title":"Security"},{"body":"","categories":["Reference"],"description":"Detailed reference information and lists, including supported OS distros, available modules, monitor metrics, extensions, cost comparison and analysis, glossary","excerpt":"Detailed reference information and lists, including supported OS …","ref":"/docs/ref/","tags":"","title":"References"},{"body":"Pigsty runs on Linux, supporting amd64/x86_64 and arm64/aarch64 arch, plus 3 major distros: EL, Debian, Ubuntu.\nPigsty runs bare-metal without containers. Supports latest 2 major releases for each of the 3 major distros across both archs.\nOverview Recommended OS versions: RockyLinux 10.0, Ubuntu 24.04, Debian 13.1.\nDistro Arch OS Code PG18 PG17 PG16 PG15 PG14 PG13 RHEL / Rocky / Alma 10 x86_64 el10.x86_64 RHEL / Rocky / Alma 10 aarch64 el10.aarch64 Ubuntu 24.04 (noble) x86_64 u24.x86_64 Ubuntu 24.04 (noble) aarch64 u24.aarch64 Debian 13 (trixie) x86_64 d13.x86_64 Debian 13 (trixie) aarch64 d13.aarch64 EL Pigsty supports RHEL / Rocky / Alma / Anolis / CentOS 8, 9, 10.\nEL Distro Arch OS Code PG18 PG17 PG16 PG15 PG14 PG13 RHEL10 / Rocky10 / Alma10 x86_64 el10.x86_64 RHEL10 / Rocky10 / Alma10 aarch64 el10.aarch64 RHEL9 / Rocky9 / Alma9 x86_64 el9.x86_64 RHEL9 / Rocky9 / Alma9 aarch64 el9.aarch64 RHEL8 / Rocky8 / Alma8 x86_64 el8.x86_64 RHEL8 / Rocky8 / Alma8 aarch64 el8.aarch64 RHEL7 / CentOS7 x86_64 el7.x86_64 RHEL7 / CentOS7 aarch64 - RockyLinux 10.0 / 9.6 Recommended RockyLinux 10.0 / 9.6 balances stability and fresh software. Recommended for EL users.\nEL8 EOL Soon EL8 goes EOL in 2029. Plan upgrade ASAP. EL10 support is ready, EL8 will be dropped in next release.\nEL 7 EOL @ 2024-06 RHEL 7 EOL since Jun 2024. PGDG stopped providing binary packages for PG 16/17/18 on EL7.\nFor extended support on legacy OS, consider Enterprise Subscription.\nUbuntu Pigsty supports Ubuntu 24.04 / 22.04:\nUbuntu Distro Arch OS Code PG18 PG17 PG16 PG15 PG14 PG13 Ubuntu 24.04 (noble) x86_64 u24.x86_64 Ubuntu 24.04 (noble) aarch64 u24.aarch64 Ubuntu 22.04 (jammy) x86_64 u22.x86_64 Ubuntu 22.04 (jammy) aarch64 u22.aarch64 Ubuntu 20.04 (focal) x86_64 u20.x86_64 Ubuntu 20.04 (focal) aarch64 - Ubuntu 22.04 / 24.04 LTS Recommended Ubuntu 24.04 balances stability and fresh software. Recommended for Ubuntu users.\nUbuntu 20.04 EOL Ubuntu 20.04 EOL since Apr 2025. For extended support on legacy OS, consider Enterprise Subscription.\nDebian Pigsty supports Debian 12 / 13, latest Debian 13.1 recommended:\nDebian Distro Arch OS Code PG18 PG17 PG16 PG15 PG14 PG13 Debian 13 (trixie) x86_64 d13.x86_64 Debian 13 (trixie) aarch64 d13.aarch64 Debian 12 (bookworm) x86_64 d12.x86_64 Debian 12 (bookworm) aarch64 d12.aarch64 Debian 11 (bullseye) x86_64 d11.x86_64 Debian 11 (bullseye) aarch64 - Debian 12.11 / 13.1 Recommended Debian 11 EOL @ 2024-07 Debian 11 EOL since Jul 2024. For extended support on legacy OS, consider Enterprise Subscription.\nVagrant For local VM deployment, use these Vagrant base images (same as used in Pigsty dev):\ngeneric/rocky8: Rocky 8.10 generic/rocky9: Rocky 9.6 generic/debian12: Debian 12.11 generic/debian13: Debian 13 generic/ubuntu2204: Ubuntu 22.04 bento/ubuntu-24.04: Ubuntu 24.04.2 Terraform For cloud deployment, use these Terraform base images (Aliyun example):\nRocky 8.10 : rockylinux_8_10_x64_20G_alibase_20240923.vhd Rocky 9.6 : rockylinux_9_6_x64_20G_alibase_20250101.vhd Ubuntu 22.04 : ubuntu_22_04_x64_20G_alibase_20240926.vhd Ubuntu 24.04 : ubuntu_24_04_x64_20G_alibase_20240923.vhd Debian 12.11 : debian_12_11_x64_20G_alibase_20241201.vhd Debian 13 : debian_13_x64_20G_alibase_20250101.vhd ","categories":["Reference"],"description":"Pigsty compatible Linux OS distribution major versions and CPU architectures","excerpt":"Pigsty compatible Linux OS distribution major versions and CPU …","ref":"/docs/ref/linux/","tags":"","title":"Supported Linux"},{"body":" Core Modules Pigsty provides four core modules that are essential for delivering fully-featured, highly available PostgreSQL services:\nPGSQL: Self-healing PostgreSQL clusters with HA, PITR, IaC, SOP, monitoring, and 440 extensions out of the box. INFRA: Local software repo, VictoriaMetrics, Grafana, VictoriaLogs, AlertManager, PushGateway, Blackbox Exporter… NODE: Tune nodes to desired state: hostname, timezone, NTP, ssh, sudo, haproxy, docker, vector, keepalived. ETCD: Distributed key-value store serving as DCS for HA PostgreSQL clusters: consensus, config management, service discovery. Kernel Modules Pigsty provides four kernel modules as optional in-place replacements for the vanilla PostgreSQL kernel, offering different database flavors:\nMSSQL: Microsoft SQL Server wire-protocol compatible PG kernel, powered by AWS, WiltonDB \u0026 Babelfish! IVORY: Oracle-compatible PostgreSQL 16 kernel, from the IvorySQL open-source project by HighGo. POLAR: “Cloud-native” PostgreSQL kernel open-sourced by Alibaba Cloud, an Aurora-style RAC PostgreSQL fork. CITUS: Distributed PostgreSQL cluster via extension (Azure Hyperscale), with native Patroni HA support! Chinese Domestic Kernel Support! Pigsty Pro Edition provides Chinese domestic database kernel support: PolarDB-O v2 — an Oracle-compatible domestic database kernel based on PolarPG.\nExtension Modules Pigsty provides four extension modules that are not essential for core functionality but can enhance PostgreSQL capabilities:\nMINIO: S3-compatible simple object storage server, serving as optional backup repository for PostgreSQL, with production deployment and monitoring support. REDIS: Redis server, high-performance data structure server, supporting standalone, sentinel, and cluster deployment modes with comprehensive monitoring. MONGO: Native FerretDB deployment support — adding MongoDB wire-protocol level API compatibility to PostgreSQL! DOCKER: Docker daemon service, enabling one-click deployment of containerized stateless software templates to extend Pigsty’s capabilities! Peripheral Modules Pigsty also supports peripheral modules that are closely related to the PostgreSQL kernel (extensions, forks, derivatives, wrappers):\nDUCKDB: Powerful embedded OLAP database. Pigsty provides binaries, dynamic libraries, and related PG extensions: pg_duckdb, pg_lakehouse, and duckdb_fdw. SUPABASE: Pigsty allows running the popular Firebase open-source alternative — Supabase — on existing HA PostgreSQL clusters! GREENPLUM: MPP data warehouse based on PostgreSQL 12 kernel, currently with monitoring and RPM installation support only. (Beta) CLOUDBERRY: Open-source fork by original Greenplum developers after it went closed-source, based on PG 14 kernel, currently RPM installation support only. (Beta) NEON: Serverless PostgreSQL kernel with database branching capabilities. (WIP) Pilot Modules Pigsty is adding support for some pilot modules related to the PostgreSQL ecosystem. These may become official Pigsty modules in the future:\nKAFKA: Deploy KRaft-powered Kafka message queues with Pigsty, with out-of-the-box monitoring support. (Beta) MYSQL: Deploy highly available MySQL 8.0 clusters with Pigsty, with out-of-the-box monitoring support (for critique/migration evaluation). (Beta) KUBE: Production-grade Kubernetes deployment and monitoring using SealOS. (Alpha) VICTORIA: Alternative Infra implementation based on VictoriaMetrics and VictoriaLogs, offering better performance and resource utilization. (Alpha) JUPYTER: Out-of-the-box Jupyter Notebook environment for data analysis and machine learning scenarios. (Alpha) Monitoring Other Databases Pigsty’s INFRA module can be used standalone as an out-of-the-box monitoring infrastructure to monitor other nodes or existing PostgreSQL databases:\nExisting PostgreSQL Services: Pigsty can monitor external PostgreSQL services not managed by Pigsty, still providing relatively complete monitoring support. RDS PG: PostgreSQL RDS services provided by cloud vendors can be monitored as standard external Postgres instances. PolarDB: Alibaba Cloud’s cloud-native database can be monitored as external PostgreSQL 11 / 14 instances. KingBase: A Chinese domestic database provided by KINGBASE, monitored as external PostgreSQL 12 instances. Greenplum / YMatrixDB monitoring: Currently monitored as horizontally sharded PostgreSQL clusters. ","categories":["Reference"],"description":"This article lists all available modules in Pigsty and the module roadmap.","excerpt":"This article lists all available modules in Pigsty and the module …","ref":"/docs/ref/module/","tags":"","title":"Pigsty Modules"},{"body":"Pigsty has 440 extensions. See PGEXT.CLOUD for details, maintained by PIGSTY.\nCategory All PGDG PIGSTY CONTRIB MISS PG18 PG17 PG16 PG15 PG14 PG13 ALL 440 149 268 71 0 408 429 428 430 415 386 EL 434 143 268 71 6 397 421 422 424 412 382 Debian 426 105 250 71 14 394 416 414 416 404 371 ","categories":["Reference"],"description":"This article lists PostgreSQL extensions supported by Pigsty and their compatibility across different systems.","excerpt":"This article lists PostgreSQL extensions supported by Pigsty and their …","ref":"/docs/ref/extension/","tags":"","title":"Extensions"},{"body":" Pigsty FHS Pigsty’s home directory is located at ~/pigsty by default. The file structure within this directory is as follows:\n#------------------------------------------------------------------------------ # pigsty # ^-----@app # Extra application resources and examples # ^-----@bin # Utility scripts # ^-----@docs # Documentation (docsify-compatible) # ^-----@files # Ansible file resources # ^-----@victoria # VictoriaMetrics rule definitions # ^-----@grafana # Grafana dashboards # ^-----@postgres # /pg/bin/ scripts # ^-----@migration # PGSQL migration task definitions # ^-----@pki # Self-signed CA and certificates # ^-----@roles # Ansible role implementations # ^-----@templates # Ansible template files # ^-----@vagrant # Vagrant sandbox VM templates # ^-----@terraform # Terraform cloud VM provisioning templates # ^-----configure # Configuration wizard script # ^-----ansible.cfg # Ansible default configuration # ^-----pigsty.yml # Pigsty default configuration file # ^-----*.yml # Ansible playbooks #------------------------------------------------------------------------------ # /etc/pigsty/ # ^-----@targets # File-based service discovery targets # ^-----@dashboards # Grafana monitoring dashboards # ^-----@datasources # Grafana data sources # ^-----@playbooks # Ansible playbooks #------------------------------------------------------------------------------ CA FHS Pigsty’s self-signed CA is located in files/pki/ under the Pigsty home directory.\nYou must keep the CA key file secure: files/pki/ca/ca.key. This key is generated by the ca role during deploy.yml or infra.yml execution.\n# pigsty/files/pki # ^-----@ca # Self-signed CA key and certificate # ^-----@ca.key # CRITICAL: Keep this secret # ^-----@ca.crt # CRITICAL: Trusted everywhere # ^-----@csr # Certificate signing requests # ^-----@misc # Miscellaneous certificates, issued certs # ^-----@etcd # ETCD server certificates # ^-----@minio # MinIO server certificates # ^-----@nginx # Nginx SSL certificates # ^-----@infra # Infra client certificates # ^-----@pgsql # PostgreSQL server certificates # ^-----@mongo # MongoDB/FerretDB server certificates # ^-----@mysql # MySQL server certificates (placeholder) Nodes managed by Pigsty will have the following certificate files installed:\n/etc/pki/ca.crt # Root certificate added to all nodes /etc/pki/ca-trust/source/anchors/ca.crt # Symlink to system trust anchors All infra nodes will have the following certificates:\n/etc/pki/infra.crt # Infra node certificate /etc/pki/infra.key # Infra node private key When your admin node fails, the files/pki directory and pigsty.yml file should be available on the backup admin node. You can use rsync to achieve this:\n# run on meta-1, rsync to meta2 cd ~/pigsty; rsync -avz ./ meta-2:~/pigsty NODE FHS The node data directory is specified by the node_data parameter, defaulting to /data, owned by root with permissions 0777.\nEach component’s default data directory is located under this data directory:\n/data # ^-----@postgres # PostgreSQL database directory # ^-----@backups # PostgreSQL backup directory (when no dedicated backup disk) # ^-----@redis # Redis data directory (shared by multiple instances) # ^-----@minio # MinIO data directory (single-node single-disk mode) # ^-----@etcd # ETCD main data directory # ^-----@infra # Infra module data directory # ^-----@docker # Docker data directory # ^-----@... # Other component data directories Prometheus FHS Prometheus main configuration file is located at roles/infra/templates/prometheus/prometheus.yml.j2 and is rendered to /etc/prometheus/prometheus.yml on all infrastructure nodes.\nVictoriaMetrics-related scripts and rule definitions are placed in the files/victoria/ directory under the Pigsty home directory, and are copied to /etc/prometheus/ on all infrastructure nodes.\n# /etc/prometheus/ # ^-----prometheus.yml # Prometheus main configuration file # ^-----@bin # Utility scripts: check config, show status, reload, rebuild # ^-----@rules # Recording and alerting rule definitions # ^-----infra.yml # Infra rules and alerts # ^-----etcd.yml # ETCD rules and alerts # ^-----node.yml # Node rules and alerts # ^-----pgsql.yml # PGSQL rules and alerts # ^-----redis.yml # Redis rules and alerts # ^-----minio.yml # MinIO rules and alerts # ^-----kafka.yml # Kafka rules and alerts # ^-----mysql.yml # MySQL rules and alerts # ^-----@targets # File-based service discovery target definitions # ^-----@infra # Infra static target definitions # ^-----@node # Node static target definitions # ^-----@pgsql # PGSQL static target definitions # ^-----@pgrds # PGSQL remote RDS targets # ^-----@redis # Redis static target definitions # ^-----@minio # MinIO static target definitions # ^-----@mongo # MongoDB static target definitions # ^-----@mysql # MySQL static target definitions # ^-----@etcd # ETCD static target definitions # ^-----@ping # Ping static target definitions # ^-----@patroni # Patroni static targets (used when Patroni SSL is enabled) # ^-----@..... # Other monitoring target definitions # /etc/alertmanager.yml # Alertmanager main configuration file # /etc/blackbox.yml # Blackbox exporter main configuration file PostgreSQL FHS The following parameters are related to PostgreSQL database directory structure:\npg_dbsu_home: Postgres default user home directory, defaults to /var/lib/pgsql pg_bin_dir: Postgres binary directory, defaults to /usr/pgsql/bin/ pg_data: Postgres database directory, defaults to /pg/data pg_fs_main: Postgres main data disk mount point, defaults to /data pg_fs_backup: Postgres backup disk mount point, defaults to /data/backups (optional, can also backup to a subdirectory on the main data disk) # Working assumptions: # {{ pg_fs_main }} main data directory, default location: `/data` [fast SSD] # {{ pg_fs_backup }} backup data disk, default location: `/data/backups` [cheap HDD] #--------------------------------------------------------------# # Default configuration: # pg_fs_main = /data High-speed SSD # pg_fs_backup = /data/backups Cheap HDD (optional) # # /pg -\u003e /data/postgres/pg-test-15 (symlink) # /pg/data -\u003e /data/postgres/pg-test-15/data #--------------------------------------------------------------# - name: create postgresql directories tags: pg_dir become: yes block: - name: make main and backup data dir file: path={{ item }} state=directory owner=root mode=0777 with_items: - \"{{ pg_fs_main }}\" - \"{{ pg_fs_backup }}\" # pg_cluster_dir: \"{{ pg_fs_main }}/postgres/{{ pg_cluster }}-{{ pg_version }}\" - name: create postgres directories file: path={{ item }} state=directory owner={{ pg_dbsu }} group=postgres mode=0700 with_items: - \"{{ pg_fs_main }}/postgres\" - \"{{ pg_cluster_dir }}\" - \"{{ pg_cluster_dir }}/bin\" - \"{{ pg_cluster_dir }}/log\" - \"{{ pg_cluster_dir }}/tmp\" - \"{{ pg_cluster_dir }}/cert\" - \"{{ pg_cluster_dir }}/conf\" - \"{{ pg_cluster_dir }}/data\" - \"{{ pg_cluster_dir }}/meta\" - \"{{ pg_cluster_dir }}/stat\" - \"{{ pg_cluster_dir }}/change\" - \"{{ pg_backup_dir }}/backup\" Data File Structure\n# Physical directories {{ pg_fs_main }} /data # Top-level data directory, typically fast SSD mount point {{ pg_dir_main }} /data/postgres # Contains all Postgres instance data (may have multiple instances/versions) {{ pg_cluster_dir }} /data/postgres/pg-test-15 # Contains `pg-test` cluster data (major version 15) /data/postgres/pg-test-15/bin # PostgreSQL utility scripts /data/postgres/pg-test-15/log # Logs: postgres/pgbouncer/patroni/pgbackrest /data/postgres/pg-test-15/tmp # Temporary files, e.g., rendered SQL files /data/postgres/pg-test-15/cert # PostgreSQL server certificates /data/postgres/pg-test-15/conf # PostgreSQL configuration file index /data/postgres/pg-test-15/data # PostgreSQL main data directory /data/postgres/pg-test-15/meta # PostgreSQL identity information /data/postgres/pg-test-15/stat # Statistics, log reports, summary digests /data/postgres/pg-test-15/change # Change records {{ pg_fs_backup }} /data/backups # Optional backup disk directory/mount point /data/backups/postgres/pg-test-15/backup # Actual storage location for cluster backups # Symlinks /pg -\u003e /data/postgres/pg-test-15 # pg root symlink /pg/data -\u003e /data/postgres/pg-test-15/data # pg data directory /pg/backup -\u003e /var/backups/postgres/pg-test-15/backup # pg backup directory Binary File Structure\nOn EL-compatible distributions (using yum), PostgreSQL default installation location is:\n/usr/pgsql-${pg_version}/ Pigsty creates a symlink named /usr/pgsql pointing to the actual version specified by the pg_version parameter, for example:\n/usr/pgsql -\u003e /usr/pgsql-15 Therefore, the default pg_bin_dir is /usr/pgsql/bin/, and this path is added to the system PATH environment variable, defined in: /etc/profile.d/pgsql.sh.\nexport PATH=\"/usr/pgsql/bin:/pg/bin:$PATH\" export PGHOME=/usr/pgsql export PGDATA=/pg/data On Ubuntu/Debian, the default PostgreSQL Deb package installation location is:\n/usr/lib/postgresql/${pg_version}/bin Pgbouncer FHS Pgbouncer runs under the same user as {{ pg_dbsu }} (defaults to postgres), with configuration files located in /etc/pgbouncer.\npgbouncer.ini: Connection pool main configuration file database.txt: Defines databases in the connection pool userlist.txt: Defines users in the connection pool pgb_hba.conf: Defines access permissions for the connection pool Redis FHS Pigsty provides basic support for Redis deployment and monitoring.\nRedis binaries are installed in /bin/ via RPM packages or binary copy, including:\nredis-server redis-server redis-cli redis-sentinel redis-check-rdb redis-check-aof redis-benchmark /usr/libexec/redis-shutdown For a Redis instance named redis-test-1-6379, the related resources are as follows:\n/usr/lib/systemd/system/redis-test-1-6379.service # Service (on Debian: /lib/systemd) /etc/redis/redis-test-1-6379.conf # Configuration /data/redis/redis-test-1-6379 # Database directory /data/redis/redis-test-1-6379/redis-test-1-6379.rdb # RDB file /data/redis/redis-test-1-6379/redis-test-1-6379.aof # AOF file /var/log/redis/redis-test-1-6379.log # Log /var/run/redis/redis-test-1-6379.pid # PID For Ubuntu/Debian, the default systemd service directory is /lib/systemd/system/ instead of /usr/lib/systemd/system/.\n","categories":["Reference"],"description":"How Pigsty's file system structure is designed and organized, and directory structures used by each module.","excerpt":"How Pigsty's file system structure is designed and organized, and …","ref":"/docs/ref/fhs/","tags":"","title":"File Hierarchy"},{"body":"Pigsty provides approximately 380+ configuration parameters distributed across 8 core modules, allowing fine-grained control over all aspects of the system.\nModule Navigation This page provides navigation and overview for all Pigsty configuration parameters. Click on a module name to jump to the detailed parameter documentation.\nModule Parameter Groups Total Parameters Description PGSQL 9 123 PostgreSQL database cluster core configuration INFRA 10 82 Infrastructure components: repo, Nginx, DNS, monitoring, Grafana, etc. NODE 11 83 Host node tuning: identity, DNS, packages, tuning, security, admin, time, VIP, etc. ETCD 2 13 Distributed configuration storage and service discovery REDIS 1 21 Redis cache and data structure server MINIO 2 21 S3-compatible object storage service FERRET 1 9 MongoDB-compatible database FerretDB DOCKER 1 8 Docker container engine PGSQL The PGSQL module provides 9 groups with 123 PostgreSQL-related configuration parameters.\nParameter Group Count Description PG_ID 11 PostgreSQL cluster and instance identity parameters PG_BUSINESS 12 Business users, databases, services, and access control rules PG_INSTALL 10 PostgreSQL installation: version, paths, packages PG_BOOTSTRAP 38 PostgreSQL cluster initialization: Patroni high availability PG_PROVISION 8 PostgreSQL cluster template provisioning: roles, privileges, extensions PG_BACKUP 6 pgBackRest backup and recovery configuration PG_ACCESS 17 Service exposure, connection pool, VIP, DNS client access configuration PG_MONITOR 17 PostgreSQL monitoring exporter configuration PG_REMOVE 4 PostgreSQL instance cleanup and uninstallation configuration INFRA The INFRA module provides 10 groups with 82 infrastructure-related configuration parameters.\nParameter Group Count Description META 5 Pigsty meta information: version, admin IP, region, language, proxy CA 3 Self-signed CA certificate management INFRA_ID 3 Infrastructure node identity and service portal REPO 10 Local software repository configuration INFRA_PACKAGE 2 Infrastructure node package installation NGINX 14 Nginx web server and reverse proxy configuration DNS 3 DNSMasq DNS resolution service configuration VICTORIA 19 VictoriaMetrics/Logs/Traces observability suite PROMETHEUS 7 Alertmanager and Blackbox Exporter GRAFANA 8 Grafana visualization platform configuration NODE The NODE module provides 11 groups with 83 host node-related configuration parameters.\nParameter Group Count Description NODE_ID 5 Node identity parameters NODE_DNS 6 Node DNS configuration NODE_PACKAGE 4 Node package management NODE_TUNE 10 Node kernel tuning parameters NODE_SEC 4 Node security parameters NODE_ADMIN 9 Node admin user configuration NODE_TIME 5 Node time synchronization NODE_VIP 8 Node VIP configuration HAPROXY 10 HAProxy load balancer configuration NODE_EXPORTER 3 Node exporter configuration VECTOR 6 Vector log collector configuration ETCD The ETCD module provides 2 groups with 13 distributed configuration storage parameters.\nParameter Group Count Description ETCD 10 ETCD cluster deployment and configuration ETCD_REMOVE 3 ETCD cluster removal: safeguard, data cleanup, etc. REDIS The REDIS module provides 21 Redis-related configuration parameters.\nParameter Group Count Description REDIS 21 Redis cluster deployment and configuration MINIO The MINIO module provides 2 groups with 21 MinIO object storage parameters.\nParameter Group Count Description MINIO 18 MinIO cluster deployment and configuration MINIO_REMOVE 3 MinIO cluster removal: safeguard, data cleanup, etc. FERRET The FERRET module provides 9 FerretDB-related configuration parameters.\nParameter Group Count Description FERRET 9 FerretDB deployment and configuration DOCKER The DOCKER module provides 8 Docker container engine configuration parameters.\nParameter Group Count Description DOCKER 8 Docker container engine configuration Parameter Overview The following tables provide a comprehensive summary of all parameters, organized by module.\nPGSQL Parameters PG_ID parameter group defines PostgreSQL cluster and instance identity, including cluster name, instance number, role, shard, etc.\nParameter Type Description pg_mode enum pgsql cluster mode: pgsql,citus,mssql,mysql,polar,ivory,oracle,gpsql pg_cluster string pgsql cluster name, required identity parameter pg_seq int pgsql instance number, required identity parameter pg_role enum pgsql instance role, required, can be primary, replica, offline pg_instances dict Define multiple pg instances on one node, using {port:ins_vars} format pg_upstream ip Upstream node IP for cascaded replica or standby cluster pg_shard string pgsql shard name, required for citus and gpsql horizontal sharding clusters pg_group int pgsql shard number, positive integer, required for citus and gpsql clusters gp_role enum Greenplum role for this cluster, can be master or segment pg_exporters dict Set up additional pg_exporters on this node to monitor remote postgres instances pg_offline_query bool Set to true to mark this replica as special offline instance for Offline service PG_BUSINESS parameter group defines business users, databases, services, access control rules, and default system user credentials.\nParameter Type Description pg_users user[] Postgres business users pg_databases database[] Postgres business databases pg_services service[] Postgres business services pg_hba_rules hba[] Postgres business HBA rules pgb_hba_rules hba[] Pgbouncer business HBA rules pg_replication_username username Postgres replication username, default replicator pg_replication_password password Postgres replication password, default DBUser.Replicator pg_admin_username username Postgres admin username, default dbuser_dba pg_admin_password password Postgres admin password, default DBUser.DBA pg_monitor_username username Postgres monitor username, default dbuser_monitor pg_monitor_password password Postgres monitor password, default DBUser.Monitor pg_dbsu_password password dbsu password, empty string means no dbsu password, best not to set PG_INSTALL parameter group configures PostgreSQL installation options, including version, paths, packages, and extensions.\nParameter Type Description pg_dbsu username OS dbsu name, default postgres, best not to change pg_dbsu_uid int OS dbsu uid and gid, default 26 for postgres user and group pg_dbsu_sudo enum dbsu sudo privilege: none, limit, all, nopass, default limit pg_dbsu_home path PostgreSQL home directory, default /var/lib/pgsql pg_dbsu_ssh_exchange bool Exchange postgres dbsu ssh keys between pgsql cluster pg_version enum Postgres major version to install, default 18 pg_bin_dir path Postgres binary directory, default /usr/pgsql/bin pg_log_dir path Postgres log directory, default /pg/log/postgres pg_packages string[] pg packages to install, ${pg_version} will be replaced pg_extensions string[] pg extensions to install, ${pg_version} will be replaced PG_BOOTSTRAP parameter group configures PostgreSQL cluster initialization, including Patroni HA, data directory, storage, connections, encoding, etc.\nParameter Type Description pg_data path Postgres data directory, default /pg/data pg_fs_main path Postgres main data mount point, default /data/postgres pg_fs_backup path pg backup data mount point, default /data/backups pg_storage_type enum pg main data storage type: SSD, HDD, default SSD pg_dummy_filesize size Size of /pg/dummy, default reserves 64MB for emergency pg_listen ip(s) postgres/pgbouncer listen address, default 0.0.0.0 pg_port port Postgres listen port, default 5432 pg_localhost path Postgres Unix socket directory for local connections pg_namespace path Top-level key namespace in etcd, used by patroni \u0026 vip patroni_enabled bool If disabled, postgres cluster won’t be created during init patroni_mode enum Patroni working mode: default, pause, remove patroni_port port Patroni listen port, default 8008 patroni_log_dir path Patroni log directory, default /pg/log/patroni patroni_ssl_enabled bool Secure patroni RestAPI with SSL? patroni_watchdog_mode enum Patroni watchdog mode: automatic, required, off, default off patroni_username username Patroni restapi username, default postgres patroni_password password Patroni restapi password, default Patroni.API pg_primary_db string Primary database name in cluster, used by Citus, default postgres pg_parameters dict Override PostgreSQL parameters in postgresql.auto.conf pg_files path[] Extra files to copy to PGDATA directory (e.g., license files) pg_conf enum Config template: oltp, olap, crit, tiny, default oltp.yml pg_max_conn int Postgres max connections, auto uses recommended value pg_shared_buffer_ratio float Postgres shared buffer memory ratio, default 0.25, range 0.1~0.4 pg_rto int Recovery Time Objective (seconds), default 30s pg_rpo int Recovery Point Objective (bytes), default 1MiB pg_libs string Preloaded libraries, default pg_stat_statements,auto_explain pg_delay interval WAL replay delay for standby cluster, for delayed replica pg_checksum bool Enable data checksums for postgres cluster? pg_pwd_enc enum Password encryption algorithm: fixed to scram-sha-256 pg_encoding enum Database cluster encoding, default UTF8 pg_locale enum Database cluster locale setting, default C pg_lc_collate enum Database cluster collation, default C pg_lc_ctype enum Database character type, default C pg_io_method enum PostgreSQL IO method: auto, sync, worker, io_uring pg_etcd_password password Password for this PostgreSQL cluster in etcd, default uses cluster name pgsodium_key string pgsodium encryption master key, 64-bit hex, default sha256(pg_cluster) pgsodium_getkey_script path pgsodium getkey script path, default uses template pgsodium_getkey PG_PROVISION parameter group configures PostgreSQL cluster template provisioning, including default roles, privileges, schemas, extensions, and HBA rules.\nParameter Type Description pg_provision bool Provision postgres cluster business objects after bootstrap? pg_init string Cluster template initialization script, default pg-init pg_default_roles role[] Default predefined roles and system users in postgres cluster pg_default_privileges string[] Default privileges when admin user creates database objects pg_default_schemas string[] List of default schemas to create pg_default_extensions extension[] List of default extensions to create pg_reload bool Reload postgres config immediately after HBA changes pg_default_hba_rules hba[] Postgres host-based authentication rules, global PG default HBA pgb_default_hba_rules hba[] Pgbouncer default host-based authentication rules, global PGB default HBA PG_BACKUP parameter group configures pgBackRest backup and recovery, including repository type, path, retention policy, etc.\nParameter Type Description pgbackrest_enabled bool Enable pgbackrest on pgsql host? pgbackrest_clean bool Delete previous pg backup data during init? pgbackrest_log_dir path pgbackrest log directory, default /pg/log/pgbackrest pgbackrest_method enum pgbackrest repo method: local, minio, etc. pgbackrest_init_backup bool Execute full backup immediately after pgbackrest init? default true pgbackrest_repo dict pgbackrest repository definition PG_ACCESS parameter group configures service exposure, connection pool, VIP, DNS, and other client access options.\nParameter Type Description pgbouncer_enabled bool If disabled, pgbouncer connection pool won’t be configured pgbouncer_port port pgbouncer listen port, default 6432 pgbouncer_log_dir path pgbouncer log directory, default /pg/log/pgbouncer pgbouncer_auth_query bool Use AuthQuery to fetch unlisted business users from postgres? pgbouncer_poolmode enum Pooling mode: transaction, session, statement, default transaction pgbouncer_sslmode enum pgbouncer client SSL mode, default disabled pgbouncer_ignore_param string[] pgbouncer ignore startup parameters list pg_weight int Relative load balancing weight in service, default 100, range 0-255 pg_service_provider string Dedicated haproxy node group name, or empty for local haproxy pg_default_service_dest enum If svc.dest=‘default’, default service points to postgres or pgbouncer pg_default_services service[] Postgres default service definition list, global shared pg_vip_enabled bool Enable L2 VIP for pgsql primary node? default disabled pg_vip_address cidr4 VIP address format \u003cipv4\u003e/\u003cmask\u003e, required when vip enabled pg_vip_interface string VIP network interface to listen, default eth0 pg_dns_suffix string pgsql dns suffix, default empty pg_dns_target enum PG DNS resolves to: auto, primary, vip, none, or specific IP PG_MONITOR parameter group configures PostgreSQL monitoring exporters, including pg_exporter, pgbouncer_exporter, and pgbackrest_exporter.\nParameter Type Description pg_exporter_enabled bool Enable pg_exporter on pgsql host? pg_exporter_config string pg_exporter config file/template name pg_exporter_cache_ttls string pg_exporter collector tiered TTL config, default ‘1,10,60,300’ pg_exporter_port port pg_exporter listen port, default 9630 pg_exporter_params string Extra URL parameters passed in pg_exporter dsn pg_exporter_url pgurl If specified, overrides auto-generated postgres DSN connection string pg_exporter_auto_discovery bool Enable monitoring auto database discovery? default enabled pg_exporter_exclude_database string Excluded database names when auto discovery enabled, comma-separated pg_exporter_include_database string Only monitor databases in this list when auto discovery enabled pg_exporter_connect_timeout int pg_exporter connection timeout in ms, default 200 pg_exporter_options arg pg_exporter extra command line options pgbouncer_exporter_enabled bool Enable pgbouncer_exporter on pgsql host? pgbouncer_exporter_port port pgbouncer_exporter listen port, default 9631 pgbouncer_exporter_url pgurl If specified, overrides auto-generated pgbouncer dsn connection string pgbouncer_exporter_options arg pgbouncer_exporter extra command line options pgbackrest_exporter_enabled bool Enable pgbackrest_exporter on pgsql host? pgbackrest_exporter_port port pgbackrest_exporter listen port, default 9854 pgbackrest_exporter_options arg pgbackrest_exporter extra command line options PG_REMOVE parameter group configures PostgreSQL instance cleanup and uninstallation behavior, including data directory, backup, and package removal control.\nParameter Type Description pg_rm_data bool Clean postgres data directory when removing pgsql instance? pg_rm_backup bool Clean pgbackrest backup when removing primary? pg_rm_pkg bool Uninstall related packages when removing pgsql instance? pg_safeguard bool Safeguard to prevent accidental pgsql cleanup? default false INFRA Parameters META parameter group defines Pigsty meta information, including version number, admin node IP, repository region, default language, and proxy settings.\nParameter Type Description version string Pigsty version string admin_ip ip Admin node IP address region enum Upstream mirror region: default, china, europe language enum Default language, en or zh proxy_env dict Global proxy environment variables for package downloads CA parameter group configures Pigsty self-signed CA certificate management, including whether to create CA, CA name, and certificate validity.\nParameter Type Description ca_create bool Create CA if not exists? default true ca_cn string CA CN name, fixed to pigsty-ca cert_validity interval Certificate validity, default 20 years INFRA_ID parameter group defines infrastructure node identity, including node sequence number, service portal configuration, and data directory.\nParameter Type Description infra_seq int Infrastructure node sequence number, required identity parameter infra_portal dict Infrastructure service list exposed via Nginx portal infra_data path Infrastructure data directory, default /data/infra REPO parameter group configures local software repository, including repository enable switch, directory path, upstream source definitions, and packages to download.\nParameter Type Description repo_enabled bool Create software repository on this infra node? repo_home path Software repository home directory, default /www repo_name string Software repository name, default pigsty repo_endpoint url Repository access point: domain or ip:port format repo_remove bool Remove existing upstream repo source definition files when building local repo? repo_modules string Enabled upstream repository module list, comma-separated repo_upstream upstream[] Upstream repository source definitions: where to download packages repo_packages string[] Which packages to download from upstream repo_extra_packages string[] Which extra packages to download from upstream repo_url_packages string[] Extra packages to download by URL INFRA_PACKAGE parameter group defines packages to install on infrastructure nodes, including RPM/DEB and PIP packages.\nParameter Type Description infra_packages string[] Packages to install on infrastructure nodes infra_packages_pip string Packages to install via pip on infrastructure nodes NGINX parameter group configures Nginx web server and reverse proxy, including enable switch, ports, SSL mode, certificates, and basic authentication.\nParameter Type Description nginx_enabled bool Enable nginx on this infra node? nginx_clean bool Clean existing nginx config during init? nginx_exporter_enabled bool Enable nginx_exporter on this infra node? nginx_exporter_port port nginx_exporter listen port, default 9113 nginx_sslmode enum nginx SSL mode: disable, enable, enforce nginx_cert_validity duration nginx self-signed certificate validity, default 397d nginx_home path nginx content directory, default /www, symlinks to nginx_data nginx_data path nginx actual data directory, default /data/nginx nginx_users dict nginx basic auth users: username and password dictionary nginx_port port nginx listen port, default 80 nginx_ssl_port port nginx SSL listen port, default 443 certbot_sign bool Use certbot to sign certificates? certbot_email string certbot notification email address certbot_options string certbot extra command line arguments DNS parameter group configures DNSMasq DNS resolution service, including enable switch, listen port, and dynamic DNS records.\nParameter Type Description dns_enabled bool Set up dnsmasq on this infra node? dns_port port DNS server listen port, default 53 dns_records string[] Dynamic DNS records resolved by dnsmasq VICTORIA parameter group configures VictoriaMetrics/Logs/Traces observability suite, including enable switches, ports, data retention policies, etc.\nParameter Type Description vmetrics_enabled bool Enable VictoriaMetrics on this infra node? vmetrics_clean bool Clean VictoriaMetrics data during init? vmetrics_port port VictoriaMetrics listen port, default 8428 vmetrics_scrape_interval interval Global scrape interval, default 10s vmetrics_scrape_timeout interval Global scrape timeout, default 8s vmetrics_options arg VictoriaMetrics extra command line arguments vlogs_enabled bool Enable VictoriaLogs on this infra node? vlogs_clean bool Clean VictoriaLogs data during init? vlogs_port port VictoriaLogs listen port, default 9428 vlogs_options arg VictoriaLogs extra command line arguments vtraces_enabled bool Enable VictoriaTraces on this infra node? vtraces_clean bool Clean VictoriaTraces data during init? vtraces_port port VictoriaTraces listen port, default 10428 vtraces_options arg VictoriaTraces extra command line arguments vmalert_enabled bool Enable VMAlert on this infra node? vmalert_port port VMAlert listen port, default 8880 vmalert_options arg VMAlert extra command line arguments PROMETHEUS parameter group configures Alertmanager and Blackbox Exporter, providing alerting management and network probing.\nParameter Type Description blackbox_enabled bool Set up blackbox_exporter on this infra node? blackbox_port port blackbox_exporter listen port, default 9115 blackbox_options arg blackbox_exporter extra command line options alertmanager_enabled bool Set up alertmanager on this infra node? alertmanager_port port AlertManager listen port, default 9059 alertmanager_options arg alertmanager extra command line options exporter_metrics_path path exporter metrics path, default /metrics GRAFANA parameter group configures Grafana visualization platform, including enable switch, port, admin credentials, and data source configuration.\nParameter Type Description grafana_enabled bool Enable Grafana on this infra node? grafana_port port Grafana listen port, default 3000 grafana_clean bool Clean data during Grafana init? grafana_admin_username username Grafana admin username, default admin grafana_admin_password password Grafana admin password, default pigsty grafana_auth_proxy bool Enable Grafana auth proxy? grafana_pgurl url External PostgreSQL database URL (for Grafana persistence) grafana_view_password password Grafana metadb PG datasource password NODE Parameters NODE_ID parameter group defines node identity parameters, including node name, cluster name, and whether to borrow identity from PostgreSQL.\nParameter Type Description nodename string Node instance identifier, uses hostname if missing, optional node_cluster string Node cluster identifier, uses ’nodes’ if missing, optional nodename_overwrite bool Overwrite node hostname with nodename? nodename_exchange bool Exchange nodename between playbook hosts? node_id_from_pg bool Borrow postgres identity as node identity if possible? NODE_DNS parameter group configures node DNS resolution, including static hosts records and dynamic DNS servers.\nParameter Type Description node_write_etc_hosts bool Modify /etc/hosts on target nodes? node_default_etc_hosts string[] Static DNS records in /etc/hosts node_etc_hosts string[] Extra static DNS records in /etc/hosts node_dns_method enum How to handle existing DNS servers: add, none, overwrite node_dns_servers string[] Dynamic DNS server list in /etc/resolv.conf node_dns_options string[] DNS resolution options in /etc/resolv.conf NODE_PACKAGE parameter group configures node software sources and package installation.\nParameter Type Description node_repo_modules enum Which repo modules to enable on node? default local node_repo_remove bool Remove existing repos on node when configuring node software repos? node_packages string[] Packages to install on current node node_default_packages string[] Default packages to install on all nodes NODE_TUNE parameter group configures node kernel parameters, feature switches, and performance tuning templates.\nParameter Type Description node_disable_numa bool Disable node NUMA, requires reboot node_disable_swap bool Disable node Swap, use with caution node_static_network bool Preserve DNS resolver settings after reboot, i.e., static network, default enabled node_disk_prefetch bool Configure disk prefetch on HDD to improve performance node_kernel_modules string[] Kernel modules to enable on this node node_hugepage_count int Number of 2MB hugepages allocated on host node, higher priority than ratio node_hugepage_ratio float Memory hugepage ratio allocated on host node, 0 disables node_overcommit_ratio float Node memory overcommit ratio (50-100), 0 disables node_tune enum Node tuning profile: none, oltp, olap, crit, tiny node_sysctl_params dict Extra sysctl config parameters, k:v format NODE_SEC parameter group configures node security options, including SELinux, firewall, etc.\nParameter Type Description node_selinux_mode enum SELinux mode: disabled, permissive, enforcing node_firewall_mode enum Firewall mode: off, none, zone node_firewall_intranet cidr[] Intranet CIDR list for firewall rules node_firewall_public_port port[] Public open port list, default [22, 80, 443, 5432] NODE_ADMIN parameter group configures node admin user, data directory, and command aliases.\nParameter Type Description node_data path Node main data directory, default /data node_admin_enabled bool Create admin user on target node? node_admin_uid int Node admin user uid and gid node_admin_username username Node admin user name, default dba node_admin_sudo enum Admin user sudo privilege: limited, nopass, all, none node_admin_ssh_exchange bool Exchange admin ssh keys between node clusters? node_admin_pk_current bool Add current user’s ssh public key to admin’s authorized_keys? node_admin_pk_list string[] ssh public keys to add to admin user node_aliases dict Shell alias commands to configure on host, KV dictionary NODE_TIME parameter group configures node timezone, NTP time sync, and cron jobs.\nParameter Type Description node_timezone string Set host node timezone, empty string skips node_ntp_enabled bool Enable chronyd time sync service? node_ntp_servers string[] NTP server list in /etc/chrony.conf node_crontab_overwrite bool Append or overwrite when writing /etc/crontab? node_crontab string[] Crontab entries in /etc/crontab NODE_VIP parameter group configures node cluster L2 VIP, implemented by keepalived.\nParameter Type Description vip_enabled bool Enable L2 VIP on this node cluster? vip_address ip Node VIP address in ipv4 format, required when vip enabled vip_vrid int Required integer 1-254, should be unique in same VLAN vip_role enum Optional, master/backup, default backup vip_preempt bool Optional, true/false, default false, enable vip preemption vip_interface string Node VIP network interface to listen, default eth0 vip_dns_suffix string Node VIP DNS name suffix, default empty string vip_auth_pass password VRRP auth password, auto-generated if empty vip_exporter_port port keepalived exporter listen port, default 9650 HAPROXY parameter group configures HAProxy load balancer and service exposure on nodes.\nParameter Type Description haproxy_enabled bool Enable haproxy on this node? haproxy_clean bool Clean all existing haproxy config? haproxy_reload bool Reload haproxy after config? haproxy_auth_enabled bool Enable haproxy admin page authentication? haproxy_admin_username username haproxy admin username, default admin haproxy_admin_password password haproxy admin password, default pigsty haproxy_exporter_port port haproxy exporter port, default 9101 haproxy_client_timeout interval haproxy client connection timeout, default 24h haproxy_server_timeout interval haproxy server connection timeout, default 24h haproxy_services service[] haproxy service list to expose on node NODE_EXPORTER parameter group configures node monitoring exporter.\nParameter Type Description node_exporter_enabled bool Configure node_exporter on this node? node_exporter_port port node exporter listen port, default 9100 node_exporter_options arg node_exporter extra server options VECTOR parameter group configures Vector log collector.\nParameter Type Description vector_enabled bool Enable vector log collector? vector_clean bool Clean vector data directory during init? vector_data path vector data directory, default /data/vector vector_port port vector metrics listen port, default 9598 vector_read_from enum vector reads logs from beginning or end vector_log_endpoint string[] Log send destination endpoint, default sends to infra group ETCD Parameters ETCD parameter group is for etcd cluster deployment and configuration, including instance identity, cluster name, data directory, ports, and authentication password.\nParameter Type Description etcd_seq int etcd instance identifier, required etcd_cluster string etcd cluster name, default fixed to etcd etcd_learner bool Initialize etcd instance as learner? etcd_data path etcd data directory, default /data/etcd etcd_port port etcd client port, default 2379 etcd_peer_port port etcd peer port, default 2380 etcd_init enum etcd initial cluster state, new or existing etcd_election_timeout int etcd election timeout, default 1000ms etcd_heartbeat_interval int etcd heartbeat interval, default 100ms etcd_root_password password etcd root user password for RBAC authentication ETCD_REMOVE parameter group controls etcd cluster removal behavior, including safeguard, data cleanup, and package uninstallation.\nParameter Type Description etcd_safeguard bool etcd safeguard to prevent cleaning running etcd instance? etcd_rm_data bool Delete etcd data when removing? default true etcd_rm_pkg bool Uninstall etcd package when removing? default false REDIS Parameters REDIS parameter group is for Redis cluster deployment and configuration, including identity, instance definitions, working mode, memory configuration, persistence, and monitoring.\nParameter Type Description redis_cluster string Redis database cluster name, required identity parameter redis_instances dict Instance definitions on Redis node redis_node int Redis node number, positive integer, unique in cluster, required redis_fs_main path Redis main data directory, default /data redis_exporter_enabled bool Redis Exporter enabled? redis_exporter_port port Redis Exporter listen port redis_exporter_options string Redis Exporter command arguments redis_safeguard bool Prevent erasing existing Redis redis_clean bool Erase existing instance when initializing Redis redis_rmdata bool Remove data when removing Redis instance? redis_mode enum Redis cluster mode: sentinel, cluster, standalone redis_conf string Redis config file template, except sentinel redis_bind_address ip Redis listen address, empty binds to host IP redis_max_memory size Redis max available memory redis_mem_policy enum Redis memory eviction policy redis_password password Redis password, empty disables password redis_rdb_save string[] Redis RDB save directives, empty array disables RDB redis_aof_enabled bool Redis AOF enabled? redis_rename_commands dict Redis dangerous command rename list redis_cluster_replicas int How many replicas per master in Redis native cluster? redis_sentinel_monitor master[] Master list monitored by Redis sentinel, only for sentinel cluster MINIO Parameters MINIO parameter group is for MinIO cluster deployment and configuration, including identity, storage paths, ports, authentication credentials, and bucket/user provisioning.\nParameter Type Description minio_seq int minio instance identifier, required minio_cluster string minio cluster name, default minio minio_user username minio OS user, default minio minio_https bool Enable HTTPS for MinIO? default true minio_node string minio node name pattern minio_data path minio data directory, use {x...y} for multiple disks minio_volumes string minio core parameter, specifies member nodes and disks minio_domain string minio external domain, default sss.pigsty minio_port port minio service port, default 9000 minio_admin_port port minio console port, default 9001 minio_access_key username Root access key, default minioadmin minio_secret_key password Root secret key, default S3User.MinIO minio_extra_vars string Extra environment variables for minio server minio_provision bool Execute minio resource provisioning task? default true minio_alias string minio deployment client alias minio_endpoint string minio deployment client alias endpoint minio_buckets bucket[] minio buckets to create minio_users user[] minio users to create MINIO_REMOVE parameter group controls MinIO cluster removal behavior, including safeguard, data cleanup, and package uninstallation.\nParameter Type Description minio_safeguard bool Prevent accidental deletion? default false minio_rm_data bool Delete minio data when removing? default true minio_rm_pkg bool Uninstall minio package when removing? default false FERRET Parameters FERRET parameter group is for FerretDB deployment and configuration, including identity, underlying PostgreSQL connection, listen port, and SSL settings.\nParameter Type Description mongo_seq int mongo instance number, required identity parameter mongo_cluster string mongo cluster name, required identity parameter mongo_pgurl pgurl PGURL connection string for FerretDB backend mongo_ssl_enabled bool Enable SSL? default false mongo_listen ip Listen address, empty listens on all addresses mongo_port port Service port, default 27017 mongo_ssl_port port TLS listen port, default 27018 mongo_exporter_port port Exporter port, default 9216 mongo_extra_vars string Extra environment variables, default empty string DOCKER Parameters DOCKER parameter group is for Docker container engine deployment and configuration, including enable switch, data directory, storage driver, registry mirrors, and monitoring.\nParameter Type Description docker_enabled bool Enable Docker on current node? default disabled docker_data path Docker data directory, default /data/docker docker_storage_driver enum Docker storage driver, default overlay2 docker_cgroups_driver enum Docker CGroup filesystem driver: cgroupfs, systemd docker_registry_mirrors string[] Docker registry mirror list docker_exporter_port port Docker monitoring metrics export port, default 9323 docker_image string[] Docker images to pull, default empty list docker_image_cache path Docker image tarball path to import, default /tmp/docker/*.tgz ","categories":["Reference"],"description":"Pigsty configuration parameter overview and navigation","excerpt":"Pigsty configuration parameter overview and navigation","ref":"/docs/ref/param/","tags":"","title":"Parameters"},{"body":"Pigsty provides a series of Ansible playbooks for automated deployment and management of various modules. This page provides navigation and summary of all playbooks.\nModule Navigation Module Description INFRA 3 Infrastructure module playbooks NODE 2 Node management module playbooks ETCD 2 ETCD cluster management playbooks PGSQL 7 PostgreSQL cluster management playbooks REDIS 2 Redis cluster management playbooks MINIO 2 MinIO object storage management playbooks FERRET 1 FerretDB management playbook DOCKER 1 Docker management playbook Playbook Summary The following table lists all available preset playbooks in Pigsty:\nPlaybook Module Function deploy.yml ADMIN Deploy pigsty on current environment infra.yml INFRA Initialize pigsty infrastructure on infra nodes infra-rm.yml INFRA Remove infrastructure components from infra nodes node.yml NODE Manage nodes, adjust nodes to desired state node-rm.yml NODE Remove managed nodes from Pigsty etcd.yml ETCD Install and configure Etcd cluster etcd-rm.yml ETCD Remove Etcd cluster or members pgsql.yml PGSQL Initialize PostgreSQL cluster or add new replicas pgsql-rm.yml PGSQL Remove PostgreSQL cluster or instance pgsql-user.yml PGSQL Add new business users to existing PostgreSQL cluster pgsql-db.yml PGSQL Add new business databases to existing PostgreSQL cluster pgsql-monitor.yml PGSQL Monitor remote PostgreSQL instances pgsql-migration.yml PGSQL Generate migration manuals and scripts for existing PostgreSQL clusters pgsql-pitr.yml PGSQL Execute PostgreSQL Point-in-Time Recovery (PITR) redis.yml REDIS Initialize Redis cluster/node/instance redis-rm.yml REDIS Remove Redis cluster/node/instance minio.yml MINIO Install MinIO cluster minio-rm.yml MINIO Remove MinIO cluster mongo.yml FERRET Install FerretDB on nodes docker.yml DOCKER Install Docker Daemon and Docker Compose Playbook Usage Notes Protection Mechanism Multiple modules provide deletion protection through *_safeguard parameters:\nPGSQL: pg_safeguard prevents accidental deletion of PostgreSQL clusters ETCD: etcd_safeguard prevents accidental deletion of Etcd clusters MINIO: minio_safeguard prevents accidental deletion of MinIO clusters By default, these safeguard parameters are not enabled (undefined). It’s recommended to explicitly set them to true for initialized clusters in production environments.\nWhen the protection switch is set to true, the corresponding *-rm.yml playbook will abort immediately. You can force override through command-line parameters:\n./pgsql-rm.yml -l pg-test -e pg_safeguard=false ./etcd-rm.yml -e etcd_safeguard=false ./minio-rm.yml -l minio -e minio_safeguard=false Limiting Execution Scope When executing playbooks, it’s recommended to use the -l parameter to limit the execution scope:\n./pgsql.yml -l pg-meta # Limit execution to pg-meta cluster ./node.yml -l 10.10.10.10 # Limit execution to specific node ./redis.yml -l redis-test # Limit execution to redis-test cluster Idempotency Most playbooks are idempotent and can be executed repeatedly. However, note:\ninfra.yml does not clear data by default and can be safely re-executed. All clean parameters (vmetrics_clean, vlogs_clean, vtraces_clean, grafana_clean, nginx_clean) default to false To clear infrastructure data for rebuild, you need to explicitly set the corresponding clean parameter to true Be extra careful when repeatedly executing *-rm.yml deletion playbooks Task Tags You can use the -t parameter to execute only specific task subsets:\n./pgsql.yml -l pg-test -t pg_service # Only refresh pg-test cluster services ./node.yml -t haproxy # Only set up haproxy on nodes ./etcd.yml -t etcd_launch # Only restart etcd service Quick Command Reference ./deploy.yml # One-pass deployment INFRA Module ./infra.yml # Initialize infrastructure ./infra-rm.yml # Remove infrastructure NODE Module ./node.yml -l \u003ccls|ip\u003e # Add node ./node-rm.yml -l \u003ccls|ip\u003e # Remove node bin/node-add \u003ccls|ip\u003e # Add node (wrapper script) bin/node-rm \u003ccls|ip\u003e # Remove node (wrapper script) ETCD Module ./etcd.yml # Initialize etcd cluster ./etcd-rm.yml # Remove etcd cluster bin/etcd-add \u003cip\u003e # Add etcd member (wrapper script) bin/etcd-rm \u003cip\u003e # Remove etcd member (wrapper script) PGSQL Module ./pgsql.yml -l \u003ccls\u003e # Initialize PostgreSQL cluster ./pgsql-rm.yml -l \u003ccls\u003e # Remove PostgreSQL cluster ./pgsql-user.yml -l \u003ccls\u003e -e username=\u003cuser\u003e # Create business user ./pgsql-db.yml -l \u003ccls\u003e -e dbname=\u003cdb\u003e # Create business database ./pgsql-monitor.yml -e clsname=\u003ccls\u003e # Monitor remote cluster ./pgsql-pitr.yml -l \u003ccls\u003e -e '{\"pg_pitr\": {}}' # Execute PITR recovery bin/pgsql-add \u003ccls\u003e # Initialize cluster (wrapper script) bin/pgsql-rm \u003ccls\u003e # Remove cluster (wrapper script) bin/pgsql-user \u003ccls\u003e \u003cuser\u003e # Create user (wrapper script) bin/pgsql-db \u003ccls\u003e \u003cdb\u003e # Create database (wrapper script) REDIS Module ./redis.yml -l \u003ccls\u003e # Initialize Redis cluster ./redis-rm.yml -l \u003ccls\u003e # Remove Redis cluster MINIO Module ./minio.yml -l \u003ccls\u003e # Initialize MinIO cluster ./minio-rm.yml -l \u003ccls\u003e # Remove MinIO cluster FERRET Module ./mongo.yml -l ferret # Install FerretDB DOCKER Module ./docker.yml -l \u003chost\u003e # Install Docker ","categories":["Reference"],"description":"Overview and navigation of Pigsty preset playbooks","excerpt":"Overview and navigation of Pigsty preset playbooks","ref":"/docs/ref/playbook/","tags":"","title":"Playbooks"},{"body":"This page lists default ports used by Pigsty module components. Adjust as needed or use as a reference for fine-grained firewall configuration.\nModule Component Port Parameter Status NODE node_exporter 9100 node_exporter_port Enabled NODE haproxy 9101 haproxy_exporter_port Enabled NODE vector 9598 vector_port Enabled NODE keepalived_exporter 9650 vip_exporter_port Optional NODE chronyd 123 - Enabled DOCKER docker 9323 docker_exporter_port Optional INFRA nginx 80 nginx_port Enabled INFRA nginx 443 nginx_ssl_port Enabled INFRA grafana 3000 grafana_port Enabled INFRA victoriaMetrics 8428 vmetrics_port Enabled INFRA victoriaLogs 9428 vlogs_port Enabled INFRA victoriaTraces 10428 vtraces_port Enabled INFRA vmalert 8880 vmalert_port Enabled INFRA alertmanager 9059 alertmanager_port Enabled INFRA blackbox_exporter 9115 blackbox_port Enabled INFRA dnsmasq 53 dns_port Enabled ETCD etcd 2379 etcd_port Enabled ETCD etcd 2380 etcd_peer_port Enabled MINIO minio 9000 minio_port Enabled MINIO minio 9001 minio_admin_port Enabled REDIS redis 6379 redis_port Optional REDIS redis_exporter 9121 redis_exporter_port Optional FERRET ferretdb 27017 mongo_port Optional FERRET mongo_exporter 9216 mongo_exporter_port Enabled PGSQL postgres 5432 pg_port Enabled PGSQL pgbouncer 6432 pgbouncer_port Enabled PGSQL patroni 8008 patroni_port Enabled PGSQL pg_exporter 9630 pg_exporter_port Enabled PGSQL pgbouncer_exporter 9631 pgbouncer_exporter_port Enabled PGSQL pgbackrest_exporter 9854 pgbackrest_exporter_port Enabled PGSQL {{ pg_cluster }}-primary 5433 pg_default_services Enabled PGSQL {{ pg_cluster }}-replica 5434 pg_default_services Enabled PGSQL {{ pg_cluster }}-default 5436 pg_default_services Enabled PGSQL {{ pg_cluster }}-offline 5438 pg_default_services Enabled PGSQL {{ pg_cluster }}-\u003cservice\u003e 543x pg_services Optional ","categories":["Reference"],"description":"Default ports used by Pigsty components, with related parameters and status.","excerpt":"Default ports used by Pigsty components, with related parameters and …","ref":"/docs/ref/port/","tags":"","title":"Port List"},{"body":"PostgreSQL is the most popular database in the world, and countless software is built on PostgreSQL, around PostgreSQL, or serves PostgreSQL itself, such as\n“Application software” that uses PostgreSQL as the preferred database “Tooling software” that serves PostgreSQL software development and management “Database software” that derives, wraps, forks, modifies, or extends PostgreSQL And Pigsty just have a series of Docker Compose templates for these software, application and databases:\nName Website Type State Port Domain Description Supabase Supabase DB GA 8000 supa.pigsty OSS Firebase Alternative, Backend as Platform PolarDB PolarDB DB GA 5532 OSS RAC for PostgreSQL FerretDB FerretDB DB GA 27017 OSS Mongo Alternative base on PostgreSQL MinIO MinIO DB GA 9000 sss.pigsty OSS AWS S3 Alternative, Simple Storage Service EdgeDB EdgeDB DB TBD OSS Graph Database base on PostgreSQL NocoDB NocoDB APP GA 8080 noco.pigsty OSS Airtable Alternative over PostgreSQL Odoo Odoo APP GA 8069 odoo.pigsty OSS ERP Software base on PostgreSQL Dify Dify APP GA 8001 dify.pigsty OSS AI Workflow Orachestration \u0026 LLMOps Platform Jupyter Jupyter APP GA lab.pigsty OSS AI Python Notebook \u0026 Data Analysis IDE Gitea Gitea APP GA 8889 git.pigsty OSS DevOps Git Service Wiki Wiki.js APP GA 9002 wiki.pigsty OSS Wiki Software GitLab GitLab APP TBD OSS GitHub Alternative, Code Management Platform Mastodon Mastodon APP TBD OSS Decentralized Social Network Keycloak Keycloak APP TBD OSS Identity \u0026 Access Management Component Harbour Harbour APP TBD OSS Docker/K8S Image Repository Confluence Confluence APP TBD Enterprise Knowledge Management System Jira Jira APP TBD Enterprise Project Management Tools Zabbix Zabbix 7 APP TBD OSS Monitoring Platform for Enterprise Grafana Grafana APP TBD Dashboard, Data Visualization \u0026 Monitoring Platform Metabase Metabase APP GA 9004 mtbs.pigsty Fast analysis of data from multiple data sources ByteBase ByteBase APP GA 8887 ddl.pigsty Database Migration Tool for PostgreSQL Kong Kong TOOL GA 8000 api.pigsty OSS API Gateway based on Nginx/OpenResty PostgREST PostgREST TOOL GA 8884 api.pigsty Generate RESTAPI from PostgreSQL Schemas pgAdmin4 pgAdmin4 TOOL GA 8885 adm.pigsty PostgreSQL GUI Admin Tools pgWeb pgWeb TOOL GA 8886 cli.pigsty PostgreSQL Web GUI Client SchemaSpy SchemaSpy TOOL TBD Dump \u0026 Visualize PostgreSQL Schema pgBadger pgBadger TOOL TBD PostgreSQL Log Analysis pg_exporter pg_exporter TOOL GA 9630 Expose PostgreSQL \u0026 Pgbouncer Metrics for Prometheus How to prepare Docker? To run docker compose templates, you need to install the DOCKER module on the node, If you don’t have the Internet access or having firewall issues, you may need to configure a DockerHub proxy, check the tutorial.\n","categories":["Reference"],"description":"Software and tools that use PostgreSQL can be managed by the docker daemon","excerpt":"Software and tools that use PostgreSQL can be managed by the docker …","ref":"/docs/app/","tags":"","title":"Applications"},{"body":"Supabase is great, but having your own Supabase is even better. Pigsty can help you deploy enterprise-grade Supabase on your own servers (physical, virtual, or cloud) with a single command — more extensions, better performance, deeper control, and more cost-effective.\nPigsty is one of three self-hosting approaches listed on the Supabase official documentation: Self-hosting: Third-Party Guides\nThis tutorial requires basic Linux knowledge. Otherwise, consider using Supabase cloud or plain Docker Compose self-hosting.\nTL;DR Prepare a Linux server, follow the Pigsty standard single-node installation process with the supabase config template:\ncurl -fsSL https://repo.pigsty.io/get | bash; cd ~/pigsty ./configure -c supabase # Use supabase config (change credentials in pigsty.yml) vi pigsty.yml # Edit domain, passwords, keys... ./deploy.yml # Standard single-node Pigsty deployment ./docker.yml # Install Docker module ./app.yml # Start Supabase stateless components (may be slow) After installation, access Supa Studio on port 8000 with username supabase and password pigsty.\nChecklist At least one 1C2G server Static internal IPv4 address Supported Linux distro installed Standard Pigsty installation Modified config file: domain, passwords, IP address Docker module installed, ensure proxy/mirror available Use Pigsty’s app.yml to start Supabase Table of Contents What is Supabase? Why Self-Host? Single-Node Quick Start Advanced: Security Hardening Advanced: Domain Configuration Advanced: External Object Storage Advanced: Using SMTP Advanced: True High Availability What is Supabase? Supabase is a BaaS (Backend as a Service), an open-source Firebase alternative, and the most popular database + backend solution in the AI Agent era. Supabase wraps PostgreSQL and provides authentication, messaging, edge functions, object storage, and automatically generates REST and GraphQL APIs based on your database schema.\nSupabase aims to provide developers with a one-stop backend solution, reducing the complexity of developing and maintaining backend infrastructure. It allows developers to skip most backend development work — you only need to understand database design and frontend to ship quickly! Developers can use vibe coding to create a frontend and database schema to rapidly build complete applications.\nCurrently, Supabase is the most popular open-source project in the PostgreSQL ecosystem, with over 90,000 GitHub stars. Supabase also offers a “generous” free tier for small startups — free 500 MB storage, more than enough for storing user tables and analytics data.\nWhy Self-Host? If Supabase cloud is so attractive, why self-host?\nThe most obvious reason is what we discussed in “Is Cloud Database an IQ Tax?”: when your data/compute scale exceeds the cloud computing sweet spot (Supabase: 4C/8G/500MB free storage), costs can explode. And nowadays, reliable local enterprise NVMe SSDs have three to four orders of magnitude cost advantage over cloud storage, and self-hosting can better leverage this.\nAnother important reason is functionality — Supabase cloud features are limited. Many powerful PostgreSQL extensions aren’t available in cloud services due to multi-tenant security challenges and licensing. Despite extensions being PostgreSQL’s core feature, only 64 extensions are available on Supabase cloud. Self-hosted Supabase with Pigsty provides up to 440 ready-to-use PostgreSQL extensions.\nAdditionally, self-control and vendor lock-in avoidance are important reasons for self-hosting. Although Supabase aims to provide a vendor-lock-free open-source Google Firebase alternative, self-hosting enterprise-grade Supabase is not trivial. Supabase includes a series of PostgreSQL extensions they develop and maintain, and plans to replace the native PostgreSQL kernel with OrioleDB (which they acquired). These kernels and extensions are not available in the official PGDG repository.\nThis is implicit vendor lock-in, preventing users from self-hosting in ways other than the supabase/postgres Docker image. Pigsty provides an open, transparent, and universal solution. We package all 10 missing Supabase extensions into ready-to-use RPM/DEB packages, ensuring they work on all major Linux distributions:\nExtension Description pg_graphql GraphQL support in PostgreSQL (Rust), provided by PIGSTY pg_jsonschema JSON Schema validation (Rust), provided by PIGSTY wrappers Supabase foreign data wrapper bundle (Rust), provided by PIGSTY index_advisor Query index advisor (SQL), provided by PIGSTY pg_net Async non-blocking HTTP/HTTPS requests (C), provided by PIGSTY vault Store encrypted credentials in Vault (C), provided by PIGSTY pgjwt JSON Web Token API implementation (SQL), provided by PIGSTY pgsodium Table data encryption TDE, provided by PIGSTY supautils Security utilities for cloud environments (C), provided by PIGSTY pg_plan_filter Filter queries by execution plan cost (C), provided by PIGSTY We also install most extensions by default in Supabase deployments. You can enable them as needed.\nPigsty also handles the underlying highly available PostgreSQL cluster, highly available MinIO object storage cluster, and even Docker deployment, Nginx reverse proxy, domain configuration, and HTTPS certificate issuance. You can spin up any number of stateless Supabase container clusters using Docker Compose and store state in external Pigsty-managed database services.\nWith this self-hosted architecture, you gain the freedom to use different kernels (PG 15-18, OrioleDB), install 437 extensions, scale Supabase/Postgres/MinIO, freedom from database operations, and freedom from vendor lock-in — running locally forever. Compared to cloud service costs, you only need to prepare servers and run a few commands.\nSingle-Node Quick Start Let’s start with single-node Supabase deployment. We’ll cover multi-node high availability later.\nPrepare a fresh Linux server, use the Pigsty supabase configuration template for standard installation, then run docker.yml and app.yml to start stateless Supabase containers (default ports 8000/8433).\ncurl -fsSL https://repo.pigsty.io/get | bash; cd ~/pigsty ./configure -c supabase # Use supabase config (change credentials in pigsty.yml) vi pigsty.yml # Edit domain, passwords, keys... ./deploy.yml # Install Pigsty ./docker.yml # Install Docker module ./app.yml # Start Supabase stateless components with Docker Before deploying Supabase, modify the auto-generated pigsty.yml configuration file (domain and passwords) according to your needs. For local development/testing, you can skip this and customize later.\nIf configured correctly, after about ten minutes, you can access the Supabase Studio GUI at http://\u003cyour_ip_address\u003e:8000 on your local network. Default username and password are supabase and pigsty.\nNotes:\nIn mainland China, Pigsty uses 1Panel and 1ms DockerHub mirrors by default, which may be slow. You can configure your own proxy and registry mirror, then manually pull images with cd /opt/supabase; docker compose pull. We also offer expert consulting services including complete offline installation packages. If you need object storage functionality, you must access Supabase via domain and HTTPS, otherwise errors will occur. For serious production deployments, always change all default passwords! Key Technical Decisions Here are some key technical decisions for self-hosting Supabase:\nSingle-node deployment doesn’t provide PostgreSQL/MinIO high availability. However, single-node deployment still has significant advantages over the official pure Docker Compose approach: out-of-the-box monitoring, freedom to install extensions, component scaling capabilities, and point-in-time recovery as a safety net.\nIf you only have one server or choose to self-host on cloud servers, Pigsty recommends using external S3 instead of local MinIO for object storage to hold PostgreSQL backups and Supabase Storage. This deployment provides a minimum safety net RTO (hour-level recovery time) / RPO (MB-level data loss) disaster recovery in single-node conditions.\nFor serious production deployments, Pigsty recommends at least 3-4 nodes, ensuring both MinIO and PostgreSQL use enterprise-grade multi-node high availability deployments. You’ll need more nodes and disks, adjusting cluster configuration in pigsty.yml and Supabase cluster configuration to use high availability endpoints.\nSome Supabase features require sending emails, so SMTP service is needed. Unless purely for internal use, production deployments should use SMTP cloud services. Self-hosted mail servers’ emails are often marked as spam.\nIf your service is directly exposed to the public internet, we strongly recommend using real domain names and HTTPS certificates via Nginx Portal.\nNext, we’ll discuss advanced topics for improving Supabase security, availability, and performance beyond single-node deployment.\nAdvanced: Security Hardening Pigsty Components\nFor serious production deployments, we strongly recommend changing Pigsty component passwords. These defaults are public and well-known — going to production without changing passwords is like running naked:\ngrafana_admin_password: pigsty, Grafana admin password pg_admin_password: DBUser.DBA, PostgreSQL superuser password pg_monitor_password: DBUser.Monitor, PostgreSQL monitoring user password pg_replication_password: DBUser.Replicator, PostgreSQL replication user password patroni_password: Patroni.API, Patroni HA component password haproxy_admin_password: pigsty, Load balancer admin password minio_secret_key: S3User.MinIO, MinIO root user secret etcd_root_password: Etcd.Root, ETCD root user password Additionally, strongly recommend changing the PostgreSQL business user password for Supabase, default is DBUser.Supa These are Pigsty component passwords. Strongly recommended to set before installation.\nSupabase Keys\nBesides Pigsty component passwords, you need to change Supabase keys, including:\nJWT_SECRET: JWT signing key, at least 32 characters ANON_KEY: Anonymous user JWT credential SERVICE_ROLE_KEY: Service role JWT credential PG_META_CRYPTO_KEY: PostgreSQL Meta service encryption key, at least 32 characters DASHBOARD_USERNAME: Supabase Studio web UI default username, default supabase DASHBOARD_PASSWORD: Supabase Studio web UI default password, default pigsty LOGFLARE_PUBLIC_ACCESS_TOKEN: Logflare public access token, 32-64 random characters LOGFLARE_PRIVATE_ACCESS_TOKEN: Logflare private access token, 32-64 random characters Please follow the Supabase tutorial: Securing your services:\nGenerate a JWT_SECRET with at least 40 characters, then use the tutorial tools to issue ANON_KEY and SERVICE_ROLE_KEY JWTs. Use the tutorial tools to generate an ANON_KEY JWT based on JWT_SECRET and expiration time — this is the anonymous user credential. Use the tutorial tools to generate a SERVICE_ROLE_KEY — this is the higher-privilege service role credential. Specify a random string of at least 32 characters for PG_META_CRYPTO_KEY to encrypt Studio UI and meta service interactions. If using different PostgreSQL business user passwords, modify POSTGRES_PASSWORD accordingly. If your object storage uses different passwords, modify S3_ACCESS_KEY and S3_SECRET_KEY accordingly. After modifying Supabase credentials, restart Docker Compose to apply:\n./app.yml -t app_config,app_launch # Using playbook cd /opt/supabase; make up # Manual execution Advanced: Domain Configuration If using Supabase locally or on LAN, you can directly connect to Kong’s HTTP port 8000 via IP:Port.\nYou can use an internal static-resolved domain, but for serious production deployments, we recommend using a real domain + HTTPS to access Supabase. In this case, your server should have a public IP, you should own a domain, use cloud/DNS/CDN provider’s DNS resolution to point to the node’s public IP (optional fallback: local /etc/hosts static resolution).\nThe simple approach is to batch-replace the placeholder domain (supa.pigsty) with your actual domain, e.g., supa.pigsty.cc:\nsed -ie 's/supa.pigsty/supa.pigsty.cc/g' ~/pigsty/pigsty.yml If not configured beforehand, reload Nginx and Supabase configuration:\nmake cert # Request certbot free HTTPS certificate ./app.yml # Reload Supabase configuration The modified configuration should look like:\nall: vars: certbot_sign: true # Use certbot to sign real certificates infra_portal: home: i.pigsty.cc # Replace with your domain! supa: domain: supa.pigsty.cc # Replace with your domain! endpoint: \"10.10.10.10:8000\" websocket: true certbot: supa.pigsty.cc # Certificate name, usually same as domain children: supabase: vars: apps: supabase: # Supabase app definition conf: # Override /opt/supabase/.env SITE_URL: https://supa.pigsty.cc # \u003c------- Change to your external domain name API_EXTERNAL_URL: https://supa.pigsty.cc # \u003c------- Otherwise the storage API may not work! SUPABASE_PUBLIC_URL: https://supa.pigsty.cc # \u003c------- Don't forget to set this in infra_portal! For complete domain/HTTPS configuration, see Certificate Management. You can also use Pigsty’s built-in local static resolution and self-signed HTTPS certificates as fallback.\nAdvanced: External Object Storage You can use S3 or S3-compatible services for PostgreSQL backups and Supabase object storage. Here we use Alibaba Cloud OSS as an example.\nPigsty provides a terraform/spec/aliyun-s3.tf template for provisioning a server and OSS bucket on Alibaba Cloud.\nFirst, modify the S3 configuration in all.children.supa.vars.apps.[supabase].conf to point to Alibaba Cloud OSS:\n# if using s3/minio as file storage S3_BUCKET: data # Replace with S3-compatible service info S3_ENDPOINT: https://sss.pigsty:9000 # Replace with S3-compatible service info S3_ACCESS_KEY: s3user_data # Replace with S3-compatible service info S3_SECRET_KEY: S3User.Data # Replace with S3-compatible service info S3_FORCE_PATH_STYLE: true # Replace with S3-compatible service info S3_REGION: stub # Replace with S3-compatible service info S3_PROTOCOL: https # Replace with S3-compatible service info Reload Supabase configuration:\n./app.yml -t app_config,app_launch You can also use S3 as PostgreSQL backup repository. Add an aliyun backup repository definition in all.vars.pgbackrest_repo:\nall: vars: pgbackrest_method: aliyun # pgbackrest backup method: local,minio,[user-defined repos...] pgbackrest_repo: # pgbackrest backup repo: https://pgbackrest.org/configuration.html#section-repository aliyun: # Define new backup repo 'aliyun' type: s3 # Alibaba Cloud OSS is S3-compatible s3_endpoint: oss-cn-beijing-internal.aliyuncs.com s3_region: oss-cn-beijing s3_bucket: pigsty-oss s3_key: xxxxxxxxxxxxxx s3_key_secret: xxxxxxxx s3_uri_style: host path: /pgbackrest bundle: y # bundle small files into a single file bundle_limit: 20MiB # Limit for file bundles, 20MiB for object storage bundle_size: 128MiB # Target size for file bundles, 128MiB for object storage cipher_type: aes-256-cbc # enable AES encryption for remote backup repo cipher_pass: pgBackRest.MyPass # Set encryption password for pgBackRest backup repo retention_full_type: time # retention full backup by time on minio repo retention_full: 14 # keep full backup for the last 14 days Then specify aliyun backup repository in all.vars.pgbackrest_method and reset pgBackrest:\n./pgsql.yml -t pgbackrest Pigsty will switch the backup repository to external object storage. For more backup configuration, see PostgreSQL Backup.\nAdvanced: Using SMTP You can use SMTP for sending emails. Modify the supabase app configuration with SMTP information:\nall: children: supabase: # supa group vars: # supa group vars apps: # supa group app list supabase: # the supabase app conf: # the supabase app conf entries SMTP_HOST: smtpdm.aliyun.com:80 SMTP_PORT: 80 SMTP_USER: no_reply@mail.your.domain.com SMTP_PASS: your_email_user_password SMTP_SENDER_NAME: MySupabase SMTP_ADMIN_EMAIL: adminxxx@mail.your.domain.com ENABLE_ANONYMOUS_USERS: false Don’t forget to reload configuration with app.yml.\nAdvanced: True High Availability After these configurations, you have enterprise-grade Supabase with public domain, HTTPS certificate, SMTP, PITR backup, monitoring, IaC, and 400+ extensions (basic single-node version). For high availability configuration, see other Pigsty documentation. We offer expert consulting services for hands-on Supabase self-hosting — $400 USD to save you the hassle.\nSingle-node RTO/RPO relies on external object storage as a safety net. If your node fails, backups in external S3 storage let you redeploy Supabase on a new node and restore from backup. This provides minimum safety net RTO (hour-level recovery) / RPO (MB-level data loss) disaster recovery.\nFor RTO \u003c 30s with zero data loss on failover, use multi-node high availability deployment:\nETCD: DCS needs three or more nodes to tolerate one node failure. PGSQL: PostgreSQL synchronous commit (no data loss) mode recommends at least three nodes. INFRA: Monitoring infrastructure failure has less impact; production recommends dual replicas. Supabase stateless containers can also be multi-node replicas for high availability. In this case, you also need to modify PostgreSQL and MinIO endpoints to use DNS / L2 VIP / HAProxy high availability endpoints. For these parts, follow the documentation for each Pigsty module. Reference conf/ha/trio.yml and conf/ha/safe.yml for upgrading to three or more nodes.\n","categories":["Reference"],"description":"Self-host enterprise-grade Supabase with Pigsty, featuring monitoring, high availability, PITR, IaC, and 440+ PostgreSQL extensions.","excerpt":"Self-host enterprise-grade Supabase with Pigsty, featuring monitoring, …","ref":"/docs/app/supabase/","tags":"","title":"Enterprise Self-Hosted Supabase"},{"body":"Odoo is an open-source enterprise resource planning (ERP) software that provides a full suite of business applications, including CRM, sales, purchasing, inventory, production, accounting, and other management functions. Odoo is a typical web application that uses PostgreSQL as its underlying database.\nAll your business on one platform — Simple, efficient, yet affordable\nPublic Demo (may not always be available): http://odoo.pigsty.io, username: test@pigsty.io, password: pigsty\nQuick Start On a fresh Linux x86/ARM server running a compatible operating system:\ncurl -fsSL https://repo.pigsty.io/get | bash; cd ~/pigsty ./bootstrap # Install Ansible ./configure -c app/odoo # Use Odoo configuration (change credentials in pigsty.yml) ./deploy.yml # Install Pigsty ./docker.yml # Install Docker Compose ./app.yml # Start Odoo stateless components with Docker Odoo listens on port 8069 by default. Access http://\u003cip\u003e:8069 in your browser. The default username and password are both admin.\nYou can add a DNS resolution record odoo.pigsty pointing to your server in the browser host’s /etc/hosts file, allowing you to access the Odoo web interface via http://odoo.pigsty.\nIf you want to access Odoo via SSL/HTTPS, you need to use a real SSL certificate or trust the self-signed CA certificate automatically generated by Pigsty. (In Chrome, you can also type thisisunsafe to bypass certificate verification)\nConfiguration Template conf/app/odoo.yml defines a template configuration file containing the resources required for a single Odoo instance.\nall: children: # Odoo application (default username and password: admin/admin) odoo: hosts: { 10.10.10.10: {} } vars: app: odoo # Specify app name to install (in apps) apps: # Define all applications odoo: # App name, should have corresponding ~/pigsty/app/odoo folder file: # Optional directories to create - { path: /data/odoo ,state: directory, owner: 100, group: 101 } - { path: /data/odoo/webdata ,state: directory, owner: 100, group: 101 } - { path: /data/odoo/addons ,state: directory, owner: 100, group: 101 } conf: # Override /opt/\u003capp\u003e/.env config file PG_HOST: 10.10.10.10 # PostgreSQL host PG_PORT: 5432 # PostgreSQL port PG_USERNAME: odoo # PostgreSQL user PG_PASSWORD: DBUser.Odoo # PostgreSQL password ODOO_PORT: 8069 # Odoo app port ODOO_DATA: /data/odoo/webdata # Odoo webdata ODOO_ADDONS: /data/odoo/addons # Odoo plugins ODOO_DBNAME: odoo # Odoo database name ODOO_VERSION: 19.0 # Odoo image version # Odoo database pg-odoo: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-odoo pg_users: - { name: odoo ,password: DBUser.Odoo ,pgbouncer: true ,roles: [ dbrole_admin ] ,createdb: true ,comment: admin user for odoo service } - { name: odoo_ro ,password: DBUser.Odoo ,pgbouncer: true ,roles: [ dbrole_readonly ] ,comment: read only user for odoo service } - { name: odoo_rw ,password: DBUser.Odoo ,pgbouncer: true ,roles: [ dbrole_readwrite ] ,comment: read write user for odoo service } pg_databases: - { name: odoo ,owner: odoo ,revokeconn: true ,comment: odoo main database } pg_hba_rules: - { user: all ,db: all ,addr: 172.17.0.0/16 ,auth: pwd ,title: 'allow access from local docker network' } - { user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes' } node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] # Full backup daily at 1am infra: { hosts: { 10.10.10.10: { infra_seq: 1 } } } etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 } }, vars: { etcd_cluster: etcd } } #minio: { hosts: { 10.10.10.10: { minio_seq: 1 } }, vars: { minio_cluster: minio } } vars: # Global variables version: v4.0.0 # Pigsty version string admin_ip: 10.10.10.10 # Admin node IP address region: default # Upstream mirror region: default|china|europe node_tune: oltp # Node tuning specs: oltp,olap,tiny,crit pg_conf: oltp.yml # PGSQL tuning specs: {oltp,olap,tiny,crit}.yml docker_enabled: true # Enable docker on app group #docker_registry_mirrors: [\"https://docker.1panel.live\",\"https://docker.1ms.run\",\"https://docker.xuanyuan.me\",\"https://registry-1.docker.io\"] proxy_env: # Global proxy env for downloading packages \u0026 pulling docker images no_proxy: \"localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,*.pigsty,*.aliyun.com,mirrors.*,*.tsinghua.edu.cn\" #http_proxy: 127.0.0.1:12345 # Add proxy env here for downloading packages or pulling images #https_proxy: 127.0.0.1:12345 # Usually format is http://user:pass@proxy.xxx.com #all_proxy: 127.0.0.1:12345 infra_portal: # Domain names and upstream servers home : { domain: i.pigsty } minio : { domain: m.pigsty ,endpoint: \"${admin_ip}:9001\" ,scheme: https ,websocket: true } odoo: # Nginx server config for odoo domain: odoo.pigsty # REPLACE WITH YOUR OWN DOMAIN! endpoint: \"10.10.10.10:8069\" # Odoo service endpoint: IP:PORT websocket: true # Add websocket support certbot: odoo.pigsty # Certbot cert name, apply with `make cert` repo_enabled: false node_repo_modules: node,infra,pgsql pg_version: 18 #----------------------------------# # Credentials: MUST CHANGE THESE! #----------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root Basics Check the configurable environment variables in the .env file:\n# https://hub.docker.com/_/odoo# PG_HOST=10.10.10.10 PG_PORT=5432 PG_USER=dbuser_odoo PG_PASS=DBUser.Odoo ODOO_PORT=8069 Then start Odoo with:\nmake up # docker compose up Access http://odoo.pigsty or http://10.10.10.10:8069\nMakefile make up # Start Odoo with docker compose in minimal mode make run # Start Odoo with docker, local data directory and external PostgreSQL make view # Print Odoo access endpoints make log # tail -f Odoo logs make info # Inspect Odoo with jq make stop # Stop Odoo container make clean # Remove Odoo container make pull # Pull latest Odoo image make rmi # Remove Odoo image make save # Save Odoo image to /tmp/docker/odoo.tgz make load # Load Odoo image from /tmp/docker/odoo.tgz Using External PostgreSQL You can use external PostgreSQL for Odoo. Odoo will create its own database during setup, so you don’t need to do that.\npg_users: [ { name: dbuser_odoo ,password: DBUser.Odoo ,pgbouncer: true ,roles: [ dbrole_admin ] ,comment: admin user for odoo database } ] pg_databases: [ { name: odoo ,owner: dbuser_odoo ,revokeconn: true ,comment: odoo primary database } ] Create the business user and database with:\nbin/pgsql-user pg-meta dbuser_odoo #bin/pgsql-db pg-meta odoo # Odoo will create the database during setup Check connectivity:\npsql postgres://dbuser_odoo:DBUser.Odoo@10.10.10.10:5432/odoo Expose Odoo Service Expose the Odoo web service via Nginx portal:\ninfra_portal: # Domain names and upstream servers home : { domain: h.pigsty } odoo : { domain: odoo.pigsty, endpoint: \"127.0.0.1:8069\", websocket: true } # \u003c------ Add this line ./infra.yml -t nginx # Setup nginx infra portal Odoo Addons There are many Odoo modules available in the community. You can install them by downloading and placing them in the addons folder.\nvolumes: - ./addons:/mnt/extra-addons You can mount the ./addons directory to /mnt/extra-addons in the container, then download and extract addons to the addons folder.\nTo enable addon modules, first enter Developer mode:\nSettings -\u003e General Settings -\u003e Developer Tools -\u003e Activate the developer mode\nThen go to Apps -\u003e Update Apps List, and you’ll find the extra addons available to install from the panel.\nFrequently used free addons: Accounting Kit\nDemo Check the public demo: http://odoo.pigsty.io, username: test@pigsty.io, password: pigsty\nIf you want to access Odoo via SSL, you must trust files/pki/ca/ca.crt in your browser (or use the dirty hack thisisunsafe in Chrome).\n","categories":["Reference"],"description":"How to spin up an out-of-the-box enterprise application suite Odoo and use Pigsty to manage its backend PostgreSQL database.","excerpt":"How to spin up an out-of-the-box enterprise application suite Odoo and …","ref":"/docs/app/odoo/","tags":"","title":"Odoo: Self-Hosted Open Source ERP"},{"body":"Dify is a Generative AI Application Innovation Engine and open-source LLM application development platform. It provides capabilities from Agent building to AI workflow orchestration, RAG retrieval, and model management, helping users easily build and operate generative AI native applications.\nPigsty provides support for self-hosted Dify, allowing you to deploy Dify with a single command while storing critical state in externally managed PostgreSQL. You can use pgvector as a vector database in the same PostgreSQL instance, further simplifying deployment.\nQuick Start Why Self-Host Installation Configuration Checklist Domain and SSL File Backup Current Pigsty v4.0 supported Dify version: v1.8.1\nQuick Start On a fresh Linux x86/ARM server running a compatible operating system:\ncurl -fsSL https://repo.pigsty.io/get | bash; cd ~/pigsty ./bootstrap # Install Pigsty dependencies ./configure -c app/dify # Use Dify configuration template vi pigsty.yml # Edit passwords, domains, keys, etc. ./deploy.yml # Install Pigsty ./docker.yml # Install Docker and Compose ./app.yml # Install Dify Dify listens on port 5001 by default. Access http://\u003cip\u003e:5001 in your browser and set up your initial user credentials to log in.\nOnce Dify starts, you can install various extensions, configure system models, and start using it!\nWhy Self-Host There are many reasons to self-host Dify, but the primary motivation is data security. The Docker Compose template provided by Dify uses basic default database images, lacking enterprise features like high availability, disaster recovery, monitoring, IaC, and PITR capabilities.\nPigsty elegantly solves these issues for Dify, deploying all components with a single command based on configuration files and using mirrors to address China region access challenges. This makes Dify deployment and delivery very smooth. It handles PostgreSQL primary database, PGVector vector database, MinIO object storage, Redis, Prometheus monitoring, Grafana visualization, Nginx reverse proxy, and free HTTPS certificates all at once.\nPigsty ensures all Dify state is stored in externally managed services, including metadata in PostgreSQL and other data in the file system. Dify instances launched via Docker Compose become stateless applications that can be destroyed and rebuilt at any time, greatly simplifying operations.\nInstallation Let’s start with single-node Dify deployment. We’ll cover production high-availability deployment methods later.\nFirst, use Pigsty’s standard installation process to install the PostgreSQL instance required by Dify:\ncurl -fsSL https://repo.pigsty.io/get | bash; cd ~/pigsty ./bootstrap # Prepare Pigsty dependencies ./configure -c app/dify # Use Dify application template vi pigsty.yml # Edit configuration file, modify domains and passwords ./deploy.yml # Install Pigsty and various databases When you use the ./configure -c app/dify command, Pigsty automatically generates a configuration file based on the conf/app/dify.yml template and your current environment. You should modify passwords, domains, and other relevant parameters in the generated pigsty.yml configuration file according to your needs, then run ./deploy.yml to execute the standard installation process.\nNext, run docker.yml to install Docker and Docker Compose, then use app.yml to complete Dify deployment:\n./docker.yml # Install Docker and Docker Compose ./app.yml # Deploy Dify stateless components with Docker You can access the Dify Web admin interface at http://\u003cyour_ip_address\u003e:5001 on your local network.\nThe first login will prompt you to set up default username, email, and password.\nYou can also use the locally resolved placeholder domain dify.pigsty, or follow the configuration below to use a real domain with an HTTPS certificate.\nConfiguration When you use the ./configure -c app/dify command for configuration, Pigsty automatically generates a configuration file based on the conf/app/dify.yml template and your current environment. Here’s a detailed explanation of the default configuration:\n--- #==============================================================# # File : dify.yml # Desc : pigsty config for running 1-node dify app # Ctime : 2025-02-24 # Mtime : 2025-12-12 # Docs : https://doc.pgsty.com/app/odoo # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# # Last Verified Dify Version: v1.8.1 on 2025-0908 # tutorial: https://doc.pgsty.com/app/dify # how to use this template: # # curl -fsSL https://repo.pigsty.io/get | bash; cd ~/pigsty # ./bootstrap # prepare local repo \u0026 ansible # ./configure -c app/dify # use this dify config template # vi pigsty.yml # IMPORTANT: CHANGE CREDENTIALS!! # ./deploy.yml # install pigsty \u0026 pgsql \u0026 minio # ./docker.yml # install docker \u0026 docker-compose # ./app.yml # install dify with docker-compose # # To replace domain name: # sed -ie 's/dify.pigsty/dify.pigsty.cc/g' pigsty.yml all: children: # the dify application dify: hosts: { 10.10.10.10: {} } vars: app: dify # specify app name to be installed (in the apps) apps: # define all applications dify: # app name, should have corresponding ~/pigsty/app/dify folder file: # data directory to be created - { path: /data/dify ,state: directory ,mode: 0755 } conf: # override /opt/dify/.env config file # change domain, mirror, proxy, secret key NGINX_SERVER_NAME: dify.pigsty # A secret key for signing and encryption, gen with `openssl rand -base64 42` (CHANGE PASSWORD!) SECRET_KEY: sk-somerandomkey # expose DIFY nginx service with port 5001 by default DIFY_PORT: 5001 # where to store dify files? the default is ./volume, we'll use another volume created above DIFY_DATA: /data/dify # proxy and mirror settings #PIP_MIRROR_URL: https://pypi.tuna.tsinghua.edu.cn/simple #SANDBOX_HTTP_PROXY: http://10.10.10.10:12345 #SANDBOX_HTTPS_PROXY: http://10.10.10.10:12345 # database credentials DB_USERNAME: dify DB_PASSWORD: difyai123456 DB_HOST: 10.10.10.10 DB_PORT: 5432 DB_DATABASE: dify VECTOR_STORE: pgvector PGVECTOR_HOST: 10.10.10.10 PGVECTOR_PORT: 5432 PGVECTOR_USER: dify PGVECTOR_PASSWORD: difyai123456 PGVECTOR_DATABASE: dify PGVECTOR_MIN_CONNECTION: 2 PGVECTOR_MAX_CONNECTION: 10 pg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta pg_users: - { name: dify ,password: difyai123456 ,pgbouncer: true ,roles: [ dbrole_admin ] ,superuser: true ,comment: dify superuser } pg_databases: - { name: dify ,owner: dify ,revokeconn: true ,comment: dify main database } - { name: dify_plugin ,owner: dify ,revokeconn: true ,comment: dify plugin_daemon database } pg_hba_rules: - { user: dify ,db: all ,addr: 172.17.0.0/16 ,auth: pwd ,title: 'allow dify access from local docker network' } node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] # make a full backup every 1am infra: { hosts: { 10.10.10.10: { infra_seq: 1 } } } etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 } }, vars: { etcd_cluster: etcd } } #minio: { hosts: { 10.10.10.10: { minio_seq: 1 } }, vars: { minio_cluster: minio } } vars: # global variables version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default|china|europe node_tune: oltp # node tuning specs: oltp,olap,tiny,crit pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml docker_enabled: true # enable docker on app group #docker_registry_mirrors: [\"https://docker.1panel.live\",\"https://docker.1ms.run\",\"https://docker.xuanyuan.me\",\"https://registry-1.docker.io\"] proxy_env: # global proxy env when downloading packages \u0026 pull docker images no_proxy: \"localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,*.pigsty,*.aliyun.com,mirrors.*,*.tsinghua.edu.cn\" #http_proxy: 127.0.0.1:12345 # add your proxy env here for downloading packages or pull images #https_proxy: 127.0.0.1:12345 # usually the proxy is format as http://user:pass@proxy.xxx.com #all_proxy: 127.0.0.1:12345 infra_portal: # domain names and upstream servers home : { domain: i.pigsty } #minio : { domain: m.pigsty ,endpoint: \"${admin_ip}:9001\" ,scheme: https ,websocket: true } dify: # nginx server config for dify domain: dify.pigsty # REPLACE WITH YOUR OWN DOMAIN! endpoint: \"10.10.10.10:5001\" # dify service endpoint: IP:PORT websocket: true # add websocket support certbot: dify.pigsty # certbot cert name, apply with `make cert` repo_enabled: false node_repo_modules: node,infra,pgsql pg_version: 18 #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root ... Checklist Here’s a checklist of configuration items you need to pay attention to:\nHardware/Software: Prepare required machine resources: Linux x86_64/arm64 server, fresh installation of a mainstream Linux OS Network/Permissions: SSH passwordless login access, user with sudo privileges without password Ensure the machine has a static IPv4 network address on the internal network and can access the internet If accessing via public network, ensure you have a domain pointing to the node’s public IP address Ensure you use the app/dify configuration template and modify parameters as needed configure -c app/dify, enter the node’s internal primary IP address, or specify via -i \u003cprimary_ip\u003e command line parameter Have you changed all password-related configuration parameters? [Optional] grafana_admin_password: pigsty, Grafana admin password pg_admin_password: DBUser.DBA, PG superuser password pg_monitor_password: DBUser.Monitor, PG monitoring user password pg_replication_password: DBUser.Replicator, PG replication user password patroni_password: Patroni.API, Patroni HA component password haproxy_admin_password: pigsty, Load balancer admin password Have you changed the PostgreSQL cluster business user password and application configurations using these passwords? Default username dify and password difyai123456 are generated by Pigsty for Dify; modify according to your needs In the Dify configuration block, modify DB_USERNAME, DB_PASSWORD, PGVECTOR_USER, PGVECTOR_PASSWORD accordingly Have you changed Dify’s default encryption key? You can randomly generate a password string with openssl rand -base64 42 and fill in the SECRET_KEY parameter Have you changed the domain used by Dify? Replace placeholder domain dify.pigsty with your actual domain, e.g., dify.pigsty.cc You can use sed -ie 's/dify.pigsty/dify.pigsty.cc/g' pigsty.yml to modify Dify’s domain Domain and SSL If you want to use a real domain with an HTTPS certificate, you need to modify the pigsty.yml configuration file:\nThe dify domain in the infra_portal parameter It’s best to specify an email address certbot_email for certificate expiration notifications Configure Dify’s NGINX_SERVER_NAME parameter to specify your actual domain all: children: # Cluster definitions dify: # Dify group vars: # Dify group variables apps: # Application configuration dify: # Dify application definition conf: # Dify application configuration NGINX_SERVER_NAME: dify.pigsty vars: # Global parameters #certbot_sign: true # Use Certbot for free HTTPS certificate certbot_email: your@email.com # Email for certificate requests, for expiration notifications, optional infra_portal: # Configure Nginx servers dify: # Dify server definition domain: dify.pigsty # Replace with your own domain here! endpoint: \"10.10.10.10:5001\" # Specify Dify's IP and port here (auto-configured by default) websocket: true # Dify requires websocket enabled certbot: dify.pigsty # Specify Certbot certificate name Use the following commands to request Nginx certificates:\n# Request certificate, can also manually run /etc/nginx/sign-cert script make cert # The above Makefile shortcut actually runs the following playbook task: ./infra.yml -t nginx_certbot,nginx_reload -e certbot_sign=true Run the app.yml playbook to redeploy Dify service for the NGINX_SERVER_NAME configuration to take effect:\n./app.yml File Backup You can use restic to backup Dify’s file storage (default location /data/dify):\nexport RESTIC_REPOSITORY=/data/backups/dify # Specify dify backup directory export RESTIC_PASSWORD=some-strong-password # Specify backup encryption password mkdir -p ${RESTIC_REPOSITORY} # Create dify backup directory restic init After creating the Restic backup repository, you can backup Dify with:\nexport RESTIC_REPOSITORY=/data/backups/dify # Specify dify backup directory export RESTIC_PASSWORD=some-strong-password # Specify backup encryption password restic backup /data/dify # Backup /dify data directory to repository restic snapshots # View backup snapshot list restic restore -t /data/dify 0b11f778 # Restore snapshot xxxxxx to /data/dify restic check # Periodically check repository integrity Another more reliable method is using JuiceFS to mount MinIO object storage to the /data/dify directory, allowing you to use MinIO/S3 for file state storage.\nIf you want to store all data in PostgreSQL, consider “storing file system data in PostgreSQL using JuiceFS”.\nFor example, you can create another dify_fs database and use it as JuiceFS metadata storage:\nMETAURL=postgres://dify:difyai123456@:5432/dify_fs OPTIONS=( --storage postgres --bucket :5432/dify_fs --access-key dify --secret-key difyai123456 ${METAURL} jfs ) juicefs format \"${OPTIONS[@]}\" # Create PG file system juicefs mount ${METAURL} /data/dify -d # Mount to /data/dify directory in background juicefs bench /data/dify # Test performance juicefs umount /data/dify # Unmount Reference Dify Self-Hosting FAQ\n","categories":["Reference"],"description":"How to self-host the AI Workflow LLMOps platform — Dify, using external PostgreSQL, PGVector, and Redis for storage with Pigsty?","excerpt":"How to self-host the AI Workflow LLMOps platform — Dify, using …","ref":"/docs/app/dify/","tags":"","title":"Dify: AI Workflow Platform"},{"body":"Enterprise-grade open source software templates\n","categories":["Application"],"description":"","excerpt":"Enterprise-grade open source software templates\n","ref":"/docs/app/_div_enterprise/","tags":"","title":"Enterprise Software"},{"body":"NocoDB is an open-source Airtable alternative that turns any database into a smart spreadsheet.\nIt provides a rich user interface that allows you to create powerful database applications without writing code. NocoDB supports PostgreSQL, MySQL, SQL Server, and more, making it ideal for building internal tools and data management systems.\nQuick Start Pigsty provides a Docker Compose configuration file for NocoDB in the software template directory:\ncd ~/pigsty/app/nocodb Review and modify the .env configuration file (adjust database connections as needed).\nStart the service:\nmake up # Start NocoDB with Docker Compose Access NocoDB:\nDefault URL: http://nocodb.pigsty Alternate URL: http://10.10.10.10:8080 First-time access requires creating an administrator account Management Commands Pigsty provides convenient Makefile commands to manage NocoDB:\nmake up # Start NocoDB service make run # Start with Docker (connect to external PostgreSQL) make view # Display NocoDB access URL make log # View container logs make info # View service details make stop # Stop the service make clean # Stop and remove containers make pull # Pull the latest image make rmi # Remove NocoDB image make save # Save image to /tmp/nocodb.tgz make load # Load image from /tmp/nocodb.tgz Connect to PostgreSQL NocoDB can connect to PostgreSQL databases managed by Pigsty.\nWhen adding a new project in the NocoDB interface, select “External Database” and enter the PostgreSQL connection information:\nHost: 10.10.10.10 Port: 5432 Database Name: your_database Username: your_username Password: your_password SSL: Disabled (or enable as needed) After successful connection, NocoDB will automatically read the database table structure, and you can manage data through the visual interface.\nFeatures Smart Spreadsheet Interface: Excel/Airtable-like user experience Multiple Views: Grid, form, kanban, calendar, gallery views Collaboration Features: Team collaboration, permission management, comments API Support: Auto-generated REST API Integration Capabilities: Webhooks, Zapier integrations Import/Export: CSV, Excel format support Formulas and Validation: Complex data calculations and validation rules Configuration NocoDB configuration is in the .env file:\n# Database connection (NocoDB metadata storage) NC_DB=pg://postgres:DBUser.Postgres@10.10.10.10:5432/nocodb # JWT secret (recommended to change) NC_AUTH_JWT_SECRET=your-secret-key # Other settings NC_PUBLIC_URL=http://nocodb.pigsty NC_DISABLE_TELE=true Data Persistence NocoDB metadata is stored by default in an external PostgreSQL database, and application data can also be stored in PostgreSQL.\nIf using local storage, data is saved in the /data/nocodb directory.\nSecurity Recommendations Change Default Secret: Modify NC_AUTH_JWT_SECRET in the .env file Use Strong Passwords: Set strong passwords for administrator accounts Configure HTTPS: Enable HTTPS for production environments Restrict Access: Limit access through firewall or Nginx Regular Backups: Regularly back up the NocoDB metadata database Related Links NocoDB Website: https://nocodb.com/ Documentation: https://docs.nocodb.com/ GitHub Repository: https://github.com/nocodb/nocodb Pigsty Software Template: https://github.com/pgsty/pigsty/tree/main/app/nocodb ","categories":["Reference"],"description":"Use NocoDB to transform PostgreSQL databases into smart spreadsheets, a no-code database application platform.","excerpt":"Use NocoDB to transform PostgreSQL databases into smart spreadsheets, …","ref":"/docs/app/nocodb/","tags":"","title":"NocoDB: Open-Source Airtable"},{"body":"Teable is an AI-powered no-code database platform designed for team collaboration and automation.\nTeable perfectly combines the power of databases with the ease of spreadsheets, integrating AI capabilities to help teams efficiently generate, automate, and collaborate on data.\nQuick Start Teable requires a complete Pigsty environment (including PostgreSQL, Redis, MinIO).\nPrepare Environment cd ~/pigsty ./bootstrap # Prepare local repo and Ansible ./configure -c app/teable # Important: modify default credentials! ./deploy.yml # Install Pigsty, PostgreSQL, MinIO ./redis.yml # Install Redis instance ./docker.yml # Install Docker and Docker Compose ./app.yml # Install Teable with Docker Compose Access Service Default URL: http://teable.pigsty Alternate URL: http://10.10.10.10:3000 First-time access requires registering an administrator account Management Commands Manage Teable in the Pigsty software template directory:\ncd ~/pigsty/app/teable make up # Start Teable service make down # Stop Teable service make log # View container logs make clean # Clean up containers and data Architecture Teable depends on the following components:\nPostgreSQL: Stores application data and metadata Redis: Caching and session management MinIO: Object storage (files, images, etc.) Docker: Container runtime environment Ensure these services are properly installed before deploying Teable.\nFeatures AI Integration: Built-in AI assistant for auto-generating data, formulas, and workflows Smart Tables: Powerful table functionality with multiple field types Automated Workflows: No-code automation to boost team efficiency Multiple Views: Grid, form, kanban, calendar, and more Team Collaboration: Real-time collaboration, permission management, comments API and Integrations: Auto-generated API with Webhook support Template Library: Rich application templates for quick project starts Configuration Teable configuration is managed through environment variables in docker-compose.yml:\n# PostgreSQL connection POSTGRES_HOST=10.10.10.10 POSTGRES_PORT=5432 POSTGRES_DB=teable POSTGRES_USER=dbuser_teable POSTGRES_PASSWORD=DBUser.Teable # Redis connection REDIS_HOST=10.10.10.10 REDIS_PORT=6379 REDIS_DB=0 # MinIO connection MINIO_ENDPOINT=http://10.10.10.10:9000 MINIO_ACCESS_KEY=minioadmin MINIO_SECRET_KEY=minioadmin # Application configuration BACKEND_URL=http://teable.pigsty PUBLIC_ORIGIN=http://teable.pigsty Important: In production environments, modify all default passwords and keys!\nData Persistence Teable data persistence relies on:\nPostgreSQL: All structured data stored in PostgreSQL MinIO: Files, images, and other unstructured data stored in MinIO Redis: Cache data (optional persistence) Regularly back up the PostgreSQL database and MinIO buckets to ensure data safety.\nSecurity Recommendations Change Default Credentials: Modify all default usernames and passwords in configuration files Enable HTTPS: Configure SSL certificates for production environments Configure Firewall: Restrict access to services Regular Backups: Regularly back up PostgreSQL and MinIO data Update Components: Keep Teable and dependent components up to date Related Links Teable Website: https://teable.io/ Documentation: https://help.teable.io/ GitHub Repository: https://github.com/teableio/teable Pigsty Software Template: https://github.com/pgsty/pigsty/tree/main/app/teable ","categories":["Reference"],"description":"Build AI-powered no-code database applications with Teable to boost team productivity.","excerpt":"Build AI-powered no-code database applications with Teable to boost …","ref":"/docs/app/teable/","tags":"","title":"Teable: AI No-Code Database"},{"body":"Public Demo: http://git.pigsty.cc\nTL;DR cd ~/pigsty/app/gitea; make up Pigsty use 8889 port for gitea by default\nhttp://git.pigsty or http://10.10.10.10:8889\nmake up # pull up gitea with docker-compose in minimal mode make run # launch gitea with docker , local data dir and external PostgreSQL make view # print gitea access point make log # tail -f gitea logs make info # introspect gitea with jq make stop # stop gitea container make clean # remove gitea container make pull # pull latest gitea image make rmi # remove gitea image make save # save gitea image to /tmp/gitea.tgz make load # load gitea image from /tmp PostgreSQL Preparation Gitea use built-in SQLite as default metadata storage, you can let Gitea use external PostgreSQL by setting connection string environment variable\n# postgres://dbuser_gitea:DBUser.gitea@10.10.10.10:5432/gitea db: { name: gitea, owner: dbuser_gitea, comment: gitea primary database } user: { name: dbuser_gitea , password: DBUser.gitea, roles: [ dbrole_admin ] } ","categories":["Reference"],"description":"Launch the self-hosting Git service with Gitea and Pigsty managed PostgreSQL","excerpt":"Launch the self-hosting Git service with Gitea and Pigsty managed …","ref":"/docs/app/gitea/","tags":"","title":"Gitea: Simple Self-Hosting Git Service"},{"body":"Public Demo: http://wiki.pigsty.cc\nTL; DR cd app/wiki ; docker-compose up -d Postgres Preparation # postgres://dbuser_wiki:DBUser.Wiki@10.10.10.10:5432/wiki - { name: wiki, owner: dbuser_wiki, revokeconn: true , comment: wiki the api gateway database } - { name: dbuser_wiki, password: DBUser.Wiki , pgbouncer: true , roles: [ dbrole_admin ] } bin/pgsql-user pg-meta dbuser_wiki bin/pgsql-db pg-meta wiki Configuration version: \"3\" services: wiki: container_name: wiki image: requarks/wiki:2 environment: DB_TYPE: postgres DB_HOST: 10.10.10.10 DB_PORT: 5432 DB_USER: dbuser_wiki DB_PASS: DBUser.Wiki DB_NAME: wiki restart: unless-stopped ports: - \"9002:3000\" Access Default Port for wiki: 9002 # add to nginx_upstream - { name: wiki , domain: wiki.pigsty.cc , endpoint: \"127.0.0.1:9002\" } ./infra.yml -t nginx_config ansible all -b -a 'nginx -s reload' ","categories":["Reference"],"description":"How to self-hosting your own wikipedia with Wiki.js and use Pigsty managed PostgreSQL as the backend database","excerpt":"How to self-hosting your own wikipedia with Wiki.js and use Pigsty …","ref":"/docs/app/wiki/","tags":"","title":"Wiki.js: OSS Wiki Software"},{"body":"Mattermost is an open-source team collaboration and messaging platform.\nMattermost provides instant messaging, file sharing, audio/video calls, and more. It’s an open-source alternative to Slack and Microsoft Teams, particularly suitable for enterprises requiring self-hosted deployment.\nQuick Start cd ~/pigsty/app/mattermost make up # Start Mattermost with Docker Compose Access URL: http://mattermost.pigsty or http://10.10.10.10:8065\nFirst-time access requires creating an administrator account.\nFeatures Instant Messaging: Personal and group chat Channel Management: Public and private channels File Sharing: Secure file storage and sharing Audio/Video Calls: Built-in calling functionality Integration Capabilities: Webhooks, Bots, and plugins support Mobile Apps: iOS and Android clients Enterprise-grade: SSO, LDAP, compliance features Connect to PostgreSQL Mattermost uses PostgreSQL for data storage. Configure the connection information:\nMM_SQLSETTINGS_DRIVERNAME=postgres MM_SQLSETTINGS_DATASOURCE=postgres://dbuser_mm:DBUser.MM@10.10.10.10:5432/mattermost Related Links Mattermost Website: https://mattermost.com/ Documentation: https://docs.mattermost.com/ GitHub Repository: https://github.com/mattermost/mattermost ","categories":["Reference"],"description":"Build a private team collaboration platform with Mattermost, the open-source Slack alternative.","excerpt":"Build a private team collaboration platform with Mattermost, the …","ref":"/docs/app/mattermost/","tags":"","title":"Mattermost: Open-Source IM"},{"body":"Maybe is an open-source personal finance management application.\nMaybe provides financial tracking, budget management, investment analysis, and more. It’s an open-source alternative to Mint and Personal Capital, giving you complete control over your financial data.\nQuick Start cd ~/pigsty/app/maybe cp .env.example .env vim .env # Must modify SECRET_KEY_BASE make up # Start Maybe service Access URL: http://maybe.pigsty or http://10.10.10.10:5002\nConfiguration Configure in the .env file:\nSECRET_KEY_BASE=your-secret-key-here # Must modify! DATABASE_URL=postgresql://... Important: You must modify SECRET_KEY_BASE before first deployment!\nFeatures Account Management: Track multiple bank accounts and credit cards Budget Planning: Set up and track budgets Investment Analysis: Monitor portfolio performance Bill Reminders: Automatic reminders for upcoming bills Privacy-first: Data is completely under your control Related Links GitHub Repository: https://github.com/maybe-finance/maybe ","categories":["Reference"],"description":"Manage personal finances with Maybe, the open-source Mint/Personal Capital alternative.","excerpt":"Manage personal finances with Maybe, the open-source Mint/Personal …","ref":"/docs/app/maybe/","tags":"","title":"Maybe: Personal Finance"},{"body":"Metabase is a fast, easy-to-use open-source business intelligence tool that lets your team explore and visualize data without SQL knowledge.\nMetabase provides a friendly user interface with rich chart types and supports connecting to various databases, making it an ideal choice for enterprise data analysis.\nQuick Start Pigsty provides a Docker Compose configuration file for Metabase in the software template directory:\ncd ~/pigsty/app/metabase Review and modify the .env configuration file:\nvim .env # Check configuration, recommend changing default credentials Start the service:\nmake up # Start Metabase with Docker Compose Access Metabase:\nDefault URL: http://metabase.pigsty Alternate URL: http://10.10.10.10:3001 First-time access requires initial setup Management Commands Pigsty provides convenient Makefile commands to manage Metabase:\nmake up # Start Metabase service make run # Start with Docker (connect to external PostgreSQL) make view # Display Metabase access URL make log # View container logs make info # View service details make stop # Stop the service make clean # Stop and remove containers make pull # Pull the latest image make rmi # Remove Metabase image make save # Save image to file make load # Load image from file Connect to PostgreSQL Metabase can connect to PostgreSQL databases managed by Pigsty.\nDuring Metabase initialization or when adding a database, select “PostgreSQL” and enter the connection information:\nDatabase Type: PostgreSQL Name: Custom name (e.g., \"Production Database\") Host: 10.10.10.10 Port: 5432 Database Name: your_database Username: dbuser_meta Password: DBUser.Meta After successful connection, Metabase will automatically scan the database schema, and you can start creating questions and dashboards.\nFeatures No SQL Required: Build queries through visual interface Rich Chart Types: Line, bar, pie, map charts, and more Interactive Dashboards: Create beautiful data dashboards Auto Refresh: Schedule data and dashboard updates Permission Management: Fine-grained user and data access control SQL Mode: Advanced users can write SQL directly Embedding: Embed charts into other applications Alerting: Automatic notifications on data changes Configuration Metabase configuration is in the .env file:\n# Metabase metadata database (PostgreSQL recommended) MB_DB_TYPE=postgres MB_DB_DBNAME=metabase MB_DB_PORT=5432 MB_DB_USER=dbuser_metabase MB_DB_PASS=DBUser.Metabase MB_DB_HOST=10.10.10.10 # Application configuration JAVA_OPTS=-Xmx2g Recommended: Use a dedicated PostgreSQL database for storing Metabase metadata.\nData Persistence Metabase metadata (users, questions, dashboards, etc.) is stored in the configured database.\nIf using H2 database (default), data is saved in the /data/metabase directory. Using PostgreSQL as the metadata database is strongly recommended for production environments.\nPerformance Optimization Use PostgreSQL: Replace the default H2 database Increase Memory: Add JVM memory with JAVA_OPTS=-Xmx4g Database Indexes: Create indexes for frequently queried fields Result Caching: Enable Metabase query result caching Scheduled Updates: Set reasonable dashboard auto-refresh frequency Security Recommendations Change Default Credentials: Modify metadata database username and password Enable HTTPS: Configure SSL certificates for production Configure Authentication: Enable SSO or LDAP authentication Restrict Access: Limit access through firewall Regular Backups: Back up the Metabase metadata database Related Links Metabase Website: https://metabase.com/ Documentation: https://www.metabase.com/docs/ GitHub Repository: https://github.com/metabase/metabase Pigsty Software Template: https://github.com/pgsty/pigsty/tree/main/app/metabase ","categories":["Reference"],"description":"Use Metabase for rapid business intelligence analysis with a user-friendly interface for team self-service data exploration.","excerpt":"Use Metabase for rapid business intelligence analysis with a …","ref":"/docs/app/metabase/","tags":"","title":"Metabase: BI Analytics Tool"},{"body":"\nTL;DR cd app/kong ; docker-compose up -d make up # pull up kong with docker-compose make ui # run swagger ui container make log # tail -f kong logs make info # introspect kong with jq make stop # stop kong container make clean # remove kong container make rmui # remove swagger ui container make pull # pull latest kong image make rmi # remove kong image make save # save kong image to /tmp/kong.tgz make load # load kong image from /tmp Scripts Default Port: 8000 Default SSL Port: 8443 Default Admin Port: 8001 Default Postgres Database: postgres://dbuser_kong:DBUser.Kong@10.10.10.10:5432/kong # postgres://dbuser_kong:DBUser.Kong@10.10.10.10:5432/kong - { name: kong, owner: dbuser_kong, revokeconn: true , comment: kong the api gateway database } - { name: dbuser_kong, password: DBUser.Kong , pgbouncer: true , roles: [ dbrole_admin ] } ","categories":["Reference"],"description":"Learn how to deploy Kong, the API gateway, with Docker Compose and use external PostgreSQL as the backend database","excerpt":"Learn how to deploy Kong, the API gateway, with Docker Compose and use …","ref":"/docs/app/kong/","tags":"","title":"Kong: the Nginx API Gateway"},{"body":"Docker Registry mirror service caches images from Docker Hub and other registries.\nParticularly useful for users in China or regions with slow Docker Hub access, significantly reducing image pull times.\nQuick Start cd ~/pigsty/app/registry make up # Start Registry mirror service Access URL: http://registry.pigsty or http://10.10.10.10:5000\nFeatures Image Caching: Cache images from Docker Hub and other registries Web Interface: Optional image management UI High Performance: Local caching dramatically improves pull speed Storage Management: Configurable cleanup and management policies Health Checks: Built-in health check endpoints Configure Docker Configure Docker to use the local mirror:\n# Edit /etc/docker/daemon.json { \"registry-mirrors\": [\"http://10.10.10.10:5000\"] } # Restart Docker systemctl restart docker Storage Management Image data is stored in the /data/registry directory. Reserve at least 100GB of space.\nRelated Links Docker Registry Documentation: https://docs.docker.com/registry/ GitHub Repository: https://github.com/distribution/distribution ","categories":["Reference"],"description":"Deploy Docker Registry mirror service to accelerate Docker image pulls, especially useful for users in China.","excerpt":"Deploy Docker Registry mirror service to accelerate Docker image …","ref":"/docs/app/registry/","tags":"","title":"Registry: Container Image Mirror"},{"body":"Database management and development tools\n","categories":["Application"],"description":"","excerpt":"Database management and development tools\n","ref":"/docs/app/_div_tools/","tags":"","title":"Database Tools"},{"body":"ByteBase ByteBase is a database schema change management tool. The following command will start a ByteBase on the meta node 8887 port by default.\nmkdir -p /data/bytebase/data; docker run --init --name bytebase --restart always --detach --publish 8887:8887 --volume /data/bytebase/data:/var/opt/bytebase \\ bytebase/bytebase:1.0.4 --data /var/opt/bytebase --host http://ddl.pigsty --port 8887 Then visit http://10.10.10.10:8887/ or http://ddl.pigsty to access bytebase console. You have to “Create Project”, “Env”, “Instance”, “Database” to perform schema migration.\nPublic Demo: http://ddl.pigsty.cc\nDefault username \u0026 password: admin / pigsty\nBytebase Overview Schema Migrator for PostgreSQL\ncd app/bytebase; make up Visit http://ddl.pigsty or http://10.10.10.10:8887\nmake up # pull up bytebase with docker-compose in minimal mode make run # launch bytebase with docker , local data dir and external PostgreSQL make view # print bytebase access point make log # tail -f bytebase logs make info # introspect bytebase with jq make stop # stop bytebase container make clean # remove bytebase container make pull # pull latest bytebase image make rmi # remove bytebase image make save # save bytebase image to /tmp/bytebase.tgz make load # load bytebase image from /tmp PostgreSQL Preparation Bytebase use its internal PostgreSQL database by default, You can use external PostgreSQL for higher durability.\n# postgres://dbuser_bytebase:DBUser.Bytebase@10.10.10.10:5432/bytebase db: { name: bytebase, owner: dbuser_bytebase, comment: bytebase primary database } user: { name: dbuser_bytebase , password: DBUser.Bytebase, roles: [ dbrole_admin ] } if you wish to user an external PostgreSQL, drop monitor extensions and views \u0026 pg_repack\nDROP SCHEMA monitor CASCADE; DROP EXTENSION pg_repack; After bytebase initialized, you can create them back with /pg/tmp/pg-init-template.sql\npsql bytebase \u003c /pg/tmp/pg-init-template.sql ","categories":["Task"],"description":"Self-hosting bytebase with PostgreSQL managed by Pigsty","excerpt":"Self-hosting bytebase with PostgreSQL managed by Pigsty","ref":"/docs/app/bytebase/","tags":"","title":"ByteBase: PG Schema Migration"},{"body":"pgAdmin4 is a useful PostgreSQL management tool. Execute the following command to launch the pgadmin service on the admin node:\ncd ~/pigsty/app/pgadmin ; docker-compose up -d The default port for pgadmin is 8885, and you can access it through the following address:\nhttp://adm.pigsty\nDemo Public Demo: http://adm.pigsty.cc\nCredentials: admin@pigsty.cc / pigsty\nTL; DR cd ~/pigsty/app/pgadmin # enter docker compose dir make up # launch pgadmin container make conf view # load pigsty server list Shortcuts:\nmake up # pull up pgadmin with docker-compose make run # launch pgadmin with docker make view # print pgadmin access point make log # tail -f pgadmin logs make info # introspect pgadmin with jq make stop # stop pgadmin container make clean # remove pgadmin container make conf # provision pgadmin with pigsty pg servers list make dump # dump servers.json from pgadmin container make pull # pull latest pgadmin image make rmi # remove pgadmin image make save # save pgadmin image to /tmp/pgadmin.tgz make load # load pgadmin image from /tmp ","categories":["Task"],"description":"Launch pgAdmin4 with docker, and load Pigsty server list into it","excerpt":"Launch pgAdmin4 with docker, and load Pigsty server list into it","ref":"/docs/app/pgadmin/","tags":"","title":"PGAdmin4: PG Admin GUI Tool"},{"body":"PGWEB: https://github.com/sosedoff/pgweb\nSimple web-based and cross-platform PostgreSQL database explorer.\nPublic Demo: http://cli.pigsty.cc\nTL; DR cd ~/pigsty/app/pgweb ; make up Visit http://cli.pigsty or http://10.10.10.10:8886\nTry connecting with example URLs:\npostgres://dbuser_meta:DBUser.Meta@10.10.10.10:5432/meta?sslmode=disable postgres://test:test@10.10.10.11:5432/test?sslmode=disable make up # pull up pgweb with docker compose make run # launch pgweb with docker make view # print pgweb access point make log # tail -f pgweb logs make info # introspect pgweb with jq make stop # stop pgweb container make clean # remove pgweb container make pull # pull latest pgweb image make rmi # remove pgweb image make save # save pgweb image to /tmp/docker/pgweb.tgz make load # load pgweb image from /tmp/docker/pgweb.tgz ","categories":["Reference"],"description":"Launch pgweb to access PostgreSQL via web browser","excerpt":"Launch pgweb to access PostgreSQL via web browser","ref":"/docs/app/pgweb/","tags":"","title":"PGWeb: Browser-based PG Client"},{"body":"PostgREST is a binary component that automatically generates a REST API based on the PostgreSQL database schema.\nFor example, the following command will launch postgrest with docker (local port 8884, using default admin user, and expose Pigsty CMDB schema):\ndocker run --init --name postgrest --restart always --detach --publish 8884:8081 postgrest/postgrest Visit http://10.10.10.10:8884 will show all auto-generated API definitions and automatically expose API documentation using Swagger Editor.\nIf you wish to perform CRUD operations and design more fine-grained permission control, please refer to Tutorial 1 - The Golden Key to generate a signed JWT.\nThis is an example of creating pigsty cmdb API with PostgREST\ncd ~/pigsty/app/postgrest ; docker-compose up -d http://10.10.10.10:8884 is the default endpoint for PostgREST\nhttp://10.10.10.10:8883 is the default api docs for PostgREST\nmake up # pull up postgrest with docker-compose make run # launch postgrest with docker make ui # run swagger ui container make view # print postgrest access point make log # tail -f postgrest logs make info # introspect postgrest with jq make stop # stop postgrest container make clean # remove postgrest container make rmui # remove swagger ui container make pull # pull latest postgrest image make rmi # remove postgrest image make save # save postgrest image to /tmp/postgrest.tgz make load # load postgrest image from /tmp Swagger UI Launch a swagger OpenAPI UI and visualize PostgREST API on 8883 with:\ndocker run --init --name postgrest --name swagger -p 8883:8080 -e API_URL=http://10.10.10.10:8884 swaggerapi/swagger-ui # docker run -d -e API_URL=http://10.10.10.10:8884 -p 8883:8080 swaggerapi/swagger-editor # swagger editor Check http://10.10.10.10:8883/\n","categories":["Reference"],"description":"Launch postgREST to generate REST API from PostgreSQL schema automatically","excerpt":"Launch postgREST to generate REST API from PostgreSQL schema …","ref":"/docs/app/postgrest/","tags":"","title":"PostgREST: Generate REST API from Schema"},{"body":"Electric is a PostgreSQL sync engine that solves complex data synchronization problems.\nElectric supports partial replication, fan-out delivery, and efficient data transfer, making it ideal for building real-time and offline-first applications.\nQuick Start cd ~/pigsty/app/electric make up # Start Electric service Access URL: http://electric.pigsty or http://10.10.10.10:3000\nFeatures Partial Replication: Sync only the data you need Real-time Sync: Millisecond-level data updates Offline-first: Work offline with automatic sync Conflict Resolution: Automatic handling of data conflicts Type Safety: TypeScript support Related Links Electric Website: https://electric-sql.com/ Documentation: https://electric-sql.com/docs GitHub Repository: https://github.com/electric-sql/electric ","categories":["Reference"],"description":"Use Electric to solve PostgreSQL data synchronization challenges with partial replication and real-time data transfer.","excerpt":"Use Electric to solve PostgreSQL data synchronization challenges with …","ref":"/docs/app/electric/","tags":"","title":"Electric: PGLite Sync Engine"},{"body":"Run jupyter notebook with docker, you have to:\nchange the default password in .env: JUPYTER_TOKEN create data dir with proper permission: make dir, owned by 1000:100 make up to pull up jupyter with docker compose cd ~/pigsty/app/jupyter ; make dir up Visit http://lab.pigsty or http://10.10.10.10:8888, the default password is pigsty\nhttp://lab.pigsty?token=pigsty Prepare Create a data directory /data/jupyter, with the default uid \u0026 gid 1000:100:\nmake dir # mkdir -p /data/jupyter; chown -R 1000:100 /data/jupyter Connect to Postgres Use the jupyter terminal to install psycopg2-binary \u0026 psycopg2 package.\npip install psycopg2-binary psycopg2 # install with a mirror pip install -i https://pypi.tuna.tsinghua.edu.cn/simple psycopg2-binary psycopg2 pip install --upgrade pip pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple Or installation with conda:\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ then use the driver in your notebook\nimport psycopg2 conn = psycopg2.connect('postgres://dbuser_dba:DBUser.DBA@10.10.10.10:5432/meta') cursor = conn.cursor() cursor.execute('SELECT * FROM pg_stat_activity') for i in cursor.fetchall(): print(i) Alias make up # pull up jupyter with docker compose make dir # create required /data/jupyter and set owner make run # launch jupyter with docker make view # print jupyter access point make log # tail -f jupyter logs make info # introspect jupyter with jq make stop # stop jupyter container make clean # remove jupyter container make pull # pull latest jupyter image make rmi # remove jupyter image make save # save jupyter image to /tmp/docker/jupyter.tgz make load # load jupyter image from /tmp/docker/jupyter.tgz ","categories":["Reference"],"description":"Run Jupyter Lab in container, and access PostgreSQL database","excerpt":"Run Jupyter Lab in container, and access PostgreSQL database","ref":"/docs/app/jupyter/","tags":"","title":"Jupyter: AI Notebook \u0026 IDE"},{"body":"PostgreSQL-based data visualization applications\n","categories":["Application"],"description":"","excerpt":"PostgreSQL-based data visualization applications\n","ref":"/docs/app/_div_applet/","tags":"","title":"Data Applications"},{"body":"PGLOG is a sample application included with Pigsty that uses the pglog.sample table in MetaDB as its data source. You simply need to load logs into this table, then access the related dashboard.\nPigsty provides convenient commands for pulling CSV logs and loading them into the sample table. On the meta node, the following shortcut commands are available by default:\ncatlog [node=localhost] [date=today] # Print CSV log to stdout pglog # Load CSVLOG from stdin pglog12 # Load PG12 format CSVLOG pglog13 # Load PG13 format CSVLOG pglog14 # Load PG14 format CSVLOG (=pglog) catlog | pglog # Analyze current node's log for today catlog node-1 '2021-07-15' | pglog # Analyze node-1's csvlog for 2021-07-15 Next, you can access the following links to view the sample log analysis interface.\nPGLOG Overview: Present the entire CSV log sample details, aggregated by multiple dimensions. PGLOG Session: Present detailed information about a specific connection in the log sample. The catlog command pulls CSV database logs from a specific node for a specific date and writes to stdout\nBy default, catlog pulls logs from the current node for today. You can specify the node and date through parameters.\nUsing pglog and catlog together, you can quickly pull database CSV logs for analysis.\ncatlog | pglog # Analyze current node's log for today catlog node-1 '2021-07-15' | pglog # Analyze node-1's csvlog for 2021-07-15 ","categories":["Reference"],"description":"A sample Applet included with Pigsty for analyzing PostgreSQL CSV log samples","excerpt":"A sample Applet included with Pigsty for analyzing PostgreSQL CSV log …","ref":"/docs/app/pglog/","tags":["Visualization"],"title":"PGLOG: PostgreSQL Log Analysis Application"},{"body":"If you have a database and don’t know what to do with it, why not try this open-source project: Vonng/isd\nYou can directly reuse the monitoring system Grafana to interactively browse sub-hourly meteorological data from nearly 30,000 surface weather stations over the past 120 years.\nThis is a fully functional data application that can query meteorological observation records from 30,000 global surface weather stations since 1901.\nProject URL: https://github.com/Vonng/isd\nOnline Demo: https://demo.pigsty.io/d/isd-overview\nQuick Start Clone this repository\ngit clone https://github.com/Vonng/isd.git; cd isd; Prepare a PostgreSQL instance\nThe PostgreSQL instance should have the PostGIS extension enabled. Use the PGURL environment variable to pass database connection information:\n# Pigsty uses dbuser_dba as the default admin account with password DBUser.DBA export PGURL=postgres://dbuser_dba:DBUser.DBA@127.0.0.1:5432/meta?sslmode=disable psql \"${PGURL}\" -c 'SELECT 1' # Check if connection is available Fetch and import ISD weather station metadata\nThis is a daily-updated weather station metadata file containing station longitude/latitude, elevation, name, country, province, and other information. Use the following command to download and import:\nmake reload-station # Equivalent to downloading the latest station data then loading: get-station + load-station Fetch and import the latest isd.daily data\nisd.daily is a daily-updated dataset containing daily observation data summaries from global weather stations. Use the following command to download and import. Note that raw data downloaded directly from the NOAA website needs to be parsed before it can be loaded into the database, so you need to download or build an ISD data parser.\nmake get-parser # Download the parser binary from Github, or you can build directly with go using make build make reload-daily # Download and import the latest isd.daily data for this year into the database Load pre-parsed CSV dataset\nThe ISD Daily dataset has some dirty data and duplicate data. If you don’t want to manually parse and clean it, a stable pre-parsed CSV dataset is also provided here.\nThis dataset contains isd.daily data up to 2023-06-24. You can download and import it directly into PostgreSQL without needing a parser.\nmake get-stable # Get the stable isd.daily historical dataset from Github make load-stable # Load the downloaded stable historical dataset into the PostgreSQL database More Data Two parts of the ISD dataset are updated daily: weather station metadata and the latest year’s isd.daily (e.g., the 2023 tarball).\nYou can use the following command to download and refresh these two parts. If the dataset hasn’t been updated, these commands won’t re-download the same data package:\nmake reload # Actually: reload-station + reload-daily You can also use the following commands to download and load isd.daily data for a specific year:\nbin/get-daily 2022 # Get daily weather observation summary for 2022 (1900-2023) bin/load-daily \"${PGURL}\" 2022 # Load daily weather observation summary for 2022 (1900-2023) In addition to the daily summary isd.daily, ISD also provides more detailed sub-hourly raw observation records isd.hourly. The download and load methods are similar:\nbin/get-hourly 2022 # Download hourly observation records for a specific year (e.g., 2022, options 1900-2023) bin/load-hourly \"${PGURL}\" 2022 # Load hourly observation records for a specific year Data Dataset Overview ISD provides four datasets: sub-hourly raw observation data, daily statistical summary data, monthly statistical summary, and yearly statistical summary\nDataset Notes ISD Hourly Sub-hourly observation records ISD Daily Daily statistical summary ISD Monthly Not used, can be calculated from isd.daily ISD Yearly Not used, can be calculated from isd.daily Daily Summary Dataset\nCompressed package size 2.8GB (as of 2023-06-24) Table size 24GB, index size 6GB, total size approximately 30GB in PostgreSQL If timescaledb compression is enabled, total size can be compressed to 4.5 GB Sub-hourly Observation Data\nTotal compressed package size 117GB After loading into database: table size 1TB+, index size 600GB+, total size 1.6TB Database Schema Weather Station Metadata Table\nCREATE TABLE isd.station ( station VARCHAR(12) PRIMARY KEY, usaf VARCHAR(6) GENERATED ALWAYS AS (substring(station, 1, 6)) STORED, wban VARCHAR(5) GENERATED ALWAYS AS (substring(station, 7, 5)) STORED, name VARCHAR(32), country VARCHAR(2), province VARCHAR(2), icao VARCHAR(4), location GEOMETRY(POINT), longitude NUMERIC GENERATED ALWAYS AS (Round(ST_X(location)::NUMERIC, 6)) STORED, latitude NUMERIC GENERATED ALWAYS AS (Round(ST_Y(location)::NUMERIC, 6)) STORED, elevation NUMERIC, period daterange, begin_date DATE GENERATED ALWAYS AS (lower(period)) STORED, end_date DATE GENERATED ALWAYS AS (upper(period)) STORED ); Daily Summary Table\nCREATE TABLE IF NOT EXISTS isd.daily ( station VARCHAR(12) NOT NULL, -- station number 6USAF+5WBAN ts DATE NOT NULL, -- observation date -- Temperature \u0026 Dew Point temp_mean NUMERIC(3, 1), -- mean temperature ℃ temp_min NUMERIC(3, 1), -- min temperature ℃ temp_max NUMERIC(3, 1), -- max temperature ℃ dewp_mean NUMERIC(3, 1), -- mean dew point ℃ -- Air Pressure slp_mean NUMERIC(5, 1), -- sea level pressure (hPa) stp_mean NUMERIC(5, 1), -- station pressure (hPa) -- Visibility vis_mean NUMERIC(6), -- visible distance (m) -- Wind Speed wdsp_mean NUMERIC(4, 1), -- average wind speed (m/s) wdsp_max NUMERIC(4, 1), -- max wind speed (m/s) gust NUMERIC(4, 1), -- max wind gust (m/s) -- Precipitation / Snow Depth prcp_mean NUMERIC(5, 1), -- precipitation (mm) prcp NUMERIC(5, 1), -- rectified precipitation (mm) sndp NuMERIC(5, 1), -- snow depth (mm) -- FRSHTT (Fog/Rain/Snow/Hail/Thunder/Tornado) is_foggy BOOLEAN, -- (F)og is_rainy BOOLEAN, -- (R)ain or Drizzle is_snowy BOOLEAN, -- (S)now or pellets is_hail BOOLEAN, -- (H)ail is_thunder BOOLEAN, -- (T)hunder is_tornado BOOLEAN, -- (T)ornado or Funnel Cloud -- Record counts used for statistical aggregation temp_count SMALLINT, -- record count for temp dewp_count SMALLINT, -- record count for dew point slp_count SMALLINT, -- record count for sea level pressure stp_count SMALLINT, -- record count for station pressure wdsp_count SMALLINT, -- record count for wind speed visib_count SMALLINT, -- record count for visible distance -- Temperature flags temp_min_f BOOLEAN, -- aggregate min temperature temp_max_f BOOLEAN, -- aggregate max temperature prcp_flag CHAR, -- precipitation flag: ABCDEFGHI PRIMARY KEY (station, ts) ); -- PARTITION BY RANGE (ts); Sub-hourly Raw Observation Data Table\nISD Hourly CREATE TABLE IF NOT EXISTS isd.hourly ( station VARCHAR(12) NOT NULL, -- station id ts TIMESTAMP NOT NULL, -- timestamp -- air temp NUMERIC(3, 1), -- [-93.2,+61.8] dewp NUMERIC(3, 1), -- [-98.2,+36.8] slp NUMERIC(5, 1), -- [8600,10900] stp NUMERIC(5, 1), -- [4500,10900] vis NUMERIC(6), -- [0,160000] -- wind wd_angle NUMERIC(3), -- [1,360] wd_speed NUMERIC(4, 1), -- [0,90] wd_gust NUMERIC(4, 1), -- [0,110] wd_code VARCHAR(1), -- code that denotes the character of the WIND-OBSERVATION. -- cloud cld_height NUMERIC(5), -- [0,22000] cld_code VARCHAR(2), -- cloud code -- water sndp NUMERIC(5, 1), -- mm snow prcp NUMERIC(5, 1), -- mm precipitation prcp_hour NUMERIC(2), -- precipitation duration in hour prcp_code VARCHAR(1), -- precipitation type code -- sky mw_code VARCHAR(2), -- manual weather observation code aw_code VARCHAR(2), -- auto weather observation code pw_code VARCHAR(1), -- weather code of past period of time pw_hour NUMERIC(2), -- duration of pw_code period -- misc -- remark TEXT, -- eqd TEXT, data JSONB -- extra data ) PARTITION BY RANGE (ts); Parser The raw data provided by NOAA ISD is in a highly compressed proprietary format that needs to be processed through a parser before it can be converted into database table format.\nFor the Daily and Hourly datasets, two parsers are provided here: isdd and isdh. Both parsers take annual data compressed packages as input, produce CSV results as output, and work in pipeline mode as shown below:\nNAME isd -- Intergrated Surface Dataset Parser SYNOPSIS isd daily [-i \u003cinput|stdin\u003e] [-o \u003coutput|stout\u003e] [-v] isd hourly [-i \u003cinput|stdin\u003e] [-o \u003coutput|stout\u003e] [-v] [-d raw|ts-first|hour-first] DESCRIPTION The isd program takes noaa isd daily/hourly raw tarball data as input. and generate parsed data in csv format as output. Works in pipe mode cat data/daily/2023.tar.gz | bin/isd daily -v | psql ${PGURL} -AXtwqc \"COPY isd.daily FROM STDIN CSV;\" isd daily -v -i data/daily/2023.tar.gz | psql ${PGURL} -AXtwqc \"COPY isd.daily FROM STDIN CSV;\" isd hourly -v -i data/hourly/2023.tar.gz | psql ${PGURL} -AXtwqc \"COPY isd.hourly FROM STDIN CSV;\" OPTIONS -i \u003cinput\u003e input file, stdin by default -o \u003coutput\u003e output file, stdout by default -p \u003cprofpath\u003e pprof file path, enable if specified -d de-duplicate rows for hourly dataset (raw, ts-first, hour-first) -v verbose mode -h print help User Interface Several dashboards made with Grafana are provided here for exploring the ISD dataset and querying weather stations and historical meteorological data.\nISD Overview\nGlobal overview with overall metrics and weather station navigation.\nISD Country\nDisplay all weather stations within a single country/region.\nISD Station\nDisplay detailed information for a single weather station, including metadata and daily/monthly/yearly summary metrics.\nISD Station Dashboard ISD Detail\nDisplay raw sub-hourly observation metric data for a weather station, requires the isd.hourly dataset.\nISD Station Dashboard ","categories":["Reference"],"description":"Demonstrate how to import data into a database using the ISD dataset as an example","excerpt":"Demonstrate how to import data into a database using the ISD dataset …","ref":"/docs/app/isd/","tags":["Visualization"],"title":"NOAA ISD Global Weather Station Historical Data Query"},{"body":"Covid is a sample Applet included with Pigsty for visualizing the World Health Organization’s official pandemic data dashboard.\nYou can browse COVID-19 infection and death cases for each country and region, as well as global pandemic trends.\nOverview GitHub Repository: https://github.com/pgsty/pigsty-app/tree/master/covid\nOnline Demo: https://demo.pigsty.io/d/covid\nInstallation Enter the application directory on the admin node and execute make to complete the installation.\nmake # Complete all configuration Other sub-tasks:\nmake reload # download latest data and pour it again make ui # install grafana dashboards make sql # install database schemas make download # download latest data make load # load downloaded data into database make reload # download latest data and pour it into database ","categories":["Reference"],"description":"A sample Applet included with Pigsty for visualizing World Health Organization official pandemic data","excerpt":"A sample Applet included with Pigsty for visualizing World Health …","ref":"/docs/app/covid/","tags":["Visualization"],"title":"WHO COVID-19 Pandemic Dashboard"},{"body":" Overview GitHub Repository: https://github.com/pgsty/pigsty-app/tree/master/db\nOnline Demo: https://demo.pigsty.io/d/sf-survey\n","categories":["Reference"],"description":"Analyze database-related data from StackOverflow's global developer survey over the past seven years","excerpt":"Analyze database-related data from StackOverflow's global developer …","ref":"/docs/app/sf-survey/","tags":["Visualization"],"title":"StackOverflow Global Developer Survey"},{"body":" Overview GitHub Repository: https://github.com/pgsty/pigsty-app/tree/master/db\nOnline Demo: https://demo.pigsty.io/d/db-engine\n","categories":["Reference"],"description":"Analyze database management systems on DB-Engines and browse their popularity evolution","excerpt":"Analyze database management systems on DB-Engines and browse their …","ref":"/docs/app/db-engine/","tags":["Visualization"],"title":"DB-Engines Database Popularity Trend Analysis"},{"body":" Overview GitHub Repository: https://github.com/pgsty/pigsty-app/tree/master/cloud\nOnline Demo: https://demo.pigsty.io/d/ecs\nArticle: Analyzing Computing Costs: Has Aliyun Really Reduced Prices?\nData Source Aliyun ECS pricing can be obtained as raw CSV data from Price Calculator - Pricing Details - Price Download.\nSchema Download Aliyun pricing details and import for analysis\nCREATE EXTENSION file_fdw; CREATE SERVER fs FOREIGN DATA WRAPPER file_fdw; DROP FOREIGN TABLE IF EXISTS aliyun_ecs CASCADE; CREATE FOREIGN TABLE aliyun_ecs ( \"region\" text, \"system\" text, \"network\" text, \"isIO\" bool, \"instanceId\" text, \"hourlyPrice\" numeric, \"weeklyPrice\" numeric, \"standard\" numeric, \"monthlyPrice\" numeric, \"yearlyPrice\" numeric, \"2yearPrice\" numeric, \"3yearPrice\" numeric, \"4yearPrice\" numeric, \"5yearPrice\" numeric, \"id\" text, \"instanceLabel\" text, \"familyId\" text, \"serverType\" text, \"cpu\" text, \"localStorage\" text, \"NvmeSupport\" text, \"InstanceFamilyLevel\" text, \"EniTrunkSupported\" text, \"InstancePpsRx\" text, \"GPUSpec\" text, \"CpuTurboFrequency\" text, \"InstancePpsTx\" text, \"InstanceTypeId\" text, \"GPUAmount\" text, \"InstanceTypeFamily\" text, \"SecondaryEniQueueNumber\" text, \"EniQuantity\" text, \"EniPrivateIpAddressQuantity\" text, \"DiskQuantity\" text, \"EniIpv6AddressQuantity\" text, \"InstanceCategory\" text, \"CpuArchitecture\" text, \"EriQuantity\" text, \"MemorySize\" numeric, \"EniTotalQuantity\" numeric, \"PhysicalProcessorModel\" text, \"InstanceBandwidthRx\" numeric, \"CpuCoreCount\" numeric, \"Generation\" text, \"CpuSpeedFrequency\" numeric, \"PrimaryEniQueueNumber\" text, \"LocalStorageCategory\" text, \"InstanceBandwidthTx\" text, \"TotalEniQueueQuantity\" text ) SERVER fs OPTIONS ( filename '/tmp/aliyun-ecs.csv', format 'csv',header 'true'); Similarly for AWS EC2, you can download the price list from Vantage:\nDROP FOREIGN TABLE IF EXISTS aws_ec2 CASCADE; CREATE FOREIGN TABLE aws_ec2 ( \"name\" TEXT, \"id\" TEXT, \"Memory\" TEXT, \"vCPUs\" TEXT, \"GPUs\" TEXT, \"ClockSpeed\" TEXT, \"InstanceStorage\" TEXT, \"NetworkPerformance\" TEXT, \"ondemand\" TEXT, \"reserve\" TEXT, \"spot\" TEXT ) SERVER fs OPTIONS ( filename '/tmp/aws-ec2.csv', format 'csv',header 'true'); DROP VIEW IF EXISTS ecs; CREATE VIEW ecs AS SELECT \"region\" AS region, \"id\" AS id, \"instanceLabel\" AS name, \"familyId\" AS family, \"CpuCoreCount\" AS cpu, \"MemorySize\" AS mem, round(\"5yearPrice\" / \"CpuCoreCount\" / 60, 2) AS ycm5, -- ¥ / (core·month) round(\"4yearPrice\" / \"CpuCoreCount\" / 48, 2) AS ycm4, -- ¥ / (core·month) round(\"3yearPrice\" / \"CpuCoreCount\" / 36, 2) AS ycm3, -- ¥ / (core·month) round(\"2yearPrice\" / \"CpuCoreCount\" / 24, 2) AS ycm2, -- ¥ / (core·month) round(\"yearlyPrice\" / \"CpuCoreCount\" / 12, 2) AS ycm1, -- ¥ / (core·month) round(\"standard\" / \"CpuCoreCount\", 2) AS ycmm, -- ¥ / (core·month) round(\"hourlyPrice\" / \"CpuCoreCount\" * 720, 2) AS ycmh, -- ¥ / (core·month) \"CpuSpeedFrequency\"::NUMERIC AS freq, \"CpuTurboFrequency\"::NUMERIC AS freq_turbo, \"Generation\" AS generation FROM aliyun_ecs WHERE system = 'linux'; DROP VIEW IF EXISTS ec2; CREATE VIEW ec2 AS SELECT id, name, split_part(id, '.', 1) as family, split_part(id, '.', 2) as spec, (regexp_match(split_part(id, '.', 1), '^[a-zA-Z]+(\\d)[a-z0-9]*'))[1] as gen, regexp_substr(\"vCPUs\", '^[0-9]+')::int as cpu, regexp_substr(\"Memory\", '^[0-9]+')::int as mem, CASE spot WHEN 'unavailable' THEN NULL ELSE round((regexp_substr(\"spot\", '([0-9]+.[0-9]+)')::NUMERIC * 7.2), 2) END AS spot, CASE ondemand WHEN 'unavailable' THEN NULL ELSE round((regexp_substr(\"ondemand\", '([0-9]+.[0-9]+)')::NUMERIC * 7.2), 2) END AS ondemand, CASE reserve WHEN 'unavailable' THEN NULL ELSE round((regexp_substr(\"reserve\", '([0-9]+.[0-9]+)')::NUMERIC * 7.2), 2) END AS reserve, \"ClockSpeed\" AS freq FROM aws_ec2; Visualization ","categories":["Reference"],"description":"Analyze compute and storage pricing on Aliyun / AWS (ECS/ESSD)","excerpt":"Analyze compute and storage pricing on Aliyun / AWS (ECS/ESSD)","ref":"/docs/app/cloud/","tags":["Visualization"],"title":"AWS \u0026 Aliyun Server Pricing"},{"body":"Pigsty provides various ready-to-use configuration templates for different deployment scenarios.\nYou can specify a configuration template with the -c option during configure. If no template is specified, the default meta template is used.\nCategory Templates Solo Templates meta, rich, fat, slim, infra Kernel Templates pgsql, citus, mssql, polar, ivory, mysql, pgtde, oriole, supabase HA Templates ha/simu, ha/full, ha/safe, ha/trio, ha/dual App Templates app/odoo, app/dify, app/electric, app/maybe, app/teable, app/registry Misc Templates demo/el, demo/debian, demo/demo, demo/minio, build/oss, build/pro ","categories":["Reference"],"description":"Batteries-included configuration templates for specific scenarios, with detailed explanations.","excerpt":"Batteries-included configuration templates for specific scenarios, …","ref":"/docs/conf/","tags":"","title":"Conf Templates"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/conf/_div_solo/","tags":"","title":"Solo Templates"},{"body":"The meta configuration template is Pigsty’s default template, designed to fulfill Pigsty’s core functionality—deploying PostgreSQL—on a single node.\nTo maximize compatibility, meta installs only the minimum required software set to ensure it runs across all operating system distributions and architectures.\nOverview Config Name: meta Node Count: Single node Description: Default single-node installation template with extensive configuration parameter descriptions and minimum required feature set. OS Distro: el8, el9, el10, d12, d13, u22, u24 OS Arch: x86_64, aarch64 Related: meta, slim, fat Usage: This is the default config template, so there’s no need to specify -c meta explicitly during configure:\n./configure [-i \u003cprimary_ip\u003e] For example, if you want to install PostgreSQL 17 rather than the default 18, you can use the -v arg in configure:\n./configure -v 17 # or 16,15,14,13.... Content Source: pigsty/conf/meta.yml\n--- #==============================================================# # File : meta.yml # Desc : Pigsty default 1-node online install config # Ctime : 2020-05-22 # Mtime : 2025-12-28 # Docs : https://doc.pgsty.com/config # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# # This is the default 1-node configuration template, with: # INFRA, NODE, PGSQL, ETCD, MINIO, DOCKER, APP (pgadmin) # with basic pg extensions: postgis, pgvector # # Work with PostgreSQL 14-18 on all supported platform # Usage: # curl https://repo.pigsty.io/get | bash # ./configure # ./deploy.yml all: #==============================================================# # Clusters, Nodes, and Modules #==============================================================# children: #----------------------------------------------# # PGSQL : https://doc.pgsty.com/pgsql #----------------------------------------------# # this is an example single-node postgres cluster with pgvector installed, with one biz database \u0026 two biz users pg-meta: hosts: 10.10.10.10: { pg_seq: 1, pg_role: primary } # \u003c---- primary instance with read-write capability #x.xx.xx.xx: { pg_seq: 2, pg_role: replica } # \u003c---- read only replica for read-only online traffic #x.xx.xx.xy: { pg_seq: 3, pg_role: offline } # \u003c---- offline instance of ETL \u0026 interactive queries vars: pg_cluster: pg-meta # install, load, create pg extensions: https://doc.pgsty.com/pgsql/extension pg_extensions: [ postgis, pgvector ] # define business users/roles : https://doc.pgsty.com/pgsql/user pg_users: - { name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [dbrole_admin ] ,comment: pigsty admin user } - { name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [dbrole_readonly] ,comment: read-only viewer } # define business databases : https://doc.pgsty.com/pgsql/db pg_databases: - name: meta baseline: cmdb.sql comment: \"pigsty meta database\" schemas: [pigsty] # define extensions in database : https://doc.pgsty.com/pgsql/extension/create extensions: [ postgis, vector ] # define HBA rules : https://doc.pgsty.com/pgsql/hba pg_hba_rules: - { user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes' } # define backup policies: https://doc.pgsty.com/pgsql/backup node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] # make a full backup every day 1am # define (OPTIONAL) L2 VIP that bind to primary #pg_vip_enabled: true #pg_vip_address: 10.10.10.2/24 #pg_vip_interface: eth1 #----------------------------------------------# # INFRA : https://doc.pgsty.com/infra #----------------------------------------------# infra: hosts: 10.10.10.10: { infra_seq: 1 } vars: repo_enabled: false # disable in 1-node mode : https://doc.pgsty.com/admin/repo #repo_extra_packages: [ pg18-main ,pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-olap ,pg18-feat ,pg18-lang ,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl] #----------------------------------------------# # ETCD : https://doc.pgsty.com/etcd #----------------------------------------------# etcd: hosts: 10.10.10.10: { etcd_seq: 1 } vars: etcd_cluster: etcd etcd_safeguard: false # prevent purging running etcd instance? #----------------------------------------------# # MINIO : https://doc.pgsty.com/minio #----------------------------------------------# #minio: # hosts: # 10.10.10.10: { minio_seq: 1 } # vars: # minio_cluster: minio # minio_users: # list of minio user to be created # - { access_key: pgbackrest ,secret_key: S3User.Backup ,policy: pgsql } # - { access_key: s3user_meta ,secret_key: S3User.Meta ,policy: meta } # - { access_key: s3user_data ,secret_key: S3User.Data ,policy: data } #----------------------------------------------# # DOCKER : https://doc.pgsty.com/docker # APP : https://doc.pgsty.com/app #----------------------------------------------# # launch example pgadmin app with: ./app.yml (http://10.10.10.10:8885 admin@pigsty.cc / pigsty) app: hosts: { 10.10.10.10: {} } vars: docker_enabled: true # enabled docker with ./docker.yml #docker_registry_mirrors: [\"https://docker.1panel.live\",\"https://docker.1ms.run\",\"https://docker.xuanyuan.me\",\"https://registry-1.docker.io\"] app: pgadmin # specify the default app name to be installed (in the apps) apps: # define all applications, appname: definition pgadmin: # pgadmin app definition (app/pgadmin -\u003e /opt/pgadmin) conf: # override /opt/pgadmin/.env PGADMIN_DEFAULT_EMAIL: admin@pigsty.cc PGADMIN_DEFAULT_PASSWORD: pigsty #==============================================================# # Global Parameters #==============================================================# vars: #----------------------------------------------# # INFRA : https://doc.pgsty.com/infra #----------------------------------------------# version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: china # upstream mirror region: default|china|europe proxy_env: # global proxy env when downloading packages no_proxy: \"localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,*.pigsty,*.aliyun.com,mirrors.*,*.myqcloud.com,*.tsinghua.edu.cn\" # http_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com # https_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com # all_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com infra_portal: # infra services exposed via portal home : { domain: i.pigsty } # default domain name pgadmin : { domain: adm.pigsty ,endpoint: \"${admin_ip}:8885\" } #minio : { domain: m.pigsty ,endpoint: \"${admin_ip}:9001\" ,scheme: https ,websocket: true } #----------------------------------------------# # NODE : https://doc.pgsty.com/node/param #----------------------------------------------# nodename_overwrite: false # do not overwrite node hostname on single node mode node_tune: oltp # node tuning specs: oltp,olap,tiny,crit node_etc_hosts: [ '${admin_ip} i.pigsty sss.pigsty' ] node_repo_modules: 'node,infra,pgsql' # add these repos directly to the singleton node #node_repo_modules: local # use this if you want to build \u0026 user local repo node_repo_remove: true # remove existing node repo for node managed by pigsty #node_packages: [openssh-server] # packages to be installed current nodes with the latest version #----------------------------------------------# # PGSQL : https://doc.pgsty.com/pgsql/param #----------------------------------------------# pg_version: 18 # default postgres version pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml pg_safeguard: false # prevent purging running postgres instance? pg_packages: [ pgsql-main, pgsql-common ] # pg kernel and common utils #pg_extensions: [ pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-olap ,pg18-feat ,pg18-lang ,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl] #----------------------------------------------# # BACKUP : https://doc.pgsty.com/pgsql/backup #----------------------------------------------# # if you want to use minio as backup repo instead of 'local' fs, uncomment this, and configure `pgbackrest_repo` # you can also use external object storage as backup repo #pgbackrest_method: minio # if you want to use minio as backup repo instead of 'local' fs, uncomment this #pgbackrest_repo: # pgbackrest repo: https://pgbackrest.org/configuration.html#section-repository # local: # default pgbackrest repo with local posix fs # path: /pg/backup # local backup directory, `/pg/backup` by default # retention_full_type: count # retention full backups by count # retention_full: 2 # keep 2, at most 3 full backup when using local fs repo # minio: # optional minio repo for pgbackrest # type: s3 # minio is s3-compatible, so s3 is used # s3_endpoint: sss.pigsty # minio endpoint domain name, `sss.pigsty` by default # s3_region: us-east-1 # minio region, us-east-1 by default, useless for minio # s3_bucket: pgsql # minio bucket name, `pgsql` by default # s3_key: pgbackrest # minio user access key for pgbackrest # s3_key_secret: S3User.Backup # minio user secret key for pgbackrest # s3_uri_style: path # use path style uri for minio rather than host style # path: /pgbackrest # minio backup path, default is `/pgbackrest` # storage_port: 9000 # minio port, 9000 by default # storage_ca_file: /etc/pki/ca.crt # minio ca file path, `/etc/pki/ca.crt` by default # block: y # Enable block incremental backup # bundle: y # bundle small files into a single file # bundle_limit: 20MiB # Limit for file bundles, 20MiB for object storage # bundle_size: 128MiB # Target size for file bundles, 128MiB for object storage # cipher_type: aes-256-cbc # enable AES encryption for remote backup repo # cipher_pass: pgBackRest # AES encryption password, default is 'pgBackRest' # retention_full_type: time # retention full backup by time on minio repo # retention_full: 14 # keep full backup for last 14 days # s3: # aliyun oss (s3 compatible) object storage service # type: s3 # oss is s3-compatible # s3_endpoint: oss-cn-beijing-internal.aliyuncs.com # s3_region: oss-cn-beijing # s3_bucket: \u003cyour_bucket_name\u003e # s3_key: \u003cyour_access_key\u003e # s3_key_secret: \u003cyour_secret_key\u003e # s3_uri_style: host # path: /pgbackrest # bundle: y # bundle small files into a single file # bundle_limit: 20MiB # Limit for file bundles, 20MiB for object storage # bundle_size: 128MiB # Target size for file bundles, 128MiB for object storage # cipher_type: aes-256-cbc # enable AES encryption for remote backup repo # cipher_pass: pgBackRest # AES encryption password, default is 'pgBackRest' # retention_full_type: time # retention full backup by time on minio repo # retention_full: 14 # keep full backup for last 14 days #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root ... Explanation The meta template is Pigsty’s default getting-started configuration, designed for quick onboarding.\nUse Cases:\nFirst-time Pigsty users Quick deployment in development and testing environments Small production environments running on a single machine As a base template for more complex deployments Key Features:\nOnline installation mode without building local software repository (repo_enabled: false) Default installs PostgreSQL 18 with postgis and pgvector extensions Includes complete monitoring infrastructure (Grafana, Prometheus, Loki, etc.) Preconfigured Docker and pgAdmin application examples MinIO backup storage disabled by default, can be enabled as needed Notes:\nDefault passwords are sample passwords; must be changed for production environments Single-node etcd has no high availability guarantee, suitable for development and testing If you need to build a local software repository, use the rich template ","categories":["Reference"],"description":"Default single-node installation template with extensive configuration parameter descriptions","excerpt":"Default single-node installation template with extensive configuration …","ref":"/docs/conf/meta/","tags":"","title":"meta"},{"body":"The rich configuration template is an enhanced version of meta, designed for users who need to experience complete functionality.\nIf you want to build a local software repository, use MinIO for backup storage, run Docker applications, or need preconfigured business databases, use this template.\nOverview Config Name: rich Node Count: Single node Description: Feature-rich single-node configuration, adding local software repository, MinIO backup, complete extensions, Docker application examples on top of meta OS Distro: el8, el9, el10, d12, d13, u22, u24 OS Arch: x86_64, aarch64 Related: meta, slim, fat This template’s main enhancements over meta:\nBuilds local software repository (repo_enabled: true), downloads all PG extensions Enables single-node MinIO as PostgreSQL backup storage Preinstalls TimescaleDB, pgvector, pg_wait_sampling and other extensions Includes detailed user/database/service definition comment examples Adds Redis primary-replica instance example Preconfigures pg-test three-node HA cluster configuration stub Usage:\n./configure -c rich [-i \u003cprimary_ip\u003e] Content Source: pigsty/conf/rich.yml\n--- #==============================================================# # File : rich.yml # Desc : Pigsty feature-rich 1-node online install config # Ctime : 2020-05-22 # Mtime : 2025-12-12 # Docs : https://doc.pgsty.com/config # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# # This is the enhanced version of default meta.yml, which has: # - almost all available postgres extensions # - build local software repo for entire env # - 1 node minio used as central backup repo # - cluster stub for 3-node pg-test / ferret / redis # - stub for nginx, certs, and website self-hosting config # - detailed comments for database / user / service # # Usage: # curl https://repo.pigsty.io/get | bash # ./configure -c rich # ./deploy.yml all: #==============================================================# # Clusters, Nodes, and Modules #==============================================================# children: #----------------------------------------------# # PGSQL : https://doc.pgsty.com/pgsql #----------------------------------------------# # this is an example single-node postgres cluster with pgvector installed, with one biz database \u0026 two biz users pg-meta: hosts: 10.10.10.10: { pg_seq: 1, pg_role: primary } # \u003c---- primary instance with read-write capability #x.xx.xx.xx: { pg_seq: 2, pg_role: replica } # \u003c---- read only replica for read-only online traffic #x.xx.xx.xy: { pg_seq: 3, pg_role: offline } # \u003c---- offline instance of ETL \u0026 interactive queries vars: pg_cluster: pg-meta # install, load, create pg extensions: https://doc.pgsty.com/pgsql/extension pg_extensions: [ postgis, timescaledb, pgvector, pg_wait_sampling ] pg_libs: 'timescaledb, pg_stat_statements, auto_explain, pg_wait_sampling' # define business users/roles : https://doc.pgsty.com/pgsql/user pg_users: - name: dbuser_meta # REQUIRED, `name` is the only mandatory field of a user definition password: DBUser.Meta # optional, the password. can be a scram-sha-256 hash string or plain text #state: create # optional, create|absent, 'create' by default, use 'absent' to drop user #login: true # optional, can log in, true by default (new biz ROLE should be false) #superuser: false # optional, is superuser? false by default #createdb: false # optional, can create databases? false by default #createrole: false # optional, can create role? false by default #inherit: true # optional, can this role use inherited privileges? true by default #replication: false # optional, can this role do replication? false by default #bypassrls: false # optional, can this role bypass row level security? false by default #pgbouncer: true # optional, add this user to the pgbouncer user-list? false by default (production user should be true explicitly) #connlimit: -1 # optional, user connection limit, default -1 disable limit #expire_in: 3650 # optional, now + n days when this role is expired (OVERWRITE expire_at) #expire_at: '2030-12-31' # optional, YYYY-MM-DD 'timestamp' when this role is expired (OVERWRITTEN by expire_in) #comment: pigsty admin user # optional, comment string for this user/role #roles: [dbrole_admin] # optional, belonged roles. default roles are: dbrole_{admin|readonly|readwrite|offline} #parameters: {} # optional, role level parameters with `ALTER ROLE SET` #pool_mode: transaction # optional, pgbouncer pool mode at user level, transaction by default #pool_connlimit: -1 # optional, max database connections at user level, default -1 disable limit # Enhanced roles syntax (PG16+): roles can be string or object with options: # - dbrole_readwrite # simple string: GRANT role # - { name: role, admin: true } # GRANT WITH ADMIN OPTION # - { name: role, set: false } # PG16: REVOKE SET OPTION # - { name: role, inherit: false } # PG16: REVOKE INHERIT OPTION # - { name: role, state: absent } # REVOKE membership - { name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [dbrole_readonly], comment: read-only viewer for meta database } #- {name: dbuser_bytebase ,password: DBUser.Bytebase ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for bytebase database } #- {name: dbuser_remove ,state: absent } # use state: absent to remove a user # define business databases : https://doc.pgsty.com/pgsql/db pg_databases: # define business databases on this cluster, array of database definition - name: meta # REQUIRED, `name` is the only mandatory field of a database definition #state: create # optional, create|absent|recreate, create by default baseline: cmdb.sql # optional, database sql baseline path, (relative path among the ansible search path, e.g.: files/) schemas: [ pigsty ] # optional, additional schemas to be created, array of schema names extensions: # optional, additional extensions to be installed: array of `{name[,schema]}` - vector # install pgvector for vector similarity search - postgis # install postgis for geospatial type \u0026 index - timescaledb # install timescaledb for time-series data - { name: pg_wait_sampling, schema: monitor } # install pg_wait_sampling on monitor schema comment: pigsty meta database # optional, comment string for this database #pgbouncer: true # optional, add this database to the pgbouncer database list? true by default #owner: postgres # optional, database owner, current user if not specified #template: template1 # optional, which template to use, template1 by default #strategy: FILE_COPY # optional, clone strategy: FILE_COPY or WAL_LOG (PG15+), default to PG's default #encoding: UTF8 # optional, inherited from template / cluster if not defined (UTF8) #locale: C # optional, inherited from template / cluster if not defined (C) #lc_collate: C # optional, inherited from template / cluster if not defined (C) #lc_ctype: C # optional, inherited from template / cluster if not defined (C) #locale_provider: libc # optional, locale provider: libc, icu, builtin (PG15+) #icu_locale: en-US # optional, icu locale for icu locale provider (PG15+) #icu_rules: '' # optional, icu rules for icu locale provider (PG16+) #builtin_locale: C.UTF-8 # optional, builtin locale for builtin locale provider (PG17+) #tablespace: pg_default # optional, default tablespace, pg_default by default #is_template: false # optional, mark database as template, allowing clone by any user with CREATEDB privilege #allowconn: true # optional, allow connection, true by default. false will disable connect at all #revokeconn: false # optional, revoke public connection privilege. false by default. (leave connect with grant option to owner) #register_datasource: true # optional, register this database to grafana datasources? true by default #connlimit: -1 # optional, database connection limit, default -1 disable limit #pool_auth_user: dbuser_meta # optional, all connection to this pgbouncer database will be authenticated by this user #pool_mode: transaction # optional, pgbouncer pool mode at database level, default transaction #pool_size: 64 # optional, pgbouncer pool size at database level, default 64 #pool_size_reserve: 32 # optional, pgbouncer pool size reserve at database level, default 32 #pool_size_min: 0 # optional, pgbouncer pool size min at database level, default 0 #pool_max_db_conn: 100 # optional, max database connections at database level, default 100 #- {name: bytebase ,owner: dbuser_bytebase ,revokeconn: true ,comment: bytebase primary database } # define HBA rules : https://doc.pgsty.com/pgsql/hba pg_hba_rules: - { user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes' } # define backup policies: https://doc.pgsty.com/pgsql/backup node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] # make a full backup every day 1am # define (OPTIONAL) L2 VIP that bind to primary #pg_vip_enabled: true #pg_vip_address: 10.10.10.2/24 #pg_vip_interface: eth1 #----------------------------------------------# # PGSQL HA Cluster Example: 3-node pg-test #----------------------------------------------# #pg-test: # hosts: # 10.10.10.11: { pg_seq: 1, pg_role: primary } # primary instance, leader of cluster # 10.10.10.12: { pg_seq: 2, pg_role: replica } # replica instance, follower of leader # 10.10.10.13: { pg_seq: 3, pg_role: replica, pg_offline_query: true } # replica with offline access # vars: # pg_cluster: pg-test # define pgsql cluster name # pg_users: [{ name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] }] # pg_databases: [{ name: test }] # # define business service here: https://doc.pgsty.com/pgsql/service # pg_services: # extra services in addition to pg_default_services, array of service definition # # standby service will route {ip|name}:5435 to sync replica's pgbouncer (5435-\u003e6432 standby) # - name: standby # required, service name, the actual svc name will be prefixed with `pg_cluster`, e.g: pg-meta-standby # port: 5435 # required, service exposed port (work as kubernetes service node port mode) # ip: \"*\" # optional, service bind ip address, `*` for all ip by default # selector: \"[]\" # required, service member selector, use JMESPath to filter inventory # dest: default # optional, destination port, default|postgres|pgbouncer|\u003cport_number\u003e, 'default' by default # check: /sync # optional, health check url path, / by default # backup: \"[? pg_role == `primary`]\" # backup server selector # maxconn: 3000 # optional, max allowed front-end connection # balance: roundrobin # optional, haproxy load balance algorithm (roundrobin by default, other: leastconn) # options: 'inter 3s fastinter 1s downinter 5s rise 3 fall 3 on-marked-down shutdown-sessions slowstart 30s maxconn 3000 maxqueue 128 weight 100' # pg_vip_enabled: true # pg_vip_address: 10.10.10.3/24 # pg_vip_interface: eth1 # node_crontab: # make a full backup on monday 1am, and an incremental backup during weekdays # - '00 01 * * 1 postgres /pg/bin/pg-backup full' # - '00 01 * * 2,3,4,5,6,7 postgres /pg/bin/pg-backup' #----------------------------------------------# # INFRA : https://doc.pgsty.com/infra #----------------------------------------------# infra: hosts: 10.10.10.10: { infra_seq: 1 } vars: repo_enabled: true # build local repo, and install everything from it: https://doc.pgsty.com/admin/repo # and download all extensions into local repo repo_extra_packages: [ pg18-main ,pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-olap ,pg18-feat ,pg18-lang ,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl] #----------------------------------------------# # ETCD : https://doc.pgsty.com/etcd #----------------------------------------------# etcd: hosts: 10.10.10.10: { etcd_seq: 1 } vars: etcd_cluster: etcd etcd_safeguard: false # prevent purging running etcd instance? #----------------------------------------------# # MINIO : https://doc.pgsty.com/minio #----------------------------------------------# minio: hosts: 10.10.10.10: { minio_seq: 1 } vars: minio_cluster: minio minio_users: # list of minio user to be created - { access_key: pgbackrest ,secret_key: S3User.Backup ,policy: pgsql } - { access_key: s3user_meta ,secret_key: S3User.Meta ,policy: meta } - { access_key: s3user_data ,secret_key: S3User.Data ,policy: data } #----------------------------------------------# # DOCKER : https://doc.pgsty.com/docker # APP : https://doc.pgsty.com/app #----------------------------------------------# # OPTIONAL, launch example pgadmin app with: ./app.yml \u0026 ./app.yml -e app=bytebase app: hosts: { 10.10.10.10: {} } vars: docker_enabled: true # enabled docker with ./docker.yml #docker_registry_mirrors: [\"https://docker.1panel.live\",\"https://docker.1ms.run\",\"https://docker.xuanyuan.me\",\"https://registry-1.docker.io\"] app: pgadmin # specify the default app name to be installed (in the apps) apps: # define all applications, appname: definition # Admin GUI for PostgreSQL, launch with: ./app.yml pgadmin: # pgadmin app definition (app/pgadmin -\u003e /opt/pgadmin) conf: # override /opt/pgadmin/.env PGADMIN_DEFAULT_EMAIL: admin@pigsty.cc # default user name PGADMIN_DEFAULT_PASSWORD: pigsty # default password # Schema Migration GUI for PostgreSQL, launch with: ./app.yml -e app=bytebase bytebase: conf: BB_DOMAIN: http://ddl.pigsty # replace it with your public domain name and postgres database url BB_PGURL: \"postgresql://dbuser_bytebase:DBUser.Bytebase@10.10.10.10:5432/bytebase?sslmode=prefer\" #----------------------------------------------# # REDIS : https://doc.pgsty.com/redis #----------------------------------------------# # OPTIONAL, launch redis clusters with: ./redis.yml redis-ms: hosts: { 10.10.10.10: { redis_node: 1 , redis_instances: { 6379: { }, 6380: { replica_of: '10.10.10.10 6379' } } } } vars: { redis_cluster: redis-ms ,redis_password: 'redis.ms' ,redis_max_memory: 64MB } #==============================================================# # Global Parameters #==============================================================# vars: #----------------------------------------------# # INFRA : https://doc.pgsty.com/infra #----------------------------------------------# version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default|china|europe proxy_env: # global proxy env when downloading packages no_proxy: \"localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,*.pigsty,*.aliyun.com,mirrors.*,*.myqcloud.com,*.tsinghua.edu.cn\" # http_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com # https_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com # all_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com certbot_sign: false # enable certbot to sign https certificate for infra portal certbot_email: your@email.com # replace your email address to receive expiration notice infra_portal: # infra services exposed via portal home : { domain: i.pigsty } # default domain name pgadmin : { domain: adm.pigsty ,endpoint: \"${admin_ip}:8885\" } bytebase : { domain: ddl.pigsty ,endpoint: \"${admin_ip}:8887\" } minio : { domain: m.pigsty ,endpoint: \"${admin_ip}:9001\" ,scheme: https ,websocket: true } #website: # static local website example stub # domain: repo.pigsty # external domain name for static site # certbot: repo.pigsty # use certbot to sign https certificate for this static site # path: /www/pigsty # path to the static site directory #supabase: # dynamic upstream service example stub # domain: supa.pigsty # external domain name for upstream service # certbot: supa.pigsty # use certbot to sign https certificate for this upstream server # endpoint: \"10.10.10.10:8000\" # path to the static site directory # websocket: true # add websocket support # certbot: supa.pigsty # certbot cert name, apply with `make cert` #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root #----------------------------------------------# # NODE : https://doc.pgsty.com/node/param #----------------------------------------------# nodename_overwrite: false # do not overwrite node hostname on single node mode node_tune: oltp # node tuning specs: oltp,olap,tiny,crit node_etc_hosts: # add static domains to all nodes /etc/hosts - '${admin_ip} i.pigsty sss.pigsty' - '${admin_ip} adm.pigsty ddl.pigsty repo.pigsty supa.pigsty' node_repo_modules: local # use pre-made local repo rather than install from upstream node_repo_remove: true # remove existing node repo for node managed by pigsty #node_packages: [openssh-server] # packages to be installed current nodes with latest version #node_timezone: Asia/Hong_Kong # overwrite node timezone #----------------------------------------------# # PGSQL : https://doc.pgsty.com/pgsql/param #----------------------------------------------# pg_version: 18 # default postgres version pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml pg_safeguard: false # prevent purging running postgres instance? pg_packages: [ pgsql-main, pgsql-common ] # pg kernel and common utils #pg_extensions: [ pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-olap ,pg18-feat ,pg18-lang ,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl] #----------------------------------------------# # BACKUP : https://doc.pgsty.com/pgsql/backup #----------------------------------------------# # if you want to use minio as backup repo instead of 'local' fs, uncomment this, and configure `pgbackrest_repo` # you can also use external object storage as backup repo pgbackrest_method: minio # if you want to use minio as backup repo instead of 'local' fs, uncomment this pgbackrest_repo: # pgbackrest repo: https://pgbackrest.org/configuration.html#section-repository local: # default pgbackrest repo with local posix fs path: /pg/backup # local backup directory, `/pg/backup` by default retention_full_type: count # retention full backups by count retention_full: 2 # keep 2, at most 3 full backups when using local fs repo minio: # optional minio repo for pgbackrest type: s3 # minio is s3-compatible, so s3 is used s3_endpoint: sss.pigsty # minio endpoint domain name, `sss.pigsty` by default s3_region: us-east-1 # minio region, us-east-1 by default, useless for minio s3_bucket: pgsql # minio bucket name, `pgsql` by default s3_key: pgbackrest # minio user access key for pgbackrest [CHANGE ACCORDING to minio_users.pgbackrest] s3_key_secret: S3User.Backup # minio user secret key for pgbackrest [CHANGE ACCORDING to minio_users.pgbackrest] s3_uri_style: path # use path style uri for minio rather than host style path: /pgbackrest # minio backup path, default is `/pgbackrest` storage_port: 9000 # minio port, 9000 by default storage_ca_file: /etc/pki/ca.crt # minio ca file path, `/etc/pki/ca.crt` by default block: y # Enable block incremental backup bundle: y # bundle small files into a single file bundle_limit: 20MiB # Limit for file bundles, 20MiB for object storage bundle_size: 128MiB # Target size for file bundles, 128MiB for object storage cipher_type: aes-256-cbc # enable AES encryption for remote backup repo cipher_pass: pgBackRest # AES encryption password, default is 'pgBackRest' retention_full_type: time # retention full backup by time on minio repo retention_full: 14 # keep full backup for the last 14 days s3: # you can use cloud object storage as backup repo type: s3 # Add your object storage credentials here! s3_endpoint: oss-cn-beijing-internal.aliyuncs.com s3_region: oss-cn-beijing s3_bucket: \u003cyour_bucket_name\u003e s3_key: \u003cyour_access_key\u003e s3_key_secret: \u003cyour_secret_key\u003e s3_uri_style: host path: /pgbackrest bundle: y # bundle small files into a single file bundle_limit: 20MiB # Limit for file bundles, 20MiB for object storage bundle_size: 128MiB # Target size for file bundles, 128MiB for object storage cipher_type: aes-256-cbc # enable AES encryption for remote backup repo cipher_pass: pgBackRest # AES encryption password, default is 'pgBackRest' retention_full_type: time # retention full backup by time on minio repo retention_full: 14 # keep full backup for the last 14 days ... Explanation The rich template is Pigsty’s complete functionality showcase configuration, suitable for users who want to deeply experience all features.\nUse Cases:\nOffline environments requiring local software repository Environments needing MinIO as PostgreSQL backup storage Pre-planning multiple business databases and users Running Docker applications (pgAdmin, Bytebase, etc.) Learners wanting to understand complete configuration parameter usage Main Differences from meta:\nEnables local software repository building (repo_enabled: true) Enables MinIO storage backup (pgbackrest_method: minio) Preinstalls TimescaleDB, pg_wait_sampling and other additional extensions Includes detailed parameter comments for understanding configuration meanings Preconfigures HA cluster stub configuration (pg-test) Notes:\nSome extensions unavailable on ARM64 architecture, adjust as needed Building local software repository requires longer time and larger disk space Default passwords are sample passwords, must be changed for production ","categories":["Reference"],"description":"Feature-rich single-node configuration with local software repository, all extensions, MinIO backup, and complete examples","excerpt":"Feature-rich single-node configuration with local software repository, …","ref":"/docs/conf/rich/","tags":"","title":"rich"},{"body":"The slim configuration template provides minimal installation capability, installing a PostgreSQL high-availability cluster directly from the internet without deploying Infra monitoring infrastructure.\nWhen you only need an available database instance without the monitoring system, consider using the Slim Installation mode.\nOverview Config Name: slim Node Count: Single node Description: Minimal installation template without monitoring infrastructure, installs PostgreSQL directly OS Distro: el8, el9, el10, d12, d13, u22, u24 OS Arch: x86_64, aarch64 Related: meta Usage:\n./configure -c slim [-i \u003cprimary_ip\u003e] ./slim.yml # Execute slim installation Content Source: pigsty/conf/slim.yml\n--- #==============================================================# # File : slim.yml # Desc : Pigsty slim installation config template # Ctime : 2020-05-22 # Mtime : 2025-12-28 # Docs : https://doc.pgsty.com/config # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# # This is the config template for slim / minimal installation # No monitoring \u0026 infra will be installed, just raw postgresql # # Usage: # curl https://repo.pigsty.io/get | bash # ./configure -c slim # ./slim.yml all: children: etcd: # dcs service for postgres/patroni ha consensus hosts: # 1 node for testing, 3 or 5 for production 10.10.10.10: { etcd_seq: 1 } # etcd_seq required #10.10.10.11: { etcd_seq: 2 } # assign from 1 ~ n #10.10.10.12: { etcd_seq: 3 } # odd number please vars: # cluster level parameter override roles/etcd etcd_cluster: etcd # mark etcd cluster name etcd #----------------------------------------------# # PostgreSQL Cluster #----------------------------------------------# pg-meta: hosts: 10.10.10.10: { pg_seq: 1, pg_role: primary } #10.10.10.11: { pg_seq: 2, pg_role: replica } # you can add more! #10.10.10.12: { pg_seq: 3, pg_role: replica, pg_offline_query: true } vars: pg_cluster: pg-meta pg_users: - { name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [dbrole_admin ] ,comment: pigsty admin user } - { name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [dbrole_readonly] ,comment: read-only viewer } pg_databases: - { name: meta, baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [pigsty] ,extensions: [ vector ]} node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] # make a full backup every 1am vars: version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default,china,europe nodename_overwrite: false # do not overwrite node hostname on single node mode node_repo_modules: node,infra,pgsql # add these repos directly to the singleton node node_tune: oltp # node tuning specs: oltp,olap,tiny,crit pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml pg_version: 18 # Default PostgreSQL Major Version is 18 pg_packages: [ pgsql-main, pgsql-common ] # pg kernel and common utils #pg_extensions: [ pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-olap ,pg18-feat ,pg18-lang ,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl] #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root ... Explanation The slim template is Pigsty’s minimal installation configuration, designed for quick deployment of bare PostgreSQL clusters.\nUse Cases:\nOnly need PostgreSQL database, no monitoring system required Resource-limited small servers or edge devices Quick deployment of temporary test databases Already have monitoring system, only need PostgreSQL HA cluster Key Features:\nUses slim.yml playbook instead of deploy.yml for installation Installs software directly from internet, no local software repository Retains core PostgreSQL HA capability (Patroni + etcd + HAProxy) Minimized package downloads, faster installation Default uses PostgreSQL 18 Differences from meta:\nslim uses dedicated slim.yml playbook, skips Infra module installation Faster installation, less resource usage Suitable for “just need a database” scenarios Notes:\nAfter slim installation, cannot view database status through Grafana If monitoring is needed, use meta or rich template Can add replicas as needed for high availability ","categories":["Reference"],"description":"Minimal installation template without monitoring infrastructure, installs PostgreSQL directly from internet","excerpt":"Minimal installation template without monitoring infrastructure, …","ref":"/docs/conf/slim/","tags":"","title":"slim"},{"body":"The fat configuration template is Pigsty’s Feature-All-Test template, installing all extension plugins on a single node and building a local software repository containing all extensions for PostgreSQL 13-18 (six major versions).\nThis is a full-featured configuration for testing and development, suitable for scenarios requiring complete software package cache or testing all extensions.\nOverview Config Name: fat Node Count: Single node Description: Feature-All-Test template, installs all extensions, builds local repo with PG 13-18 all versions OS Distro: el8, el9, el10, d12, d13, u22, u24 OS Arch: x86_64, aarch64 Related: meta, slim, fat Usage:\n./configure -c fat [-i \u003cprimary_ip\u003e] To specify a particular PostgreSQL version:\n./configure -c fat -v 17 # Use PostgreSQL 17 Content Source: pigsty/conf/fat.yml\n--- #==============================================================# # File : fat.yml # Desc : Pigsty Feature-All-Test config template # Ctime : 2020-05-22 # Mtime : 2025-12-28 # Docs : https://doc.pgsty.com/config # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# # This is the 4-node sandbox for pigsty # # Usage: # curl https://repo.pigsty.io/get | bash # ./configure -c fat [-v 18|17|16|15] # ./deploy.yml all: #==============================================================# # Clusters, Nodes, and Modules #==============================================================# children: #----------------------------------------------# # PGSQL : https://doc.pgsty.com/pgsql #----------------------------------------------# # this is an example single-node postgres cluster with pgvector installed, with one biz database \u0026 two biz users pg-meta: hosts: 10.10.10.10: { pg_seq: 1, pg_role: primary } # \u003c---- primary instance with read-write capability #x.xx.xx.xx: { pg_seq: 2, pg_role: replica } # \u003c---- read only replica for read-only online traffic #x.xx.xx.xy: { pg_seq: 3, pg_role: offline } # \u003c---- offline instance of ETL \u0026 interactive queries vars: pg_cluster: pg-meta # install, load, create pg extensions: https://doc.pgsty.com/pgsql/extension pg_extensions: [ pg18-main ,pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-olap ,pg18-feat ,pg18-lang ,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl] pg_libs: 'timescaledb, pg_stat_statements, auto_explain, pg_wait_sampling' # define business users/roles : https://doc.pgsty.com/pgsql/user pg_users: - name: dbuser_meta # REQUIRED, `name` is the only mandatory field of a user definition password: DBUser.Meta # optional, the password. can be a scram-sha-256 hash string or plain text #state: create # optional, create|absent, 'create' by default, use 'absent' to drop user #login: true # optional, can log in, true by default (new biz ROLE should be false) #superuser: false # optional, is superuser? false by default #createdb: false # optional, can create databases? false by default #createrole: false # optional, can create role? false by default #inherit: true # optional, can this role use inherited privileges? true by default #replication: false # optional, can this role do replication? false by default #bypassrls: false # optional, can this role bypass row level security? false by default #pgbouncer: true # optional, add this user to the pgbouncer user-list? false by default (production user should be true explicitly) #connlimit: -1 # optional, user connection limit, default -1 disable limit #expire_in: 3650 # optional, now + n days when this role is expired (OVERWRITE expire_at) #expire_at: '2030-12-31' # optional, YYYY-MM-DD 'timestamp' when this role is expired (OVERWRITTEN by expire_in) #comment: pigsty admin user # optional, comment string for this user/role #roles: [dbrole_admin] # optional, belonged roles. default roles are: dbrole_{admin|readonly|readwrite|offline} #parameters: {} # optional, role level parameters with `ALTER ROLE SET` #pool_mode: transaction # optional, pgbouncer pool mode at user level, transaction by default #pool_connlimit: -1 # optional, max database connections at user level, default -1 disable limit # Enhanced roles syntax (PG16+): roles can be string or object with options: # - dbrole_readwrite # simple string: GRANT role # - { name: role, admin: true } # GRANT WITH ADMIN OPTION # - { name: role, set: false } # PG16: REVOKE SET OPTION # - { name: role, inherit: false } # PG16: REVOKE INHERIT OPTION # - { name: role, state: absent } # REVOKE membership - { name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [dbrole_readonly], comment: read-only viewer for meta database } #- {name: dbuser_bytebase ,password: DBUser.Bytebase ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for bytebase database } #- {name: dbuser_remove ,state: absent } # use state: absent to remove a user # define business databases : https://doc.pgsty.com/pgsql/db pg_databases: # define business databases on this cluster, array of database definition - name: meta # REQUIRED, `name` is the only mandatory field of a database definition #state: create # optional, create|absent|recreate, create by default baseline: cmdb.sql # optional, database sql baseline path, (relative path among the ansible search path, e.g.: files/) schemas: [ pigsty ] # optional, additional schemas to be created, array of schema names extensions: # optional, additional extensions to be installed: array of `{name[,schema]}` - vector # install pgvector for vector similarity search - postgis # install postgis for geospatial type \u0026 index - timescaledb # install timescaledb for time-series data - { name: pg_wait_sampling, schema: monitor } # install pg_wait_sampling on monitor schema comment: pigsty meta database # optional, comment string for this database #pgbouncer: true # optional, add this database to the pgbouncer database list? true by default #owner: postgres # optional, database owner, current user if not specified #template: template1 # optional, which template to use, template1 by default #strategy: FILE_COPY # optional, clone strategy: FILE_COPY or WAL_LOG (PG15+), default to PG's default #encoding: UTF8 # optional, inherited from template / cluster if not defined (UTF8) #locale: C # optional, inherited from template / cluster if not defined (C) #lc_collate: C # optional, inherited from template / cluster if not defined (C) #lc_ctype: C # optional, inherited from template / cluster if not defined (C) #locale_provider: libc # optional, locale provider: libc, icu, builtin (PG15+) #icu_locale: en-US # optional, icu locale for icu locale provider (PG15+) #icu_rules: '' # optional, icu rules for icu locale provider (PG16+) #builtin_locale: C.UTF-8 # optional, builtin locale for builtin locale provider (PG17+) #tablespace: pg_default # optional, default tablespace, pg_default by default #is_template: false # optional, mark database as template, allowing clone by any user with CREATEDB privilege #allowconn: true # optional, allow connection, true by default. false will disable connect at all #revokeconn: false # optional, revoke public connection privilege. false by default. (leave connect with grant option to owner) #register_datasource: true # optional, register this database to grafana datasources? true by default #connlimit: -1 # optional, database connection limit, default -1 disable limit #pool_auth_user: dbuser_meta # optional, all connection to this pgbouncer database will be authenticated by this user #pool_mode: transaction # optional, pgbouncer pool mode at database level, default transaction #pool_size: 64 # optional, pgbouncer pool size at database level, default 64 #pool_size_reserve: 32 # optional, pgbouncer pool size reserve at database level, default 32 #pool_size_min: 0 # optional, pgbouncer pool size min at database level, default 0 #pool_max_db_conn: 100 # optional, max database connections at database level, default 100 #- {name: bytebase ,owner: dbuser_bytebase ,revokeconn: true ,comment: bytebase primary database } # define HBA rules : https://doc.pgsty.com/pgsql/hba pg_hba_rules: - { user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes' } # define backup policies: https://doc.pgsty.com/pgsql/backup node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] # make a full backup every day 1am # define (OPTIONAL) L2 VIP that bind to primary pg_vip_enabled: true pg_vip_address: 10.10.10.2/24 pg_vip_interface: eth1 #----------------------------------------------# # INFRA : https://doc.pgsty.com/infra #----------------------------------------------# infra: hosts: 10.10.10.10: { infra_seq: 1 } vars: repo_enabled: true # build local repo: https://doc.pgsty.com/admin/repo #repo_extra_packages: [ pg18-main ,pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-olap ,pg18-feat ,pg18-lang,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl] repo_packages: [ node-bootstrap, infra-package, infra-addons, node-package1, node-package2, pgsql-utility, extra-modules, pg18-full,pg18-time,pg18-gis,pg18-rag,pg18-fts,pg18-olap,pg18-feat,pg18-lang,pg18-type,pg18-util,pg18-func,pg18-admin,pg18-stat,pg18-sec,pg18-fdw,pg18-sim,pg18-etl, pg17-full,pg17-time,pg17-gis,pg17-rag,pg17-fts,pg17-olap,pg17-feat,pg17-lang,pg17-type,pg17-util,pg17-func,pg17-admin,pg17-stat,pg17-sec,pg17-fdw,pg17-sim,pg17-etl, pg16-full,pg16-time,pg16-gis,pg16-rag,pg16-fts,pg16-olap,pg16-feat,pg16-lang,pg16-type,pg16-util,pg16-func,pg16-admin,pg16-stat,pg16-sec,pg16-fdw,pg16-sim,pg16-etl, pg15-full,pg15-time,pg15-gis,pg15-rag,pg15-fts,pg15-olap,pg15-feat,pg15-lang,pg15-type,pg15-util,pg15-func,pg15-admin,pg15-stat,pg15-sec,pg15-fdw,pg15-sim,pg15-etl, pg14-full,pg14-time,pg14-gis,pg14-rag,pg14-fts,pg14-olap,pg14-feat,pg14-lang,pg14-type,pg14-util,pg14-func,pg14-admin,pg14-stat,pg14-sec,pg14-fdw,pg14-sim,pg14-etl, pg13-full,pg13-time,pg13-gis,pg13-rag,pg13-fts,pg13-olap,pg13-feat,pg13-lang,pg13-type,pg13-util,pg13-func,pg13-admin,pg13-stat,pg13-sec,pg13-fdw,pg13-sim,pg13-etl, infra-extra, kafka, java-runtime, sealos, tigerbeetle, polardb, ivorysql ] #----------------------------------------------# # ETCD : https://doc.pgsty.com/etcd #----------------------------------------------# etcd: hosts: 10.10.10.10: { etcd_seq: 1 } vars: etcd_cluster: etcd etcd_safeguard: false # prevent purging running etcd instance? #----------------------------------------------# # MINIO : https://doc.pgsty.com/minio #----------------------------------------------# minio: hosts: 10.10.10.10: { minio_seq: 1 } vars: minio_cluster: minio minio_users: # list of minio user to be created - { access_key: pgbackrest ,secret_key: S3User.Backup ,policy: pgsql } - { access_key: s3user_meta ,secret_key: S3User.Meta ,policy: meta } - { access_key: s3user_data ,secret_key: S3User.Data ,policy: data } #----------------------------------------------# # DOCKER : https://doc.pgsty.com/docker # APP : https://doc.pgsty.com/app #----------------------------------------------# # OPTIONAL, launch example pgadmin app with: ./app.yml \u0026 ./app.yml -e app=bytebase app: hosts: { 10.10.10.10: {} } vars: docker_enabled: true # enabled docker with ./docker.yml #docker_registry_mirrors: [\"https://docker.1panel.live\",\"https://docker.1ms.run\",\"https://docker.xuanyuan.me\",\"https://registry-1.docker.io\"] app: pgadmin # specify the default app name to be installed (in the apps) apps: # define all applications, appname: definition # Admin GUI for PostgreSQL, launch with: ./app.yml pgadmin: # pgadmin app definition (app/pgadmin -\u003e /opt/pgadmin) conf: # override /opt/pgadmin/.env PGADMIN_DEFAULT_EMAIL: admin@pigsty.cc # default user name PGADMIN_DEFAULT_PASSWORD: pigsty # default password # Schema Migration GUI for PostgreSQL, launch with: ./app.yml -e app=bytebase bytebase: conf: BB_DOMAIN: http://ddl.pigsty # replace it with your public domain name and postgres database url BB_PGURL: \"postgresql://dbuser_bytebase:DBUser.Bytebase@10.10.10.10:5432/bytebase?sslmode=prefer\" #==============================================================# # Global Parameters #==============================================================# vars: #----------------------------------------------# # INFRA : https://doc.pgsty.com/infra #----------------------------------------------# version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default|china|europe proxy_env: # global proxy env when downloading packages no_proxy: \"localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,*.pigsty,*.aliyun.com,mirrors.*,*.myqcloud.com,*.tsinghua.edu.cn\" # http_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com # https_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com # all_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com certbot_sign: false # enable certbot to sign https certificate for infra portal certbot_email: your@email.com # replace your email address to receive expiration notice infra_portal: # domain names and upstream servers home : { domain: i.pigsty } pgadmin : { domain: adm.pigsty ,endpoint: \"${admin_ip}:8885\" } bytebase : { domain: ddl.pigsty ,endpoint: \"${admin_ip}:8887\" ,websocket: true} minio : { domain: m.pigsty ,endpoint: \"${admin_ip}:9001\" ,scheme: https ,websocket: true } #website: # static local website example stub # domain: repo.pigsty # external domain name for static site # certbot: repo.pigsty # use certbot to sign https certificate for this static site # path: /www/pigsty # path to the static site directory #supabase: # dynamic upstream service example stub # domain: supa.pigsty # external domain name for upstream service # certbot: supa.pigsty # use certbot to sign https certificate for this upstream server # endpoint: \"10.10.10.10:8000\" # path to the static site directory # websocket: true # add websocket support # certbot: supa.pigsty # certbot cert name, apply with `make cert` #----------------------------------------------# # NODE : https://doc.pgsty.com/node/param #----------------------------------------------# nodename_overwrite: true # overwrite node hostname on multi-node template node_tune: oltp # node tuning specs: oltp,olap,tiny,crit node_etc_hosts: # add static domains to all nodes /etc/hosts - 10.10.10.10 i.pigsty sss.pigsty - 10.10.10.10 adm.pigsty ddl.pigsty repo.pigsty supa.pigsty node_repo_modules: local,node,infra,pgsql # use pre-made local repo rather than install from upstream node_repo_remove: true # remove existing node repo for node managed by pigsty #node_packages: [openssh-server] # packages to be installed current nodes with latest version #node_timezone: Asia/Hong_Kong # overwrite node timezone #----------------------------------------------# # PGSQL : https://doc.pgsty.com/pgsql/param #----------------------------------------------# pg_version: 18 # default postgres version pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml pg_safeguard: false # prevent purging running postgres instance? pg_packages: [ pgsql-main, pgsql-common ] # pg kernel and common utils #pg_extensions: [ pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-olap ,pg18-feat ,pg18-lang ,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl] #----------------------------------------------# # BACKUP : https://doc.pgsty.com/pgsql/backup #----------------------------------------------# # if you want to use minio as backup repo instead of 'local' fs, uncomment this, and configure `pgbackrest_repo` # you can also use external object storage as backup repo pgbackrest_method: minio # if you want to use minio as backup repo instead of 'local' fs, uncomment this pgbackrest_repo: # pgbackrest repo: https://pgbackrest.org/configuration.html#section-repository local: # default pgbackrest repo with local posix fs path: /pg/backup # local backup directory, `/pg/backup` by default retention_full_type: count # retention full backups by count retention_full: 2 # keep 2, at most 3 full backups when using local fs repo minio: # optional minio repo for pgbackrest type: s3 # minio is s3-compatible, so s3 is used s3_endpoint: sss.pigsty # minio endpoint domain name, `sss.pigsty` by default s3_region: us-east-1 # minio region, us-east-1 by default, useless for minio s3_bucket: pgsql # minio bucket name, `pgsql` by default s3_key: pgbackrest # minio user access key for pgbackrest [CHANGE ACCORDING to minio_users.pgbackrest] s3_key_secret: S3User.Backup # minio user secret key for pgbackrest [CHANGE ACCORDING to minio_users.pgbackrest] s3_uri_style: path # use path style uri for minio rather than host style path: /pgbackrest # minio backup path, default is `/pgbackrest` storage_port: 9000 # minio port, 9000 by default storage_ca_file: /etc/pki/ca.crt # minio ca file path, `/etc/pki/ca.crt` by default block: y # Enable block incremental backup bundle: y # bundle small files into a single file bundle_limit: 20MiB # Limit for file bundles, 20MiB for object storage bundle_size: 128MiB # Target size for file bundles, 128MiB for object storage cipher_type: aes-256-cbc # enable AES encryption for remote backup repo cipher_pass: pgBackRest # AES encryption password, default is 'pgBackRest' retention_full_type: time # retention full backup by time on minio repo retention_full: 14 # keep full backup for the last 14 days s3: # you can use cloud object storage as backup repo type: s3 # Add your object storage credentials here! s3_endpoint: oss-cn-beijing-internal.aliyuncs.com s3_region: oss-cn-beijing s3_bucket: \u003cyour_bucket_name\u003e s3_key: \u003cyour_access_key\u003e s3_key_secret: \u003cyour_secret_key\u003e s3_uri_style: host path: /pgbackrest bundle: y # bundle small files into a single file bundle_limit: 20MiB # Limit for file bundles, 20MiB for object storage bundle_size: 128MiB # Target size for file bundles, 128MiB for object storage cipher_type: aes-256-cbc # enable AES encryption for remote backup repo cipher_pass: pgBackRest # AES encryption password, default is 'pgBackRest' retention_full_type: time # retention full backup by time on minio repo retention_full: 14 # keep full backup for the last 14 days #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root ... Explanation The fat template is Pigsty’s full-featured test configuration, designed for completeness testing and offline package building.\nKey Features:\nAll Extensions: Installs all categorized extension packages for PostgreSQL 18 Multi-version Repository: Local repo contains all six major versions of PostgreSQL 13-18 Complete Component Stack: Includes MinIO backup, Docker applications, VIP, etc. Enterprise Components: Includes Kafka, PolarDB, IvorySQL, TigerBeetle, etc. Repository Contents:\nCategory Description PostgreSQL 13-18 Six major versions’ kernels and all extensions Extension Categories time, gis, rag, fts, olap, feat, lang, type, util, func, admin, stat, sec, fdw, sim, etl Enterprise Components Kafka, Java Runtime, Sealos, TigerBeetle Database Kernels PolarDB, IvorySQL Differences from rich:\nfat contains all six versions of PostgreSQL 13-18, rich only contains current default version fat contains additional enterprise components (Kafka, PolarDB, IvorySQL, etc.) fat requires larger disk space and longer build time Use Cases:\nPigsty development testing and feature validation Building complete multi-version offline software packages Testing all extension compatibility scenarios Enterprise environments pre-caching all software packages Notes:\nRequires large disk space (100GB+ recommended) for storing all packages Building local software repository requires longer time Some extensions unavailable on ARM64 architecture Default passwords are sample passwords, must be changed for production ","categories":["Reference"],"description":"Feature-All-Test template, single-node installation of all extensions, builds local repo with PG 13-18 all versions","excerpt":"Feature-All-Test template, single-node installation of all extensions, …","ref":"/docs/conf/fat/","tags":"","title":"fat"},{"body":"The infra configuration template only deploys Pigsty’s observability infrastructure components (VictoriaMetrics/Grafana/Loki/Nginx, etc.), without PostgreSQL and etcd.\nSuitable for scenarios requiring a standalone monitoring stack, such as monitoring external PostgreSQL/RDS instances or other data sources.\nOverview Config Name: infra Node Count: Single or multiple nodes Description: Only installs observability infrastructure, without PostgreSQL and etcd OS Distro: el8, el9, el10, d12, d13, u22, u24 OS Arch: x86_64, aarch64 Related: meta Usage:\n./configure -c infra [-i \u003cprimary_ip\u003e] ./infra.yml # Only execute infra playbook Content Source: pigsty/conf/infra.yml\n--- #==============================================================# # File : infra.yml # Desc : Infra Only Config # Ctime : 2025-12-16 # Mtime : 2025-12-30 # Docs : https://doc.pgsty.com/infra # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# # This is the config template for deploy victoria stack alone # tutorial: https://doc.pgsty.com/infra # # Usage: # curl https://repo.pigsty.io/get | bash # ./configure -c infra # ./infra.yml all: children: infra: hosts: 10.10.10.10: { infra_seq: 1 } #10.10.10.11: { infra_seq: 2 } # you can add more nodes if you want #10.10.10.12: { infra_seq: 3 } # don't forget to assign unique infra_seq for each node vars: docker_enabled: true # enabled docker with ./docker.yml docker_registry_mirrors: [\"https://docker.1panel.live\",\"https://docker.1ms.run\",\"https://docker.xuanyuan.me\",\"https://registry-1.docker.io\"] pg_exporters: # bin/pgmon-add pg-rds 20001: { pg_cluster: pg-rds ,pg_seq: 1 ,pg_host: 10.10.10.10 ,pg_exporter_url: 'postgres://postgres:postgres@10.10.10.10:5432/postgres' } vars: # global variables version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default,china,europe node_tune: oltp # node tuning specs: oltp,olap,tiny,crit infra_portal: # infra services exposed via portal home : { domain: i.pigsty } # default domain name repo_enabled: false # online installation without repo node_repo_modules: node,infra,pgsql # add these repos directly #haproxy_enabled: false # enable haproxy on infra node? #vector_enabled: false # enable vector on infra node? # DON't FORGET TO CHANGE DEFAULT PASSWORDS! grafana_admin_password: pigsty ... Explanation The infra template is Pigsty’s pure monitoring stack configuration, designed for standalone deployment of observability infrastructure.\nUse Cases:\nMonitoring external PostgreSQL instances (RDS, self-hosted, etc.) Need standalone monitoring/alerting platform Already have PostgreSQL clusters, only need to add monitoring As a central console for multi-cluster monitoring Included Components:\nVictoriaMetrics: Time series database for storing metrics VictoriaLogs: Log aggregation system VictoriaTraces: Distributed tracing system Grafana: Visualization dashboards Alertmanager: Alert management Nginx: Reverse proxy and web entry Not Included:\nPostgreSQL database cluster etcd distributed coordination service MinIO object storage Monitoring External Instances: After configuration, add monitoring for external PostgreSQL instances via the pgsql-monitor.yml playbook:\npg_exporters: 20001: { pg_cluster: pg-foo, pg_seq: 1, pg_host: 10.10.10.100 } 20002: { pg_cluster: pg-bar, pg_seq: 1, pg_host: 10.10.10.101 } Notes:\nThis template will not install any databases For full functionality, use meta or rich template Can add multiple infra nodes for high availability as needed ","categories":["Reference"],"description":"Only installs observability infrastructure, dedicated template without PostgreSQL and etcd","excerpt":"Only installs observability infrastructure, dedicated template without …","ref":"/docs/conf/infra/","tags":"","title":"infra"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/conf/_div_kernel/","tags":"","title":"Kernel Templates"},{"body":"The pgsql configuration template uses the native PostgreSQL kernel, which is Pigsty’s default database kernel, supporting PostgreSQL versions 13 to 18.\nOverview Config Name: pgsql Node Count: Single node Description: Native PostgreSQL kernel configuration template OS Distro: el8, el9, el10, d12, d13, u22, u24 OS Arch: x86_64, aarch64 Related: meta Usage:\n./configure -c pgsql [-i \u003cprimary_ip\u003e] To specify a particular PostgreSQL version (e.g., 17):\n./configure -c pgsql -v 17 Content Source: pigsty/conf/pgsql.yml\n--- #==============================================================# # File : pgsql.yml # Desc : 1-node PostgreSQL Config template # Ctime : 2025-02-23 # Mtime : 2025-12-28 # Docs : https://doc.pgsty.com/config # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# # This is the config template for basical PostgreSQL Kernel. # Nothing special, just a basic setup with one node. # tutorial: https://doc.pgsty.com/pgsql/kernel/postgres # # Usage: # curl https://repo.pigsty.io/get | bash # ./configure -c pgsql # ./deploy.yml all: children: infra: { hosts: { 10.10.10.10: { infra_seq: 1 }} ,vars: { repo_enabled: false }} etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 }} ,vars: { etcd_cluster: etcd }} #minio: { hosts: { 10.10.10.10: { minio_seq: 1 }} ,vars: { minio_cluster: minio }} #----------------------------------------------# # PostgreSQL Cluster #----------------------------------------------# pg-meta: hosts: 10.10.10.10: { pg_seq: 1, pg_role: primary } vars: pg_cluster: pg-meta pg_users: - { name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [dbrole_admin ] ,comment: pigsty admin user } - { name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [dbrole_readonly] ,comment: read-only viewer } pg_databases: - { name: meta, baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [pigsty] ,extensions: [ postgis, timescaledb, vector ]} pg_extensions: [ postgis, timescaledb, pgvector, pg_wait_sampling ] pg_libs: 'timescaledb, pg_stat_statements, auto_explain, pg_wait_sampling' pg_hba_rules: - { user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes' } node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] # make a full backup every 1am vars: #----------------------------------------------# # INFRA : https://doc.pgsty.com/infra/param #----------------------------------------------# version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default,china,europe infra_portal: # infra services exposed via portal home : { domain: i.pigsty } # default domain name #----------------------------------------------# # NODE : https://doc.pgsty.com/node/param #----------------------------------------------# nodename_overwrite: false # do not overwrite node hostname on single node mode node_repo_modules: node,infra,pgsql # add these repos directly to the singleton node node_tune: oltp # node tuning specs: oltp,olap,tiny,crit #----------------------------------------------# # PGSQL : https://doc.pgsty.com/pgsql/param #----------------------------------------------# pg_version: 18 # Default PostgreSQL Major Version is 18 pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml pg_packages: [ pgsql-main, pgsql-common ] # pg kernel and common utils #pg_extensions: [ pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-olap ,pg18-feat ,pg18-lang ,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl] #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root ... Explanation The pgsql template is Pigsty’s standard kernel configuration, using community-native PostgreSQL.\nVersion Support:\nPostgreSQL 18 (default) PostgreSQL 17, 16, 15, 14, 13 Use Cases:\nNeed to use the latest PostgreSQL features Need the widest extension support Standard production environment deployment Same functionality as meta template, explicitly declaring native kernel usage Differences from meta:\npgsql template explicitly declares using native PostgreSQL kernel Suitable for scenarios needing clear distinction between different kernel types ","categories":["Reference"],"description":"Native PostgreSQL kernel, supports deployment of PostgreSQL versions 13 to 18","excerpt":"Native PostgreSQL kernel, supports deployment of PostgreSQL versions …","ref":"/docs/conf/pgsql/","tags":"","title":"pgsql"},{"body":"The citus configuration template deploys a distributed PostgreSQL cluster using the Citus extension, providing transparent horizontal scaling and data sharding capabilities.\nOverview Config Name: citus Node Count: Five nodes (1 coordinator + 4 data nodes) Description: Citus distributed PostgreSQL cluster OS Distro: el8, el9, el10, d12, d13, u22, u24 OS Arch: x86_64 Related: meta Usage:\n./configure -c citus [-i \u003cprimary_ip\u003e] Content Source: pigsty/conf/citus.yml\n--- #==============================================================# # File : citus.yml # Desc : 1-node Citus (Distributive) Config Template # Ctime : 2020-05-22 # Mtime : 2025-12-28 # Docs : https://doc.pgsty.com/config # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# # This is the config template for Citus Distributive Cluster # tutorial: https://doc.pgsty.com/pgsql/kernel/citus # # Usage: # curl https://repo.pigsty.io/get | bash # ./configure -c citus # ./deploy.yml all: children: infra: { hosts: { 10.10.10.10: { infra_seq: 1 }} ,vars: { repo_enabled: false }} etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 }} ,vars: { etcd_cluster: etcd }} #minio: { hosts: { 10.10.10.10: { minio_seq: 1 }} ,vars: { minio_cluster: minio }} #----------------------------------------------# # pg-citus: 10 node citus cluster #----------------------------------------------# pg-citus: # the citus group contains 5 clusters hosts: 10.10.10.10: { pg_group: 0, pg_cluster: pg-citus0 ,pg_vip_address: 10.10.10.60/24 ,pg_seq: 0, pg_role: primary } #10.10.10.11: { pg_group: 0, pg_cluster: pg-citus0 ,pg_vip_address: 10.10.10.60/24 ,pg_seq: 1, pg_role: replica } #10.10.10.12: { pg_group: 1, pg_cluster: pg-citus1 ,pg_vip_address: 10.10.10.61/24 ,pg_seq: 0, pg_role: primary } #10.10.10.13: { pg_group: 1, pg_cluster: pg-citus1 ,pg_vip_address: 10.10.10.61/24 ,pg_seq: 1, pg_role: replica } #10.10.10.14: { pg_group: 2, pg_cluster: pg-citus2 ,pg_vip_address: 10.10.10.62/24 ,pg_seq: 0, pg_role: primary } #10.10.10.15: { pg_group: 2, pg_cluster: pg-citus2 ,pg_vip_address: 10.10.10.62/24 ,pg_seq: 1, pg_role: replica } #10.10.10.16: { pg_group: 3, pg_cluster: pg-citus3 ,pg_vip_address: 10.10.10.63/24 ,pg_seq: 0, pg_role: primary } #10.10.10.17: { pg_group: 3, pg_cluster: pg-citus3 ,pg_vip_address: 10.10.10.63/24 ,pg_seq: 1, pg_role: replica } #10.10.10.18: { pg_group: 4, pg_cluster: pg-citus4 ,pg_vip_address: 10.10.10.64/24 ,pg_seq: 0, pg_role: primary } #10.10.10.19: { pg_group: 4, pg_cluster: pg-citus4 ,pg_vip_address: 10.10.10.64/24 ,pg_seq: 1, pg_role: replica } vars: pg_mode: citus # pgsql cluster mode: citus pg_shard: pg-citus # citus shard name: pg-citus pg_primary_db: citus # primary database used by citus pg_dbsu_password: DBUser.Postgres # enable dbsu password access for citus pg_extensions: [ citus, postgis, pgvector, topn, pg_cron, hll ] # install these extensions pg_libs: 'citus, pg_cron, pg_stat_statements' # citus will be added by patroni automatically pg_users: [{ name: dbuser_citus ,password: DBUser.Citus ,pgbouncer: true ,roles: [ dbrole_admin ] }] pg_databases: [{ name: citus ,owner: dbuser_citus ,extensions: [ citus, vector, topn, pg_cron, hll ] }] pg_parameters: cron.database_name: citus citus.node_conninfo: 'sslrootcert=/pg/cert/ca.crt sslmode=verify-full' pg_hba_rules: - { user: 'all' ,db: all ,addr: 127.0.0.1/32 ,auth: ssl ,title: 'all user ssl access from localhost' } - { user: 'all' ,db: all ,addr: intra ,auth: ssl ,title: 'all user ssl access from intranet' } pg_vip_enabled: true # enable vip for citus cluster pg_vip_interface: eth1 # vip interface for all members (you can override this in each host) vars: # global variables #----------------------------------------------# # INFRA : https://doc.pgsty.com/infra/param #----------------------------------------------# version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default,china,europe infra_portal: # infra services exposed via portal home : { domain: i.pigsty } # default domain name #----------------------------------------------# # NODE : https://doc.pgsty.com/node/param #----------------------------------------------# nodename_overwrite: true # overwrite hostname since this is a multi-node tempalte node_repo_modules: node,infra,pgsql # add these repos directly to the singleton node node_tune: oltp # node tuning specs: oltp,olap,tiny,crit #----------------------------------------------# # PGSQL : https://doc.pgsty.com/pgsql/param #----------------------------------------------# pg_version: 17 # Default PostgreSQL Major Version is 17 pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml pg_packages: [ pgsql-main, pgsql-common ] # pg kernel and common utils #pg_extensions: [ pg17-time ,pg17-gis ,pg17-rag ,pg17-fts ,pg17-olap ,pg17-feat ,pg17-lang ,pg17-type ,pg17-util ,pg17-func ,pg17-admin ,pg17-stat ,pg17-sec ,pg17-fdw ,pg17-sim ,pg17-etl] #repo_extra_packages: [ pgsql-main, citus, postgis, pgvector, pg_cron, hll, topn ] #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root ... Explanation The citus template deploys a Citus distributed PostgreSQL cluster, suitable for large-scale data scenarios requiring horizontal scaling.\nKey Features:\nTransparent data sharding, automatically distributes data to multiple nodes Parallel query execution, aggregates results from multiple nodes Supports distributed transactions (2PC) Maintains PostgreSQL SQL compatibility Architecture:\nCoordinator Node (pg-citus0): Receives queries, routes to data nodes Data Nodes (pg-citus1~3): Stores sharded data Use Cases:\nSingle table data volume exceeds single-node capacity Need horizontal scaling for write and query performance Multi-tenant SaaS applications Real-time analytical workloads Notes:\nCitus supports PostgreSQL 14~17 Distributed tables require specifying a distribution column Some PostgreSQL features may be limited (e.g., cross-shard foreign keys) ARM64 architecture not supported ","categories":["Reference"],"description":"Citus distributed PostgreSQL cluster, provides horizontal scaling and sharding capabilities","excerpt":"Citus distributed PostgreSQL cluster, provides horizontal scaling and …","ref":"/docs/conf/citus/","tags":"","title":"citus"},{"body":"The mssql configuration template uses WiltonDB / Babelfish database kernel instead of native PostgreSQL, providing Microsoft SQL Server wire protocol (TDS) and T-SQL syntax compatibility.\nFor the complete tutorial, see: Babelfish (MSSQL) Kernel Guide\nOverview Config Name: mssql Node Count: Single node Description: WiltonDB / Babelfish configuration template, provides SQL Server protocol compatibility OS Distro: el8, el9, el10, u22, u24 (Debian not available) OS Arch: x86_64 Related: meta Usage:\n./configure -c mssql [-i \u003cprimary_ip\u003e] Content Source: pigsty/conf/mssql.yml\n--- #==============================================================# # File : mssql.yml # Desc : Babelfish: WiltonDB (MSSQL Compatible) template # Ctime : 2020-08-01 # Mtime : 2025-12-28 # Docs : https://doc.pgsty.com/config # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# # This is the config template for Babelfish Kernel (WiltonDB), # Which is a PostgreSQL 15 fork with SQL Server Compatibility # tutorial: https://doc.pgsty.com/pgsql/kernel/babelfish # # Usage: # curl https://repo.pigsty.io/get | bash # ./configure -c mssql # ./deploy.yml all: children: infra: { hosts: { 10.10.10.10: { infra_seq: 1 }} ,vars: { repo_enabled: false }} etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 }} ,vars: { etcd_cluster: etcd }} #minio: { hosts: { 10.10.10.10: { minio_seq: 1 }} ,vars: { minio_cluster: minio }} #----------------------------------------------# # Babelfish Database Cluster #----------------------------------------------# pg-meta: hosts: 10.10.10.10: { pg_seq: 1, pg_role: primary } vars: pg_cluster: pg-meta pg_users: - {name: dbuser_mssql ,password: DBUser.MSSQL ,superuser: true, pgbouncer: true ,roles: [dbrole_admin], comment: superuser \u0026 owner for babelfish } pg_databases: - name: mssql baseline: mssql.sql extensions: [uuid-ossp, babelfishpg_common, babelfishpg_tsql, babelfishpg_tds, babelfishpg_money, pg_hint_plan, system_stats, tds_fdw] owner: dbuser_mssql parameters: { 'babelfishpg_tsql.migration_mode' : 'multi-db' } comment: babelfish cluster, a MSSQL compatible pg cluster node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] # make a full backup every 1am # Babelfish / WiltonDB Ad Hoc Settings pg_mode: mssql # Microsoft SQL Server Compatible Mode pg_version: 15 pg_packages: [ wiltondb, pgsql-common, sqlcmd ] pg_libs: 'babelfishpg_tds, pg_stat_statements, auto_explain' # add timescaledb to shared_preload_libraries pg_default_hba_rules: # overwrite default HBA rules for babelfish cluster, order by `order` - { user: '${dbsu}' ,db: all ,addr: local ,auth: ident ,title: 'dbsu access via local os user ident' ,order: 100} - { user: '${dbsu}' ,db: replication ,addr: local ,auth: ident ,title: 'dbsu replication from local os ident' ,order: 150} - { user: '${repl}' ,db: replication ,addr: localhost ,auth: pwd ,title: 'replicator replication from localhost' ,order: 200} - { user: '${repl}' ,db: replication ,addr: intra ,auth: pwd ,title: 'replicator replication from intranet' ,order: 250} - { user: '${repl}' ,db: postgres ,addr: intra ,auth: pwd ,title: 'replicator postgres db from intranet' ,order: 300} - { user: '${monitor}' ,db: all ,addr: localhost ,auth: pwd ,title: 'monitor from localhost with password' ,order: 350} - { user: '${monitor}' ,db: all ,addr: infra ,auth: pwd ,title: 'monitor from infra host with password' ,order: 400} - { user: '${admin}' ,db: all ,addr: infra ,auth: ssl ,title: 'admin @ infra nodes with pwd \u0026 ssl' ,order: 450} - { user: '${admin}' ,db: all ,addr: world ,auth: ssl ,title: 'admin @ everywhere with ssl \u0026 pwd' ,order: 500} - { user: dbuser_mssql ,db: mssql ,addr: intra ,auth: md5 ,title: 'allow mssql dbsu intranet access' ,order: 525} # \u003c--- use md5 auth method for mssql user - { user: '+dbrole_readonly',db: all ,addr: localhost ,auth: pwd ,title: 'pgbouncer read/write via local socket' ,order: 550} - { user: '+dbrole_readonly',db: all ,addr: intra ,auth: pwd ,title: 'read/write biz user via password' ,order: 600} - { user: '+dbrole_offline' ,db: all ,addr: intra ,auth: pwd ,title: 'allow etl offline tasks from intranet' ,order: 650} pg_default_services: # route primary \u0026 replica service to mssql port 1433 - { name: primary ,port: 5433 ,dest: 1433 ,check: /primary ,selector: \"[]\" } - { name: replica ,port: 5434 ,dest: 1433 ,check: /read-only ,selector: \"[]\" , backup: \"[? pg_role == `primary` || pg_role == `offline` ]\" } - { name: default ,port: 5436 ,dest: postgres ,check: /primary ,selector: \"[]\" } - { name: offline ,port: 5438 ,dest: postgres ,check: /replica ,selector: \"[? pg_role == `offline` || pg_offline_query ]\" , backup: \"[? pg_role == `replica` \u0026\u0026 !pg_offline_query]\" } vars: #----------------------------------------------# # INFRA : https://doc.pgsty.com/infra/param #----------------------------------------------# version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default,china,europe infra_portal: # infra services exposed via portal home : { domain: i.pigsty } # default domain name #----------------------------------------------# # NODE : https://doc.pgsty.com/node/param #----------------------------------------------# nodename_overwrite: false # do not overwrite node hostname on single node mode node_repo_modules: node,infra,pgsql,mssql # extra mssql repo is required node_tune: oltp # node tuning specs: oltp,olap,tiny,crit #----------------------------------------------# # PGSQL : https://doc.pgsty.com/pgsql/param #----------------------------------------------# pg_version: 15 # Babelfish kernel is compatible with postgres 15 pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root ... Explanation The mssql template allows you to use SQL Server Management Studio (SSMS) or other SQL Server client tools to connect to PostgreSQL.\nKey Features:\nUses TDS protocol (port 1433), compatible with SQL Server clients Supports T-SQL syntax, low migration cost Retains PostgreSQL’s ACID properties and extension ecosystem Supports multi-db and single-db migration modes Connection Methods:\n# Using sqlcmd command line tool sqlcmd -S 10.10.10.10,1433 -U dbuser_mssql -P DBUser.MSSQL -d mssql # Using SSMS or Azure Data Studio # Server: 10.10.10.10,1433 # Authentication: SQL Server Authentication # Login: dbuser_mssql # Password: DBUser.MSSQL Use Cases:\nMigrating from SQL Server to PostgreSQL Applications needing to support both SQL Server and PostgreSQL clients Leveraging PostgreSQL ecosystem while maintaining T-SQL compatibility Notes:\nWiltonDB is based on PostgreSQL 15, does not support higher version features Some T-SQL syntax may have compatibility differences, refer to Babelfish compatibility documentation Must use md5 authentication method (not scram-sha-256) ","categories":["Reference"],"description":"WiltonDB / Babelfish kernel, provides Microsoft SQL Server protocol and syntax compatibility","excerpt":"WiltonDB / Babelfish kernel, provides Microsoft SQL Server protocol …","ref":"/docs/conf/mssql/","tags":"","title":"mssql"},{"body":"The polar configuration template uses Alibaba Cloud’s PolarDB for PostgreSQL database kernel instead of native PostgreSQL, providing “cloud-native” Aurora-style storage-compute separation capability.\nFor the complete tutorial, see: PolarDB for PostgreSQL (POLAR) Kernel Guide\nOverview Config Name: polar Node Count: Single node Description: Uses PolarDB for PostgreSQL kernel OS Distro: el8, el9, el10, d12, d13, u22, u24 OS Arch: x86_64 Related: meta Usage:\n./configure -c polar [-i \u003cprimary_ip\u003e] Content Source: pigsty/conf/polar.yml\n--- #==============================================================# # File : polar.yml # Desc : Pigsty 1-node PolarDB Kernel Config Template # Ctime : 2020-08-05 # Mtime : 2025-12-28 # Docs : https://doc.pgsty.com/config # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# # This is the config template for PolarDB PG Kernel, # Which is a PostgreSQL 15 fork with RAC flavor features # tutorial: https://doc.pgsty.com/pgsql/kernel/polardb # # Usage: # curl https://repo.pigsty.io/get | bash # ./configure -c polar # ./deploy.yml all: children: infra: { hosts: { 10.10.10.10: { infra_seq: 1 }} ,vars: { repo_enabled: false }} etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 }} ,vars: { etcd_cluster: etcd }} #minio: { hosts: { 10.10.10.10: { minio_seq: 1 }} ,vars: { minio_cluster: minio }} #----------------------------------------------# # PolarDB Database Cluster #----------------------------------------------# pg-meta: hosts: 10.10.10.10: { pg_seq: 1, pg_role: primary } vars: pg_cluster: pg-meta pg_users: - {name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [dbrole_admin] ,comment: pigsty admin user } - {name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [dbrole_readonly] ,comment: read-only viewer for meta database } pg_databases: - {name: meta ,baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [pigsty]} pg_hba_rules: - {user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes'} node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] # make a full backup every 1am # PolarDB Ad Hoc Settings pg_version: 15 # PolarDB PG is based on PG 15 pg_mode: polar # PolarDB PG Compatible mode pg_packages: [ polardb, pgsql-common ] # Replace PG kernel with PolarDB kernel pg_exporter_exclude_database: 'template0,template1,postgres,polardb_admin' pg_default_roles: # PolarDB require replicator as superuser - { name: dbrole_readonly ,login: false ,comment: role for global read-only access } - { name: dbrole_offline ,login: false ,comment: role for restricted read-only access } - { name: dbrole_readwrite ,login: false ,roles: [dbrole_readonly] ,comment: role for global read-write access } - { name: dbrole_admin ,login: false ,roles: [pg_monitor, dbrole_readwrite] ,comment: role for object creation } - { name: postgres ,superuser: true ,comment: system superuser } - { name: replicator ,superuser: true ,replication: true ,roles: [pg_monitor, dbrole_readonly] ,comment: system replicator } # \u003c- superuser is required for replication - { name: dbuser_dba ,superuser: true ,roles: [dbrole_admin] ,pgbouncer: true ,pool_mode: session, pool_connlimit: 16 ,comment: pgsql admin user } - { name: dbuser_monitor ,roles: [pg_monitor] ,pgbouncer: true ,parameters: {log_min_duration_statement: 1000 } ,pool_mode: session ,pool_connlimit: 8 ,comment: pgsql monitor user } vars: # global variables #----------------------------------------------# # INFRA : https://doc.pgsty.com/infra/param #----------------------------------------------# version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default,china,europe infra_portal: # infra services exposed via portal home : { domain: i.pigsty } # default domain name #----------------------------------------------# # NODE : https://doc.pgsty.com/node/param #----------------------------------------------# nodename_overwrite: false # do not overwrite node hostname on single node mode node_repo_modules: node,infra,pgsql # add these repos directly to the singleton node node_tune: oltp # node tuning specs: oltp,olap,tiny,crit #----------------------------------------------# # PGSQL : https://doc.pgsty.com/pgsql/param #----------------------------------------------# pg_version: 15 # PolarDB is compatible with PG 15 pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root ... Explanation The polar template uses Alibaba Cloud’s open-source PolarDB for PostgreSQL kernel, providing cloud-native database capabilities.\nKey Features:\nStorage-compute separation architecture, compute and storage nodes can scale independently Supports one-write-multiple-read, read replicas scale in seconds Compatible with PostgreSQL ecosystem, maintains SQL compatibility Supports shared storage scenarios, suitable for cloud environment deployment Use Cases:\nCloud-native scenarios requiring storage-compute separation architecture Read-heavy write-light workloads Scenarios requiring quick scaling of read replicas Test environments for evaluating PolarDB features Notes:\nPolarDB is based on PostgreSQL 15, does not support higher version features Replication user requires superuser privileges (different from native PostgreSQL) Some PostgreSQL extensions may have compatibility issues ARM64 architecture not supported ","categories":["Reference"],"description":"PolarDB for PostgreSQL kernel, provides Aurora-style storage-compute separation capability","excerpt":"PolarDB for PostgreSQL kernel, provides Aurora-style storage-compute …","ref":"/docs/conf/polar/","tags":"","title":"polar"},{"body":"The ivory configuration template uses Highgo’s IvorySQL database kernel instead of native PostgreSQL, providing Oracle syntax and PL/SQL compatibility.\nFor the complete tutorial, see: IvorySQL (Oracle Compatible) Kernel Guide\nOverview Config Name: ivory Node Count: Single node Description: Uses IvorySQL Oracle-compatible kernel OS Distro: el8, el9, el10, d12, d13, u22, u24 OS Arch: x86_64, aarch64 Related: meta Usage:\n./configure -c ivory [-i \u003cprimary_ip\u003e] Content Source: pigsty/conf/ivory.yml\n--- #==============================================================# # File : ivory.yml # Desc : IvorySQL 4 (Oracle Compatible) template # Ctime : 2024-08-05 # Mtime : 2025-12-28 # Docs : https://doc.pgsty.com/pgsql/kernel/ivorysql # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# # This is the config template for IvorySQL 5 Kernel, # Which is a PostgreSQL 18 fork with Oracle Compatibility # tutorial: https://doc.pgsty.com/pgsql/kernel/ivorysql # # Usage: # curl https://repo.pigsty.io/get | bash # ./configure -c ivory # ./deploy.yml all: children: infra: { hosts: { 10.10.10.10: { infra_seq: 1 }} ,vars: { repo_enabled: false }} etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 }} ,vars: { etcd_cluster: etcd }} #minio: { hosts: { 10.10.10.10: { minio_seq: 1 }} ,vars: { minio_cluster: minio }} #----------------------------------------------# # IvorySQL Database Cluster #----------------------------------------------# pg-meta: hosts: 10.10.10.10: { pg_seq: 1, pg_role: primary } vars: pg_cluster: pg-meta pg_users: - {name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [dbrole_admin] ,comment: pigsty admin user } - {name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [dbrole_readonly] ,comment: read-only viewer for meta database } pg_databases: - {name: meta ,baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [pigsty]} pg_hba_rules: - {user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes'} node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] # make a full backup every 1am # IvorySQL Ad Hoc Settings pg_mode: ivory # Use IvorySQL Oracle Compatible Mode pg_packages: [ ivorysql, pgsql-common ] # install IvorySQL instead of postgresql kernel pg_libs: 'liboracle_parser, pg_stat_statements, auto_explain' # pre-load oracle parser vars: # global variables #----------------------------------------------# # INFRA : https://doc.pgsty.com/infra/param #----------------------------------------------# version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default,china,europe infra_portal: # infra services exposed via portal home : { domain: i.pigsty } # default domain name #----------------------------------------------# # NODE : https://doc.pgsty.com/node/param #----------------------------------------------# nodename_overwrite: false # do not overwrite node hostname on single node mode node_repo_modules: node,infra,pgsql # add these repos directly to the singleton node node_tune: oltp # node tuning specs: oltp,olap,tiny,crit #----------------------------------------------# # PGSQL : https://doc.pgsty.com/pgsql/param #----------------------------------------------# pg_version: 18 # IvorySQL kernel is compatible with postgres 18 pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root ... Explanation The ivory template uses Highgo’s open-source IvorySQL kernel, providing Oracle database compatibility.\nKey Features:\nSupports Oracle PL/SQL syntax Compatible with Oracle data types (NUMBER, VARCHAR2, etc.) Supports Oracle-style packages Retains all standard PostgreSQL functionality Use Cases:\nMigrating from Oracle to PostgreSQL Applications needing both Oracle and PostgreSQL syntax support Leveraging PostgreSQL ecosystem while maintaining PL/SQL compatibility Test environments for evaluating IvorySQL features Notes:\nIvorySQL 4 is based on PostgreSQL 18 Using liboracle_parser requires loading into shared_preload_libraries pgbackrest may have checksum issues in Oracle-compatible mode, PITR capability is limited Only supports EL8/EL9 systems, Debian/Ubuntu not supported ","categories":["Reference"],"description":"IvorySQL kernel, provides Oracle syntax and PL/SQL compatibility","excerpt":"IvorySQL kernel, provides Oracle syntax and PL/SQL compatibility","ref":"/docs/conf/ivory/","tags":"","title":"ivory"},{"body":"The mysql configuration template uses OpenHalo database kernel instead of native PostgreSQL, providing MySQL wire protocol and SQL syntax compatibility.\nOverview Config Name: mysql Node Count: Single node Description: OpenHalo MySQL-compatible kernel configuration OS Distro: el8, el9, el10, d12, d13, u22, u24 OS Arch: x86_64 Related: meta Usage:\n./configure -c mysql [-i \u003cprimary_ip\u003e] Content Source: pigsty/conf/mysql.yml\n--- #==============================================================# # File : mysql.yml # Desc : 1-node OpenHaloDB (MySQL Compatible) template # Ctime : 2025-04-03 # Mtime : 2025-12-28 # Docs : https://doc.pgsty.com/config # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# # This is the config template for OpenHalo PG Kernel, # Which is a PostgreSQL 14 fork with MySQL Wire Compatibility # tutorial: https://doc.pgsty.com/pgsql/kernel/openhalo # # Usage: # curl https://repo.pigsty.io/get | bash # ./configure -c mysql # ./deploy.yml all: children: infra: { hosts: { 10.10.10.10: { infra_seq: 1 }} ,vars: { repo_enabled: false }} etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 }} ,vars: { etcd_cluster: etcd }} #minio: { hosts: { 10.10.10.10: { minio_seq: 1 }} ,vars: { minio_cluster: minio }} #----------------------------------------------# # OpenHalo Database Cluster #----------------------------------------------# # connect with mysql client: mysql -h 10.10.10.10 -u dbuser_meta -D mysql (the actual database is 'postgres', and 'mysql' is a schema) pg-meta: hosts: 10.10.10.10: { pg_seq: 1, pg_role: primary } vars: pg_cluster: pg-meta pg_users: - {name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [dbrole_admin] ,comment: pigsty admin user } - {name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [dbrole_readonly] ,comment: read-only viewer for meta database } pg_databases: - {name: postgres, extensions: [aux_mysql]} # the mysql compatible database - {name: meta ,baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [pigsty]} pg_hba_rules: - {user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes'} node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] # make a full backup every 1am # OpenHalo Ad Hoc Setting pg_mode: mysql # MySQL Compatible Mode by HaloDB pg_version: 14 # The current HaloDB is compatible with PG Major Version 14 pg_packages: [ openhalodb, pgsql-common ] # install openhalodb instead of postgresql kernel vars: #----------------------------------------------# # INFRA : https://doc.pgsty.com/infra/param #----------------------------------------------# version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default,china,europe infra_portal: # infra services exposed via portal home : { domain: i.pigsty } # default domain name #----------------------------------------------# # NODE : https://doc.pgsty.com/node/param #----------------------------------------------# nodename_overwrite: false # do not overwrite node hostname on single node mode node_repo_modules: node,infra,pgsql # add these repos directly to the singleton node node_tune: oltp # node tuning specs: oltp,olap,tiny,crit #----------------------------------------------# # PGSQL : https://doc.pgsty.com/pgsql/param #----------------------------------------------# pg_version: 14 # OpenHalo is compatible with PG 14 pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root ... Explanation The mysql template uses the OpenHalo kernel, allowing you to connect to PostgreSQL using MySQL client tools.\nKey Features:\nUses MySQL protocol (port 3306), compatible with MySQL clients Supports a subset of MySQL SQL syntax Retains PostgreSQL’s ACID properties and storage engine Supports both PostgreSQL and MySQL protocol connections simultaneously Connection Methods:\n# Using MySQL client mysql -h 10.10.10.10 -P 3306 -u dbuser_meta -pDBUser.Meta # Also retains PostgreSQL connection capability psql postgres://dbuser_meta:DBUser.Meta@10.10.10.10:5432/meta Use Cases:\nMigrating from MySQL to PostgreSQL Applications needing to support both MySQL and PostgreSQL clients Leveraging PostgreSQL ecosystem while maintaining MySQL compatibility Notes:\nOpenHalo is based on PostgreSQL 14, does not support higher version features Some MySQL syntax may have compatibility differences Only supports EL8/EL9 systems ARM64 architecture not supported ","categories":["Reference"],"description":"OpenHalo kernel, provides MySQL protocol and syntax compatibility","excerpt":"OpenHalo kernel, provides MySQL protocol and syntax compatibility","ref":"/docs/conf/mysql/","tags":"","title":"mysql"},{"body":"The pgtde configuration template uses Percona PostgreSQL database kernel, providing Transparent Data Encryption (TDE) capability.\nOverview Config Name: pgtde Node Count: Single node Description: Percona PostgreSQL transparent data encryption configuration OS Distro: el8, el9, el10, d12, d13, u22, u24 OS Arch: x86_64 Related: meta Usage:\n./configure -c pgtde [-i \u003cprimary_ip\u003e] Content Source: pigsty/conf/pgtde.yml\n--- #==============================================================# # File : pgtde.yml # Desc : PG TDE with Percona PostgreSQL 1-node template # Ctime : 2025-07-04 # Mtime : 2025-12-28 # Docs : https://doc.pgsty.com/config # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# # This is the config template for Percona PostgreSQL Distribution # With pg_tde extension, which is compatible with PostgreSQL 18.1 # tutorial: https://doc.pgsty.com/pgsql/kernel/percona # # Usage: # curl https://repo.pigsty.io/get | bash # ./configure -c pgtde # ./deploy.yml all: children: infra: { hosts: { 10.10.10.10: { infra_seq: 1 }} ,vars: { repo_enabled: false }} etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 }} ,vars: { etcd_cluster: etcd }} #minio: { hosts: { 10.10.10.10: { minio_seq: 1 }} ,vars: { minio_cluster: minio }} #----------------------------------------------# # Percona Postgres Database Cluster #----------------------------------------------# pg-meta: hosts: 10.10.10.10: { pg_seq: 1, pg_role: primary } vars: pg_cluster: pg-meta pg_users: - { name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [dbrole_admin ] ,comment: pigsty admin user } - { name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [dbrole_readonly] ,comment: read-only viewer } pg_databases: - name: meta baseline: cmdb.sql comment: pigsty tde database schemas: [pigsty] extensions: [ vector, postgis, pg_tde ,pgaudit, { name: pg_stat_monitor, schema: monitor } ] pg_hba_rules: - { user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes' } node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] # make a full backup every 1am # Percona PostgreSQL TDE Ad Hoc Settings pg_packages: [ percona-main, pgsql-common ] # install percona postgres packages pg_libs: 'pg_tde, pgaudit, pg_stat_statements, pg_stat_monitor, auto_explain' vars: #----------------------------------------------# # INFRA : https://doc.pgsty.com/infra/param #----------------------------------------------# version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default,china,europe infra_portal: # infra services exposed via portal home : { domain: i.pigsty } # default domain name #----------------------------------------------# # NODE : https://doc.pgsty.com/node/param #----------------------------------------------# nodename_overwrite: false # do not overwrite node hostname on single node mode node_repo_modules: node,infra,pgsql,percona node_tune: oltp #----------------------------------------------# # PGSQL : https://doc.pgsty.com/pgsql/param #----------------------------------------------# pg_version: 18 # Default Percona TDE PG Major Version is 18 pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root ... Explanation The pgtde template uses Percona PostgreSQL kernel, providing enterprise-grade transparent data encryption capability.\nKey Features:\nTransparent Data Encryption: Data automatically encrypted on disk, transparent to applications Key Management: Supports local keys and external Key Management Systems (KMS) Table-level Encryption: Selectively encrypt sensitive tables Full Compatibility: Fully compatible with native PostgreSQL Use Cases:\nMeeting data security compliance requirements (e.g., PCI-DSS, HIPAA) Storing sensitive data (e.g., personal information, financial data) Scenarios requiring data-at-rest encryption Enterprise environments with strict data security requirements Usage:\n-- Create encrypted table CREATE TABLE sensitive_data ( id SERIAL PRIMARY KEY, ssn VARCHAR(11) ) USING pg_tde; -- Or enable encryption on existing table ALTER TABLE existing_table SET ACCESS METHOD pg_tde; Notes:\nPercona PostgreSQL is based on PostgreSQL 18 Encryption brings some performance overhead (typically 5-15%) Encryption keys must be properly managed ARM64 architecture not supported ","categories":["Reference"],"description":"Percona PostgreSQL kernel, provides Transparent Data Encryption (pg_tde) capability","excerpt":"Percona PostgreSQL kernel, provides Transparent Data Encryption …","ref":"/docs/conf/pgtde/","tags":"","title":"pgtde"},{"body":"The oriole configuration template uses OrioleDB storage engine instead of PostgreSQL’s default Heap storage, providing bloat-free, high-performance OLTP capability.\nOverview Config Name: oriole Node Count: Single node Description: OrioleDB bloat-free storage engine configuration OS Distro: el8, el9, el10, d12, d13, u22, u24 OS Arch: x86_64 Related: meta Usage:\n./configure -c oriole [-i \u003cprimary_ip\u003e] Content Source: pigsty/conf/oriole.yml\n--- #==============================================================# # File : oriole.yml # Desc : 1-node OrioleDB (OLTP Enhancement) template # Ctime : 2025-04-05 # Mtime : 2025-12-28 # Docs : https://doc.pgsty.com/config # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# # This is the config template for OrioleDB Kernel, # Which is a Patched PostgreSQL 17 fork without bloat # tutorial: https://doc.pgsty.com/pgsql/kernel/oriole # # Usage: # curl https://repo.pigsty.io/get | bash # ./configure -c oriole # ./deploy.yml all: children: infra: { hosts: { 10.10.10.10: { infra_seq: 1 }} ,vars: { repo_enabled: false }} etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 }} ,vars: { etcd_cluster: etcd }} #minio: { hosts: { 10.10.10.10: { minio_seq: 1 }} ,vars: { minio_cluster: minio }} #----------------------------------------------# # OrioleDB Database Cluster #----------------------------------------------# pg-meta: hosts: 10.10.10.10: { pg_seq: 1, pg_role: primary } vars: pg_cluster: pg-meta pg_users: - {name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [dbrole_admin] ,comment: pigsty admin user } - {name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [dbrole_readonly] ,comment: read-only viewer for meta database } pg_databases: - {name: meta ,baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [pigsty], extensions: [orioledb]} pg_hba_rules: - {user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes'} node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] # make a full backup every 1am # OrioleDB Ad Hoc Settings pg_mode: oriole # oriole compatible mode pg_packages: [ oriole, pgsql-common ] # install OrioleDB kernel pg_libs: 'orioledb, pg_stat_statements, auto_explain' # Load OrioleDB Extension vars: # global variables #----------------------------------------------# # INFRA : https://doc.pgsty.com/infra/param #----------------------------------------------# version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default,china,europe infra_portal: # infra services exposed via portal home : { domain: i.pigsty } # default domain name #----------------------------------------------# # NODE : https://doc.pgsty.com/node/param #----------------------------------------------# nodename_overwrite: false # do not overwrite node hostname on single node mode node_repo_modules: node,infra,pgsql # add these repos directly to the singleton node node_tune: oltp # node tuning specs: oltp,olap,tiny,crit #----------------------------------------------# # PGSQL : https://doc.pgsty.com/pgsql/param #----------------------------------------------# pg_version: 17 # OrioleDB Kernel is based on PG 17 pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root ... Explanation The oriole template uses OrioleDB storage engine, fundamentally solving PostgreSQL table bloat problems.\nKey Features:\nBloat-free Design: Uses UNDO logs instead of Multi-Version Concurrency Control (MVCC) No VACUUM Required: Eliminates performance jitter from autovacuum Row-level WAL: More efficient logging and replication Compressed Storage: Built-in data compression, reduces storage space Use Cases:\nHigh-frequency update OLTP workloads Applications sensitive to write latency Need for stable response times (eliminates VACUUM impact) Large tables with frequent updates causing bloat Usage:\n-- Create table using OrioleDB storage CREATE TABLE orders ( id SERIAL PRIMARY KEY, customer_id INT, amount DECIMAL(10,2) ) USING orioledb; -- Existing tables cannot be directly converted, need to be rebuilt Notes:\nOrioleDB is based on PostgreSQL 17 Need to add orioledb to shared_preload_libraries Some PostgreSQL features may not be fully supported ARM64 architecture not supported ","categories":["Reference"],"description":"OrioleDB kernel, provides bloat-free OLTP enhanced storage engine","excerpt":"OrioleDB kernel, provides bloat-free OLTP enhanced storage engine","ref":"/docs/conf/oriole/","tags":"","title":"oriole"},{"body":"The supabase configuration template provides a reference configuration for self-hosting Supabase, using Pigsty-managed PostgreSQL as the underlying storage.\nFor more details, see Supabase Self-Hosting Tutorial\nOverview Config Name: supabase Node Count: Single node Description: Self-host Supabase using Pigsty-managed PostgreSQL OS Distro: el8, el9, el10, d12, d13, u22, u24 OS Arch: x86_64, aarch64 Related: meta, rich Usage:\n./configure -c supabase [-i \u003cprimary_ip\u003e] Content Source: pigsty/conf/supabase.yml\n--- #==============================================================# # File : supabase.yml # Desc : Pigsty configuration for self-hosting supabase # Ctime : 2023-09-19 # Mtime : 2025-12-28 # Docs : https://doc.pgsty.com/config # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# # supabase is available on el8/el9/u22/u24/d12 with pg15,16,17,18 # tutorial: https://doc.pgsty.com/app/supabase # Usage: # curl https://repo.pigsty.io/get | bash # install pigsty # ./configure -c supabase # use this supabase conf template # ./deploy.yml # install pigsty \u0026 pgsql \u0026 minio # ./docker.yml # install docker \u0026 docker compose # ./app.yml # launch supabase with docker compose all: children: #----------------------------------------------# # INFRA : https://doc.pgsty.com/infra #----------------------------------------------# infra: hosts: 10.10.10.10: { infra_seq: 1 } vars: repo_enabled: false # disable local repo #----------------------------------------------# # ETCD : https://doc.pgsty.com/etcd #----------------------------------------------# etcd: hosts: 10.10.10.10: { etcd_seq: 1 } vars: etcd_cluster: etcd etcd_safeguard: false # enable to prevent purging running etcd instance #----------------------------------------------# # MINIO : https://doc.pgsty.com/minio #----------------------------------------------# minio: hosts: 10.10.10.10: { minio_seq: 1 } vars: minio_cluster: minio minio_users: # list of minio user to be created - { access_key: pgbackrest ,secret_key: S3User.Backup ,policy: pgsql } - { access_key: s3user_meta ,secret_key: S3User.Meta ,policy: meta } - { access_key: s3user_data ,secret_key: S3User.Data ,policy: data } #----------------------------------------------# # PostgreSQL cluster for Supabase self-hosting #----------------------------------------------# pg-meta: hosts: 10.10.10.10: { pg_seq: 1, pg_role: primary } vars: pg_cluster: pg-meta pg_users: # supabase roles: anon, authenticated, dashboard_user - { name: anon ,login: false } - { name: authenticated ,login: false } - { name: dashboard_user ,login: false ,replication: true ,createdb: true ,createrole: true } - { name: service_role ,login: false ,bypassrls: true } # supabase users: please use the same password - { name: supabase_admin ,password: 'DBUser.Supa' ,pgbouncer: true ,inherit: true ,roles: [ dbrole_admin ] ,superuser: true ,replication: true ,createdb: true ,createrole: true ,bypassrls: true } - { name: authenticator ,password: 'DBUser.Supa' ,pgbouncer: true ,inherit: false ,roles: [ dbrole_admin, authenticated ,anon ,service_role ] } - { name: supabase_auth_admin ,password: 'DBUser.Supa' ,pgbouncer: true ,inherit: false ,roles: [ dbrole_admin ] ,createrole: true } - { name: supabase_storage_admin ,password: 'DBUser.Supa' ,pgbouncer: true ,inherit: false ,roles: [ dbrole_admin, authenticated ,anon ,service_role ] ,createrole: true } - { name: supabase_functions_admin ,password: 'DBUser.Supa' ,pgbouncer: true ,inherit: false ,roles: [ dbrole_admin ] ,createrole: true } - { name: supabase_replication_admin ,password: 'DBUser.Supa' ,replication: true ,roles: [ dbrole_admin ]} - { name: supabase_etl_admin ,password: 'DBUser.Supa' ,replication: true ,roles: [ pg_read_all_data ]} - { name: supabase_read_only_user ,password: 'DBUser.Supa' ,bypassrls: true ,roles: [ pg_read_all_data, dbrole_readonly ]} pg_databases: - name: postgres baseline: supabase.sql owner: supabase_admin comment: supabase postgres database schemas: [ extensions ,auth ,realtime ,storage ,graphql_public ,supabase_functions ,_analytics ,_realtime ] extensions: - { name: pgcrypto ,schema: extensions } # cryptographic functions - { name: pg_net ,schema: extensions } # async HTTP - { name: pgjwt ,schema: extensions } # json web token API for postgres - { name: uuid-ossp ,schema: extensions } # generate universally unique identifiers (UUIDs) - { name: pgsodium ,schema: extensions } # pgsodium is a modern cryptography library for Postgres. - { name: supabase_vault ,schema: extensions } # Supabase Vault Extension - { name: pg_graphql ,schema: extensions } # pg_graphql: GraphQL support - { name: pg_jsonschema ,schema: extensions } # pg_jsonschema: Validate json schema - { name: wrappers ,schema: extensions } # wrappers: FDW collections - { name: http ,schema: extensions } # http: allows web page retrieval inside the database. - { name: pg_cron ,schema: extensions } # pg_cron: Job scheduler for PostgreSQL - { name: timescaledb ,schema: extensions } # timescaledb: Enables scalable inserts and complex queries for time-series data - { name: pg_tle ,schema: extensions } # pg_tle: Trusted Language Extensions for PostgreSQL - { name: vector ,schema: extensions } # pgvector: the vector similarity search - { name: pgmq ,schema: extensions } # pgmq: A lightweight message queue like AWS SQS and RSMQ - { name: supabase ,owner: supabase_admin ,comment: supabase analytics database ,schemas: [ extensions, _analytics ] } # supabase required extensions pg_libs: 'timescaledb, pgsodium, plpgsql, plpgsql_check, pg_cron, pg_net, pg_stat_statements, auto_explain, pg_wait_sampling, pg_tle, plan_filter' pg_extensions: [ pg18-main ,pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-olap ,pg18-feat ,pg18-lang ,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl] pg_parameters: { cron.database_name: postgres } pg_hba_rules: # supabase hba rules, require access from docker network - { user: all ,db: postgres ,addr: intra ,auth: pwd ,title: 'allow supabase access from intranet' } - { user: all ,db: postgres ,addr: 172.17.0.0/16 ,auth: pwd ,title: 'allow access from local docker network' } node_crontab: - '00 01 * * * postgres /pg/bin/pg-backup full' # make a full backup every 1am - '* * * * * postgres /pg/bin/supa-kick' # kick supabase _analytics lag per minute: https://github.com/pgsty/pigsty/issues/581 #----------------------------------------------# # Supabase #----------------------------------------------# # ./docker.yml # ./app.yml # the supabase stateless containers (default username \u0026 password: supabase/pigsty) supabase: hosts: 10.10.10.10: {} vars: docker_enabled: true # enable docker on this group #docker_registry_mirrors: [\"https://docker.1panel.live\",\"https://docker.1ms.run\",\"https://docker.xuanyuan.me\",\"https://registry-1.docker.io\"] app: supabase # specify app name (supa) to be installed (in the apps) apps: # define all applications supabase: # the definition of supabase app conf: # override /opt/supabase/.env # IMPORTANT: CHANGE JWT_SECRET AND REGENERATE CREDENTIAL ACCORDING!!!!!!!!!!! # https://supabase.com/docs/guides/self-hosting/docker#securing-your-services JWT_SECRET: your-super-secret-jwt-token-with-at-least-32-characters-long ANON_KEY: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJhbm9uIiwKICAgICJpc3MiOiAic3VwYWJhc2UtZGVtbyIsCiAgICAiaWF0IjogMTY0MTc2OTIwMCwKICAgICJleHAiOiAxNzk5NTM1NjAwCn0.dc_X5iR_VP_qT0zsiyj_I_OZ2T9FtRU2BBNWN8Bu4GE SERVICE_ROLE_KEY: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJzZXJ2aWNlX3JvbGUiLAogICAgImlzcyI6ICJzdXBhYmFzZS1kZW1vIiwKICAgICJpYXQiOiAxNjQxNzY5MjAwLAogICAgImV4cCI6IDE3OTk1MzU2MDAKfQ.DaYlNEoUrrEn2Ig7tqibS-PHK5vgusbcbo7X36XVt4Q PG_META_CRYPTO_KEY: your-encryption-key-32-chars-min DASHBOARD_USERNAME: supabase DASHBOARD_PASSWORD: pigsty # 32~64 random characters string for logflare LOGFLARE_PUBLIC_ACCESS_TOKEN: 1234567890abcdef1234567890abcdef LOGFLARE_PRIVATE_ACCESS_TOKEN: fedcba0987654321fedcba0987654321 # postgres connection string (use the correct ip and port) POSTGRES_HOST: 10.10.10.10 # point to the local postgres node POSTGRES_PORT: 5436 # access via the 'default' service, which always route to the primary postgres POSTGRES_DB: postgres # the supabase underlying database POSTGRES_PASSWORD: DBUser.Supa # password for supabase_admin and multiple supabase users # expose supabase via domain name SITE_URL: https://supa.pigsty # \u003c------- Change This to your external domain name API_EXTERNAL_URL: https://supa.pigsty # \u003c------- Otherwise the storage api may not work! SUPABASE_PUBLIC_URL: https://supa.pigsty # \u003c------- DO NOT FORGET TO PUT IT IN infra_portal! # if using s3/minio as file storage S3_BUCKET: data S3_ENDPOINT: https://sss.pigsty:9000 S3_ACCESS_KEY: s3user_data S3_SECRET_KEY: S3User.Data S3_FORCE_PATH_STYLE: true S3_PROTOCOL: https S3_REGION: stub MINIO_DOMAIN_IP: 10.10.10.10 # sss.pigsty domain name will resolve to this ip statically # if using SMTP (optional) #SMTP_ADMIN_EMAIL: admin@example.com #SMTP_HOST: supabase-mail #SMTP_PORT: 2500 #SMTP_USER: fake_mail_user #SMTP_PASS: fake_mail_password #SMTP_SENDER_NAME: fake_sender #ENABLE_ANONYMOUS_USERS: false #==============================================================# # Global Parameters #==============================================================# vars: #----------------------------------------------# # INFRA : https://doc.pgsty.com/infra #----------------------------------------------# version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default|china|europe proxy_env: # global proxy env when downloading packages no_proxy: \"localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,*.pigsty,*.aliyun.com,mirrors.*,*.myqcloud.com,*.tsinghua.edu.cn\" # http_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com # https_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com # all_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com certbot_sign: false # enable certbot to sign https certificate for infra portal certbot_email: your@email.com # replace your email address to receive expiration notice infra_portal: # infra services exposed via portal home : { domain: i.pigsty } # default domain name pgadmin : { domain: adm.pigsty ,endpoint: \"${admin_ip}:8885\" } bytebase : { domain: ddl.pigsty ,endpoint: \"${admin_ip}:8887\" } #minio : { domain: m.pigsty ,endpoint: \"${admin_ip}:9001\" ,scheme: https ,websocket: true } # Nginx / Domain / HTTPS : https://doc.pgsty.com/admin/portal supa : # nginx server config for supabase domain: supa.pigsty # REPLACE IT WITH YOUR OWN DOMAIN! endpoint: \"10.10.10.10:8000\" # supabase service endpoint: IP:PORT websocket: true # add websocket support certbot: supa.pigsty # certbot cert name, apply with `make cert` #----------------------------------------------# # NODE : https://doc.pgsty.com/node/param #----------------------------------------------# nodename_overwrite: false # do not overwrite node hostname on single node mode node_tune: oltp # node tuning specs: oltp,olap,tiny,crit node_etc_hosts: # add static domains to all nodes /etc/hosts - 10.10.10.10 i.pigsty sss.pigsty supa.pigsty node_repo_modules: node,pgsql,infra # use pre-made local repo rather than install from upstream node_repo_remove: true # remove existing node repo for node managed by pigsty #node_packages: [openssh-server] # packages to be installed current nodes with latest version #node_timezone: Asia/Hong_Kong # overwrite node timezone #----------------------------------------------# # PGSQL : https://doc.pgsty.com/pgsql/param #----------------------------------------------# pg_version: 18 # default postgres version pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml pg_safeguard: false # prevent purging running postgres instance? pg_default_schemas: [ monitor, extensions ] # add new schema: exxtensions pg_default_extensions: # default extensions to be created - { name: pg_stat_statements ,schema: monitor } - { name: pgstattuple ,schema: monitor } - { name: pg_buffercache ,schema: monitor } - { name: pageinspect ,schema: monitor } - { name: pg_prewarm ,schema: monitor } - { name: pg_visibility ,schema: monitor } - { name: pg_freespacemap ,schema: monitor } - { name: pg_wait_sampling ,schema: monitor } # move default extensions to `extensions` schema for supabase - { name: postgres_fdw ,schema: extensions } - { name: file_fdw ,schema: extensions } - { name: btree_gist ,schema: extensions } - { name: btree_gin ,schema: extensions } - { name: pg_trgm ,schema: extensions } - { name: intagg ,schema: extensions } - { name: intarray ,schema: extensions } - { name: pg_repack ,schema: extensions } #----------------------------------------------# # BACKUP : https://doc.pgsty.com/pgsql/backup #----------------------------------------------# minio_endpoint: https://sss.pigsty:9000 # explicit overwrite minio endpoint with haproxy port pgbackrest_method: minio # pgbackrest repo method: local,minio,[user-defined...] pgbackrest_repo: # pgbackrest repo: https://pgbackrest.org/configuration.html#section-repository local: # default pgbackrest repo with local posix fs path: /pg/backup # local backup directory, `/pg/backup` by default retention_full_type: count # retention full backups by count retention_full: 2 # keep 2, at most 3 full backups when using local fs repo minio: # optional minio repo for pgbackrest type: s3 # minio is s3-compatible, so s3 is used s3_endpoint: sss.pigsty # minio endpoint domain name, `sss.pigsty` by default s3_region: us-east-1 # minio region, us-east-1 by default, useless for minio s3_bucket: pgsql # minio bucket name, `pgsql` by default s3_key: pgbackrest # minio user access key for pgbackrest s3_key_secret: S3User.Backup # minio user secret key for pgbackrest \u003c------------------ HEY, DID YOU CHANGE THIS? s3_uri_style: path # use path style uri for minio rather than host style path: /pgbackrest # minio backup path, default is `/pgbackrest` storage_port: 9000 # minio port, 9000 by default storage_ca_file: /etc/pki/ca.crt # minio ca file path, `/etc/pki/ca.crt` by default block: y # Enable block incremental backup bundle: y # bundle small files into a single file bundle_limit: 20MiB # Limit for file bundles, 20MiB for object storage bundle_size: 128MiB # Target size for file bundles, 128MiB for object storage cipher_type: aes-256-cbc # enable AES encryption for remote backup repo cipher_pass: pgBackRest # AES encryption password, default is 'pgBackRest' \u003c----- HEY, DID YOU CHANGE THIS? retention_full_type: time # retention full backup by time on minio repo retention_full: 14 # keep full backup for the last 14 days s3: # you can use cloud object storage as backup repo type: s3 # Add your object storage credentials here! s3_endpoint: oss-cn-beijing-internal.aliyuncs.com s3_region: oss-cn-beijing s3_bucket: \u003cyour_bucket_name\u003e s3_key: \u003cyour_access_key\u003e s3_key_secret: \u003cyour_secret_key\u003e s3_uri_style: host path: /pgbackrest bundle: y # bundle small files into a single file bundle_limit: 20MiB # Limit for file bundles, 20MiB for object storage bundle_size: 128MiB # Target size for file bundles, 128MiB for object storage cipher_type: aes-256-cbc # enable AES encryption for remote backup repo cipher_pass: pgBackRest # AES encryption password, default is 'pgBackRest' retention_full_type: time # retention full backup by time on minio repo retention_full: 14 # keep full backup for the last 14 days #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root ... Installation Demo Explanation The supabase template provides a complete self-hosted Supabase solution, allowing you to run this open-source Firebase alternative on your own infrastructure.\nArchitecture:\nPostgreSQL: Production-grade Pigsty-managed PostgreSQL (with HA support) Docker Containers: Supabase stateless services (Auth, Storage, Realtime, Edge Functions, etc.) MinIO: S3-compatible object storage for file storage and PostgreSQL backup Nginx: Reverse proxy and HTTPS termination Key Features:\nUses Pigsty-managed PostgreSQL instead of Supabase’s built-in database container Supports PostgreSQL high availability (can be expanded to three-node cluster) Installs all Supabase-required extensions (pg_net, pgjwt, pg_graphql, vector, etc.) Integrated MinIO object storage for file uploads and backups HTTPS support with Let’s Encrypt automatic certificates Deployment Steps:\ncurl https://repo.pigsty.io/get | bash # Download Pigsty ./configure -c supabase # Use supabase config template ./deploy.yml # Install Pigsty, PostgreSQL, MinIO ./docker.yml # Install Docker ./app.yml # Start Supabase containers Access:\n# Supabase Studio https://supa.pigsty (username: supabase, password: pigsty) # Direct PostgreSQL connection psql postgres://supabase_admin:DBUser.Supa@10.10.10.10:5432/postgres Use Cases:\nNeed to self-host BaaS (Backend as a Service) platform Want full control over data and infrastructure Need enterprise-grade PostgreSQL HA and backups Compliance or cost concerns with Supabase cloud service Notes:\nMust change JWT_SECRET: Use at least 32-character random string, and regenerate ANON_KEY and SERVICE_ROLE_KEY Configure proper domain names (SITE_URL, API_EXTERNAL_URL) Production environments should enable HTTPS (can use certbot for auto certificates) Docker network needs access to PostgreSQL (172.17.0.0/16 HBA rule configured) ","categories":["Reference"],"description":"Self-host Supabase using Pigsty-managed PostgreSQL, an open-source Firebase alternative","excerpt":"Self-host Supabase using Pigsty-managed PostgreSQL, an open-source …","ref":"/docs/conf/supabase/","tags":"","title":"supabase"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/conf/_div_ha/","tags":"","title":"HA Templates"},{"body":"The ha/simu configuration template is a 20-node production environment simulation, requiring a powerful host machine to run.\nOverview Config Name: ha/simu Node Count: 20 nodes, pigsty/vagrant/spec/simu.rb Description: 20-node production environment simulation, requires powerful host machine OS Distro: el8, el9, el10, d12, d13, u22, u24 OS Arch: x86_64, aarch64 Usage:\n./configure -c ha/simu [-i \u003cprimary_ip\u003e] Content Source: pigsty/conf/ha/simu.yml\n--- #==============================================================# # File : simu.yml # Desc : Pigsty Simubox: a 20 node prod simulation env # Ctime : 2023-07-20 # Mtime : 2025-12-23 # Docs : https://doc.pgsty.com/config # License : AGPLv3 @ https://doc.pgsty.com/about/license # Copyright : 2018-2025 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# all: children: #==========================================================# # infra: 3 nodes #==========================================================# # ./infra.yml -l infra # ./docker.yml -l infra (optional) infra: hosts: 10.10.10.10: {} 10.10.10.11: { repo_enabled: false } 10.10.10.12: { repo_enabled: false } vars: docker_enabled: true node_conf: oltp # use oltp template for infra nodes pg_conf: oltp.yml # use oltp template for infra pgsql pg_exporters: # bin/pgmon-add pg-meta2/pg-src2/pg-dst2 20001: {pg_cluster: pg-meta2 ,pg_seq: 1 ,pg_host: 10.10.10.10, pg_databases: [{ name: meta }]} 20002: {pg_cluster: pg-meta2 ,pg_seq: 2 ,pg_host: 10.10.10.11, pg_databases: [{ name: meta }]} 20003: {pg_cluster: pg-meta2 ,pg_seq: 3 ,pg_host: 10.10.10.12, pg_databases: [{ name: meta }]} 20004: {pg_cluster: pg-src2 ,pg_seq: 1 ,pg_host: 10.10.10.31, pg_databases: [{ name: src }]} 20005: {pg_cluster: pg-src2 ,pg_seq: 2 ,pg_host: 10.10.10.32, pg_databases: [{ name: src }]} 20006: {pg_cluster: pg-src2 ,pg_seq: 3 ,pg_host: 10.10.10.33, pg_databases: [{ name: src }]} 20007: {pg_cluster: pg-dst2 ,pg_seq: 1 ,pg_host: 10.10.10.41, pg_databases: [{ name: dst }]} 20008: {pg_cluster: pg-dst2 ,pg_seq: 2 ,pg_host: 10.10.10.42, pg_databases: [{ name: dst }]} 20009: {pg_cluster: pg-dst2 ,pg_seq: 3 ,pg_host: 10.10.10.43, pg_databases: [{ name: dst }]} #==========================================================# # nodes: 23 nodes #==========================================================# # ./node.yml nodes: hosts: 10.10.10.10 : { nodename: meta1 ,node_cluster: meta ,pg_cluster: pg-meta ,pg_seq: 1 ,pg_role: primary, infra_seq: 1 } 10.10.10.11 : { nodename: meta2 ,node_cluster: meta ,pg_cluster: pg-meta ,pg_seq: 2 ,pg_role: replica, infra_seq: 2 } 10.10.10.12 : { nodename: meta3 ,node_cluster: meta ,pg_cluster: pg-meta ,pg_seq: 3 ,pg_role: replica, infra_seq: 3 } 10.10.10.18 : { nodename: proxy1 ,node_cluster: proxy ,vip_address: 10.10.10.20 ,vip_vrid: 20 ,vip_interface: eth1 ,vip_role: master } 10.10.10.19 : { nodename: proxy2 ,node_cluster: proxy ,vip_address: 10.10.10.20 ,vip_vrid: 20 ,vip_interface: eth1 ,vip_role: backup } 10.10.10.21 : { nodename: minio1 ,node_cluster: minio ,minio_cluster: minio ,minio_seq: 1 } 10.10.10.22 : { nodename: minio2 ,node_cluster: minio ,minio_cluster: minio ,minio_seq: 2 } 10.10.10.23 : { nodename: minio3 ,node_cluster: minio ,minio_cluster: minio ,minio_seq: 3 } 10.10.10.24 : { nodename: minio4 ,node_cluster: minio ,minio_cluster: minio ,minio_seq: 4 } 10.10.10.25 : { nodename: etcd1 ,node_cluster: etcd ,etcd_cluster: etcd ,etcd_seq: 1 } 10.10.10.26 : { nodename: etcd2 ,node_cluster: etcd ,etcd_cluster: etcd ,etcd_seq: 2 } 10.10.10.27 : { nodename: etcd3 ,node_cluster: etcd ,etcd_cluster: etcd ,etcd_seq: 3 } 10.10.10.28 : { nodename: etcd4 ,node_cluster: etcd ,etcd_cluster: etcd ,etcd_seq: 4 } 10.10.10.29 : { nodename: etcd5 ,node_cluster: etcd ,etcd_cluster: etcd ,etcd_seq: 5 } 10.10.10.31 : { nodename: pg-src-1 ,node_cluster: pg-src ,node_id_from_pg: true } 10.10.10.32 : { nodename: pg-src-2 ,node_cluster: pg-src ,node_id_from_pg: true } 10.10.10.33 : { nodename: pg-src-3 ,node_cluster: pg-src ,node_id_from_pg: true } 10.10.10.41 : { nodename: pg-dst-1 ,node_cluster: pg-dst ,node_id_from_pg: true } 10.10.10.42 : { nodename: pg-dst-2 ,node_cluster: pg-dst ,node_id_from_pg: true } 10.10.10.43 : { nodename: pg-dst-3 ,node_cluster: pg-dst ,node_id_from_pg: true } #==========================================================# # etcd: 5 nodes dedicated etcd cluster #==========================================================# # ./etcd.yml -l etcd; etcd: hosts: 10.10.10.25: {} 10.10.10.26: {} 10.10.10.27: {} 10.10.10.28: {} 10.10.10.29: {} vars: {} #==========================================================# # minio: 4 nodes dedicated minio cluster #==========================================================# # ./minio.yml -l minio; minio: hosts: 10.10.10.21: {} 10.10.10.22: {} 10.10.10.23: {} 10.10.10.24: {} vars: minio_data: '/data{1...4}' # 4 node x 4 disk minio_users: # list of minio user to be created - { access_key: pgbackrest ,secret_key: S3User.Backup ,policy: pgsql } - { access_key: s3user_meta ,secret_key: S3User.Meta ,policy: meta } - { access_key: s3user_data ,secret_key: S3User.Data ,policy: data } #==========================================================# # proxy: 2 nodes used as dedicated haproxy server #==========================================================# # ./node.yml -l proxy proxy: hosts: 10.10.10.18: {} 10.10.10.19: {} vars: vip_enabled: true haproxy_services: # expose minio service : sss.pigsty:9000 - name: minio # [REQUIRED] service name, unique port: 9000 # [REQUIRED] service port, unique balance: leastconn # Use leastconn algorithm and minio health check options: [ \"option httpchk\", \"option http-keep-alive\", \"http-check send meth OPTIONS uri /minio/health/live\", \"http-check expect status 200\" ] servers: # reload service with ./node.yml -t haproxy_config,haproxy_reload - { name: minio-1 ,ip: 10.10.10.21 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-2 ,ip: 10.10.10.22 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-3 ,ip: 10.10.10.23 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-4 ,ip: 10.10.10.24 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } #==========================================================# # pg-meta: reuse infra node as meta cmdb #==========================================================# # ./pgsql.yml -l pg-meta pg-meta: hosts: 10.10.10.10: { pg_seq: 1 , pg_role: primary } 10.10.10.11: { pg_seq: 2 , pg_role: replica } 10.10.10.12: { pg_seq: 3 , pg_role: replica } vars: pg_cluster: pg-meta pg_vip_enabled: true pg_vip_address: 10.10.10.2/24 pg_vip_interface: eth1 pg_users: - {name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [dbrole_admin] ,comment: pigsty admin user } - {name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [dbrole_readonly] ,comment: read-only viewer for meta database } - {name: dbuser_grafana ,password: DBUser.Grafana ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for grafana database } - {name: dbuser_bytebase ,password: DBUser.Bytebase ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for bytebase database } - {name: dbuser_kong ,password: DBUser.Kong ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for kong api gateway } - {name: dbuser_gitea ,password: DBUser.Gitea ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for gitea service } - {name: dbuser_wiki ,password: DBUser.Wiki ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for wiki.js service } - {name: dbuser_noco ,password: DBUser.Noco ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for nocodb service } pg_databases: - { name: meta ,baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [pigsty] ,extensions: [{name: vector}]} - { name: grafana ,owner: dbuser_grafana ,revokeconn: true ,comment: grafana primary database } - { name: bytebase ,owner: dbuser_bytebase ,revokeconn: true ,comment: bytebase primary database } - { name: kong ,owner: dbuser_kong ,revokeconn: true ,comment: kong the api gateway database } - { name: gitea ,owner: dbuser_gitea ,revokeconn: true ,comment: gitea meta database } - { name: wiki ,owner: dbuser_wiki ,revokeconn: true ,comment: wiki meta database } - { name: noco ,owner: dbuser_noco ,revokeconn: true ,comment: nocodb database } pg_hba_rules: - { user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes' } pg_libs: 'pg_stat_statements, auto_explain' # add timescaledb to shared_preload_libraries node_crontab: # make a full backup on monday 1am, and an incremental backup during weekdays - '00 01 * * 1 postgres /pg/bin/pg-backup full' - '00 01 * * 2,3,4,5,6,7 postgres /pg/bin/pg-backup' #==========================================================# # pg-src: dedicate 3 node source cluster #==========================================================# # ./pgsql.yml -l pg-src pg-src: hosts: 10.10.10.31: { pg_seq: 1 ,pg_role: primary } 10.10.10.32: { pg_seq: 2 ,pg_role: replica } 10.10.10.33: { pg_seq: 3 ,pg_role: replica } vars: pg_cluster: pg-src pg_vip_enabled: true pg_vip_address: 10.10.10.3/24 pg_vip_interface: eth1 pg_users: [{ name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] }] pg_databases: [{ name: src }] #==========================================================# # pg-dst: dedicate 3 node destination cluster #==========================================================# # ./pgsql.yml -l pg-dst pg-dst: hosts: 10.10.10.41: { pg_seq: 1 ,pg_role: primary } 10.10.10.42: { pg_seq: 2 ,pg_role: replica } 10.10.10.43: { pg_seq: 3 ,pg_role: replica } vars: pg_cluster: pg-dst pg_vip_enabled: true pg_vip_address: 10.10.10.4/24 pg_vip_interface: eth1 pg_users: [ { name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] } ] pg_databases: [ { name: dst } ] #==========================================================# # redis-meta: reuse the 5 etcd nodes as redis sentinel #==========================================================# # ./redis.yml -l redis-meta redis-meta: hosts: 10.10.10.25: { redis_node: 1 , redis_instances: { 26379: {} } } 10.10.10.26: { redis_node: 2 , redis_instances: { 26379: {} } } 10.10.10.27: { redis_node: 3 , redis_instances: { 26379: {} } } 10.10.10.28: { redis_node: 4 , redis_instances: { 26379: {} } } 10.10.10.29: { redis_node: 5 , redis_instances: { 26379: {} } } vars: redis_cluster: redis-meta redis_password: 'redis.meta' redis_mode: sentinel redis_max_memory: 256MB redis_sentinel_monitor: # primary list for redis sentinel, use cls as name, primary ip:port - { name: redis-src, host: 10.10.10.31, port: 6379 ,password: redis.src, quorum: 1 } - { name: redis-dst, host: 10.10.10.41, port: 6379 ,password: redis.dst, quorum: 1 } #==========================================================# # redis-src: reuse pg-src 3 nodes for redis #==========================================================# # ./redis.yml -l redis-src redis-src: hosts: 10.10.10.31: { redis_node: 1 , redis_instances: {6379: { } }} 10.10.10.32: { redis_node: 2 , redis_instances: {6379: { replica_of: '10.10.10.31 6379' }, 6380: { replica_of: '10.10.10.32 6379' } }} 10.10.10.33: { redis_node: 3 , redis_instances: {6379: { replica_of: '10.10.10.31 6379' }, 6380: { replica_of: '10.10.10.33 6379' } }} vars: redis_cluster: redis-src redis_password: 'redis.src' redis_max_memory: 64MB #==========================================================# # redis-dst: reuse pg-dst 3 nodes for redis #==========================================================# # ./redis.yml -l redis-dst redis-dst: hosts: 10.10.10.41: { redis_node: 1 , redis_instances: {6379: { } }} 10.10.10.42: { redis_node: 2 , redis_instances: {6379: { replica_of: '10.10.10.41 6379' } }} 10.10.10.43: { redis_node: 3 , redis_instances: {6379: { replica_of: '10.10.10.41 6379' } }} vars: redis_cluster: redis-dst redis_password: 'redis.dst' redis_max_memory: 64MB #==========================================================# # pg-tmp: reuse proxy nodes as pgsql cluster #==========================================================# # ./pgsql.yml -l pg-tmp pg-tmp: hosts: 10.10.10.18: { pg_seq: 1 ,pg_role: primary } 10.10.10.19: { pg_seq: 2 ,pg_role: replica } vars: pg_cluster: pg-tmp pg_users: [ { name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] } ] pg_databases: [ { name: tmp } ] #==========================================================# # pg-etcd: reuse etcd nodes as pgsql cluster #==========================================================# # ./pgsql.yml -l pg-etcd pg-etcd: hosts: 10.10.10.25: { pg_seq: 1 ,pg_role: primary } 10.10.10.26: { pg_seq: 2 ,pg_role: replica } 10.10.10.27: { pg_seq: 3 ,pg_role: replica } 10.10.10.28: { pg_seq: 4 ,pg_role: replica } 10.10.10.29: { pg_seq: 5 ,pg_role: offline } vars: pg_cluster: pg-etcd pg_users: [ { name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] } ] pg_databases: [ { name: etcd } ] #==========================================================# # pg-minio: reuse minio nodes as pgsql cluster #==========================================================# # ./pgsql.yml -l pg-minio pg-minio: hosts: 10.10.10.21: { pg_seq: 1 ,pg_role: primary } 10.10.10.22: { pg_seq: 2 ,pg_role: replica } 10.10.10.23: { pg_seq: 3 ,pg_role: replica } 10.10.10.24: { pg_seq: 4 ,pg_role: replica } vars: pg_cluster: pg-minio pg_users: [ { name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] } ] pg_databases: [ { name: minio } ] #==========================================================# # ferret: reuse pg-src as mongo (ferretdb) #==========================================================# # ./mongo.yml -l ferret ferret: hosts: 10.10.10.31: { mongo_seq: 1 } 10.10.10.32: { mongo_seq: 2 } 10.10.10.33: { mongo_seq: 3 } vars: mongo_cluster: ferret mongo_pgurl: 'postgres://test:test@10.10.10.31:5432/src' #============================================================# # Global Variables #============================================================# vars: #==========================================================# # INFRA #==========================================================# version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: china # upstream mirror region: default|china|europe infra_portal: # infra services exposed via portal home : { domain: i.pigsty } # default domain name minio : { domain: m.pigsty ,endpoint: \"10.10.10.21:9001\" ,scheme: https ,websocket: true } postgrest : { domain: api.pigsty ,endpoint: \"127.0.0.1:8884\" } pgadmin : { domain: adm.pigsty ,endpoint: \"127.0.0.1:8885\" } pgweb : { domain: cli.pigsty ,endpoint: \"127.0.0.1:8886\" } bytebase : { domain: ddl.pigsty ,endpoint: \"127.0.0.1:8887\" } jupyter : { domain: lab.pigsty ,endpoint: \"127.0.0.1:8888\" , websocket: true } supa : { domain: supa.pigsty ,endpoint: \"10.10.10.10:8000\", websocket: true } #==========================================================# # NODE #==========================================================# node_id_from_pg: false # use nodename rather than pg identity as hostname node_conf: tiny # use small node template node_timezone: Asia/Hong_Kong # use Asia/Hong_Kong Timezone node_dns_servers: # DNS servers in /etc/resolv.conf - 10.10.10.10 - 10.10.10.11 node_etc_hosts: - 10.10.10.10 i.pigsty - 10.10.10.20 sss.pigsty # point minio service domain to the L2 VIP of proxy cluster node_ntp_servers: # NTP servers in /etc/chrony.conf - pool cn.pool.ntp.org iburst - pool 10.10.10.10 iburst node_admin_ssh_exchange: false # exchange admin ssh key among node cluster #==========================================================# # PGSQL #==========================================================# pg_conf: tiny.yml pgbackrest_method: minio # USE THE HA MINIO THROUGH A LOAD BALANCER pg_dbsu_ssh_exchange: false # do not exchange dbsu ssh key among pgsql cluster pgbackrest_repo: # pgbackrest repo: https://pgbackrest.org/configuration.html#section-repository local: # default pgbackrest repo with local posix fs path: /pg/backup # local backup directory, `/pg/backup` by default retention_full_type: count # retention full backups by count retention_full: 2 # keep 2, at most 3 full backup when using local fs repo minio: # optional minio repo for pgbackrest type: s3 # minio is s3-compatible, so s3 is used s3_endpoint: sss.pigsty # minio endpoint domain name, `sss.pigsty` by default s3_region: us-east-1 # minio region, us-east-1 by default, useless for minio s3_bucket: pgsql # minio bucket name, `pgsql` by default s3_key: pgbackrest # minio user access key for pgbackrest s3_key_secret: S3User.Backup # minio user secret key for pgbackrest s3_uri_style: path # use path style uri for minio rather than host style path: /pgbackrest # minio backup path, default is `//pgbackrest` storage_port: 9000 # minio port, 9000 by default storage_ca_file: /etc/pki/ca.crt # minio ca file path, `/etc/pki/ca.crt` by default block: y # Enable block incremental backup bundle: y # bundle small files into a single file bundle_limit: 20MiB # Limit for file bundles, 20MiB for object storage bundle_size: 128MiB # Target size for file bundles, 128MiB for object storage cipher_type: aes-256-cbc # enable AES encryption for remote backup repo cipher_pass: pgBackRest # AES encryption password, default is 'pgBackRest' retention_full_type: time # retention full backup by time on minio repo retention_full: 14 # keep full backup for last 14 days #==========================================================# # Repo #==========================================================# repo_packages: [ node-bootstrap, infra-package, infra-addons, node-package1, node-package2, pgsql-utility, extra-modules, pg18-core ,pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-olap ,pg18-feat ,pg18-lang ,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl ] #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root ... Explanation The ha/simu template is a large-scale production environment simulation for testing and validating complex scenarios.\nArchitecture:\n2-node HA INFRA (monitoring/alerting/Nginx/DNS) 5-node HA ETCD and MinIO (multi-disk) 2-node Proxy (HAProxy + Keepalived VIP) Multiple PostgreSQL clusters: pg-meta: 2-node HA pg-v12~v17: Single-node multi-version testing pg-pitr: Single-node PITR testing pg-test: 4-node HA pg-src/pg-dst: 3+2 node replication testing pg-citus: 10-node distributed cluster Multiple Redis modes: primary-replica, sentinel, cluster Use Cases:\nLarge-scale deployment testing and validation High availability failover drills Performance benchmarking New feature preview and evaluation Notes:\nRequires powerful host machine (64GB+ RAM recommended) Uses Vagrant virtual machines for simulation ","categories":["Reference"],"description":"20-node production environment simulation for large-scale deployment testing","excerpt":"20-node production environment simulation for large-scale deployment …","ref":"/docs/conf/simu/","tags":"","title":"ha/simu"},{"body":"The ha/full configuration template is Pigsty’s recommended sandbox demonstration environment, deploying two PostgreSQL clusters across four nodes for testing and demonstrating various Pigsty capabilities.\nMost Pigsty tutorials and examples are based on this template’s sandbox environment.\nOverview Config Name: ha/full Node Count: Four nodes Description: Four-node complete feature demonstration environment with two PostgreSQL clusters, MinIO, Redis, etc. OS Distro: el8, el9, el10, d12, d13, u22, u24 OS Arch: x86_64, aarch64 Related: ha/trio, ha/safe, demo/demo Usage:\n./configure -c ha/full [-i \u003cprimary_ip\u003e] After configuration, modify the IP addresses of the other three nodes.\nContent Source: pigsty/conf/ha/full.yml\n--- #==============================================================# # File : full.yml # Desc : Pigsty Local Sandbox 4-node Demo Config # Ctime : 2020-05-22 # Mtime : 2025-12-12 # Docs : https://doc.pgsty.com/config # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# all: #==============================================================# # Clusters, Nodes, and Modules #==============================================================# children: # infra: monitor, alert, repo, etc.. infra: hosts: 10.10.10.10: { infra_seq: 1 } vars: docker_enabled: true # enabled docker with ./docker.yml #docker_registry_mirrors: [\"https://docker.1panel.live\",\"https://docker.1ms.run\",\"https://docker.xuanyuan.me\",\"https://registry-1.docker.io\"] # etcd cluster for HA postgres DCS etcd: hosts: 10.10.10.10: { etcd_seq: 1 } vars: etcd_cluster: etcd # minio (single node, used as backup repo) minio: hosts: 10.10.10.10: { minio_seq: 1 } vars: minio_cluster: minio minio_users: # list of minio user to be created - { access_key: pgbackrest ,secret_key: S3User.Backup ,policy: pgsql } - { access_key: s3user_meta ,secret_key: S3User.Meta ,policy: meta } - { access_key: s3user_data ,secret_key: S3User.Data ,policy: data } # postgres cluster: pg-meta pg-meta: hosts: 10.10.10.10: { pg_seq: 1, pg_role: primary } vars: pg_cluster: pg-meta pg_users: - { name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [ dbrole_admin ] ,comment: pigsty admin user } - { name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [ dbrole_readonly ] ,comment: read-only viewer for meta database } pg_databases: - { name: meta ,baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [ pigsty ] } pg_hba_rules: - { user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes' } pg_vip_enabled: true pg_vip_address: 10.10.10.2/24 pg_vip_interface: eth1 node_crontab: # make a full backup 1 am everyday - '00 01 * * * postgres /pg/bin/pg-backup full' # pgsql 3 node ha cluster: pg-test pg-test: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } # primary instance, leader of cluster 10.10.10.12: { pg_seq: 2, pg_role: replica } # replica instance, follower of leader 10.10.10.13: { pg_seq: 3, pg_role: replica, pg_offline_query: true } # replica with offline access vars: pg_cluster: pg-test # define pgsql cluster name pg_users: [{ name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] }] pg_databases: [{ name: test }] pg_vip_enabled: true pg_vip_address: 10.10.10.3/24 pg_vip_interface: eth1 node_crontab: # make a full backup on monday 1am, and an incremental backup during weekdays - '00 01 * * 1 postgres /pg/bin/pg-backup full' - '00 01 * * 2,3,4,5,6,7 postgres /pg/bin/pg-backup' #----------------------------------# # redis ms, sentinel, native cluster #----------------------------------# redis-ms: # redis classic primary \u0026 replica hosts: { 10.10.10.10: { redis_node: 1 , redis_instances: { 6379: { }, 6380: { replica_of: '10.10.10.10 6379' } } } } vars: { redis_cluster: redis-ms ,redis_password: 'redis.ms' ,redis_max_memory: 64MB } redis-meta: # redis sentinel x 3 hosts: { 10.10.10.11: { redis_node: 1 , redis_instances: { 26379: { } ,26380: { } ,26381: { } } } } vars: redis_cluster: redis-meta redis_password: 'redis.meta' redis_mode: sentinel redis_max_memory: 16MB redis_sentinel_monitor: # primary list for redis sentinel, use cls as name, primary ip:port - { name: redis-ms, host: 10.10.10.10, port: 6379 ,password: redis.ms, quorum: 2 } redis-test: # redis native cluster: 3m x 3s hosts: 10.10.10.12: { redis_node: 1 ,redis_instances: { 6379: { } ,6380: { } ,6381: { } } } 10.10.10.13: { redis_node: 2 ,redis_instances: { 6379: { } ,6380: { } ,6381: { } } } vars: { redis_cluster: redis-test ,redis_password: 'redis.test' ,redis_mode: cluster, redis_max_memory: 32MB } #==============================================================# # Global Parameters #==============================================================# vars: version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default|china|europe node_tune: oltp # node tuning specs: oltp,olap,tiny,crit pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml proxy_env: # global proxy env when downloading packages no_proxy: \"localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,*.pigsty,*.aliyun.com,mirrors.*,*.myqcloud.com,*.tsinghua.edu.cn\" # http_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com # https_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com # all_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com infra_portal: # infra services exposed via portal home : { domain: i.pigsty } # default domain name #minio : { domain: m.pigsty ,endpoint: \"${admin_ip}:9001\" ,scheme: https ,websocket: true } #----------------------------------# # MinIO Related Options #----------------------------------# node_etc_hosts: [ '${admin_ip} i.pigsty sss.pigsty' ] pgbackrest_method: minio # if you want to use minio as backup repo instead of 'local' fs, uncomment this pgbackrest_repo: # pgbackrest repo: https://pgbackrest.org/configuration.html#section-repository local: # default pgbackrest repo with local posix fs path: /pg/backup # local backup directory, `/pg/backup` by default retention_full_type: count # retention full backups by count retention_full: 2 # keep 2, at most 3 full backup when using local fs repo minio: # optional minio repo for pgbackrest type: s3 # minio is s3-compatible, so s3 is used s3_endpoint: sss.pigsty # minio endpoint domain name, `sss.pigsty` by default s3_region: us-east-1 # minio region, us-east-1 by default, useless for minio s3_bucket: pgsql # minio bucket name, `pgsql` by default s3_key: pgbackrest # minio user access key for pgbackrest s3_key_secret: S3User.Backup # minio user secret key for pgbackrest s3_uri_style: path # use path style uri for minio rather than host style path: /pgbackrest # minio backup path, default is `/pgbackrest` storage_port: 9000 # minio port, 9000 by default storage_ca_file: /etc/pki/ca.crt # minio ca file path, `/etc/pki/ca.crt` by default block: y # Enable block incremental backup bundle: y # bundle small files into a single file bundle_limit: 20MiB # Limit for file bundles, 20MiB for object storage bundle_size: 128MiB # Target size for file bundles, 128MiB for object storage cipher_type: aes-256-cbc # enable AES encryption for remote backup repo cipher_pass: pgBackRest # AES encryption password, default is 'pgBackRest' retention_full_type: time # retention full backup by time on minio repo retention_full: 14 # keep full backup for last 14 days #----------------------------------# # Repo, Node, Packages #----------------------------------# repo_remove: true # remove existing repo on admin node during repo bootstrap node_repo_remove: true # remove existing node repo for node managed by pigsty repo_extra_packages: [ pg18-main ] #,pg18-core ,pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-olap ,pg18-feat ,pg18-lang ,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl] pg_version: 18 # default postgres version #pg_extensions: [pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-feat ,pg18-lang ,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl ,pg18-olap] #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root ... Explanation The ha/full template is Pigsty’s complete feature demonstration configuration, showcasing the collaboration of various components.\nComponents Overview:\nComponent Node Distribution Description INFRA Node 1 Monitoring/Alerting/Nginx/DNS ETCD Node 1 DCS Service MinIO Node 1 S3-compatible Storage pg-meta Node 1 Single-node PostgreSQL pg-test Nodes 2-4 Three-node HA PostgreSQL redis-ms Node 1 Redis Primary-Replica Mode redis-meta Node 2 Redis Sentinel Mode redis-test Nodes 3-4 Redis Native Cluster Mode Use Cases:\nPigsty feature demonstration and learning Development testing environments Evaluating HA architecture Comparing different Redis modes Differences from ha/trio:\nAdded second PostgreSQL cluster (pg-test) Added three Redis cluster mode examples Infrastructure uses single node (instead of three nodes) Notes:\nThis template is mainly for demonstration and testing; for production, refer to ha/trio or ha/safe MinIO backup enabled by default; comment out related config if not needed ","categories":["Reference"],"description":"Four-node complete feature demonstration environment with two PostgreSQL clusters, MinIO, Redis, etc.","excerpt":"Four-node complete feature demonstration environment with two …","ref":"/docs/conf/full/","tags":"","title":"ha/full"},{"body":"The ha/safe configuration template is based on the ha/trio template, providing a security-hardened configuration with high-standard security best practices.\nOverview Config Name: ha/safe Node Count: Three nodes (optional delayed replica) Description: Security-hardened HA configuration with high-standard security best practices OS Distro: el8, el9, el10, d12, d13, u22, u24 OS Arch: x86_64 (some security extensions unavailable on ARM64) Related: ha/trio, ha/full Usage:\n./configure -c ha/safe [-i \u003cprimary_ip\u003e] Security Hardening Measures The ha/safe template implements the following security hardening:\nMandatory SSL Encryption: SSL enabled for both PostgreSQL and PgBouncer Strong Password Policy: passwordcheck extension enforces password complexity User Expiration: All users set to 20-year expiration Minimal Connection Scope: Limit PostgreSQL/Patroni/PgBouncer listen addresses Strict HBA Rules: Mandatory SSL authentication, admin requires certificate Audit Logs: Record connection and disconnection events Delayed Replica: Optional 1-hour delayed replica for recovery from mistakes Critical Template: Uses crit.yml tuning template for zero data loss Content Source: pigsty/conf/ha/safe.yml\n--- #==============================================================# # File : safe.yml # Desc : Pigsty 3-node security enhance template # Ctime : 2020-05-22 # Mtime : 2025-12-12 # Docs : https://doc.pgsty.com/config # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# #===== SECURITY ENHANCEMENT CONFIG TEMPLATE WITH 3 NODES ======# # * 3 infra nodes, 3 etcd nodes, single minio node # * 3-instance pgsql cluster with an extra delayed instance # * crit.yml templates, no data loss, checksum enforced # * enforce ssl on postgres \u0026 pgbouncer, use postgres by default # * enforce an expiration date for all users (20 years by default) # * enforce strong password policy with passwordcheck extension # * enforce changing default password for all users # * log connections and disconnections # * restrict listen ip address for postgres/patroni/pgbouncer all: children: infra: # infra cluster for proxy, monitor, alert, etc hosts: # 1 for common usage, 3 nodes for production 10.10.10.10: { infra_seq: 1 } # identity required 10.10.10.11: { infra_seq: 2, repo_enabled: false } 10.10.10.12: { infra_seq: 3, repo_enabled: false } vars: { patroni_watchdog_mode: off } minio: # minio cluster, s3 compatible object storage hosts: { 10.10.10.10: { minio_seq: 1 } } vars: { minio_cluster: minio } etcd: # dcs service for postgres/patroni ha consensus hosts: # 1 node for testing, 3 or 5 for production 10.10.10.10: { etcd_seq: 1 } # etcd_seq required 10.10.10.11: { etcd_seq: 2 } # assign from 1 ~ n 10.10.10.12: { etcd_seq: 3 } # odd number please vars: # cluster level parameter override roles/etcd etcd_cluster: etcd # mark etcd cluster name etcd etcd_safeguard: false # safeguard against purging etcd_clean: true # purge etcd during init process pg-meta: # 3 instance postgres cluster `pg-meta` hosts: 10.10.10.10: { pg_seq: 1, pg_role: primary } 10.10.10.11: { pg_seq: 2, pg_role: replica } 10.10.10.12: { pg_seq: 3, pg_role: replica , pg_offline_query: true } vars: pg_cluster: pg-meta pg_conf: crit.yml pg_users: - { name: dbuser_meta , password: Pleas3-ChangeThisPwd ,expire_in: 7300 ,pgbouncer: true ,roles: [ dbrole_admin ] ,comment: pigsty admin user } - { name: dbuser_view , password: Make.3ure-Compl1ance ,expire_in: 7300 ,pgbouncer: true ,roles: [ dbrole_readonly ] ,comment: read-only viewer for meta database } pg_databases: - { name: meta ,baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [ pigsty ] ,extensions: [ { name: vector } ] } pg_services: - { name: standby , ip: \"*\" ,port: 5435 , dest: default ,selector: \"[]\" , backup: \"[? pg_role == `primary`]\" } pg_listen: '${ip},${vip},${lo}' pg_vip_enabled: true pg_vip_address: 10.10.10.2/24 pg_vip_interface: eth1 # OPTIONAL delayed cluster for pg-meta #pg-meta-delay: # delayed instance for pg-meta (1 hour ago) # hosts: { 10.10.10.13: { pg_seq: 1, pg_role: primary, pg_upstream: 10.10.10.10, pg_delay: 1h } } # vars: { pg_cluster: pg-meta-delay } #################################################################### # Parameters # #################################################################### vars: # global variables version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default|china|europe node_tune: oltp # node tuning specs: oltp,olap,tiny,crit pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml #docker_registry_mirrors: [\"https://docker.1panel.live\",\"https://docker.1ms.run\",\"https://docker.xuanyuan.me\",\"https://registry-1.docker.io\"] patroni_ssl_enabled: true # secure patroni RestAPI communications with SSL? pgbouncer_sslmode: require # pgbouncer client ssl mode: disable|allow|prefer|require|verify-ca|verify-full, disable by default pg_default_service_dest: postgres # default service destination to postgres instead of pgbouncer pgbackrest_method: minio # pgbackrest repo method: local,minio,[user-defined...] #----------------------------------# # MinIO Related Options #----------------------------------# minio_users: # and configure `pgbackrest_repo` \u0026 `minio_users` accordingly - { access_key: dba , secret_key: S3User.DBA.Strong.Password, policy: consoleAdmin } - { access_key: pgbackrest , secret_key: Min10.bAckup ,policy: readwrite } pgbackrest_repo: # pgbackrest repo: https://pgbackrest.org/configuration.html#section-repository local: # default pgbackrest repo with local posix fs path: /pg/backup # local backup directory, `/pg/backup` by default retention_full_type: count # retention full backups by count retention_full: 2 # keep 2, at most 3 full backup when using local fs repo minio: # optional minio repo for pgbackrest s3_key: pgbackrest # \u003c-------- CHANGE THIS, SAME AS `minio_users` access_key s3_key_secret: Min10.bAckup # \u003c-------- CHANGE THIS, SAME AS `minio_users` secret_key cipher_pass: 'pgBR.${pg_cluster}' # \u003c-------- CHANGE THIS, you can use cluster name as part of password type: s3 # minio is s3-compatible, so s3 is used s3_endpoint: sss.pigsty # minio endpoint domain name, `sss.pigsty` by default s3_region: us-east-1 # minio region, us-east-1 by default, useless for minio s3_bucket: pgsql # minio bucket name, `pgsql` by default s3_uri_style: path # use path style uri for minio rather than host style path: /pgbackrest # minio backup path, default is `/pgbackrest` storage_port: 9000 # minio port, 9000 by default storage_ca_file: /etc/pki/ca.crt # minio ca file path, `/etc/pki/ca.crt` by default block: y # Enable block incremental backup bundle: y # bundle small files into a single file bundle_limit: 20MiB # Limit for file bundles, 20MiB for object storage bundle_size: 128MiB # Target size for file bundles, 128MiB for object storage cipher_type: aes-256-cbc # enable AES encryption for remote backup repo retention_full_type: time # retention full backup by time on minio repo retention_full: 14 # keep full backup for last 14 days #----------------------------------# # Access Control #----------------------------------# # add passwordcheck extension to enforce strong password policy pg_libs: '$libdir/passwordcheck, pg_stat_statements, auto_explain' pg_extensions: - passwordcheck, supautils, pgsodium, pg_vault, pg_session_jwt, anonymizer, pgsmcrypto, pgauditlogtofile, pgaudit #, pgaudit17, pgaudit16, pgaudit15, pgaudit14 - pg_auth_mon, credcheck, pgcryptokey, pg_jobmon, logerrors, login_hook, set_user, pgextwlist, pg_auditor, sslutils, noset #pg_tde #pg_snakeoil pg_default_roles: # default roles and users in postgres cluster - { name: dbrole_readonly ,login: false ,comment: role for global read-only access } - { name: dbrole_offline ,login: false ,comment: role for restricted read-only access } - { name: dbrole_readwrite ,login: false ,roles: [ dbrole_readonly ] ,comment: role for global read-write access } - { name: dbrole_admin ,login: false ,roles: [ pg_monitor, dbrole_readwrite ] ,comment: role for object creation } - { name: postgres ,superuser: true ,expire_in: 7300 ,comment: system superuser } - { name: replicator ,replication: true ,expire_in: 7300 ,roles: [ pg_monitor, dbrole_readonly ] ,comment: system replicator } - { name: dbuser_dba ,superuser: true ,expire_in: 7300 ,roles: [ dbrole_admin ] ,pgbouncer: true ,pool_mode: session, pool_connlimit: 16 , comment: pgsql admin user } - { name: dbuser_monitor ,roles: [ pg_monitor ] ,expire_in: 7300 ,pgbouncer: true ,parameters: { log_min_duration_statement: 1000 } ,pool_mode: session ,pool_connlimit: 8 ,comment: pgsql monitor user } pg_default_hba_rules: # postgres host-based auth rules by default, order by `order` - { user: '${dbsu}' ,db: all ,addr: local ,auth: ident ,title: 'dbsu access via local os user ident' ,order: 100} - { user: '${dbsu}' ,db: replication ,addr: local ,auth: ident ,title: 'dbsu replication from local os ident' ,order: 150} - { user: '${repl}' ,db: replication ,addr: localhost ,auth: ssl ,title: 'replicator replication from localhost' ,order: 200} - { user: '${repl}' ,db: replication ,addr: intra ,auth: ssl ,title: 'replicator replication from intranet' ,order: 250} - { user: '${repl}' ,db: postgres ,addr: intra ,auth: ssl ,title: 'replicator postgres db from intranet' ,order: 300} - { user: '${monitor}' ,db: all ,addr: localhost ,auth: pwd ,title: 'monitor from localhost with password' ,order: 350} - { user: '${monitor}' ,db: all ,addr: infra ,auth: ssl ,title: 'monitor from infra host with password' ,order: 400} - { user: '${admin}' ,db: all ,addr: infra ,auth: ssl ,title: 'admin @ infra nodes with pwd \u0026 ssl' ,order: 450} - { user: '${admin}' ,db: all ,addr: world ,auth: cert ,title: 'admin @ everywhere with ssl \u0026 cert' ,order: 500} - { user: '+dbrole_readonly',db: all ,addr: localhost ,auth: ssl ,title: 'pgbouncer read/write via local socket' ,order: 550} - { user: '+dbrole_readonly',db: all ,addr: intra ,auth: ssl ,title: 'read/write biz user via password' ,order: 600} - { user: '+dbrole_offline' ,db: all ,addr: intra ,auth: ssl ,title: 'allow etl offline tasks from intranet' ,order: 650} pgb_default_hba_rules: # pgbouncer host-based authentication rules, order by `order` - { user: '${dbsu}' ,db: pgbouncer ,addr: local ,auth: peer ,title: 'dbsu local admin access with os ident' ,order: 100} - { user: 'all' ,db: all ,addr: localhost ,auth: pwd ,title: 'allow all user local access with pwd' ,order: 150} - { user: '${monitor}' ,db: pgbouncer ,addr: intra ,auth: ssl ,title: 'monitor access via intranet with pwd' ,order: 200} - { user: '${monitor}' ,db: all ,addr: world ,auth: deny ,title: 'reject all other monitor access addr' ,order: 250} - { user: '${admin}' ,db: all ,addr: intra ,auth: ssl ,title: 'admin access via intranet with pwd' ,order: 300} - { user: '${admin}' ,db: all ,addr: world ,auth: deny ,title: 'reject all other admin access addr' ,order: 350} - { user: 'all' ,db: all ,addr: intra ,auth: ssl ,title: 'allow all user intra access with pwd' ,order: 400} #----------------------------------# # Repo, Node, Packages #----------------------------------# repo_remove: true # remove existing repo on admin node during repo bootstrap node_repo_remove: true # remove existing node repo for node managed by pigsty #node_selinux_mode: enforcing # set selinux mode: enforcing,permissive,disabled node_firewall_mode: zone # firewall mode: off, none, zone, zone by default repo_extra_packages: [ pg18-main ] #,pg18-core ,pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-olap ,pg18-feat ,pg18-lang ,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl] pg_version: 18 # default postgres version #pg_extensions: [ pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-olap ,pg18-feat ,pg18-lang ,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl] #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# #grafana_admin_username: admin grafana_admin_password: You.Have2Use-A_VeryStrongPassword grafana_view_password: DBUser.Viewer #pg_admin_username: dbuser_dba pg_admin_password: PessWorb.Should8eStrong-eNough #pg_monitor_username: dbuser_monitor pg_monitor_password: MekeSuerYour.PassWordI5secured #pg_replication_username: replicator pg_replication_password: doNotUseThis-PasswordFor.AnythingElse #patroni_username: postgres patroni_password: don.t-forget-to-change-thEs3-password #haproxy_admin_username: admin haproxy_admin_password: GneratePasswordWith-pwgen-s-16-1 minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root ... Explanation The ha/safe template is Pigsty’s security-hardened configuration, designed for production environments with high security requirements.\nSecurity Features Summary:\nSecurity Measure Description SSL Encryption Full-chain SSL for PostgreSQL/PgBouncer/Patroni Strong Password passwordcheck extension enforces complexity User Expiration All users expire in 20 years (expire_in: 7300) Strict HBA Admin remote access requires certificate Encrypted Backup MinIO backup with AES-256-CBC encryption Audit Logs pgaudit extension for SQL audit logging Delayed Replica 1-hour delayed replica for mistake recovery Use Cases:\nFinance, healthcare, government sectors with high security requirements Environments needing compliance audit requirements Critical business with extremely high data security demands Notes:\nSome security extensions unavailable on ARM64 architecture, enable appropriately All default passwords must be changed to strong passwords Recommend using with regular security audits ","categories":["Reference"],"description":"Security-hardened HA configuration template with high-standard security best practices","excerpt":"Security-hardened HA configuration template with high-standard …","ref":"/docs/conf/safe/","tags":"","title":"ha/safe"},{"body":"Three nodes is the minimum scale for achieving true high availability. The ha/trio template uses a three-node standard HA architecture, with INFRA, ETCD, and PGSQL all deployed across three nodes, tolerating any single server failure.\nOverview Config Name: ha/trio Node Count: Three nodes Description: Three-node standard HA architecture, tolerates any single server failure OS Distro: el8, el9, el10, d12, d13, u22, u24 OS Arch: x86_64, aarch64 Related: ha/dual, ha/full, ha/safe Usage:\n./configure -c ha/trio [-i \u003cprimary_ip\u003e] After configuration, modify placeholder IPs 10.10.10.11 and 10.10.10.12 to actual node IP addresses.\nContent Source: pigsty/conf/ha/trio.yml\n--- #==============================================================# # File : trio.yml # Desc : Pigsty 3-node security enhance template # Ctime : 2020-05-22 # Mtime : 2025-12-12 # Docs : https://doc.pgsty.com/config # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# # 3 infra node, 3 etcd node, 3 pgsql node, and 1 minio node all: #==============================================================# # Clusters, Nodes, and Modules #==============================================================# children: #----------------------------------# # infra: monitor, alert, repo, etc.. #----------------------------------# infra: # infra cluster for proxy, monitor, alert, etc hosts: # 1 for common usage, 3 nodes for production 10.10.10.10: { infra_seq: 1 } # identity required 10.10.10.11: { infra_seq: 2, repo_enabled: false } 10.10.10.12: { infra_seq: 3, repo_enabled: false } vars: patroni_watchdog_mode: off # do not fencing infra etcd: # dcs service for postgres/patroni ha consensus hosts: # 1 node for testing, 3 or 5 for production 10.10.10.10: { etcd_seq: 1 } # etcd_seq required 10.10.10.11: { etcd_seq: 2 } # assign from 1 ~ n 10.10.10.12: { etcd_seq: 3 } # odd number please vars: # cluster level parameter override roles/etcd etcd_cluster: etcd # mark etcd cluster name etcd etcd_safeguard: false # safeguard against purging etcd_clean: true # purge etcd during init process minio: # minio cluster, s3 compatible object storage hosts: { 10.10.10.10: { minio_seq: 1 } } vars: { minio_cluster: minio } pg-meta: # 3 instance postgres cluster `pg-meta` hosts: 10.10.10.10: { pg_seq: 1, pg_role: primary } 10.10.10.11: { pg_seq: 2, pg_role: replica } 10.10.10.12: { pg_seq: 3, pg_role: replica , pg_offline_query: true } vars: pg_cluster: pg-meta pg_users: - { name: dbuser_meta , password: DBUser.Meta ,pgbouncer: true ,roles: [ dbrole_admin ] ,comment: pigsty admin user } - { name: dbuser_view , password: DBUser.View ,pgbouncer: true ,roles: [ dbrole_readonly ] ,comment: read-only viewer for meta database } pg_databases: - { name: meta ,baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [ pigsty ] ,extensions: [ { name: vector } ] } pg_vip_enabled: true pg_vip_address: 10.10.10.2/24 pg_vip_interface: eth1 #==============================================================# # Global Parameters #==============================================================# vars: #----------------------------------# # Meta Data #----------------------------------# version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default|china|europe node_tune: oltp # node tuning specs: oltp,olap,tiny,crit pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml #docker_registry_mirrors: [\"https://docker.1panel.live\",\"https://docker.1ms.run\",\"https://docker.xuanyuan.me\",\"https://registry-1.docker.io\"] proxy_env: # global proxy env when downloading packages no_proxy: \"localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,*.pigsty,*.aliyun.com,mirrors.*,*.myqcloud.com,*.tsinghua.edu.cn\" # http_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com # https_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com # all_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com infra_portal: # infra services exposed via portal home : { domain: i.pigsty } # default domain name #minio : { domain: m.pigsty ,endpoint: \"${admin_ip}:9001\" ,scheme: https ,websocket: true } #----------------------------------# # Repo, Node, Packages #----------------------------------# repo_remove: true # remove existing repo on admin node during repo bootstrap node_repo_remove: true # remove existing node repo for node managed by pigsty repo_extra_packages: [ pg18-main ] #,pg18-core ,pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-olap ,pg18-feat ,pg18-lang ,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl] pg_version: 18 # default postgres version #pg_extensions: [ pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-olap ,pg18-feat ,pg18-lang ,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl] #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root ... Explanation The ha/trio template is Pigsty’s standard HA configuration, providing true automatic failover capability.\nArchitecture:\nThree-node INFRA: Distributed deployment of Prometheus/Grafana/Nginx Three-node ETCD: DCS majority election, tolerates single-point failure Three-node PostgreSQL: One primary, two replicas, automatic failover Single-node MinIO: Can be expanded to multi-node as needed HA Guarantees:\nThree-node ETCD tolerates one node failure, maintains majority PostgreSQL primary failure triggers automatic Patroni election for new primary L2 VIP follows primary, applications don’t need to modify connection config Use Cases:\nMinimum HA deployment for production environments Critical business requiring automatic failover Foundation architecture for larger scale deployments Extension Suggestions:\nFor stronger data security, refer to ha/safe template For more demo features, refer to ha/full template Production environments should enable pgbackrest_method: minio for remote backup ","categories":["Reference"],"description":"Three-node standard HA configuration, tolerates any single server failure","excerpt":"Three-node standard HA configuration, tolerates any single server …","ref":"/docs/conf/trio/","tags":"","title":"ha/trio"},{"body":"The ha/dual template uses two-node deployment, implementing a “semi-HA” architecture with one primary and one standby. If you only have two servers, this is a pragmatic choice.\nOverview Config Name: ha/dual Node Count: Two nodes Description: Two-node limited HA deployment, tolerates specific server failure OS Distro: el8, el9, el10, d12, d13, u22, u24 OS Arch: x86_64, aarch64 Related: ha/trio, slim Usage:\n./configure -c ha/dual [-i \u003cprimary_ip\u003e] After configuration, modify placeholder IP 10.10.10.11 to actual standby node IP address.\nContent Source: pigsty/conf/ha/dual.yml\n--- #==============================================================# # File : dual.yml # Desc : Pigsty deployment example for two nodes # Ctime : 2020-05-22 # Mtime : 2025-12-12 # Docs : https://doc.pgsty.com/config # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# # It is recommended to use at least three nodes in production deployment. # But sometimes, there are only two nodes available, that's dual.yml for # # In this setup, we have two nodes, .10 (admin_node) and .11 (pgsql_priamry): # # If .11 is down, .10 will take over since the dcs:etcd is still alive # If .10 is down, .11 (pgsql primary) will still be functioning as a primary if: # - Only dcs:etcd is down # - Only pgsql is down # if both etcd \u0026 pgsql are down (e.g. node down), the primary will still demote itself. all: children: # infra cluster for proxy, monitor, alert, etc.. infra: { hosts: { 10.10.10.10: { infra_seq: 1 } } } # etcd cluster for ha postgres etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 } }, vars: { etcd_cluster: etcd } } # minio cluster, optional backup repo for pgbackrest #minio: { hosts: { 10.10.10.10: { minio_seq: 1 } }, vars: { minio_cluster: minio } } # postgres cluster 'pg-meta' with single primary instance pg-meta: hosts: 10.10.10.10: { pg_seq: 1, pg_role: replica } 10.10.10.11: { pg_seq: 2, pg_role: primary } # \u003c----- use this as primary by default vars: pg_cluster: pg-meta pg_databases: [ { name: meta ,baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [ pigsty ] ,extensions: [ { name: vector }] } ] pg_users: - { name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [ dbrole_admin ] ,comment: pigsty admin user } - { name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [ dbrole_readonly ] ,comment: read-only viewer for meta database } node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] # make a full backup every 1am pg_vip_enabled: true pg_vip_address: 10.10.10.2/24 pg_vip_interface: eth1 vars: # global parameters version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default,china,europe node_tune: oltp # node tuning specs: oltp,olap,tiny,crit pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml #docker_registry_mirrors: [\"https://docker.1panel.live\",\"https://docker.1ms.run\",\"https://docker.xuanyuan.me\",\"https://registry-1.docker.io\"] infra_portal: # domain names and upstream servers home : { domain: i.pigsty } #minio : { domain: m.pigsty ,endpoint: \"${admin_ip}:9001\" ,scheme: https ,websocket: true } #----------------------------------# # Repo, Node, Packages #----------------------------------# repo_remove: true # remove existing repo on admin node during repo bootstrap node_repo_remove: true # remove existing node repo for node managed by pigsty repo_extra_packages: [ pg18-main ] #,pg18-core ,pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-olap ,pg18-feat ,pg18-lang ,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl] pg_version: 18 # default postgres version #pg_extensions: [ pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-olap ,pg18-feat ,pg18-lang ,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl] #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root ... Explanation The ha/dual template is Pigsty’s two-node limited HA configuration, designed for scenarios with only two servers.\nArchitecture:\nNode A (10.10.10.10): Admin node, runs Infra + etcd + PostgreSQL replica Node B (10.10.10.11): Data node, runs PostgreSQL primary only Failure Scenario Analysis:\nFailed Node Impact Auto Recovery Node B down Primary switches to Node A Auto Node A etcd down Primary continues running (no DCS) Manual Node A pgsql down Primary continues running Manual Node A complete failure Primary degrades to standalone Manual Use Cases:\nBudget-limited environments with only two servers Acceptable that some failure scenarios need manual intervention Transitional solution before upgrading to three-node HA Notes:\nTrue HA requires at least three nodes (DCS needs majority) Recommend upgrading to three-node architecture as soon as possible L2 VIP requires network environment support (same broadcast domain) ","categories":["Reference"],"description":"Two-node configuration, limited HA deployment tolerating specific server failure","excerpt":"Two-node configuration, limited HA deployment tolerating specific …","ref":"/docs/conf/dual/","tags":"","title":"ha/dual"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/conf/_div_app/","tags":"","title":"App Templates"},{"body":"The app/odoo configuration template provides a reference configuration for self-hosting Odoo open-source ERP system, using Pigsty-managed PostgreSQL as the database.\nFor more details, see Odoo Deployment Tutorial\nOverview Config Name: app/odoo Node Count: Single node Description: Deploy Odoo ERP using Pigsty-managed PostgreSQL OS Distro: el8, el9, el10, d12, d13, u22, u24 OS Arch: x86_64, aarch64 Related: meta Usage:\n./configure -c app/odoo [-i \u003cprimary_ip\u003e] Content Source: pigsty/conf/app/odoo.yml\n--- #==============================================================# # File : odoo.yml # Desc : pigsty config for running 1-node odoo app # Ctime : 2025-01-11 # Mtime : 2025-12-12 # Docs : https://doc.pgsty.com/app/odoo # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# # tutorial: https://doc.pgsty.com/app/odoo # how to use this template: # # curl -fsSL https://repo.pigsty.io/get | bash; cd ~/pigsty # ./bootstrap # prepare local repo \u0026 ansible # ./configure -c app/odoo # Use this odoo config template # vi pigsty.yml # IMPORTANT: CHANGE CREDENTIALS!! # ./deploy.yml # install pigsty \u0026 pgsql \u0026 minio # ./docker.yml # install docker \u0026 docker-compose # ./app.yml # install odoo all: children: # the odoo application (default username \u0026 password: admin/admin) odoo: hosts: { 10.10.10.10: {} } vars: app: odoo # specify app name to be installed (in the apps) apps: # define all applications odoo: # app name, should have corresponding ~/pigsty/app/odoo folder file: # optional directory to be created - { path: /data/odoo ,state: directory, owner: 100, group: 101 } - { path: /data/odoo/webdata ,state: directory, owner: 100, group: 101 } - { path: /data/odoo/addons ,state: directory, owner: 100, group: 101 } conf: # override /opt/\u003capp\u003e/.env config file PG_HOST: 10.10.10.10 # postgres host PG_PORT: 5432 # postgres port PG_USERNAME: odoo # postgres user PG_PASSWORD: DBUser.Odoo # postgres password ODOO_PORT: 8069 # odoo app port ODOO_DATA: /data/odoo/webdata # odoo webdata ODOO_ADDONS: /data/odoo/addons # odoo plugins ODOO_DBNAME: odoo # odoo database name ODOO_VERSION: 19.0 # odoo image version # the odoo database pg-odoo: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-odoo pg_users: - { name: odoo ,password: DBUser.Odoo ,pgbouncer: true ,roles: [ dbrole_admin ] ,createdb: true ,comment: admin user for odoo service } - { name: odoo_ro ,password: DBUser.Odoo ,pgbouncer: true ,roles: [ dbrole_readonly ] ,comment: read only user for odoo service } - { name: odoo_rw ,password: DBUser.Odoo ,pgbouncer: true ,roles: [ dbrole_readwrite ] ,comment: read write user for odoo service } pg_databases: - { name: odoo ,owner: odoo ,revokeconn: true ,comment: odoo main database } pg_hba_rules: - { user: all ,db: all ,addr: 172.17.0.0/16 ,auth: pwd ,title: 'allow access from local docker network' } - { user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes' } node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] # make a full backup every 1am infra: { hosts: { 10.10.10.10: { infra_seq: 1 } } } etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 } }, vars: { etcd_cluster: etcd } } #minio: { hosts: { 10.10.10.10: { minio_seq: 1 } }, vars: { minio_cluster: minio } } vars: # global variables version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default|china|europe node_tune: oltp # node tuning specs: oltp,olap,tiny,crit pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml docker_enabled: true # enable docker on app group #docker_registry_mirrors: [\"https://docker.1panel.live\",\"https://docker.1ms.run\",\"https://docker.xuanyuan.me\",\"https://registry-1.docker.io\"] proxy_env: # global proxy env when downloading packages \u0026 pull docker images no_proxy: \"localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,*.pigsty,*.aliyun.com,mirrors.*,*.tsinghua.edu.cn\" #http_proxy: 127.0.0.1:12345 # add your proxy env here for downloading packages or pull images #https_proxy: 127.0.0.1:12345 # usually the proxy is format as http://user:pass@proxy.xxx.com #all_proxy: 127.0.0.1:12345 infra_portal: # domain names and upstream servers home : { domain: i.pigsty } minio : { domain: m.pigsty ,endpoint: \"${admin_ip}:9001\" ,scheme: https ,websocket: true } odoo: # nginx server config for odoo domain: odoo.pigsty # REPLACE WITH YOUR OWN DOMAIN! endpoint: \"10.10.10.10:8069\" # odoo service endpoint: IP:PORT websocket: true # add websocket support certbot: odoo.pigsty # certbot cert name, apply with `make cert` repo_enabled: false node_repo_modules: node,infra,pgsql pg_version: 18 #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root ... Explanation The app/odoo template provides a one-click deployment solution for Odoo open-source ERP system.\nWhat is Odoo:\nWorld’s most popular open-source ERP system Covers CRM, Sales, Purchasing, Inventory, Finance, HR, and other enterprise management modules Supports thousands of community and official application extensions Provides web interface and mobile support Key Features:\nUses Pigsty-managed PostgreSQL instead of Odoo’s built-in database Supports Odoo 19.0 latest version Data persisted to independent directory /data/odoo Supports custom plugin directory /data/odoo/addons Access:\n# Odoo Web interface http://odoo.pigsty:8069 # Default admin account Username: admin Password: admin (set on first login) Use Cases:\nSMB ERP systems Alternative to SAP, Oracle ERP and other commercial solutions Enterprise applications requiring customized business processes Notes:\nOdoo container runs as uid=100, gid=101, data directory needs correct permissions First access requires creating database and setting admin password Production environments should enable HTTPS Custom modules can be installed via /data/odoo/addons ","categories":["Reference"],"description":"Deploy Odoo open-source ERP system using Pigsty-managed PostgreSQL","excerpt":"Deploy Odoo open-source ERP system using Pigsty-managed PostgreSQL","ref":"/docs/conf/odoo/","tags":"","title":"app/odoo"},{"body":"The app/dify configuration template provides a reference configuration for self-hosting Dify AI application development platform, using Pigsty-managed PostgreSQL and pgvector as vector storage.\nFor more details, see Dify Deployment Tutorial\nOverview Config Name: app/dify Node Count: Single node Description: Deploy Dify using Pigsty-managed PostgreSQL OS Distro: el8, el9, el10, d12, d13, u22, u24 OS Arch: x86_64, aarch64 Related: meta Usage:\n./configure -c app/dify [-i \u003cprimary_ip\u003e] Content Source: pigsty/conf/app/dify.yml\n--- #==============================================================# # File : dify.yml # Desc : pigsty config for running 1-node dify app # Ctime : 2025-02-24 # Mtime : 2025-12-12 # Docs : https://doc.pgsty.com/app/odoo # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# # Last Verified Dify Version: v1.8.1 on 2025-0908 # tutorial: https://doc.pgsty.com/app/dify # how to use this template: # # curl -fsSL https://repo.pigsty.io/get | bash; cd ~/pigsty # ./bootstrap # prepare local repo \u0026 ansible # ./configure -c app/dify # use this dify config template # vi pigsty.yml # IMPORTANT: CHANGE CREDENTIALS!! # ./deploy.yml # install pigsty \u0026 pgsql \u0026 minio # ./docker.yml # install docker \u0026 docker-compose # ./app.yml # install dify with docker-compose # # To replace domain name: # sed -ie 's/dify.pigsty/dify.pigsty.cc/g' pigsty.yml all: children: # the dify application dify: hosts: { 10.10.10.10: {} } vars: app: dify # specify app name to be installed (in the apps) apps: # define all applications dify: # app name, should have corresponding ~/pigsty/app/dify folder file: # data directory to be created - { path: /data/dify ,state: directory ,mode: 0755 } conf: # override /opt/dify/.env config file # change domain, mirror, proxy, secret key NGINX_SERVER_NAME: dify.pigsty # A secret key for signing and encryption, gen with `openssl rand -base64 42` (CHANGE PASSWORD!) SECRET_KEY: sk-somerandomkey # expose DIFY nginx service with port 5001 by default DIFY_PORT: 5001 # where to store dify files? the default is ./volume, we'll use another volume created above DIFY_DATA: /data/dify # proxy and mirror settings #PIP_MIRROR_URL: https://pypi.tuna.tsinghua.edu.cn/simple #SANDBOX_HTTP_PROXY: http://10.10.10.10:12345 #SANDBOX_HTTPS_PROXY: http://10.10.10.10:12345 # database credentials DB_USERNAME: dify DB_PASSWORD: difyai123456 DB_HOST: 10.10.10.10 DB_PORT: 5432 DB_DATABASE: dify VECTOR_STORE: pgvector PGVECTOR_HOST: 10.10.10.10 PGVECTOR_PORT: 5432 PGVECTOR_USER: dify PGVECTOR_PASSWORD: difyai123456 PGVECTOR_DATABASE: dify PGVECTOR_MIN_CONNECTION: 2 PGVECTOR_MAX_CONNECTION: 10 pg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta pg_users: - { name: dify ,password: difyai123456 ,pgbouncer: true ,roles: [ dbrole_admin ] ,superuser: true ,comment: dify superuser } pg_databases: - { name: dify ,owner: dify ,revokeconn: true ,comment: dify main database } - { name: dify_plugin ,owner: dify ,revokeconn: true ,comment: dify plugin_daemon database } pg_hba_rules: - { user: dify ,db: all ,addr: 172.17.0.0/16 ,auth: pwd ,title: 'allow dify access from local docker network' } node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] # make a full backup every 1am infra: { hosts: { 10.10.10.10: { infra_seq: 1 } } } etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 } }, vars: { etcd_cluster: etcd } } #minio: { hosts: { 10.10.10.10: { minio_seq: 1 } }, vars: { minio_cluster: minio } } vars: # global variables version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default|china|europe node_tune: oltp # node tuning specs: oltp,olap,tiny,crit pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml docker_enabled: true # enable docker on app group #docker_registry_mirrors: [\"https://docker.1panel.live\",\"https://docker.1ms.run\",\"https://docker.xuanyuan.me\",\"https://registry-1.docker.io\"] proxy_env: # global proxy env when downloading packages \u0026 pull docker images no_proxy: \"localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,*.pigsty,*.aliyun.com,mirrors.*,*.tsinghua.edu.cn\" #http_proxy: 127.0.0.1:12345 # add your proxy env here for downloading packages or pull images #https_proxy: 127.0.0.1:12345 # usually the proxy is format as http://user:pass@proxy.xxx.com #all_proxy: 127.0.0.1:12345 infra_portal: # domain names and upstream servers home : { domain: i.pigsty } #minio : { domain: m.pigsty ,endpoint: \"${admin_ip}:9001\" ,scheme: https ,websocket: true } dify: # nginx server config for dify domain: dify.pigsty # REPLACE WITH YOUR OWN DOMAIN! endpoint: \"10.10.10.10:5001\" # dify service endpoint: IP:PORT websocket: true # add websocket support certbot: dify.pigsty # certbot cert name, apply with `make cert` repo_enabled: false node_repo_modules: node,infra,pgsql pg_version: 18 #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root ... Explanation The app/dify template provides a one-click deployment solution for Dify AI application development platform.\nWhat is Dify:\nOpen-source LLM application development platform Supports RAG, Agent, Workflow and other AI application modes Provides visual Prompt orchestration and application building interface Supports multiple LLM backends (OpenAI, Claude, local models, etc.) Key Features:\nUses Pigsty-managed PostgreSQL instead of Dify’s built-in database Uses pgvector as vector storage (replaces Weaviate/Qdrant) Supports HTTPS and custom domain names Data persisted to independent directory /data/dify Access:\n# Dify Web interface http://dify.pigsty:5001 # Or via Nginx proxy https://dify.pigsty Use Cases:\nEnterprise internal AI application development platform RAG knowledge base Q\u0026A systems LLM-driven automated workflows AI Agent development and deployment Notes:\nMust change SECRET_KEY, generate with openssl rand -base64 42 Configure LLM API keys (e.g., OpenAI API Key) Docker network needs access to PostgreSQL (172.17.0.0/16 HBA rule configured) Recommend configuring proxy to accelerate Python package downloads ","categories":["Reference"],"description":"Deploy Dify AI application development platform using Pigsty-managed PostgreSQL","excerpt":"Deploy Dify AI application development platform using Pigsty-managed …","ref":"/docs/conf/dify/","tags":"","title":"app/dify"},{"body":"The app/electric configuration template provides a reference configuration for deploying Electric SQL real-time sync service, enabling real-time data synchronization from PostgreSQL to clients.\nOverview Config Name: app/electric Node Count: Single node Description: Deploy Electric real-time sync using Pigsty-managed PostgreSQL OS Distro: el8, el9, el10, d12, d13, u22, u24 OS Arch: x86_64, aarch64 Related: meta Usage:\n./configure -c app/electric [-i \u003cprimary_ip\u003e] Content Source: pigsty/conf/app/electric.yml\n--- #==============================================================# # File : electric.yml # Desc : pigsty config for running 1-node electric app # Ctime : 2025-03-29 # Mtime : 2025-12-12 # Docs : https://doc.pgsty.com/app/odoo # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# # tutorial: https://doc.pgsty.com/app/electric # quick start: https://electric-sql.com/docs/quickstart # how to use this template: # # curl -fsSL https://repo.pigsty.io/get | bash; cd ~/pigsty # ./bootstrap # prepare local repo \u0026 ansible # ./configure -c app/electric # use this dify config template # vi pigsty.yml # IMPORTANT: CHANGE CREDENTIALS!! # ./deploy.yml # install pigsty \u0026 pgsql \u0026 minio # ./docker.yml # install docker \u0026 docker-compose # ./app.yml # install dify with docker-compose all: children: # infra cluster for proxy, monitor, alert, etc.. infra: hosts: { 10.10.10.10: { infra_seq: 1 } } vars: app: electric apps: # define all applications electric: # app name, should have corresponding ~/pigsty/app/electric folder conf: # override /opt/electric/.env config file : https://electric-sql.com/docs/api/config DATABASE_URL: 'postgresql://electric:DBUser.Electric@10.10.10.10:5432/electric?sslmode=require' ELECTRIC_PORT: 8002 ELECTRIC_PROMETHEUS_PORT: 8003 ELECTRIC_INSECURE: true #ELECTRIC_SECRET: 1U6ItbhoQb4kGUU5wXBLbxvNf # etcd cluster for ha postgres etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 } }, vars: { etcd_cluster: etcd } } # minio cluster, s3 compatible object storage #minio: { hosts: { 10.10.10.10: { minio_seq: 1 } }, vars: { minio_cluster: minio } } # postgres example cluster: pg-meta pg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta pg_users: - {name: electric ,password: DBUser.Electric ,pgbouncer: true , replication: true ,roles: [dbrole_admin] ,comment: electric main user } pg_databases: [{ name: electric , owner: electric }] pg_hba_rules: - { user: electric , db: replication ,addr: infra ,auth: ssl ,title: 'allow electric intranet/docker ssl access' } #==============================================================# # Global Parameters #==============================================================# vars: #----------------------------------# # Meta Data #----------------------------------# version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default|china|europe node_tune: oltp # node tuning specs: oltp,olap,tiny,crit pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml docker_enabled: true # enable docker on app group #docker_registry_mirrors: [\"https://docker.1panel.live\",\"https://docker.1ms.run\",\"https://docker.xuanyuan.me\",\"https://registry-1.docker.io\"] proxy_env: # global proxy env when downloading packages no_proxy: \"localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,*.pigsty,*.aliyun.com,mirrors.*,*.myqcloud.com,*.tsinghua.edu.cn\" # http_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com # https_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com # all_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com infra_portal: # domain names and upstream servers home : { domain: i.pigsty } electric: domain: elec.pigsty endpoint: \"${admin_ip}:8002\" websocket: true # apply free ssl cert with certbot: make cert certbot: odoo.pigsty # \u003c----- replace with your own domain name! #----------------------------------# # Safe Guard #----------------------------------# # you can enable these flags after bootstrap, to prevent purging running etcd / pgsql instances etcd_safeguard: false # prevent purging running etcd instance? pg_safeguard: false # prevent purging running postgres instance? false by default #----------------------------------# # Repo, Node, Packages #----------------------------------# repo_enabled: false node_repo_modules: node,infra,pgsql pg_version: 18 # default postgres version #pg_extensions: [ pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-olap ,pg18-feat ,pg18-lang ,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl] #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root ... Explanation The app/electric template provides a one-click deployment solution for Electric SQL real-time sync service.\nWhat is Electric:\nPostgreSQL to client real-time data sync service Supports Local-first application architecture Real-time syncs data changes via logical replication Provides HTTP API for frontend application consumption Key Features:\nUses Pigsty-managed PostgreSQL as data source Captures data changes via Logical Replication Supports SSL encrypted connections Built-in Prometheus metrics endpoint Access:\n# Electric API endpoint http://elec.pigsty:8002 # Prometheus metrics http://elec.pigsty:8003/metrics Use Cases:\nBuilding Local-first applications Real-time data sync to clients Mobile and PWA data synchronization Real-time updates for collaborative applications Notes:\nElectric user needs replication permission PostgreSQL logical replication must be enabled Production environments should use SSL connection (configured with sslmode=require) ","categories":["Reference"],"description":"Deploy Electric real-time sync service using Pigsty-managed PostgreSQL","excerpt":"Deploy Electric real-time sync service using Pigsty-managed PostgreSQL","ref":"/docs/conf/electric/","tags":"","title":"app/electric"},{"body":"The app/maybe configuration template provides a reference configuration for deploying Maybe open-source personal finance management system, using Pigsty-managed PostgreSQL as the database.\nOverview Config Name: app/maybe Node Count: Single node Description: Deploy Maybe finance management using Pigsty-managed PostgreSQL OS Distro: el8, el9, el10, d12, d13, u22, u24 OS Arch: x86_64, aarch64 Related: meta Usage:\n./configure -c app/maybe [-i \u003cprimary_ip\u003e] Content Source: pigsty/conf/app/maybe.yml\n--- #==============================================================# # File : maybe.yml # Desc : pigsty config for running 1-node maybe app # Ctime : 2025-09-08 # Mtime : 2025-12-12 # Docs : https://doc.pgsty.com/app/maybe # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# # tutorial: https://doc.pgsty.com/app/maybe # how to use this template: # # curl -fsSL https://repo.pigsty.io/get | bash; cd ~/pigsty # ./bootstrap # prepare local repo \u0026 ansible # ./configure -c app/maybe # Use this maybe config template # vi pigsty.yml # IMPORTANT: CHANGE CREDENTIALS!! # ./deploy.yml # install pigsty \u0026 pgsql # ./docker.yml # install docker \u0026 docker-compose # ./app.yml # install maybe all: children: # the maybe application (personal finance management) maybe: hosts: { 10.10.10.10: {} } vars: app: maybe # specify app name to be installed (in the apps) apps: # define all applications maybe: # app name, should have corresponding ~/pigsty/app/maybe folder file: # optional directory to be created - { path: /data/maybe ,state: directory ,mode: 0755 } - { path: /data/maybe/storage ,state: directory ,mode: 0755 } conf: # override /opt/\u003capp\u003e/.env config file # Core Configuration MAYBE_VERSION: latest # Maybe image version MAYBE_PORT: 5002 # Port to expose Maybe service MAYBE_DATA: /data/maybe # Data directory for Maybe APP_DOMAIN: maybe.pigsty # Domain name for Maybe # REQUIRED: Generate with: openssl rand -hex 64 SECRET_KEY_BASE: sk-somerandomkey # Database Configuration DB_HOST: 10.10.10.10 # PostgreSQL host DB_PORT: 5432 # PostgreSQL port DB_USERNAME: maybe # PostgreSQL username DB_PASSWORD: MaybeFinance2025 # PostgreSQL password (CHANGE THIS!) DB_DATABASE: maybe_production # PostgreSQL database name # Optional: API Integration #SYNTH_API_KEY: # Get from synthfinance.com # the maybe database pg-maybe: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-maybe pg_users: - { name: maybe ,password: MaybeFinance2025 ,pgbouncer: true ,roles: [ dbrole_admin ] ,createdb: true ,comment: admin user for maybe service } - { name: maybe_ro ,password: MaybeFinance2025 ,pgbouncer: true ,roles: [ dbrole_readonly ] ,comment: read only user for maybe service } - { name: maybe_rw ,password: MaybeFinance2025 ,pgbouncer: true ,roles: [ dbrole_readwrite ] ,comment: read write user for maybe service } pg_databases: - { name: maybe_production ,owner: maybe ,revokeconn: true ,comment: maybe main database } pg_hba_rules: - { user: maybe ,db: all ,addr: 172.17.0.0/16 ,auth: pwd ,title: 'allow maybe access from local docker network' } - { user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes' } node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] # make a full backup every 1am infra: { hosts: { 10.10.10.10: { infra_seq: 1 } } } etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 } }, vars: { etcd_cluster: etcd } } #minio: { hosts: { 10.10.10.10: { minio_seq: 1 } }, vars: { minio_cluster: minio } } vars: # global variables version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default|china|europe node_tune: oltp # node tuning specs: oltp,olap,tiny,crit pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml docker_enabled: true # enable docker on app group #docker_registry_mirrors: [\"https://docker.1panel.live\",\"https://docker.1ms.run\",\"https://docker.xuanyuan.me\",\"https://registry-1.docker.io\"] proxy_env: # global proxy env when downloading packages \u0026 pull docker images no_proxy: \"localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,*.pigsty,*.aliyun.com,mirrors.*,*.tsinghua.edu.cn\" #http_proxy: 127.0.0.1:12345 # add your proxy env here for downloading packages or pull images #https_proxy: 127.0.0.1:12345 # usually the proxy is format as http://user:pass@proxy.xxx.com #all_proxy: 127.0.0.1:12345 infra_portal: # infra services exposed via portal home : { domain: i.pigsty } # default domain name minio : { domain: m.pigsty ,endpoint: \"${admin_ip}:9001\" ,scheme: https ,websocket: true } maybe: # nginx server config for maybe domain: maybe.pigsty # REPLACE WITH YOUR OWN DOMAIN! endpoint: \"10.10.10.10:5002\" # maybe service endpoint: IP:PORT websocket: true # add websocket support repo_enabled: false node_repo_modules: node,infra,pgsql #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root ... Explanation The app/maybe template provides a one-click deployment solution for Maybe open-source personal finance management system.\nWhat is Maybe:\nOpen-source personal and family finance management system Supports multi-account, multi-currency asset tracking Provides investment portfolio analysis and net worth calculation Beautiful modern web interface Key Features:\nUses Pigsty-managed PostgreSQL instead of Maybe’s built-in database Data persisted to independent directory /data/maybe Supports HTTPS and custom domain names Multi-user permission management Access:\n# Maybe Web interface http://maybe.pigsty:5002 # Or via Nginx proxy https://maybe.pigsty Use Cases:\nPersonal or family finance management Investment portfolio tracking and analysis Multi-account asset aggregation Alternative to commercial services like Mint, YNAB Notes:\nMust change SECRET_KEY_BASE, generate with openssl rand -hex 64 First access requires registering an admin account Optionally configure Synth API for stock price data ","categories":["Reference"],"description":"Deploy Maybe personal finance management system using Pigsty-managed PostgreSQL","excerpt":"Deploy Maybe personal finance management system using Pigsty-managed …","ref":"/docs/conf/maybe/","tags":"","title":"app/maybe"},{"body":"The app/teable configuration template provides a reference configuration for deploying Teable open-source no-code database, using Pigsty-managed PostgreSQL as the database.\nOverview Config Name: app/teable Node Count: Single node Description: Deploy Teable using Pigsty-managed PostgreSQL OS Distro: el8, el9, el10, d12, d13, u22, u24 OS Arch: x86_64, aarch64 Related: meta Usage:\n./configure -c app/teable [-i \u003cprimary_ip\u003e] Content Source: pigsty/conf/app/teable.yml\n--- #==============================================================# # File : teable.yml # Desc : pigsty config for running 1-node teable app # Ctime : 2025-02-24 # Mtime : 2025-12-12 # Docs : https://doc.pgsty.com/app/odoo # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# # tutorial: https://doc.pgsty.com/app/teable # how to use this template: # # curl -fsSL https://repo.pigsty.io/get | bash; cd ~/pigsty # ./bootstrap # prepare local repo \u0026 ansible # ./configure -c app/teable # use this teable config template # vi pigsty.yml # IMPORTANT: CHANGE CREDENTIALS!! # ./deploy.yml # install pigsty \u0026 pgsql \u0026 minio # ./docker.yml # install docker \u0026 docker-compose # ./app.yml # install teable with docker-compose # # To replace domain name: # sed -ie 's/teable.pigsty/teable.pigsty.cc/g' pigsty.yml all: children: # the teable application teable: hosts: { 10.10.10.10: {} } vars: app: teable # specify app name to be installed (in the apps) apps: # define all applications teable: # app name, ~/pigsty/app/teable folder conf: # override /opt/teable/.env config file # https://github.com/teableio/teable/blob/develop/dockers/examples/standalone/.env # https://help.teable.io/en/deploy/env POSTGRES_HOST: \"10.10.10.10\" POSTGRES_PORT: \"5432\" POSTGRES_DB: \"teable\" POSTGRES_USER: \"dbuser_teable\" POSTGRES_PASSWORD: \"DBUser.Teable\" PRISMA_DATABASE_URL: \"postgresql://dbuser_teable:DBUser.Teable@10.10.10.10:5432/teable\" PUBLIC_ORIGIN: \"http://tea.pigsty\" PUBLIC_DATABASE_PROXY: \"10.10.10.10:5432\" TIMEZONE: \"UTC\" # Need to support sending emails to enable the following configurations #BACKEND_MAIL_HOST: smtp.teable.io #BACKEND_MAIL_PORT: 465 #BACKEND_MAIL_SECURE: true #BACKEND_MAIL_SENDER: noreply.teable.io #BACKEND_MAIL_SENDER_NAME: Teable #BACKEND_MAIL_AUTH_USER: username #BACKEND_MAIL_AUTH_PASS: password pg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta pg_users: - { name: dbuser_teable ,password: DBUser.Teable ,pgbouncer: true ,roles: [ dbrole_admin ] ,superuser: true ,comment: teable superuser } pg_databases: - { name: teable ,owner: dbuser_teable ,comment: teable database } pg_hba_rules: - { user: teable ,db: all ,addr: 172.17.0.0/16 ,auth: pwd ,title: 'allow teable access from local docker network' } node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] # make a full backup every 1am infra: { hosts: { 10.10.10.10: { infra_seq: 1 } } } etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 } }, vars: { etcd_cluster: etcd } } minio: { hosts: { 10.10.10.10: { minio_seq: 1 } }, vars: { minio_cluster: minio } } vars: # global variables version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default|china|europe node_tune: oltp # node tuning specs: oltp,olap,tiny,crit pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml docker_enabled: true # enable docker on app group #docker_registry_mirrors: [\"https://docker.1panel.live\",\"https://docker.1ms.run\",\"https://docker.xuanyuan.me\",\"https://registry-1.docker.io\"] proxy_env: # global proxy env when downloading packages \u0026 pull docker images no_proxy: \"localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,*.pigsty,*.aliyun.com,mirrors.*,*.tsinghua.edu.cn\" #http_proxy: 127.0.0.1:12345 # add your proxy env here for downloading packages or pull images #https_proxy: 127.0.0.1:12345 # usually the proxy is format as http://user:pass@proxy.xxx.com #all_proxy: 127.0.0.1:12345 infra_portal: # domain names and upstream servers home : { domain: i.pigsty } #minio : { domain: m.pigsty ,endpoint: \"${admin_ip}:9001\" ,scheme: https ,websocket: true } teable: # nginx server config for teable domain: tea.pigsty # REPLACE IT WITH YOUR OWN DOMAIN! endpoint: \"10.10.10.10:8890\" # teable service endpoint: IP:PORT websocket: true # add websocket support certbot: tea.pigsty # certbot cert name, apply with `make cert` repo_enabled: false node_repo_modules: node,infra,pgsql pg_version: 18 #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root ... Explanation The app/teable template provides a one-click deployment solution for Teable open-source no-code database.\nWhat is Teable:\nOpen-source Airtable alternative No-code database built on PostgreSQL Supports table, kanban, calendar, form, and other views Provides API and automation workflows Key Features:\nUses Pigsty-managed PostgreSQL as underlying storage Data is stored in real PostgreSQL tables Supports direct SQL queries Can integrate with other PostgreSQL tools and extensions Access:\n# Teable Web interface http://tea.pigsty:8890 # Or via Nginx proxy https://tea.pigsty # Direct SQL access to underlying data psql postgresql://dbuser_teable:DBUser.Teable@10.10.10.10:5432/teable Use Cases:\nNeed Airtable-like functionality but want to self-host Team collaboration data management Need both API and SQL access Want data stored in real PostgreSQL Notes:\nTeable user needs superuser privileges Must configure PUBLIC_ORIGIN to external access address Supports email notifications (optional SMTP configuration) ","categories":["Reference"],"description":"Deploy Teable open-source Airtable alternative using Pigsty-managed PostgreSQL","excerpt":"Deploy Teable open-source Airtable alternative using Pigsty-managed …","ref":"/docs/conf/teable/","tags":"","title":"app/teable"},{"body":"The app/registry configuration template provides a reference configuration for deploying Docker Registry as an image proxy, usable as Docker Hub mirror acceleration or private image registry.\nOverview Config Name: app/registry Node Count: Single node Description: Deploy Docker Registry image proxy and private registry OS Distro: el8, el9, el10, d12, d13, u22, u24 OS Arch: x86_64, aarch64 Related: meta Usage:\n./configure -c app/registry [-i \u003cprimary_ip\u003e] Content Source: pigsty/conf/app/registry.yml\n--- #==============================================================# # File : registry.yml # Desc : pigsty config for running Docker Registry Mirror # Ctime : 2025-07-01 # Mtime : 2025-12-12 # Docs : https://doc.pgsty.com/app/registry # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# # tutorial: https://doc.pgsty.com/app/registry # how to use this template: # # curl -fsSL https://repo.pigsty.io/get | bash; cd ~/pigsty # ./configure -c app/registry # use this registry config template # vi pigsty.yml # IMPORTANT: CHANGE DOMAIN \u0026 CREDENTIALS! # ./deploy.yml # install pigsty # ./docker.yml # install docker \u0026 docker-compose # ./app.yml # install registry with docker-compose # # To replace domain name: # sed -ie 's/registry.pigsty/registry.your-domain.com/g' pigsty.yml #==============================================================# # Usage Instructions: #==============================================================# # # 1. Deploy the registry: # ./configure -c conf/app/registry.yml \u0026\u0026 ./deploy.yml \u0026\u0026 ./docker.yml \u0026\u0026 ./app.yml # # 2. Configure Docker clients to use the mirror: # Edit /etc/docker/daemon.json: # { # \"registry-mirrors\": [\"https://registry.your-domain.com\"], # \"insecure-registries\": [\"registry.your-domain.com\"] # } # # 3. Restart Docker daemon: # sudo systemctl restart docker # # 4. Test the registry: # docker pull nginx:latest # This will now use your mirror # # 5. Access the web UI (optional): # https://registry-ui.your-domain.com # # 6. Monitor the registry: # curl https://registry.your-domain.com/v2/_catalog # curl https://registry.your-domain.com/v2/nginx/tags/list # #==============================================================# all: children: # the docker registry mirror application registry: hosts: { 10.10.10.10: {} } vars: app: registry # specify app name to be installed apps: # define all applications registry: file: # create data directory for registry - { path: /data/registry ,state: directory ,mode: 0755 } conf: # environment variables for registry REGISTRY_DATA: /data/registry REGISTRY_PORT: 5000 REGISTRY_UI_PORT: 5080 REGISTRY_STORAGE_DELETE_ENABLED: true REGISTRY_LOG_LEVEL: info REGISTRY_PROXY_REMOTEURL: https://registry-1.docker.io REGISTRY_PROXY_TTL: 168h # basic infrastructure infra: { hosts: { 10.10.10.10: { infra_seq: 1 } } } etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 } }, vars: { etcd_cluster: etcd } } vars: #----------------------------------------------# # INFRA : https://doc.pgsty.com/infra/param #----------------------------------------------# version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default,china,europe infra_portal: # infra services exposed via portal home : { domain: i.pigsty } # default domain name # Docker Registry Mirror service configuration registry: # nginx server config for registry domain: d.pigsty # REPLACE IT WITH YOUR OWN DOMAIN! endpoint: \"10.10.10.10:5000\" # registry service endpoint: IP:PORT websocket: false # registry doesn't need websocket certbot: d.pigsty # certbot cert name, apply with `make cert` # Optional: Registry Web UI registry-ui: # nginx server config for registry UI domain: dui.pigsty # REPLACE IT WITH YOUR OWN DOMAIN! endpoint: \"10.10.10.10:5080\" # registry UI endpoint: IP:PORT websocket: false # UI doesn't need websocket certbot: d.pigsty # certbot cert name for UI #----------------------------------------------# # NODE : https://doc.pgsty.com/node/param #----------------------------------------------# repo_enabled: false node_repo_modules: node,infra,pgsql node_tune: oltp # node tuning specs: oltp,olap,tiny,crit #----------------------------------------------# # PGSQL : https://doc.pgsty.com/pgsql/param #----------------------------------------------# pg_version: 18 # Default PostgreSQL Major Version is 18 pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml pg_packages: [ pgsql-main, pgsql-common ] # pg kernel and common utils #pg_extensions: [ pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-olap ,pg18-feat ,pg18-lang ,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl] #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root ... Explanation The app/registry template provides a one-click deployment solution for Docker Registry image proxy.\nWhat is Registry:\nDocker’s official image registry implementation Can serve as Docker Hub pull-through cache Can also serve as private image registry Supports image caching and local storage Key Features:\nActs as proxy cache for Docker Hub to accelerate image pulls Caches images to local storage /data/registry Provides Web UI to view cached images Supports custom cache expiration time Configure Docker Client:\n# Edit /etc/docker/daemon.json { \"registry-mirrors\": [\"https://d.pigsty\"], \"insecure-registries\": [\"d.pigsty\"] } # Restart Docker sudo systemctl restart docker Access:\n# Registry API https://d.pigsty/v2/_catalog # Web UI http://dui.pigsty:5080 # Pull images (automatically uses proxy) docker pull nginx:latest Use Cases:\nAccelerate Docker image pulls (especially in mainland China) Reduce external network dependency Enterprise internal private image registry Offline environment image distribution Notes:\nRequires sufficient disk space to store cached images Default cache TTL is 7 days (REGISTRY_PROXY_TTL: 168h) Can configure HTTPS certificates (via certbot) ","categories":["Reference"],"description":"Deploy Docker Registry image proxy and private registry using Pigsty","excerpt":"Deploy Docker Registry image proxy and private registry using Pigsty","ref":"/docs/conf/registry/","tags":"","title":"app/registry"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/conf/_div_misc/","tags":"","title":"Misc Templates"},{"body":"The demo/el configuration template is optimized for Enterprise Linux family distributions (RHEL, Rocky Linux, Alma Linux, Oracle Linux).\nOverview Config Name: demo/el Node Count: Single node Description: Enterprise Linux optimized configuration template OS Distro: el8, el9, el10 OS Arch: x86_64, aarch64 Related: meta, demo/debian Usage:\n./configure -c demo/el [-i \u003cprimary_ip\u003e] Content Source: pigsty/conf/demo/el.yml\n--- #==============================================================# # File : el.yml # Desc : Default parameters for EL System in Pigsty # Ctime : 2020-05-22 # Mtime : 2025-12-27 # Docs : https://doc.pgsty.com/config # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# #==============================================================# # Sandbox (4-node) # #==============================================================# # admin user : vagrant (nopass ssh \u0026 sudo already set) # # 1. meta : 10.10.10.10 (2 Core | 4GB) pg-meta # # 2. node-1 : 10.10.10.11 (1 Core | 1GB) pg-test-1 # # 3. node-2 : 10.10.10.12 (1 Core | 1GB) pg-test-2 # # 4. node-3 : 10.10.10.13 (1 Core | 1GB) pg-test-3 # # (replace these ip if your 4-node env have different ip addr) # # VIP 2: (l2 vip is available inside same LAN ) # # pg-meta ---\u003e 10.10.10.2 ---\u003e 10.10.10.10 # # pg-test ---\u003e 10.10.10.3 ---\u003e 10.10.10.1{1,2,3} # #==============================================================# all: ################################################################## # CLUSTERS # ################################################################## # meta nodes, nodes, pgsql, redis, pgsql clusters are defined as # k:v pair inside `all.children`. Where the key is cluster name # and value is cluster definition consist of two parts: # `hosts`: cluster members ip and instance level variables # `vars` : cluster level variables ################################################################## children: # groups definition # infra cluster for proxy, monitor, alert, etc.. infra: { hosts: { 10.10.10.10: { infra_seq: 1 } } } # etcd cluster for ha postgres etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 } }, vars: { etcd_cluster: etcd } } # minio cluster, s3 compatible object storage minio: { hosts: { 10.10.10.10: { minio_seq: 1 } }, vars: { minio_cluster: minio } } #----------------------------------# # pgsql cluster: pg-meta (CMDB) # #----------------------------------# pg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary , pg_offline_query: true } } vars: pg_cluster: pg-meta # define business databases here: https://doc.pgsty.com/pgsql/db pg_databases: # define business databases on this cluster, array of database definition - name: meta # REQUIRED, `name` is the only mandatory field of a database definition #state: create # optional, create|absent|recreate, create by default baseline: cmdb.sql # optional, database sql baseline path, (relative path among ansible search path, e.g: files/) schemas: [pigsty] # optional, additional schemas to be created, array of schema names extensions: # optional, additional extensions to be installed: array of `{name[,schema]}` - { name: vector } # install pgvector extension on this database by default comment: pigsty meta database # optional, comment string for this database #pgbouncer: true # optional, add this database to pgbouncer database list? true by default #owner: postgres # optional, database owner, current user if not specified #template: template1 # optional, which template to use, template1 by default #strategy: FILE_COPY # optional, clone strategy: FILE_COPY or WAL_LOG (PG15+), default to PG's default #encoding: UTF8 # optional, inherited from template / cluster if not defined (UTF8) #locale: C # optional, inherited from template / cluster if not defined (C) #lc_collate: C # optional, inherited from template / cluster if not defined (C) #lc_ctype: C # optional, inherited from template / cluster if not defined (C) #locale_provider: libc # optional, locale provider: libc, icu, builtin (PG15+) #icu_locale: en-US # optional, icu locale for icu locale provider (PG15+) #icu_rules: '' # optional, icu rules for icu locale provider (PG16+) #builtin_locale: C.UTF-8 # optional, builtin locale for builtin locale provider (PG17+) #tablespace: pg_default # optional, default tablespace, pg_default by default #is_template: false # optional, mark database as template, allowing clone by any user with CREATEDB privilege #allowconn: true # optional, allow connection, true by default. false will disable connect at all #revokeconn: false # optional, revoke public connection privilege. false by default. (leave connect with grant option to owner) #register_datasource: true # optional, register this database to grafana datasources? true by default #connlimit: -1 # optional, database connection limit, default -1 disable limit #pool_auth_user: dbuser_meta # optional, all connection to this pgbouncer database will be authenticated by this user #pool_mode: transaction # optional, pgbouncer pool mode at database level, default transaction #pool_size: 64 # optional, pgbouncer pool size at database level, default 64 #pool_size_reserve: 32 # optional, pgbouncer pool size reserve at database level, default 32 #pool_size_min: 0 # optional, pgbouncer pool size min at database level, default 0 #pool_max_db_conn: 100 # optional, max database connections at database level, default 100 #- { name: grafana ,owner: dbuser_grafana ,revokeconn: true ,comment: grafana primary database } #- { name: bytebase ,owner: dbuser_bytebase ,revokeconn: true ,comment: bytebase primary database } #- { name: kong ,owner: dbuser_kong ,revokeconn: true ,comment: kong the api gateway database } #- { name: gitea ,owner: dbuser_gitea ,revokeconn: true ,comment: gitea meta database } #- { name: wiki ,owner: dbuser_wiki ,revokeconn: true ,comment: wiki meta database } # define business users here: https://doc.pgsty.com/pgsql/user pg_users: # define business users/roles on this cluster, array of user definition - name: dbuser_meta # REQUIRED, `name` is the only mandatory field of a user definition password: DBUser.Meta # optional, password, can be a scram-sha-256 hash string or plain text #login: true # optional, can log in, true by default (new biz ROLE should be false) #superuser: false # optional, is superuser? false by default #createdb: false # optional, can create database? false by default #createrole: false # optional, can create role? false by default #inherit: true # optional, can this role use inherited privileges? true by default #replication: false # optional, can this role do replication? false by default #bypassrls: false # optional, can this role bypass row level security? false by default #pgbouncer: true # optional, add this user to pgbouncer user-list? false by default (production user should be true explicitly) #connlimit: -1 # optional, user connection limit, default -1 disable limit #expire_in: 3650 # optional, now + n days when this role is expired (OVERWRITE expire_at) #expire_at: '2030-12-31' # optional, YYYY-MM-DD 'timestamp' when this role is expired (OVERWRITTEN by expire_in) #comment: pigsty admin user # optional, comment string for this user/role #roles: [dbrole_admin] # optional, belonged roles. default roles are: dbrole_{admin,readonly,readwrite,offline} #parameters: {} # optional, role level parameters with `ALTER ROLE SET` #pool_mode: transaction # optional, pgbouncer pool mode at user level, transaction by default #pool_connlimit: -1 # optional, max database connections at user level, default -1 disable limit - {name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [dbrole_readonly], comment: read-only viewer for meta database} #- {name: dbuser_grafana ,password: DBUser.Grafana ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for grafana database } #- {name: dbuser_bytebase ,password: DBUser.Bytebase ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for bytebase database } #- {name: dbuser_gitea ,password: DBUser.Gitea ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for gitea service } #- {name: dbuser_wiki ,password: DBUser.Wiki ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for wiki.js service } # define business service here: https://doc.pgsty.com/pgsql/service pg_services: # extra services in addition to pg_default_services, array of service definition # standby service will route {ip|name}:5435 to sync replica's pgbouncer (5435-\u003e6432 standby) - name: standby # required, service name, the actual svc name will be prefixed with `pg_cluster`, e.g: pg-meta-standby port: 5435 # required, service exposed port (work as kubernetes service node port mode) ip: \"*\" # optional, service bind ip address, `*` for all ip by default selector: \"[]\" # required, service member selector, use JMESPath to filter inventory dest: default # optional, destination port, default|postgres|pgbouncer|\u003cport_number\u003e, 'default' by default check: /sync # optional, health check url path, / by default backup: \"[? pg_role == `primary`]\" # backup server selector maxconn: 3000 # optional, max allowed front-end connection balance: roundrobin # optional, haproxy load balance algorithm (roundrobin by default, other: leastconn) options: 'inter 3s fastinter 1s downinter 5s rise 3 fall 3 on-marked-down shutdown-sessions slowstart 30s maxconn 3000 maxqueue 128 weight 100' # define pg extensions: https://doc.pgsty.com/pgsql/extension pg_libs: 'pg_stat_statements, auto_explain' # add timescaledb to shared_preload_libraries #pg_extensions: [] # extensions to be installed on this cluster # define HBA rules here: https://doc.pgsty.com/pgsql/hba pg_hba_rules: - {user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes'} pg_vip_enabled: true pg_vip_address: 10.10.10.2/24 pg_vip_interface: eth1 node_crontab: # make a full backup 1 am everyday - '00 01 * * * postgres /pg/bin/pg-backup full' #----------------------------------# # pgsql cluster: pg-test (3 nodes) # #----------------------------------# # pg-test ---\u003e 10.10.10.3 ---\u003e 10.10.10.1{1,2,3} pg-test: # define the new 3-node cluster pg-test hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } # primary instance, leader of cluster 10.10.10.12: { pg_seq: 2, pg_role: replica } # replica instance, follower of leader 10.10.10.13: { pg_seq: 3, pg_role: replica, pg_offline_query: true } # replica with offline access vars: pg_cluster: pg-test # define pgsql cluster name pg_users: [{ name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] }] pg_databases: [{ name: test }] # create a database and user named 'test' node_tune: tiny pg_conf: tiny.yml pg_vip_enabled: true pg_vip_address: 10.10.10.3/24 pg_vip_interface: eth1 node_crontab: # make a full backup on monday 1am, and an incremental backup during weekdays - '00 01 * * 1 postgres /pg/bin/pg-backup full' - '00 01 * * 2,3,4,5,6,7 postgres /pg/bin/pg-backup' #----------------------------------# # redis ms, sentinel, native cluster #----------------------------------# redis-ms: # redis classic primary \u0026 replica hosts: { 10.10.10.10: { redis_node: 1 , redis_instances: { 6379: { }, 6380: { replica_of: '10.10.10.10 6379' } } } } vars: { redis_cluster: redis-ms ,redis_password: 'redis.ms' ,redis_max_memory: 64MB } redis-meta: # redis sentinel x 3 hosts: { 10.10.10.11: { redis_node: 1 , redis_instances: { 26379: { } ,26380: { } ,26381: { } } } } vars: redis_cluster: redis-meta redis_password: 'redis.meta' redis_mode: sentinel redis_max_memory: 16MB redis_sentinel_monitor: # primary list for redis sentinel, use cls as name, primary ip:port - { name: redis-ms, host: 10.10.10.10, port: 6379 ,password: redis.ms, quorum: 2 } redis-test: # redis native cluster: 3m x 3s hosts: 10.10.10.12: { redis_node: 1 ,redis_instances: { 6379: { } ,6380: { } ,6381: { } } } 10.10.10.13: { redis_node: 2 ,redis_instances: { 6379: { } ,6380: { } ,6381: { } } } vars: { redis_cluster: redis-test ,redis_password: 'redis.test' ,redis_mode: cluster, redis_max_memory: 32MB } #################################################################### # VARS # #################################################################### vars: # global variables #================================================================# # VARS: INFRA # #================================================================# #----------------------------------------------------------------- # META #----------------------------------------------------------------- version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default,china,europe language: en # default language: en, zh proxy_env: # global proxy env when downloading packages no_proxy: \"localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,*.pigsty,*.aliyun.com,mirrors.*,*.myqcloud.com,*.tsinghua.edu.cn\" # http_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com # https_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com # all_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com #----------------------------------------------------------------- # CA #----------------------------------------------------------------- ca_create: true # create ca if not exists? or just abort ca_cn: pigsty-ca # ca common name, fixed as pigsty-ca cert_validity: 7300d # cert validity, 20 years by default #----------------------------------------------------------------- # INFRA_IDENTITY #----------------------------------------------------------------- #infra_seq: 1 # infra node identity, explicitly required infra_portal: # infra services exposed via portal home : { domain: i.pigsty } # default domain name infra_data: /data/infra # default data path for infrastructure data #----------------------------------------------------------------- # REPO #----------------------------------------------------------------- repo_enabled: true # create a yum repo on this infra node? repo_home: /www # repo home dir, `/www` by default repo_name: pigsty # repo name, pigsty by default repo_endpoint: http://${admin_ip}:80 # access point to this repo by domain or ip:port repo_remove: true # remove existing upstream repo repo_modules: infra,node,pgsql # which repo modules are installed in repo_upstream repo_upstream: # where to download - { name: pigsty-local ,description: 'Pigsty Local' ,module: local ,releases: [8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'http://${admin_ip}/pigsty' }} # used by intranet nodes - { name: pigsty-infra ,description: 'Pigsty INFRA' ,module: infra ,releases: [8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://repo.pigsty.io/yum/infra/$basearch' ,china: 'https://repo.pigsty.cc/yum/infra/$basearch' }} - { name: pigsty-pgsql ,description: 'Pigsty PGSQL' ,module: pgsql ,releases: [8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://repo.pigsty.io/yum/pgsql/el$releasever.$basearch' ,china: 'https://repo.pigsty.cc/yum/pgsql/el$releasever.$basearch' }} - { name: nginx ,description: 'Nginx Repo' ,module: infra ,releases: [8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://nginx.org/packages/rhel/$releasever/$basearch/' }} - { name: docker-ce ,description: 'Docker CE' ,module: infra ,releases: [8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://download.docker.com/linux/centos/$releasever/$basearch/stable' ,china: 'https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/$basearch/stable' ,europe: 'https://mirrors.xtom.de/docker-ce/linux/centos/$releasever/$basearch/stable' }} - { name: baseos ,description: 'EL 8+ BaseOS' ,module: node ,releases: [8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://dl.rockylinux.org/pub/rocky/$releasever/BaseOS/$basearch/os/' ,china: 'https://mirrors.aliyun.com/rockylinux/$releasever/BaseOS/$basearch/os/' ,europe: 'https://mirrors.xtom.de/rocky/$releasever/BaseOS/$basearch/os/' }} - { name: appstream ,description: 'EL 8+ AppStream' ,module: node ,releases: [8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://dl.rockylinux.org/pub/rocky/$releasever/AppStream/$basearch/os/' ,china: 'https://mirrors.aliyun.com/rockylinux/$releasever/AppStream/$basearch/os/' ,europe: 'https://mirrors.xtom.de/rocky/$releasever/AppStream/$basearch/os/' }} - { name: extras ,description: 'EL 8+ Extras' ,module: node ,releases: [8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://dl.rockylinux.org/pub/rocky/$releasever/extras/$basearch/os/' ,china: 'https://mirrors.aliyun.com/rockylinux/$releasever/extras/$basearch/os/' ,europe: 'https://mirrors.xtom.de/rocky/$releasever/extras/$basearch/os/' }} - { name: powertools ,description: 'EL 8 PowerTools' ,module: node ,releases: [8 ] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://dl.rockylinux.org/pub/rocky/$releasever/PowerTools/$basearch/os/' ,china: 'https://mirrors.aliyun.com/rockylinux/$releasever/PowerTools/$basearch/os/' ,europe: 'https://mirrors.xtom.de/rocky/$releasever/PowerTools/$basearch/os/' }} - { name: crb ,description: 'EL 9 CRB' ,module: node ,releases: [ 9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://dl.rockylinux.org/pub/rocky/$releasever/CRB/$basearch/os/' ,china: 'https://mirrors.aliyun.com/rockylinux/$releasever/CRB/$basearch/os/' ,europe: 'https://mirrors.xtom.de/rocky/$releasever/CRB/$basearch/os/' }} - { name: epel ,description: 'EL 8+ EPEL' ,module: node ,releases: [8,9 ] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://mirrors.edge.kernel.org/fedora-epel/$releasever/Everything/$basearch/' ,china: 'https://mirrors.aliyun.com/epel/$releasever/Everything/$basearch/' ,europe: 'https://mirrors.xtom.de/epel/$releasever/Everything/$basearch/' }} - { name: epel ,description: 'EL 10 EPEL' ,module: node ,releases: [ 10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://mirrors.edge.kernel.org/fedora-epel/$releasever.0/Everything/$basearch/' ,china: 'https://mirrors.aliyun.com/epel/$releasever.0/Everything/$basearch/' ,europe: 'https://mirrors.xtom.de/epel/$releasever.0/Everything/$basearch/' }} - { name: pgdg-common ,description: 'PostgreSQL Common' ,module: pgsql ,releases: [8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://download.postgresql.org/pub/repos/yum/common/redhat/rhel-$releasever-$basearch' ,china: 'https://mirrors.aliyun.com/postgresql/repos/yum/common/redhat/rhel-$releasever-$basearch' ,europe: 'https://mirrors.xtom.de/postgresql/repos/yum/common/redhat/rhel-$releasever-$basearch' }} - { name: pgdg-el8fix ,description: 'PostgreSQL EL8FIX' ,module: pgsql ,releases: [8 ] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://download.postgresql.org/pub/repos/yum/common/pgdg-centos8-sysupdates/redhat/rhel-8-$basearch/' ,china: 'https://mirrors.aliyun.com/postgresql/repos/yum/common/pgdg-centos8-sysupdates/redhat/rhel-8-$basearch/' ,europe: 'https://mirrors.xtom.de/postgresql/repos/yum/common/pgdg-centos8-sysupdates/redhat/rhel-8-$basearch/' }} - { name: pgdg-el9fix ,description: 'PostgreSQL EL9FIX' ,module: pgsql ,releases: [ 9 ] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://download.postgresql.org/pub/repos/yum/common/pgdg-rocky9-sysupdates/redhat/rhel-9-$basearch/' ,china: 'https://mirrors.aliyun.com/postgresql/repos/yum/common/pgdg-rocky9-sysupdates/redhat/rhel-9-$basearch/' ,europe: 'https://mirrors.xtom.de/postgresql/repos/yum/common/pgdg-rocky9-sysupdates/redhat/rhel-9-$basearch/' }} - { name: pgdg-el10fix ,description: 'PostgreSQL EL10FIX' ,module: pgsql ,releases: [ 10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://download.postgresql.org/pub/repos/yum/common/pgdg-rocky10-sysupdates/redhat/rhel-10-$basearch/' ,china: 'https://mirrors.aliyun.com/postgresql/repos/yum/common/pgdg-rocky10-sysupdates/redhat/rhel-10-$basearch/' ,europe: 'https://mirrors.xtom.de/postgresql/repos/yum/common/pgdg-rocky10-sysupdates/redhat/rhel-10-$basearch/' }} - { name: pgdg13 ,description: 'PostgreSQL 13' ,module: pgsql ,releases: [8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://download.postgresql.org/pub/repos/yum/13/redhat/rhel-$releasever-$basearch' ,china: 'https://mirrors.aliyun.com/postgresql/repos/yum/13/redhat/rhel-$releasever-$basearch' ,europe: 'https://mirrors.xtom.de/postgresql/repos/yum/13/redhat/rhel-$releasever-$basearch' }} - { name: pgdg14 ,description: 'PostgreSQL 14' ,module: pgsql ,releases: [8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://download.postgresql.org/pub/repos/yum/14/redhat/rhel-$releasever-$basearch' ,china: 'https://mirrors.aliyun.com/postgresql/repos/yum/14/redhat/rhel-$releasever-$basearch' ,europe: 'https://mirrors.xtom.de/postgresql/repos/yum/14/redhat/rhel-$releasever-$basearch' }} - { name: pgdg15 ,description: 'PostgreSQL 15' ,module: pgsql ,releases: [8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://download.postgresql.org/pub/repos/yum/15/redhat/rhel-$releasever-$basearch' ,china: 'https://mirrors.aliyun.com/postgresql/repos/yum/15/redhat/rhel-$releasever-$basearch' ,europe: 'https://mirrors.xtom.de/postgresql/repos/yum/15/redhat/rhel-$releasever-$basearch' }} - { name: pgdg16 ,description: 'PostgreSQL 16' ,module: pgsql ,releases: [8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://download.postgresql.org/pub/repos/yum/16/redhat/rhel-$releasever-$basearch' ,china: 'https://mirrors.aliyun.com/postgresql/repos/yum/16/redhat/rhel-$releasever-$basearch' ,europe: 'https://mirrors.xtom.de/postgresql/repos/yum/16/redhat/rhel-$releasever-$basearch' }} - { name: pgdg17 ,description: 'PostgreSQL 17' ,module: pgsql ,releases: [8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://download.postgresql.org/pub/repos/yum/17/redhat/rhel-$releasever-$basearch' ,china: 'https://mirrors.aliyun.com/postgresql/repos/yum/17/redhat/rhel-$releasever-$basearch' ,europe: 'https://mirrors.xtom.de/postgresql/repos/yum/17/redhat/rhel-$releasever-$basearch' }} - { name: pgdg18 ,description: 'PostgreSQL 18' ,module: pgsql ,releases: [8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://download.postgresql.org/pub/repos/yum/18/redhat/rhel-$releasever-$basearch' ,china: 'https://mirrors.aliyun.com/postgresql/repos/yum/18/redhat/rhel-$releasever-$basearch' ,europe: 'https://mirrors.xtom.de/postgresql/repos/yum/18/redhat/rhel-$releasever-$basearch' }} - { name: pgdg-beta ,description: 'PostgreSQL Testing' ,module: beta ,releases: [8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://download.postgresql.org/pub/repos/yum/testing/19/redhat/rhel-$releasever-$basearch' ,china: 'https://mirrors.aliyun.com/postgresql/repos/yum/testing/19/redhat/rhel-$releasever-$basearch' ,europe: 'https://mirrors.xtom.de/postgresql/repos/yum/testing/19/redhat/rhel-$releasever-$basearch' }} - { name: pgdg-extras ,description: 'PostgreSQL Extra' ,module: extra ,releases: [8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://download.postgresql.org/pub/repos/yum/extras/redhat/rhel-$releasever-$basearch' ,china: 'https://mirrors.aliyun.com/postgresql/repos/yum/extras/redhat/rhel-$releasever-$basearch' ,europe: 'https://mirrors.xtom.de/postgresql/repos/yum/extras/redhat/rhel-$releasever-$basearch' }} - { name: pgdg13-nonfree ,description: 'PostgreSQL 13+' ,module: extra ,releases: [8,9,10] ,arch: [x86_64 ] ,baseurl: { default: 'https://download.postgresql.org/pub/repos/yum/non-free/13/redhat/rhel-$releasever-$basearch' ,china: 'https://mirrors.aliyun.com/postgresql/repos/yum/non-free/13/redhat/rhel-$releasever-$basearch' ,europe: 'https://mirrors.xtom.de/postgresql/repos/yum/non-free/13/redhat/rhel-$releasever-$basearch' }} - { name: pgdg14-nonfree ,description: 'PostgreSQL 14+' ,module: extra ,releases: [8,9,10] ,arch: [x86_64 ] ,baseurl: { default: 'https://download.postgresql.org/pub/repos/yum/non-free/14/redhat/rhel-$releasever-$basearch' ,china: 'https://mirrors.aliyun.com/postgresql/repos/yum/non-free/14/redhat/rhel-$releasever-$basearch' ,europe: 'https://mirrors.xtom.de/postgresql/repos/yum/non-free/14/redhat/rhel-$releasever-$basearch' }} - { name: pgdg15-nonfree ,description: 'PostgreSQL 15+' ,module: extra ,releases: [8,9,10] ,arch: [x86_64 ] ,baseurl: { default: 'https://download.postgresql.org/pub/repos/yum/non-free/15/redhat/rhel-$releasever-$basearch' ,china: 'https://mirrors.aliyun.com/postgresql/repos/yum/non-free/15/redhat/rhel-$releasever-$basearch' ,europe: 'https://mirrors.xtom.de/postgresql/repos/yum/non-free/15/redhat/rhel-$releasever-$basearch' }} - { name: pgdg16-nonfree ,description: 'PostgreSQL 16+' ,module: extra ,releases: [8,9,10] ,arch: [x86_64 ] ,baseurl: { default: 'https://download.postgresql.org/pub/repos/yum/non-free/16/redhat/rhel-$releasever-$basearch' ,china: 'https://mirrors.aliyun.com/postgresql/repos/yum/non-free/16/redhat/rhel-$releasever-$basearch' ,europe: 'https://mirrors.xtom.de/postgresql/repos/yum/non-free/16/redhat/rhel-$releasever-$basearch' }} - { name: pgdg17-nonfree ,description: 'PostgreSQL 17+' ,module: extra ,releases: [8,9,10] ,arch: [x86_64 ] ,baseurl: { default: 'https://download.postgresql.org/pub/repos/yum/non-free/17/redhat/rhel-$releasever-$basearch' ,china: 'https://mirrors.aliyun.com/postgresql/repos/yum/non-free/17/redhat/rhel-$releasever-$basearch' ,europe: 'https://mirrors.xtom.de/postgresql/repos/yum/non-free/17/redhat/rhel-$releasever-$basearch' }} - { name: pgdg18-nonfree ,description: 'PostgreSQL 18+' ,module: extra ,releases: [8,9,10] ,arch: [x86_64 ] ,baseurl: { default: 'https://download.postgresql.org/pub/repos/yum/non-free/18/redhat/rhel-$releasever-$basearch' ,china: 'https://mirrors.aliyun.com/postgresql/repos/yum/non-free/18/redhat/rhel-$releasever-$basearch' ,europe: 'https://mirrors.xtom.de/postgresql/repos/yum/non-free/18/redhat/rhel-$releasever-$basearch' }} - { name: timescaledb ,description: 'TimescaleDB' ,module: extra ,releases: [8,9 ] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://packagecloud.io/timescale/timescaledb/el/$releasever/$basearch' }} - { name: percona ,description: 'Percona TDE' ,module: percona ,releases: [8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://repo.pigsty.io/yum/percona/el$releasever.$basearch' ,china: 'https://repo.pigsty.cc/yum/percona/el$releasever.$basearch' ,origin: 'http://repo.percona.com/ppg-18.1/yum/release/$releasever/RPMS/$basearch' }} - { name: wiltondb ,description: 'WiltonDB' ,module: mssql ,releases: [8,9 ] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://repo.pigsty.io/yum/mssql/el$releasever.$basearch', china: 'https://repo.pigsty.cc/yum/mssql/el$releasever.$basearch' , origin: 'https://download.copr.fedorainfracloud.org/results/wiltondb/wiltondb/epel-$releasever-$basearch/' }} - { name: groonga ,description: 'Groonga' ,module: groonga ,releases: [8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://packages.groonga.org/almalinux/$releasever/$basearch/' }} - { name: mysql ,description: 'MySQL' ,module: mysql ,releases: [8,9 ] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://repo.mysql.com/yum/mysql-8.4-community/el/$releasever/$basearch/' }} - { name: mongo ,description: 'MongoDB' ,module: mongo ,releases: [8,9 ] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/8.0/$basearch/' ,china: 'https://mirrors.aliyun.com/mongodb/yum/redhat/$releasever/mongodb-org/8.0/$basearch/' }} - { name: redis ,description: 'Redis' ,module: redis ,releases: [8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://rpmfind.net/linux/remi/enterprise/$releasever/redis72/$basearch/' }} - { name: grafana ,description: 'Grafana' ,module: grafana ,releases: [8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://rpm.grafana.com', china: 'https://mirrors.aliyun.com/grafana/yum/' }} - { name: kubernetes ,description: 'Kubernetes' ,module: kube ,releases: [8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://pkgs.k8s.io/core:/stable:/v1.33/rpm/', china: 'https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.33/rpm/' }} - { name: gitlab-ee ,description: 'Gitlab EE' ,module: gitlab ,releases: [8,9 ] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://packages.gitlab.com/gitlab/gitlab-ee/el/$releasever/$basearch' }} - { name: gitlab-ce ,description: 'Gitlab CE' ,module: gitlab ,releases: [8,9 ] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://packages.gitlab.com/gitlab/gitlab-ce/el/$releasever/$basearch' }} - { name: clickhouse ,description: 'ClickHouse' ,module: click ,releases: [8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://packages.clickhouse.com/rpm/stable/', china: 'https://mirrors.aliyun.com/clickhouse/rpm/stable/' }} repo_packages: [ node-bootstrap, infra-package, infra-addons, node-package1, node-package2, pgsql-utility, extra-modules ] repo_extra_packages: [ pgsql-main ] repo_url_packages: [] #----------------------------------------------------------------- # INFRA_PACKAGE #----------------------------------------------------------------- infra_packages: # packages to be installed on infra nodes - grafana,grafana-plugins,grafana-victorialogs-ds,grafana-victoriametrics-ds,victoria-metrics,victoria-logs,victoria-traces,vmutils,vlogscli,alertmanager - node_exporter,blackbox_exporter,nginx_exporter,pg_exporter,pev2,nginx,dnsmasq,ansible,etcd,python3-requests,redis,mcli,restic,certbot,python3-certbot-nginx infra_packages_pip: '' # pip installed packages for infra nodes #----------------------------------------------------------------- # NGINX #----------------------------------------------------------------- nginx_enabled: true # enable nginx on this infra node? nginx_clean: false # clean existing nginx config during init? nginx_exporter_enabled: true # enable nginx_exporter on this infra node? nginx_exporter_port: 9113 # nginx_exporter listen port, 9113 by default nginx_sslmode: enable # nginx ssl mode? disable,enable,enforce nginx_cert_validity: 397d # nginx self-signed cert validity, 397d by default nginx_home: /www # nginx content dir, `/www` by default (soft link to nginx_data) nginx_data: /data/nginx # nginx actual data dir, /data/nginx by default nginx_users: { admin : pigsty } # nginx basic auth users: name and pass dict nginx_port: 80 # nginx listen port, 80 by default nginx_ssl_port: 443 # nginx ssl listen port, 443 by default certbot_sign: false # sign nginx cert with certbot during setup? certbot_email: your@email.com # certbot email address, used for free ssl certbot_options: '' # certbot extra options #----------------------------------------------------------------- # DNS #----------------------------------------------------------------- dns_enabled: true # setup dnsmasq on this infra node? dns_port: 53 # dns server listen port, 53 by default dns_records: # dynamic dns records resolved by dnsmasq - \"${admin_ip} i.pigsty\" - \"${admin_ip} m.pigsty supa.pigsty api.pigsty adm.pigsty cli.pigsty ddl.pigsty\" #----------------------------------------------------------------- # VICTORIA #----------------------------------------------------------------- vmetrics_enabled: true # enable victoria-metrics on this infra node? vmetrics_clean: false # whether clean existing victoria metrics data during init? vmetrics_port: 8428 # victoria-metrics listen port, 8428 by default vmetrics_scrape_interval: 10s # victoria global scrape interval, 10s by default vmetrics_scrape_timeout: 8s # victoria global scrape timeout, 8s by default vmetrics_options: \u003e- -retentionPeriod=15d -promscrape.fileSDCheckInterval=5s vlogs_enabled: true # enable victoria-logs on this infra node? vlogs_clean: false # clean victoria-logs data during init? vlogs_port: 9428 # victoria-logs listen port, 9428 by default vlogs_options: \u003e- -retentionPeriod=15d -retention.maxDiskSpaceUsageBytes=50GiB -insert.maxLineSizeBytes=1MB -search.maxQueryDuration=120s vtraces_enabled: true # enable victoria-traces on this infra node? vtraces_clean: false # clean victoria-trace data during inti? vtraces_port: 10428 # victoria-traces listen port, 10428 by default vtraces_options: \u003e- -retentionPeriod=15d -retention.maxDiskSpaceUsageBytes=50GiB vmalert_enabled: true # enable vmalert on this infra node? vmalert_port: 8880 # vmalert listen port, 8880 by default vmalert_options: '' # vmalert extra server options #----------------------------------------------------------------- # PROMETHEUS #----------------------------------------------------------------- blackbox_enabled: true # setup blackbox_exporter on this infra node? blackbox_port: 9115 # blackbox_exporter listen port, 9115 by default blackbox_options: '' # blackbox_exporter extra server options alertmanager_enabled: true # setup alertmanager on this infra node? alertmanager_port: 9059 # alertmanager listen port, 9059 by default alertmanager_options: '' # alertmanager extra server options exporter_metrics_path: /metrics # exporter metric path, `/metrics` by default #----------------------------------------------------------------- # GRAFANA #----------------------------------------------------------------- grafana_enabled: true # enable grafana on this infra node? grafana_port: 3000 # default listen port for grafana grafana_clean: false # clean grafana data during init? grafana_admin_username: admin # grafana admin username, `admin` by default grafana_admin_password: pigsty # grafana admin password, `pigsty` by default grafana_auth_proxy: false # enable grafana auth proxy? grafana_pgurl: '' # external postgres database url for grafana if given grafana_view_password: DBUser.Viewer # password for grafana meta pg datasource #================================================================# # VARS: NODE # #================================================================# #----------------------------------------------------------------- # NODE_IDENTITY #----------------------------------------------------------------- #nodename: # [INSTANCE] # node instance identity, use hostname if missing, optional node_cluster: nodes # [CLUSTER] # node cluster identity, use 'nodes' if missing, optional nodename_overwrite: true # overwrite node's hostname with nodename? nodename_exchange: false # exchange nodename among play hosts? node_id_from_pg: true # use postgres identity as node identity if applicable? #----------------------------------------------------------------- # NODE_DNS #----------------------------------------------------------------- node_write_etc_hosts: true # modify `/etc/hosts` on target node? node_default_etc_hosts: # static dns records in `/etc/hosts` - \"${admin_ip} i.pigsty\" node_etc_hosts: [] # extra static dns records in `/etc/hosts` node_dns_method: add # how to handle dns servers: add,none,overwrite node_dns_servers: ['${admin_ip}'] # dynamic nameserver in `/etc/resolv.conf` node_dns_options: # dns resolv options in `/etc/resolv.conf` - options single-request-reopen timeout:1 #----------------------------------------------------------------- # NODE_PACKAGE #----------------------------------------------------------------- node_repo_modules: local # upstream repo to be added on node, local by default node_repo_remove: true # remove existing repo on node? node_packages: [openssh-server] # packages to be installed current nodes with latest version node_default_packages: # default packages to be installed on all nodes - lz4,unzip,bzip2,pv,jq,git,ncdu,make,patch,bash,lsof,wget,uuid,tuned,nvme-cli,numactl,sysstat,iotop,htop,rsync,tcpdump - python3,python3-pip,socat,lrzsz,net-tools,ipvsadm,telnet,ca-certificates,openssl,keepalived,etcd,haproxy,chrony,pig - zlib,yum,audit,bind-utils,readline,vim-minimal,node_exporter,grubby,openssh-server,openssh-clients,chkconfig,vector #----------------------------------------------------------------- # NODE_SEC #----------------------------------------------------------------- node_selinux_mode: permissive # set selinux mode: enforcing,permissive,disabled node_firewall_mode: zone # firewall mode: off, none, zone, zone by default node_firewall_intranet: # which intranet cidr considered as internal network - 10.0.0.0/8 - 192.168.0.0/16 - 172.16.0.0/12 node_firewall_public_port: # expose these ports to public network in (zone, strict) mode - 22 # enable ssh access - 80 # enable http access - 443 # enable https access - 5432 # enable postgresql access (think twice before exposing it!) #----------------------------------------------------------------- # NODE_TUNE #----------------------------------------------------------------- node_disable_numa: false # disable node numa, reboot required node_disable_swap: false # disable node swap, use with caution node_static_network: true # preserve dns resolver settings after reboot node_disk_prefetch: false # setup disk prefetch on HDD to increase performance node_kernel_modules: [ softdog, ip_vs, ip_vs_rr, ip_vs_wrr, ip_vs_sh ] node_hugepage_count: 0 # number of 2MB hugepage, take precedence over ratio node_hugepage_ratio: 0 # node mem hugepage ratio, 0 disable it by default node_overcommit_ratio: 0 # node mem overcommit ratio, 0 disable it by default node_tune: oltp # node tuned profile: none,oltp,olap,crit,tiny node_sysctl_params: { } # sysctl parameters in k:v format in addition to tuned #----------------------------------------------------------------- # NODE_ADMIN #----------------------------------------------------------------- node_data: /data # node main data directory, `/data` by default node_admin_enabled: true # create a admin user on target node? node_admin_uid: 88 # uid and gid for node admin user node_admin_username: dba # name of node admin user, `dba` by default node_admin_sudo: nopass # admin sudo privilege, all,nopass. nopass by default node_admin_ssh_exchange: true # exchange admin ssh key among node cluster node_admin_pk_current: true # add current user's ssh pk to admin authorized_keys node_admin_pk_list: [] # ssh public keys to be added to admin user node_aliases: {} # extra shell aliases to be added, k:v dict #----------------------------------------------------------------- # NODE_TIME #----------------------------------------------------------------- node_timezone: '' # setup node timezone, empty string to skip node_ntp_enabled: true # enable chronyd time sync service? node_ntp_servers: # ntp servers in `/etc/chrony.conf` - pool pool.ntp.org iburst node_crontab_overwrite: true # overwrite or append to `/etc/crontab`? node_crontab: [ ] # crontab entries in `/etc/crontab` #----------------------------------------------------------------- # NODE_VIP #----------------------------------------------------------------- vip_enabled: false # enable vip on this node cluster? # vip_address: [IDENTITY] # node vip address in ipv4 format, required if vip is enabled # vip_vrid: [IDENTITY] # required, integer, 1-254, should be unique among same VLAN vip_role: backup # optional, `master|backup`, backup by default, use as init role vip_preempt: false # optional, `true/false`, false by default, enable vip preemption vip_interface: eth0 # node vip network interface to listen, `eth0` by default vip_dns_suffix: '' # node vip dns name suffix, empty string by default vip_exporter_port: 9650 # keepalived exporter listen port, 9650 by default #----------------------------------------------------------------- # HAPROXY #----------------------------------------------------------------- haproxy_enabled: true # enable haproxy on this node? haproxy_clean: false # cleanup all existing haproxy config? haproxy_reload: true # reload haproxy after config? haproxy_auth_enabled: true # enable authentication for haproxy admin page haproxy_admin_username: admin # haproxy admin username, `admin` by default haproxy_admin_password: pigsty # haproxy admin password, `pigsty` by default haproxy_exporter_port: 9101 # haproxy admin/exporter port, 9101 by default haproxy_client_timeout: 24h # client side connection timeout, 24h by default haproxy_server_timeout: 24h # server side connection timeout, 24h by default haproxy_services: [] # list of haproxy service to be exposed on node #----------------------------------------------------------------- # NODE_EXPORTER #----------------------------------------------------------------- node_exporter_enabled: true # setup node_exporter on this node? node_exporter_port: 9100 # node exporter listen port, 9100 by default node_exporter_options: '--no-collector.softnet --no-collector.nvme --collector.tcpstat --collector.processes' #----------------------------------------------------------------- # VECTOR #----------------------------------------------------------------- vector_enabled: true # enable vector log collector? vector_clean: false # purge vector data dir during init? vector_data: /data/vector # vector data dir, /data/vector by default vector_port: 9598 # vector metrics port, 9598 by default vector_read_from: beginning # vector read from beginning or end vector_log_endpoint: [ infra ] # if defined, sending vector log to this endpoint. #================================================================# # VARS: DOCKER # #================================================================# docker_enabled: false # enable docker on this node? docker_data: /data/docker # docker data directory, /data/docker by default docker_storage_driver: overlay2 # docker storage driver, can be zfs, btrfs docker_cgroups_driver: systemd # docker cgroup fs driver: cgroupfs,systemd docker_registry_mirrors: [] # docker registry mirror list docker_exporter_port: 9323 # docker metrics exporter port, 9323 by default docker_image: [] # docker image to be pulled after bootstrap docker_image_cache: /tmp/docker/*.tgz # docker image cache glob pattern #================================================================# # VARS: ETCD # #================================================================# #etcd_seq: 1 # etcd instance identifier, explicitly required etcd_cluster: etcd # etcd cluster \u0026 group name, etcd by default etcd_safeguard: false # prevent purging running etcd instance? etcd_clean: true # purging existing etcd during initialization? etcd_data: /data/etcd # etcd data directory, /data/etcd by default etcd_port: 2379 # etcd client port, 2379 by default etcd_peer_port: 2380 # etcd peer port, 2380 by default etcd_init: new # etcd initial cluster state, new or existing etcd_election_timeout: 1000 # etcd election timeout, 1000ms by default etcd_heartbeat_interval: 100 # etcd heartbeat interval, 100ms by default etcd_root_password: Etcd.Root # etcd root password for RBAC, change it! #================================================================# # VARS: MINIO # #================================================================# #minio_seq: 1 # minio instance identifier, REQUIRED minio_cluster: minio # minio cluster identifier, REQUIRED minio_clean: false # cleanup minio during init?, false by default minio_user: minio # minio os user, `minio` by default minio_https: true # use https for minio, true by default minio_node: '${minio_cluster}-${minio_seq}.pigsty' # minio node name pattern minio_data: '/data/minio' # minio data dir(s), use {x...y} to specify multi drivers #minio_volumes: # minio data volumes, override defaults if specified minio_domain: sss.pigsty # minio external domain name, `sss.pigsty` by default minio_port: 9000 # minio service port, 9000 by default minio_admin_port: 9001 # minio console port, 9001 by default minio_access_key: minioadmin # root access key, `minioadmin` by default minio_secret_key: S3User.MinIO # root secret key, `S3User.MinIO` by default minio_extra_vars: '' # extra environment variables minio_provision: true # run minio provisioning tasks? minio_alias: sss # alias name for local minio deployment #minio_endpoint: https://sss.pigsty:9000 # if not specified, overwritten by defaults minio_buckets: # list of minio bucket to be created - { name: pgsql } - { name: meta ,versioning: true } - { name: data } minio_users: # list of minio user to be created - { access_key: pgbackrest ,secret_key: S3User.Backup ,policy: pgsql } - { access_key: s3user_meta ,secret_key: S3User.Meta ,policy: meta } - { access_key: s3user_data ,secret_key: S3User.Data ,policy: data } #================================================================# # VARS: REDIS # #================================================================# #redis_cluster: \u003cCLUSTER\u003e # redis cluster name, required identity parameter #redis_node: 1 \u003cNODE\u003e # redis node sequence number, node int id required #redis_instances: {} \u003cNODE\u003e # redis instances definition on this redis node redis_fs_main: /data # redis main data mountpoint, `/data` by default redis_exporter_enabled: true # install redis exporter on redis nodes? redis_exporter_port: 9121 # redis exporter listen port, 9121 by default redis_exporter_options: '' # cli args and extra options for redis exporter redis_safeguard: false # prevent purging running redis instance? redis_clean: true # purging existing redis during init? redis_rmdata: true # remove redis data when purging redis server? redis_mode: standalone # redis mode: standalone,cluster,sentinel redis_conf: redis.conf # redis config template path, except sentinel redis_bind_address: '0.0.0.0' # redis bind address, empty string will use host ip redis_max_memory: 1GB # max memory used by each redis instance redis_mem_policy: allkeys-lru # redis memory eviction policy redis_password: '' # redis password, empty string will disable password redis_rdb_save: ['1200 1'] # redis rdb save directives, disable with empty list redis_aof_enabled: false # enable redis append only file? redis_rename_commands: {} # rename redis dangerous commands redis_cluster_replicas: 1 # replica number for one master in redis cluster redis_sentinel_monitor: [] # sentinel master list, works on sentinel cluster only #================================================================# # VARS: PGSQL # #================================================================# #----------------------------------------------------------------- # PG_IDENTITY #----------------------------------------------------------------- pg_mode: pgsql #CLUSTER # pgsql cluster mode: pgsql,citus,gpsql,mssql,mysql,ivory,polar # pg_cluster: #CLUSTER # pgsql cluster name, required identity parameter # pg_seq: 0 #INSTANCE # pgsql instance seq number, required identity parameter # pg_role: replica #INSTANCE # pgsql role, required, could be primary,replica,offline # pg_instances: {} #INSTANCE # define multiple pg instances on node in `{port:ins_vars}` format # pg_upstream: #INSTANCE # repl upstream ip addr for standby cluster or cascade replica # pg_shard: #CLUSTER # pgsql shard name, optional identity for sharding clusters # pg_group: 0 #CLUSTER # pgsql shard index number, optional identity for sharding clusters # gp_role: master #CLUSTER # greenplum role of this cluster, could be master or segment pg_offline_query: false #INSTANCE # set to true to enable offline queries on this instance #----------------------------------------------------------------- # PG_BUSINESS #----------------------------------------------------------------- # postgres business object definition, overwrite in group vars pg_users: [] # postgres business users pg_databases: [] # postgres business databases pg_services: [] # postgres business services pg_hba_rules: [] # business hba rules for postgres pgb_hba_rules: [] # business hba rules for pgbouncer # global credentials, overwrite in global vars pg_dbsu_password: '' # dbsu password, empty string means no dbsu password by default pg_replication_username: replicator pg_replication_password: DBUser.Replicator pg_admin_username: dbuser_dba pg_admin_password: DBUser.DBA pg_monitor_username: dbuser_monitor pg_monitor_password: DBUser.Monitor #----------------------------------------------------------------- # PG_INSTALL #----------------------------------------------------------------- pg_dbsu: postgres # os dbsu name, postgres by default, better not change it pg_dbsu_uid: 26 # os dbsu uid and gid, 26 for default postgres users and groups pg_dbsu_sudo: limit # dbsu sudo privilege, none,limit,all,nopass. limit by default pg_dbsu_home: /var/lib/pgsql # postgresql home directory, `/var/lib/pgsql` by default pg_dbsu_ssh_exchange: true # exchange postgres dbsu ssh key among same pgsql cluster pg_version: 18 # postgres major version to be installed, 17 by default pg_bin_dir: /usr/pgsql/bin # postgres binary dir, `/usr/pgsql/bin` by default pg_log_dir: /pg/log/postgres # postgres log dir, `/pg/log/postgres` by default pg_packages: # pg packages to be installed, alias can be used - pgsql-main pgsql-common pg_extensions: [] # pg extensions to be installed, alias can be used #----------------------------------------------------------------- # PG_BOOTSTRAP #----------------------------------------------------------------- pg_data: /pg/data # postgres data directory, `/pg/data` by default pg_fs_main: /data/postgres # postgres main data directory, `/data/postgres` by default pg_fs_backup: /data/backups # postgres backup data directory, `/data/backups` by default pg_storage_type: SSD # storage type for pg main data, SSD,HDD, SSD by default pg_dummy_filesize: 64MiB # size of `/pg/dummy`, hold 64MB disk space for emergency use pg_listen: '0.0.0.0' # postgres/pgbouncer listen addresses, comma separated list pg_port: 5432 # postgres listen port, 5432 by default pg_localhost: /var/run/postgresql # postgres unix socket dir for localhost connection patroni_enabled: true # if disabled, no postgres cluster will be created during init patroni_mode: default # patroni working mode: default,pause,remove pg_namespace: /pg # top level key namespace in etcd, used by patroni \u0026 vip patroni_port: 8008 # patroni listen port, 8008 by default patroni_log_dir: /pg/log/patroni # patroni log dir, `/pg/log/patroni` by default patroni_ssl_enabled: false # secure patroni RestAPI communications with SSL? patroni_watchdog_mode: off # patroni watchdog mode: automatic,required,off. off by default patroni_username: postgres # patroni restapi username, `postgres` by default patroni_password: Patroni.API # patroni restapi password, `Patroni.API` by default pg_etcd_password: '' # etcd password for this pg cluster, '' to use pg_cluster pg_primary_db: postgres # primary database name, used by citus,etc... ,postgres by default pg_parameters: {} # extra parameters in postgresql.auto.conf pg_files: [] # extra files to be copied to postgres data directory (e.g. license) pg_conf: oltp.yml # config template: oltp,olap,crit,tiny. `oltp.yml` by default pg_max_conn: auto # postgres max connections, `auto` will use recommended value pg_shared_buffer_ratio: 0.25 # postgres shared buffers ratio, 0.25 by default, 0.1~0.4 pg_io_method: worker # io method for postgres, auto,fsync,worker,io_uring, worker by default pg_rto: 30 # recovery time objective in seconds, `30s` by default pg_rpo: 1048576 # recovery point objective in bytes, `1MiB` at most by default pg_libs: 'pg_stat_statements, auto_explain' # preloaded libraries, `pg_stat_statements,auto_explain` by default pg_delay: 0 # replication apply delay for standby cluster leader pg_checksum: true # enable data checksum for postgres cluster? pg_encoding: UTF8 # database cluster encoding, `UTF8` by default pg_locale: C # database cluster local, `C` by default pg_lc_collate: C # database cluster collate, `C` by default pg_lc_ctype: C # database character type, `C` by default #pgsodium_key: \"\" # pgsodium key, 64 hex digit, default to sha256(pg_cluster) #pgsodium_getkey_script: \"\" # pgsodium getkey script path, pgsodium_getkey by default #----------------------------------------------------------------- # PG_PROVISION #----------------------------------------------------------------- pg_provision: true # provision postgres cluster after bootstrap pg_init: pg-init # provision init script for cluster template, `pg-init` by default pg_default_roles: # default roles and users in postgres cluster - { name: dbrole_readonly ,login: false ,comment: role for global read-only access } - { name: dbrole_offline ,login: false ,comment: role for restricted read-only access } - { name: dbrole_readwrite ,login: false ,roles: [dbrole_readonly] ,comment: role for global read-write access } - { name: dbrole_admin ,login: false ,roles: [pg_monitor, dbrole_readwrite] ,comment: role for object creation } - { name: postgres ,superuser: true ,comment: system superuser } - { name: replicator ,replication: true ,roles: [pg_monitor, dbrole_readonly] ,comment: system replicator } - { name: dbuser_dba ,superuser: true ,roles: [dbrole_admin] ,pgbouncer: true ,pool_mode: session, pool_connlimit: 16 ,comment: pgsql admin user } - { name: dbuser_monitor ,roles: [pg_monitor] ,pgbouncer: true ,parameters: {log_min_duration_statement: 1000 } ,pool_mode: session ,pool_connlimit: 8 ,comment: pgsql monitor user } pg_default_privileges: # default privileges when created by admin user - GRANT USAGE ON SCHEMAS TO dbrole_readonly - GRANT SELECT ON TABLES TO dbrole_readonly - GRANT SELECT ON SEQUENCES TO dbrole_readonly - GRANT EXECUTE ON FUNCTIONS TO dbrole_readonly - GRANT USAGE ON SCHEMAS TO dbrole_offline - GRANT SELECT ON TABLES TO dbrole_offline - GRANT SELECT ON SEQUENCES TO dbrole_offline - GRANT EXECUTE ON FUNCTIONS TO dbrole_offline - GRANT INSERT ON TABLES TO dbrole_readwrite - GRANT UPDATE ON TABLES TO dbrole_readwrite - GRANT DELETE ON TABLES TO dbrole_readwrite - GRANT USAGE ON SEQUENCES TO dbrole_readwrite - GRANT UPDATE ON SEQUENCES TO dbrole_readwrite - GRANT TRUNCATE ON TABLES TO dbrole_admin - GRANT REFERENCES ON TABLES TO dbrole_admin - GRANT TRIGGER ON TABLES TO dbrole_admin - GRANT CREATE ON SCHEMAS TO dbrole_admin pg_default_schemas: [ monitor ] # default schemas to be created pg_default_extensions: # default extensions to be created - { name: pg_stat_statements ,schema: monitor } - { name: pgstattuple ,schema: monitor } - { name: pg_buffercache ,schema: monitor } - { name: pageinspect ,schema: monitor } - { name: pg_prewarm ,schema: monitor } - { name: pg_visibility ,schema: monitor } - { name: pg_freespacemap ,schema: monitor } - { name: postgres_fdw ,schema: public } - { name: file_fdw ,schema: public } - { name: btree_gist ,schema: public } - { name: btree_gin ,schema: public } - { name: pg_trgm ,schema: public } - { name: intagg ,schema: public } - { name: intarray ,schema: public } - { name: pg_repack } pg_reload: true # reload postgres after hba changes pg_default_hba_rules: # postgres default host-based authentication rules, order by `order` - {user: '${dbsu}' ,db: all ,addr: local ,auth: ident ,title: 'dbsu access via local os user ident' ,order: 100} - {user: '${dbsu}' ,db: replication ,addr: local ,auth: ident ,title: 'dbsu replication from local os ident' ,order: 150} - {user: '${repl}' ,db: replication ,addr: localhost ,auth: pwd ,title: 'replicator replication from localhost',order: 200} - {user: '${repl}' ,db: replication ,addr: intra ,auth: pwd ,title: 'replicator replication from intranet' ,order: 250} - {user: '${repl}' ,db: postgres ,addr: intra ,auth: pwd ,title: 'replicator postgres db from intranet' ,order: 300} - {user: '${monitor}' ,db: all ,addr: localhost ,auth: pwd ,title: 'monitor from localhost with password' ,order: 350} - {user: '${monitor}' ,db: all ,addr: infra ,auth: pwd ,title: 'monitor from infra host with password',order: 400} - {user: '${admin}' ,db: all ,addr: infra ,auth: ssl ,title: 'admin @ infra nodes with pwd \u0026 ssl' ,order: 450} - {user: '${admin}' ,db: all ,addr: world ,auth: ssl ,title: 'admin @ everywhere with ssl \u0026 pwd' ,order: 500} - {user: '+dbrole_readonly',db: all ,addr: localhost ,auth: pwd ,title: 'pgbouncer read/write via local socket',order: 550} - {user: '+dbrole_readonly',db: all ,addr: intra ,auth: pwd ,title: 'read/write biz user via password' ,order: 600} - {user: '+dbrole_offline' ,db: all ,addr: intra ,auth: pwd ,title: 'allow etl offline tasks from intranet',order: 650} pgb_default_hba_rules: # pgbouncer default host-based authentication rules, order by `order` - {user: '${dbsu}' ,db: pgbouncer ,addr: local ,auth: peer ,title: 'dbsu local admin access with os ident',order: 100} - {user: 'all' ,db: all ,addr: localhost ,auth: pwd ,title: 'allow all user local access with pwd' ,order: 150} - {user: '${monitor}' ,db: pgbouncer ,addr: intra ,auth: pwd ,title: 'monitor access via intranet with pwd' ,order: 200} - {user: '${monitor}' ,db: all ,addr: world ,auth: deny ,title: 'reject all other monitor access addr' ,order: 250} - {user: '${admin}' ,db: all ,addr: intra ,auth: pwd ,title: 'admin access via intranet with pwd' ,order: 300} - {user: '${admin}' ,db: all ,addr: world ,auth: deny ,title: 'reject all other admin access addr' ,order: 350} - {user: 'all' ,db: all ,addr: intra ,auth: pwd ,title: 'allow all user intra access with pwd' ,order: 400} #----------------------------------------------------------------- # PG_BACKUP #----------------------------------------------------------------- pgbackrest_enabled: true # enable pgbackrest on pgsql host? pgbackrest_log_dir: /pg/log/pgbackrest # pgbackrest log dir, `/pg/log/pgbackrest` by default pgbackrest_method: local # pgbackrest repo method: local,minio,[user-defined...] pgbackrest_init_backup: true # take a full backup after pgbackrest is initialized? pgbackrest_repo: # pgbackrest repo: https://pgbackrest.org/configuration.html#section-repository local: # default pgbackrest repo with local posix fs path: /pg/backup # local backup directory, `/pg/backup` by default retention_full_type: count # retention full backups by count retention_full: 2 # keep 2, at most 3 full backups when using local fs repo minio: # optional minio repo for pgbackrest type: s3 # minio is s3-compatible, so s3 is used s3_endpoint: sss.pigsty # minio endpoint domain name, `sss.pigsty` by default s3_region: us-east-1 # minio region, us-east-1 by default, useless for minio s3_bucket: pgsql # minio bucket name, `pgsql` by default s3_key: pgbackrest # minio user access key for pgbackrest s3_key_secret: S3User.Backup # minio user secret key for pgbackrest s3_uri_style: path # use path style uri for minio rather than host style path: /pgbackrest # minio backup path, default is `/pgbackrest` storage_port: 9000 # minio port, 9000 by default storage_ca_file: /etc/pki/ca.crt # minio ca file path, `/etc/pki/ca.crt` by default block: y # Enable block incremental backup bundle: y # bundle small files into a single file bundle_limit: 20MiB # Limit for file bundles, 20MiB for object storage bundle_size: 128MiB # Target size for file bundles, 128MiB for object storage cipher_type: aes-256-cbc # enable AES encryption for remote backup repo cipher_pass: pgBackRest # AES encryption password, default is 'pgBackRest' retention_full_type: time # retention full backup by time on minio repo retention_full: 14 # keep full backup for the the last 14 days #----------------------------------------------------------------- # PG_ACCESS #----------------------------------------------------------------- pgbouncer_enabled: true # if disabled, pgbouncer will not be launched on pgsql host pgbouncer_port: 6432 # pgbouncer listen port, 6432 by default pgbouncer_log_dir: /pg/log/pgbouncer # pgbouncer log dir, `/pg/log/pgbouncer` by default pgbouncer_auth_query: false # query postgres to retrieve unlisted business users? pgbouncer_poolmode: transaction # pooling mode: transaction,session,statement, transaction by default pgbouncer_sslmode: disable # pgbouncer client ssl mode, disable by default pgbouncer_ignore_param: [ extra_float_digits, application_name, TimeZone, DateStyle, IntervalStyle, search_path ] pg_weight: 100 #INSTANCE # relative load balance weight in service, 100 by default, 0-255 pg_service_provider: '' # dedicate haproxy node group name, or empty string for local nodes by default pg_default_service_dest: pgbouncer # default service destination if svc.dest='default' pg_default_services: # postgres default service definitions - { name: primary ,port: 5433 ,dest: default ,check: /primary ,selector: \"[]\" } - { name: replica ,port: 5434 ,dest: default ,check: /read-only ,selector: \"[]\" , backup: \"[? pg_role == `primary` || pg_role == `offline` ]\" } - { name: default ,port: 5436 ,dest: postgres ,check: /primary ,selector: \"[]\" } - { name: offline ,port: 5438 ,dest: postgres ,check: /replica ,selector: \"[? pg_role == `offline` || pg_offline_query ]\" , backup: \"[? pg_role == `replica` \u0026\u0026 !pg_offline_query]\"} pg_vip_enabled: false # enable a l2 vip for pgsql primary? false by default pg_vip_address: 127.0.0.1/24 # vip address in `\u003cipv4\u003e/\u003cmask\u003e` format, require if vip is enabled pg_vip_interface: eth0 # vip network interface to listen, eth0 by default pg_dns_suffix: '' # pgsql dns suffix, '' by default pg_dns_target: auto # auto, primary, vip, none, or ad hoc ip #----------------------------------------------------------------- # PG_MONITOR #----------------------------------------------------------------- pg_exporter_enabled: true # enable pg_exporter on pgsql hosts? pg_exporter_config: pg_exporter.yml # pg_exporter configuration file name pg_exporter_cache_ttls: '1,10,60,300' # pg_exporter collector ttl stage in seconds, '1,10,60,300' by default pg_exporter_port: 9630 # pg_exporter listen port, 9630 by default pg_exporter_params: 'sslmode=disable' # extra url parameters for pg_exporter dsn pg_exporter_url: '' # overwrite auto-generate pg dsn if specified pg_exporter_auto_discovery: true # enable auto database discovery? enabled by default pg_exporter_exclude_database: 'template0,template1,postgres' # csv of database that WILL NOT be monitored during auto-discovery pg_exporter_include_database: '' # csv of database that WILL BE monitored during auto-discovery pg_exporter_connect_timeout: 200 # pg_exporter connect timeout in ms, 200 by default pg_exporter_options: '' # overwrite extra options for pg_exporter pgbouncer_exporter_enabled: true # enable pgbouncer_exporter on pgsql hosts? pgbouncer_exporter_port: 9631 # pgbouncer_exporter listen port, 9631 by default pgbouncer_exporter_url: '' # overwrite auto-generate pgbouncer dsn if specified pgbouncer_exporter_options: '' # overwrite extra options for pgbouncer_exporter pgbackrest_exporter_enabled: true # enable pgbackrest_exporter on pgsql hosts? pgbackrest_exporter_port: 9854 # pgbackrest_exporter listen port, 9854 by default pgbackrest_exporter_options: \u003e --collect.interval=120 --log.level=info #----------------------------------------------------------------- # PG_REMOVE #----------------------------------------------------------------- pg_safeguard: false # stop pg_remove running if pg_safeguard is enabled, false by default pg_rm_data: true # remove postgres data during remove? true by default pg_rm_backup: true # remove pgbackrest backup during primary remove? true by default pg_rm_pkg: true # uninstall postgres packages during remove? true by default ... Explanation The demo/el template is optimized for Enterprise Linux family distributions.\nSupported Distributions:\nRHEL 8/9/10 Rocky Linux 8/9/10 Alma Linux 8/9/10 Oracle Linux 8/9 Key Features:\nUses EPEL and PGDG repositories Optimized for YUM/DNF package manager Supports EL-specific package names Use Cases:\nEnterprise production environments (RHEL/Rocky/Alma recommended) Long-term support and stability requirements Environments using Red Hat ecosystem ","categories":["Reference"],"description":"Configuration template optimized for Enterprise Linux (RHEL/Rocky/Alma)","excerpt":"Configuration template optimized for Enterprise Linux …","ref":"/docs/conf/el/","tags":"","title":"demo/el"},{"body":"The demo/debian configuration template is optimized for Debian and Ubuntu distributions.\nOverview Config Name: demo/debian Node Count: Single node Description: Debian/Ubuntu optimized configuration template OS Distro: d12, d13, u22, u24 OS Arch: x86_64, aarch64 Related: meta, demo/el Usage:\n./configure -c demo/debian [-i \u003cprimary_ip\u003e] Content Source: pigsty/conf/demo/debian.yml\n--- #==============================================================# # File : debian.yml # Desc : Default parameters for Debian/Ubuntu in Pigsty # Ctime : 2020-05-22 # Mtime : 2025-12-27 # Docs : https://doc.pgsty.com/config # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# #==============================================================# # Sandbox (4-node) # #==============================================================# # admin user : vagrant (nopass ssh \u0026 sudo already set) # # 1. meta : 10.10.10.10 (2 Core | 4GB) pg-meta # # 2. node-1 : 10.10.10.11 (1 Core | 1GB) pg-test-1 # # 3. node-2 : 10.10.10.12 (1 Core | 1GB) pg-test-2 # # 4. node-3 : 10.10.10.13 (1 Core | 1GB) pg-test-3 # # (replace these ip if your 4-node env have different ip addr) # # VIP 2: (l2 vip is available inside same LAN ) # # pg-meta ---\u003e 10.10.10.2 ---\u003e 10.10.10.10 # # pg-test ---\u003e 10.10.10.3 ---\u003e 10.10.10.1{1,2,3} # #==============================================================# all: ################################################################## # CLUSTERS # ################################################################## # meta nodes, nodes, pgsql, redis, pgsql clusters are defined as # k:v pair inside `all.children`. Where the key is cluster name # and value is cluster definition consist of two parts: # `hosts`: cluster members ip and instance level variables # `vars` : cluster level variables ################################################################## children: # groups definition # infra cluster for proxy, monitor, alert, etc.. infra: { hosts: { 10.10.10.10: { infra_seq: 1 } } } # etcd cluster for ha postgres etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 } }, vars: { etcd_cluster: etcd } } # minio cluster, s3 compatible object storage minio: { hosts: { 10.10.10.10: { minio_seq: 1 } }, vars: { minio_cluster: minio } } #----------------------------------# # pgsql cluster: pg-meta (CMDB) # #----------------------------------# pg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary , pg_offline_query: true } } vars: pg_cluster: pg-meta # define business databases here: https://doc.pgsty.com/pgsql/db pg_databases: # define business databases on this cluster, array of database definition - name: meta # REQUIRED, `name` is the only mandatory field of a database definition #state: create # optional, create|absent|recreate, create by default baseline: cmdb.sql # optional, database sql baseline path, (relative path among ansible search path, e.g: files/) schemas: [pigsty] # optional, additional schemas to be created, array of schema names extensions: # optional, additional extensions to be installed: array of `{name[,schema]}` - { name: vector } # install pgvector extension on this database by default comment: pigsty meta database # optional, comment string for this database #pgbouncer: true # optional, add this database to pgbouncer database list? true by default #owner: postgres # optional, database owner, current user if not specified #template: template1 # optional, which template to use, template1 by default #strategy: FILE_COPY # optional, clone strategy: FILE_COPY or WAL_LOG (PG15+), default to PG's default #encoding: UTF8 # optional, inherited from template / cluster if not defined (UTF8) #locale: C # optional, inherited from template / cluster if not defined (C) #lc_collate: C # optional, inherited from template / cluster if not defined (C) #lc_ctype: C # optional, inherited from template / cluster if not defined (C) #locale_provider: libc # optional, locale provider: libc, icu, builtin (PG15+) #icu_locale: en-US # optional, icu locale for icu locale provider (PG15+) #icu_rules: '' # optional, icu rules for icu locale provider (PG16+) #builtin_locale: C.UTF-8 # optional, builtin locale for builtin locale provider (PG17+) #tablespace: pg_default # optional, default tablespace, pg_default by default #is_template: false # optional, mark database as template, allowing clone by any user with CREATEDB privilege #allowconn: true # optional, allow connection, true by default. false will disable connect at all #revokeconn: false # optional, revoke public connection privilege. false by default. (leave connect with grant option to owner) #register_datasource: true # optional, register this database to grafana datasources? true by default #connlimit: -1 # optional, database connection limit, default -1 disable limit #pool_auth_user: dbuser_meta # optional, all connection to this pgbouncer database will be authenticated by this user #pool_mode: transaction # optional, pgbouncer pool mode at database level, default transaction #pool_size: 64 # optional, pgbouncer pool size at database level, default 64 #pool_size_reserve: 32 # optional, pgbouncer pool size reserve at database level, default 32 #pool_size_min: 0 # optional, pgbouncer pool size min at database level, default 0 #pool_max_db_conn: 100 # optional, max database connections at database level, default 100 #- { name: grafana ,owner: dbuser_grafana ,revokeconn: true ,comment: grafana primary database } #- { name: bytebase ,owner: dbuser_bytebase ,revokeconn: true ,comment: bytebase primary database } #- { name: kong ,owner: dbuser_kong ,revokeconn: true ,comment: kong the api gateway database } #- { name: gitea ,owner: dbuser_gitea ,revokeconn: true ,comment: gitea meta database } #- { name: wiki ,owner: dbuser_wiki ,revokeconn: true ,comment: wiki meta database } # define business users here: https://doc.pgsty.com/pgsql/user pg_users: # define business users/roles on this cluster, array of user definition - name: dbuser_meta # REQUIRED, `name` is the only mandatory field of a user definition password: DBUser.Meta # optional, password, can be a scram-sha-256 hash string or plain text #login: true # optional, can log in, true by default (new biz ROLE should be false) #superuser: false # optional, is superuser? false by default #createdb: false # optional, can create database? false by default #createrole: false # optional, can create role? false by default #inherit: true # optional, can this role use inherited privileges? true by default #replication: false # optional, can this role do replication? false by default #bypassrls: false # optional, can this role bypass row level security? false by default #pgbouncer: true # optional, add this user to pgbouncer user-list? false by default (production user should be true explicitly) #connlimit: -1 # optional, user connection limit, default -1 disable limit #expire_in: 3650 # optional, now + n days when this role is expired (OVERWRITE expire_at) #expire_at: '2030-12-31' # optional, YYYY-MM-DD 'timestamp' when this role is expired (OVERWRITTEN by expire_in) #comment: pigsty admin user # optional, comment string for this user/role #roles: [dbrole_admin] # optional, belonged roles. default roles are: dbrole_{admin,readonly,readwrite,offline} #parameters: {} # optional, role level parameters with `ALTER ROLE SET` #pool_mode: transaction # optional, pgbouncer pool mode at user level, transaction by default #pool_connlimit: -1 # optional, max database connections at user level, default -1 disable limit - {name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [dbrole_readonly], comment: read-only viewer for meta database} #- {name: dbuser_grafana ,password: DBUser.Grafana ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for grafana database } #- {name: dbuser_bytebase ,password: DBUser.Bytebase ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for bytebase database } #- {name: dbuser_gitea ,password: DBUser.Gitea ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for gitea service } #- {name: dbuser_wiki ,password: DBUser.Wiki ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for wiki.js service } # define business service here: https://doc.pgsty.com/pgsql/service pg_services: # extra services in addition to pg_default_services, array of service definition # standby service will route {ip|name}:5435 to sync replica's pgbouncer (5435-\u003e6432 standby) - name: standby # required, service name, the actual svc name will be prefixed with `pg_cluster`, e.g: pg-meta-standby port: 5435 # required, service exposed port (work as kubernetes service node port mode) ip: \"*\" # optional, service bind ip address, `*` for all ip by default selector: \"[]\" # required, service member selector, use JMESPath to filter inventory dest: default # optional, destination port, default|postgres|pgbouncer|\u003cport_number\u003e, 'default' by default check: /sync # optional, health check url path, / by default backup: \"[? pg_role == `primary`]\" # backup server selector maxconn: 3000 # optional, max allowed front-end connection balance: roundrobin # optional, haproxy load balance algorithm (roundrobin by default, other: leastconn) options: 'inter 3s fastinter 1s downinter 5s rise 3 fall 3 on-marked-down shutdown-sessions slowstart 30s maxconn 3000 maxqueue 128 weight 100' # define pg extensions: https://doc.pgsty.com/pgsql/extension pg_libs: 'pg_stat_statements, auto_explain' # add timescaledb to shared_preload_libraries #pg_extensions: [] # extensions to be installed on this cluster # define HBA rules here: https://doc.pgsty.com/pgsql/hba pg_hba_rules: - {user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes'} pg_vip_enabled: true pg_vip_address: 10.10.10.2/24 pg_vip_interface: eth1 node_crontab: # make a full backup 1 am everyday - '00 01 * * * postgres /pg/bin/pg-backup full' #----------------------------------# # pgsql cluster: pg-test (3 nodes) # #----------------------------------# # pg-test ---\u003e 10.10.10.3 ---\u003e 10.10.10.1{1,2,3} pg-test: # define the new 3-node cluster pg-test hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } # primary instance, leader of cluster 10.10.10.12: { pg_seq: 2, pg_role: replica } # replica instance, follower of leader 10.10.10.13: { pg_seq: 3, pg_role: replica, pg_offline_query: true } # replica with offline access vars: pg_cluster: pg-test # define pgsql cluster name pg_users: [{ name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] }] pg_databases: [{ name: test }] # create a database and user named 'test' node_tune: tiny pg_conf: tiny.yml pg_vip_enabled: true pg_vip_address: 10.10.10.3/24 pg_vip_interface: eth1 node_crontab: # make a full backup on monday 1am, and an incremental backup during weekdays - '00 01 * * 1 postgres /pg/bin/pg-backup full' - '00 01 * * 2,3,4,5,6,7 postgres /pg/bin/pg-backup' #----------------------------------# # redis ms, sentinel, native cluster #----------------------------------# redis-ms: # redis classic primary \u0026 replica hosts: { 10.10.10.10: { redis_node: 1 , redis_instances: { 6379: { }, 6380: { replica_of: '10.10.10.10 6379' } } } } vars: { redis_cluster: redis-ms ,redis_password: 'redis.ms' ,redis_max_memory: 64MB } redis-meta: # redis sentinel x 3 hosts: { 10.10.10.11: { redis_node: 1 , redis_instances: { 26379: { } ,26380: { } ,26381: { } } } } vars: redis_cluster: redis-meta redis_password: 'redis.meta' redis_mode: sentinel redis_max_memory: 16MB redis_sentinel_monitor: # primary list for redis sentinel, use cls as name, primary ip:port - { name: redis-ms, host: 10.10.10.10, port: 6379 ,password: redis.ms, quorum: 2 } redis-test: # redis native cluster: 3m x 3s hosts: 10.10.10.12: { redis_node: 1 ,redis_instances: { 6379: { } ,6380: { } ,6381: { } } } 10.10.10.13: { redis_node: 2 ,redis_instances: { 6379: { } ,6380: { } ,6381: { } } } vars: { redis_cluster: redis-test ,redis_password: 'redis.test' ,redis_mode: cluster, redis_max_memory: 32MB } #################################################################### # VARS # #################################################################### vars: # global variables #================================================================# # VARS: INFRA # #================================================================# #----------------------------------------------------------------- # META #----------------------------------------------------------------- version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default,china,europe language: en # default language: en, zh proxy_env: # global proxy env when downloading packages no_proxy: \"localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,*.pigsty,*.aliyun.com,mirrors.*,*.myqcloud.com,*.tsinghua.edu.cn\" # http_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com # https_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com # all_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com #----------------------------------------------------------------- # CA #----------------------------------------------------------------- ca_create: true # create ca if not exists? or just abort ca_cn: pigsty-ca # ca common name, fixed as pigsty-ca cert_validity: 7300d # cert validity, 20 years by default #----------------------------------------------------------------- # INFRA_IDENTITY #----------------------------------------------------------------- #infra_seq: 1 # infra node identity, explicitly required infra_portal: # infra services exposed via portal home : { domain: i.pigsty } # default domain name infra_data: /data/infra # default data path for infrastructure data #----------------------------------------------------------------- # REPO #----------------------------------------------------------------- repo_enabled: true # create a yum repo on this infra node? repo_home: /www # repo home dir, `/www` by default repo_name: pigsty # repo name, pigsty by default repo_endpoint: http://${admin_ip}:80 # access point to this repo by domain or ip:port repo_remove: true # remove existing upstream repo repo_modules: infra,node,pgsql # which repo modules are installed in repo_upstream repo_upstream: # where to download - { name: pigsty-local ,description: 'Pigsty Local' ,module: local ,releases: [11,12,13,20,22,24] ,arch: [x86_64, aarch64] ,baseurl: { default: 'http://${admin_ip}/pigsty ./' }} - { name: pigsty-pgsql ,description: 'Pigsty PgSQL' ,module: pgsql ,releases: [11,12,13,20,22,24] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://repo.pigsty.io/apt/pgsql/${distro_codename} ${distro_codename} main', china: 'https://repo.pigsty.cc/apt/pgsql/${distro_codename} ${distro_codename} main' }} - { name: pigsty-infra ,description: 'Pigsty Infra' ,module: infra ,releases: [11,12,13,20,22,24] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://repo.pigsty.io/apt/infra/ generic main' ,china: 'https://repo.pigsty.cc/apt/infra/ generic main' }} - { name: nginx ,description: 'Nginx' ,module: infra ,releases: [11,12,13,20,22,24] ,arch: [x86_64, aarch64] ,baseurl: { default: 'http://nginx.org/packages/${distro_name} ${distro_codename} nginx' }} - { name: docker-ce ,description: 'Docker' ,module: infra ,releases: [11,12,13,20,22,24] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://download.docker.com/linux/${distro_name} ${distro_codename} stable' ,china: 'https://mirrors.aliyun.com/docker-ce/linux/${distro_name} ${distro_codename} stable' }} - { name: base ,description: 'Debian Basic' ,module: node ,releases: [11,12,13 ] ,arch: [x86_64, aarch64] ,baseurl: { default: 'http://deb.debian.org/debian/ ${distro_codename} main non-free-firmware' ,china: 'https://mirrors.aliyun.com/debian/ ${distro_codename} main restricted universe multiverse' }} - { name: updates ,description: 'Debian Updates' ,module: node ,releases: [11,12,13 ] ,arch: [x86_64, aarch64] ,baseurl: { default: 'http://deb.debian.org/debian/ ${distro_codename}-updates main non-free-firmware' ,china: 'https://mirrors.aliyun.com/debian/ ${distro_codename}-updates main restricted universe multiverse' }} - { name: security ,description: 'Debian Security' ,module: node ,releases: [11,12,13 ] ,arch: [x86_64, aarch64] ,baseurl: { default: 'http://security.debian.org/debian-security ${distro_codename}-security main non-free-firmware' ,china: 'https://mirrors.aliyun.com/debian-security/ ${distro_codename}-security main non-free-firmware' }} - { name: base ,description: 'Ubuntu Basic' ,module: node ,releases: [ 20,22,24] ,arch: [x86_64 ] ,baseurl: { default: 'https://mirrors.edge.kernel.org/ubuntu/ ${distro_codename} main universe multiverse restricted' ,china: 'https://mirrors.aliyun.com/ubuntu/ ${distro_codename} main restricted universe multiverse' }} - { name: updates ,description: 'Ubuntu Updates' ,module: node ,releases: [ 20,22,24] ,arch: [x86_64 ] ,baseurl: { default: 'https://mirrors.edge.kernel.org/ubuntu/ ${distro_codename}-backports main restricted universe multiverse' ,china: 'https://mirrors.aliyun.com/ubuntu/ ${distro_codename}-updates main restricted universe multiverse' }} - { name: backports ,description: 'Ubuntu Backports' ,module: node ,releases: [ 20,22,24] ,arch: [x86_64 ] ,baseurl: { default: 'https://mirrors.edge.kernel.org/ubuntu/ ${distro_codename}-security main restricted universe multiverse' ,china: 'https://mirrors.aliyun.com/ubuntu/ ${distro_codename}-backports main restricted universe multiverse' }} - { name: security ,description: 'Ubuntu Security' ,module: node ,releases: [ 20,22,24] ,arch: [x86_64 ] ,baseurl: { default: 'https://mirrors.edge.kernel.org/ubuntu/ ${distro_codename}-updates main restricted universe multiverse' ,china: 'https://mirrors.aliyun.com/ubuntu/ ${distro_codename}-security main restricted universe multiverse' }} - { name: base ,description: 'Ubuntu Basic' ,module: node ,releases: [ 20,22,24] ,arch: [ aarch64] ,baseurl: { default: 'http://ports.ubuntu.com/ubuntu-ports/ ${distro_codename} main universe multiverse restricted' ,china: 'https://mirrors.aliyun.com/ubuntu-ports/ ${distro_codename} main restricted universe multiverse' }} - { name: updates ,description: 'Ubuntu Updates' ,module: node ,releases: [ 20,22,24] ,arch: [ aarch64] ,baseurl: { default: 'http://ports.ubuntu.com/ubuntu-ports/ ${distro_codename}-backports main restricted universe multiverse' ,china: 'https://mirrors.aliyun.com/ubuntu-ports/ ${distro_codename}-updates main restricted universe multiverse' }} - { name: backports ,description: 'Ubuntu Backports' ,module: node ,releases: [ 20,22,24] ,arch: [ aarch64] ,baseurl: { default: 'http://ports.ubuntu.com/ubuntu-ports/ ${distro_codename}-security main restricted universe multiverse' ,china: 'https://mirrors.aliyun.com/ubuntu-ports/ ${distro_codename}-backports main restricted universe multiverse' }} - { name: security ,description: 'Ubuntu Security' ,module: node ,releases: [ 20,22,24] ,arch: [ aarch64] ,baseurl: { default: 'http://ports.ubuntu.com/ubuntu-ports/ ${distro_codename}-updates main restricted universe multiverse' ,china: 'https://mirrors.aliyun.com/ubuntu-ports/ ${distro_codename}-security main restricted universe multiverse' }} - { name: pgdg ,description: 'PGDG' ,module: pgsql ,releases: [11,12,13, 22,24] ,arch: [x86_64, aarch64] ,baseurl: { default: 'http://apt.postgresql.org/pub/repos/apt/ ${distro_codename}-pgdg main' ,china: 'https://mirrors.aliyun.com/postgresql/repos/apt/ ${distro_codename}-pgdg main' }} - { name: pgdg-beta ,description: 'PGDG Beta' ,module: beta ,releases: [11,12,13, 22,24] ,arch: [x86_64, aarch64] ,baseurl: { default: 'http://apt.postgresql.org/pub/repos/apt/ ${distro_codename}-pgdg-testing main 19' ,china: 'https://mirrors.aliyun.com/postgresql/repos/apt/ ${distro_codename}-pgdg-testing main 19' }} - { name: timescaledb ,description: 'TimescaleDB' ,module: extra ,releases: [11,12,13,20,22,24] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://packagecloud.io/timescale/timescaledb/${distro_name}/ ${distro_codename} main' }} - { name: citus ,description: 'Citus' ,module: extra ,releases: [11,12, 20,22 ] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://packagecloud.io/citusdata/community/${distro_name}/ ${distro_codename} main' } } - { name: percona ,description: 'Percona TDE' ,module: percona ,releases: [11,12,13,20,22,24] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://repo.pigsty.io/apt/percona ${distro_codename} main' ,china: 'https://repo.pigsty.cc/apt/percona ${distro_codename} main' ,origin: 'http://repo.percona.com/ppg-18.1/apt ${distro_codename} main' }} - { name: wiltondb ,description: 'WiltonDB' ,module: mssql ,releases: [ 20,22,24] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://repo.pigsty.io/apt/mssql/ ${distro_codename} main' ,china: 'https://repo.pigsty.cc/apt/mssql/ ${distro_codename} main' ,origin: 'https://ppa.launchpadcontent.net/wiltondb/wiltondb/ubuntu/ ${distro_codename} main' }} - { name: groonga ,description: 'Groonga Debian' ,module: groonga ,releases: [11,12,13 ] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://packages.groonga.org/debian/ ${distro_codename} main' }} - { name: groonga ,description: 'Groonga Ubuntu' ,module: groonga ,releases: [ 20,22,24] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://ppa.launchpadcontent.net/groonga/ppa/ubuntu/ ${distro_codename} main' }} - { name: mysql ,description: 'MySQL' ,module: mysql ,releases: [11,12, 20,22,24] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://repo.mysql.com/apt/${distro_name} ${distro_codename} mysql-8.0 mysql-tools', china: 'https://mirrors.tuna.tsinghua.edu.cn/mysql/apt/${distro_name} ${distro_codename} mysql-8.0 mysql-tools' }} - { name: mongo ,description: 'MongoDB' ,module: mongo ,releases: [11,12, 20,22,24] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://repo.mongodb.org/apt/${distro_name} ${distro_codename}/mongodb-org/8.0 multiverse', china: 'https://mirrors.aliyun.com/mongodb/apt/${distro_name} ${distro_codename}/mongodb-org/8.0 multiverse' }} - { name: redis ,description: 'Redis' ,module: redis ,releases: [11,12, 20,22,24] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://packages.redis.io/deb ${distro_codename} main' }} - { name: llvm ,description: 'LLVM' ,module: llvm ,releases: [11,12,13,20,22,24] ,arch: [x86_64, aarch64] ,baseurl: { default: 'http://apt.llvm.org/${distro_codename}/ llvm-toolchain-${distro_codename} main' ,china: 'https://mirrors.tuna.tsinghua.edu.cn/llvm-apt/${distro_codename}/ llvm-toolchain-${distro_codename} main' }} - { name: haproxyd ,description: 'Haproxy Debian' ,module: haproxy ,releases: [11,12 ] ,arch: [x86_64, aarch64] ,baseurl: { default: 'http://haproxy.debian.net/ ${distro_codename}-backports-3.1 main' }} - { name: haproxyu ,description: 'Haproxy Ubuntu' ,module: haproxy ,releases: [ 20,22,24] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://ppa.launchpadcontent.net/vbernat/haproxy-3.1/ubuntu/ ${distro_codename} main' }} - { name: grafana ,description: 'Grafana' ,module: grafana ,releases: [11,12,13,20,22,24] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://apt.grafana.com stable main' ,china: 'https://mirrors.aliyun.com/grafana/apt/ stable main' }} - { name: kubernetes ,description: 'Kubernetes' ,module: kube ,releases: [11,12,13,20,22,24] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /', china: 'https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.33/deb/ /' }} - { name: gitlab-ee ,description: 'Gitlab EE' ,module: gitlab ,releases: [11,12,13,20,22,24] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://packages.gitlab.com/gitlab/gitlab-ee/${distro_name}/ ${distro_codename} main' }} - { name: gitlab-ce ,description: 'Gitlab CE' ,module: gitlab ,releases: [11,12,13,20,22,24] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://packages.gitlab.com/gitlab/gitlab-ce/${distro_name}/ ${distro_codename} main' }} - { name: clickhouse ,description: 'ClickHouse' ,module: click ,releases: [11,12,13,20,22,24] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://packages.clickhouse.com/deb/ stable main', china: 'https://mirrors.aliyun.com/clickhouse/deb/ stable main' }} repo_packages: [ node-bootstrap, infra-package, infra-addons, node-package1, node-package2, pgsql-utility, extra-modules ] repo_extra_packages: [ pgsql-main ] repo_url_packages: [] #----------------------------------------------------------------- # INFRA_PACKAGE #----------------------------------------------------------------- infra_packages: # packages to be installed on infra nodes - grafana,grafana-plugins,grafana-victorialogs-ds,grafana-victoriametrics-ds,victoria-metrics,victoria-logs,victoria-traces,vmutils,vlogscli,alertmanager - node-exporter,blackbox-exporter,nginx-exporter,pg-exporter,pev2,nginx,dnsmasq,ansible,etcd,python3-requests,redis,mcli,restic,certbot,python3-certbot-nginx infra_packages_pip: '' # pip installed packages for infra nodes #----------------------------------------------------------------- # NGINX #----------------------------------------------------------------- nginx_enabled: true # enable nginx on this infra node? nginx_clean: false # clean existing nginx config during init? nginx_exporter_enabled: true # enable nginx_exporter on this infra node? nginx_exporter_port: 9113 # nginx_exporter listen port, 9113 by default nginx_sslmode: enable # nginx ssl mode? disable,enable,enforce nginx_cert_validity: 397d # nginx self-signed cert validity, 397d by default nginx_home: /www # nginx content dir, `/www` by default (soft link to nginx_data) nginx_data: /data/nginx # nginx actual data dir, /data/nginx by default nginx_users: { admin : pigsty } # nginx basic auth users: name and pass dict nginx_port: 80 # nginx listen port, 80 by default nginx_ssl_port: 443 # nginx ssl listen port, 443 by default certbot_sign: false # sign nginx cert with certbot during setup? certbot_email: your@email.com # certbot email address, used for free ssl certbot_options: '' # certbot extra options #----------------------------------------------------------------- # DNS #----------------------------------------------------------------- dns_enabled: true # setup dnsmasq on this infra node? dns_port: 53 # dns server listen port, 53 by default dns_records: # dynamic dns records resolved by dnsmasq - \"${admin_ip} i.pigsty\" - \"${admin_ip} m.pigsty supa.pigsty api.pigsty adm.pigsty cli.pigsty ddl.pigsty\" #----------------------------------------------------------------- # VICTORIA #----------------------------------------------------------------- vmetrics_enabled: true # enable victoria-metrics on this infra node? vmetrics_clean: false # whether clean existing victoria metrics data during init? vmetrics_port: 8428 # victoria-metrics listen port, 8428 by default vmetrics_scrape_interval: 10s # victoria global scrape interval, 10s by default vmetrics_scrape_timeout: 8s # victoria global scrape timeout, 8s by default vmetrics_options: \u003e- -retentionPeriod=15d -promscrape.fileSDCheckInterval=5s vlogs_enabled: true # enable victoria-logs on this infra node? vlogs_clean: false # clean victoria-logs data during init? vlogs_port: 9428 # victoria-logs listen port, 9428 by default vlogs_options: \u003e- -retentionPeriod=15d -retention.maxDiskSpaceUsageBytes=50GiB -insert.maxLineSizeBytes=1MB -search.maxQueryDuration=120s vtraces_enabled: true # enable victoria-traces on this infra node? vtraces_clean: false # clean victoria-trace data during inti? vtraces_port: 10428 # victoria-traces listen port, 10428 by default vtraces_options: \u003e- -retentionPeriod=15d -retention.maxDiskSpaceUsageBytes=50GiB vmalert_enabled: true # enable vmalert on this infra node? vmalert_port: 8880 # vmalert listen port, 8880 by default vmalert_options: '' # vmalert extra server options #----------------------------------------------------------------- # PROMETHEUS #----------------------------------------------------------------- blackbox_enabled: true # setup blackbox_exporter on this infra node? blackbox_port: 9115 # blackbox_exporter listen port, 9115 by default blackbox_options: '' # blackbox_exporter extra server options alertmanager_enabled: true # setup alertmanager on this infra node? alertmanager_port: 9059 # alertmanager listen port, 9059 by default alertmanager_options: '' # alertmanager extra server options exporter_metrics_path: /metrics # exporter metric path, `/metrics` by default #----------------------------------------------------------------- # GRAFANA #----------------------------------------------------------------- grafana_enabled: true # enable grafana on this infra node? grafana_port: 3000 # default listen port for grafana grafana_clean: false # clean grafana data during init? grafana_admin_username: admin # grafana admin username, `admin` by default grafana_admin_password: pigsty # grafana admin password, `pigsty` by default grafana_auth_proxy: false # enable grafana auth proxy? grafana_pgurl: '' # external postgres database url for grafana if given grafana_view_password: DBUser.Viewer # password for grafana meta pg datasource #================================================================# # VARS: NODE # #================================================================# #----------------------------------------------------------------- # NODE_IDENTITY #----------------------------------------------------------------- #nodename: # [INSTANCE] # node instance identity, use hostname if missing, optional node_cluster: nodes # [CLUSTER] # node cluster identity, use 'nodes' if missing, optional nodename_overwrite: true # overwrite node's hostname with nodename? nodename_exchange: false # exchange nodename among play hosts? node_id_from_pg: true # use postgres identity as node identity if applicable? #----------------------------------------------------------------- # NODE_DNS #----------------------------------------------------------------- node_write_etc_hosts: true # modify `/etc/hosts` on target node? node_default_etc_hosts: # static dns records in `/etc/hosts` - \"${admin_ip} i.pigsty\" node_etc_hosts: [] # extra static dns records in `/etc/hosts` node_dns_method: add # how to handle dns servers: add,none,overwrite node_dns_servers: ['${admin_ip}'] # dynamic nameserver in `/etc/resolv.conf` node_dns_options: # dns resolv options in `/etc/resolv.conf` - options single-request-reopen timeout:1 #----------------------------------------------------------------- # NODE_PACKAGE #----------------------------------------------------------------- node_repo_modules: local # upstream repo to be added on node, local by default node_repo_remove: true # remove existing repo on node? node_packages: [openssh-server] # packages to be installed current nodes with latest version node_default_packages: # default packages to be installed on all nodes - lz4,unzip,bzip2,pv,jq,git,ncdu,make,patch,bash,lsof,wget,uuid,tuned,nvme-cli,numactl,sysstat,iotop,htop,rsync,tcpdump - python3,python3-pip,socat,lrzsz,net-tools,ipvsadm,telnet,ca-certificates,openssl,keepalived,etcd,haproxy,chrony,pig - zlib1g,acl,dnsutils,libreadline-dev,vim-tiny,node-exporter,openssh-server,openssh-client,vector #----------------------------------------------------------------- # NODE_SEC #----------------------------------------------------------------- node_selinux_mode: permissive # set selinux mode: enforcing,permissive,disabled node_firewall_mode: zone # firewall mode: off, none, zone, zone by default node_firewall_intranet: # which intranet cidr considered as internal network - 10.0.0.0/8 - 192.168.0.0/16 - 172.16.0.0/12 node_firewall_public_port: # expose these ports to public network in (zone, strict) mode - 22 # enable ssh access - 80 # enable http access - 443 # enable https access - 5432 # enable postgresql access (think twice before exposing it!) #----------------------------------------------------------------- # NODE_TUNE #----------------------------------------------------------------- node_disable_numa: false # disable node numa, reboot required node_disable_swap: false # disable node swap, use with caution node_static_network: true # preserve dns resolver settings after reboot node_disk_prefetch: false # setup disk prefetch on HDD to increase performance node_kernel_modules: [ softdog, ip_vs, ip_vs_rr, ip_vs_wrr, ip_vs_sh ] node_hugepage_count: 0 # number of 2MB hugepage, take precedence over ratio node_hugepage_ratio: 0 # node mem hugepage ratio, 0 disable it by default node_overcommit_ratio: 0 # node mem overcommit ratio, 0 disable it by default node_tune: oltp # node tuned profile: none,oltp,olap,crit,tiny node_sysctl_params: { } # sysctl parameters in k:v format in addition to tuned #----------------------------------------------------------------- # NODE_ADMIN #----------------------------------------------------------------- node_data: /data # node main data directory, `/data` by default node_admin_enabled: true # create a admin user on target node? node_admin_uid: 88 # uid and gid for node admin user node_admin_username: dba # name of node admin user, `dba` by default node_admin_sudo: nopass # admin sudo privilege, all,nopass. nopass by default node_admin_ssh_exchange: true # exchange admin ssh key among node cluster node_admin_pk_current: true # add current user's ssh pk to admin authorized_keys node_admin_pk_list: [] # ssh public keys to be added to admin user node_aliases: {} # extra shell aliases to be added, k:v dict #----------------------------------------------------------------- # NODE_TIME #----------------------------------------------------------------- node_timezone: '' # setup node timezone, empty string to skip node_ntp_enabled: true # enable chronyd time sync service? node_ntp_servers: # ntp servers in `/etc/chrony.conf` - pool pool.ntp.org iburst node_crontab_overwrite: true # overwrite or append to `/etc/crontab`? node_crontab: [ ] # crontab entries in `/etc/crontab` #----------------------------------------------------------------- # NODE_VIP #----------------------------------------------------------------- vip_enabled: false # enable vip on this node cluster? # vip_address: [IDENTITY] # node vip address in ipv4 format, required if vip is enabled # vip_vrid: [IDENTITY] # required, integer, 1-254, should be unique among same VLAN vip_role: backup # optional, `master|backup`, backup by default, use as init role vip_preempt: false # optional, `true/false`, false by default, enable vip preemption vip_interface: eth0 # node vip network interface to listen, `eth0` by default vip_dns_suffix: '' # node vip dns name suffix, empty string by default vip_exporter_port: 9650 # keepalived exporter listen port, 9650 by default #----------------------------------------------------------------- # HAPROXY #----------------------------------------------------------------- haproxy_enabled: true # enable haproxy on this node? haproxy_clean: false # cleanup all existing haproxy config? haproxy_reload: true # reload haproxy after config? haproxy_auth_enabled: true # enable authentication for haproxy admin page haproxy_admin_username: admin # haproxy admin username, `admin` by default haproxy_admin_password: pigsty # haproxy admin password, `pigsty` by default haproxy_exporter_port: 9101 # haproxy admin/exporter port, 9101 by default haproxy_client_timeout: 24h # client side connection timeout, 24h by default haproxy_server_timeout: 24h # server side connection timeout, 24h by default haproxy_services: [] # list of haproxy service to be exposed on node #----------------------------------------------------------------- # NODE_EXPORTER #----------------------------------------------------------------- node_exporter_enabled: true # setup node_exporter on this node? node_exporter_port: 9100 # node exporter listen port, 9100 by default node_exporter_options: '--no-collector.softnet --no-collector.nvme --collector.tcpstat --collector.processes' #----------------------------------------------------------------- # VECTOR #----------------------------------------------------------------- vector_enabled: true # enable vector log collector? vector_clean: false # purge vector data dir during init? vector_data: /data/vector # vector data dir, /data/vector by default vector_port: 9598 # vector metrics port, 9598 by default vector_read_from: beginning # vector read from beginning or end vector_log_endpoint: [ infra ] # if defined, sending vector log to this endpoint. #================================================================# # VARS: DOCKER # #================================================================# docker_enabled: false # enable docker on this node? docker_data: /data/docker # docker data directory, /data/docker by default docker_storage_driver: overlay2 # docker storage driver, can be zfs, btrfs docker_cgroups_driver: systemd # docker cgroup fs driver: cgroupfs,systemd docker_registry_mirrors: [] # docker registry mirror list docker_exporter_port: 9323 # docker metrics exporter port, 9323 by default docker_image: [] # docker image to be pulled after bootstrap docker_image_cache: /tmp/docker/*.tgz # docker image cache glob pattern #================================================================# # VARS: ETCD # #================================================================# #etcd_seq: 1 # etcd instance identifier, explicitly required etcd_cluster: etcd # etcd cluster \u0026 group name, etcd by default etcd_safeguard: false # prevent purging running etcd instance? etcd_clean: true # purging existing etcd during initialization? etcd_data: /data/etcd # etcd data directory, /data/etcd by default etcd_port: 2379 # etcd client port, 2379 by default etcd_peer_port: 2380 # etcd peer port, 2380 by default etcd_init: new # etcd initial cluster state, new or existing etcd_election_timeout: 1000 # etcd election timeout, 1000ms by default etcd_heartbeat_interval: 100 # etcd heartbeat interval, 100ms by default etcd_root_password: Etcd.Root # etcd root password for RBAC, change it! #================================================================# # VARS: MINIO # #================================================================# #minio_seq: 1 # minio instance identifier, REQUIRED minio_cluster: minio # minio cluster identifier, REQUIRED minio_clean: false # cleanup minio during init?, false by default minio_user: minio # minio os user, `minio` by default minio_https: true # use https for minio, true by default minio_node: '${minio_cluster}-${minio_seq}.pigsty' # minio node name pattern minio_data: '/data/minio' # minio data dir(s), use {x...y} to specify multi drivers #minio_volumes: # minio data volumes, override defaults if specified minio_domain: sss.pigsty # minio external domain name, `sss.pigsty` by default minio_port: 9000 # minio service port, 9000 by default minio_admin_port: 9001 # minio console port, 9001 by default minio_access_key: minioadmin # root access key, `minioadmin` by default minio_secret_key: S3User.MinIO # root secret key, `S3User.MinIO` by default minio_extra_vars: '' # extra environment variables minio_provision: true # run minio provisioning tasks? minio_alias: sss # alias name for local minio deployment #minio_endpoint: https://sss.pigsty:9000 # if not specified, overwritten by defaults minio_buckets: # list of minio bucket to be created - { name: pgsql } - { name: meta ,versioning: true } - { name: data } minio_users: # list of minio user to be created - { access_key: pgbackrest ,secret_key: S3User.Backup ,policy: pgsql } - { access_key: s3user_meta ,secret_key: S3User.Meta ,policy: meta } - { access_key: s3user_data ,secret_key: S3User.Data ,policy: data } #================================================================# # VARS: REDIS # #================================================================# #redis_cluster: \u003cCLUSTER\u003e # redis cluster name, required identity parameter #redis_node: 1 \u003cNODE\u003e # redis node sequence number, node int id required #redis_instances: {} \u003cNODE\u003e # redis instances definition on this redis node redis_fs_main: /data # redis main data mountpoint, `/data` by default redis_exporter_enabled: true # install redis exporter on redis nodes? redis_exporter_port: 9121 # redis exporter listen port, 9121 by default redis_exporter_options: '' # cli args and extra options for redis exporter redis_safeguard: false # prevent purging running redis instance? redis_clean: true # purging existing redis during init? redis_rmdata: true # remove redis data when purging redis server? redis_mode: standalone # redis mode: standalone,cluster,sentinel redis_conf: redis.conf # redis config template path, except sentinel redis_bind_address: '0.0.0.0' # redis bind address, empty string will use host ip redis_max_memory: 1GB # max memory used by each redis instance redis_mem_policy: allkeys-lru # redis memory eviction policy redis_password: '' # redis password, empty string will disable password redis_rdb_save: ['1200 1'] # redis rdb save directives, disable with empty list redis_aof_enabled: false # enable redis append only file? redis_rename_commands: {} # rename redis dangerous commands redis_cluster_replicas: 1 # replica number for one master in redis cluster redis_sentinel_monitor: [] # sentinel master list, works on sentinel cluster only #================================================================# # VARS: PGSQL # #================================================================# #----------------------------------------------------------------- # PG_IDENTITY #----------------------------------------------------------------- pg_mode: pgsql #CLUSTER # pgsql cluster mode: pgsql,citus,gpsql,mssql,mysql,ivory,polar # pg_cluster: #CLUSTER # pgsql cluster name, required identity parameter # pg_seq: 0 #INSTANCE # pgsql instance seq number, required identity parameter # pg_role: replica #INSTANCE # pgsql role, required, could be primary,replica,offline # pg_instances: {} #INSTANCE # define multiple pg instances on node in `{port:ins_vars}` format # pg_upstream: #INSTANCE # repl upstream ip addr for standby cluster or cascade replica # pg_shard: #CLUSTER # pgsql shard name, optional identity for sharding clusters # pg_group: 0 #CLUSTER # pgsql shard index number, optional identity for sharding clusters # gp_role: master #CLUSTER # greenplum role of this cluster, could be master or segment pg_offline_query: false #INSTANCE # set to true to enable offline queries on this instance #----------------------------------------------------------------- # PG_BUSINESS #----------------------------------------------------------------- # postgres business object definition, overwrite in group vars pg_users: [] # postgres business users pg_databases: [] # postgres business databases pg_services: [] # postgres business services pg_hba_rules: [] # business hba rules for postgres pgb_hba_rules: [] # business hba rules for pgbouncer # global credentials, overwrite in global vars pg_dbsu_password: '' # dbsu password, empty string means no dbsu password by default pg_replication_username: replicator pg_replication_password: DBUser.Replicator pg_admin_username: dbuser_dba pg_admin_password: DBUser.DBA pg_monitor_username: dbuser_monitor pg_monitor_password: DBUser.Monitor #----------------------------------------------------------------- # PG_INSTALL #----------------------------------------------------------------- pg_dbsu: postgres # os dbsu name, postgres by default, better not change it pg_dbsu_uid: 543 # os dbsu uid and gid, 26 for default postgres users and groups pg_dbsu_sudo: limit # dbsu sudo privilege, none,limit,all,nopass. limit by default pg_dbsu_home: /var/lib/pgsql # postgresql home directory, `/var/lib/pgsql` by default pg_dbsu_ssh_exchange: true # exchange postgres dbsu ssh key among same pgsql cluster pg_version: 18 # postgres major version to be installed, 18 by default pg_bin_dir: /usr/pgsql/bin # postgres binary dir, `/usr/pgsql/bin` by default pg_log_dir: /pg/log/postgres # postgres log dir, `/pg/log/postgres` by default pg_packages: # pg packages to be installed, alias can be used - pgsql-main pgsql-common pg_extensions: [] # pg extensions to be installed, alias can be used #----------------------------------------------------------------- # PG_BOOTSTRAP #----------------------------------------------------------------- pg_data: /pg/data # postgres data directory, `/pg/data` by default pg_fs_main: /data/postgres # postgres main data directory, `/data/postgres` by default pg_fs_backup: /data/backups # postgres backup data directory, `/data/backups` by default pg_storage_type: SSD # storage type for pg main data, SSD,HDD, SSD by default pg_dummy_filesize: 64MiB # size of `/pg/dummy`, hold 64MB disk space for emergency use pg_listen: '0.0.0.0' # postgres/pgbouncer listen addresses, comma separated list pg_port: 5432 # postgres listen port, 5432 by default pg_localhost: /var/run/postgresql # postgres unix socket dir for localhost connection patroni_enabled: true # if disabled, no postgres cluster will be created during init patroni_mode: default # patroni working mode: default,pause,remove pg_namespace: /pg # top level key namespace in etcd, used by patroni \u0026 vip patroni_port: 8008 # patroni listen port, 8008 by default patroni_log_dir: /pg/log/patroni # patroni log dir, `/pg/log/patroni` by default patroni_ssl_enabled: false # secure patroni RestAPI communications with SSL? patroni_watchdog_mode: off # patroni watchdog mode: automatic,required,off. off by default patroni_username: postgres # patroni restapi username, `postgres` by default patroni_password: Patroni.API # patroni restapi password, `Patroni.API` by default pg_etcd_password: '' # etcd password for this pg cluster, '' to use pg_cluster pg_primary_db: postgres # primary database name, used by citus,etc... ,postgres by default pg_parameters: {} # extra parameters in postgresql.auto.conf pg_files: [] # extra files to be copied to postgres data directory (e.g. license) pg_conf: oltp.yml # config template: oltp,olap,crit,tiny. `oltp.yml` by default pg_max_conn: auto # postgres max connections, `auto` will use recommended value pg_shared_buffer_ratio: 0.25 # postgres shared buffers ratio, 0.25 by default, 0.1~0.4 pg_io_method: worker # io method for postgres, auto,fsync,worker,io_uring, worker by default pg_rto: 30 # recovery time objective in seconds, `30s` by default pg_rpo: 1048576 # recovery point objective in bytes, `1MiB` at most by default pg_libs: 'pg_stat_statements, auto_explain' # preloaded libraries, `pg_stat_statements,auto_explain` by default pg_delay: 0 # replication apply delay for standby cluster leader pg_checksum: true # enable data checksum for postgres cluster? pg_encoding: UTF8 # database cluster encoding, `UTF8` by default pg_locale: C # database cluster local, `C` by default pg_lc_collate: C # database cluster collate, `C` by default pg_lc_ctype: C # database character type, `C` by default #pgsodium_key: \"\" # pgsodium key, 64 hex digit, default to sha256(pg_cluster) #pgsodium_getkey_script: \"\" # pgsodium getkey script path, pgsodium_getkey by default #----------------------------------------------------------------- # PG_PROVISION #----------------------------------------------------------------- pg_provision: true # provision postgres cluster after bootstrap pg_init: pg-init # provision init script for cluster template, `pg-init` by default pg_default_roles: # default roles and users in postgres cluster - { name: dbrole_readonly ,login: false ,comment: role for global read-only access } - { name: dbrole_offline ,login: false ,comment: role for restricted read-only access } - { name: dbrole_readwrite ,login: false ,roles: [dbrole_readonly] ,comment: role for global read-write access } - { name: dbrole_admin ,login: false ,roles: [pg_monitor, dbrole_readwrite] ,comment: role for object creation } - { name: postgres ,superuser: true ,comment: system superuser } - { name: replicator ,replication: true ,roles: [pg_monitor, dbrole_readonly] ,comment: system replicator } - { name: dbuser_dba ,superuser: true ,roles: [dbrole_admin] ,pgbouncer: true ,pool_mode: session, pool_connlimit: 16 ,comment: pgsql admin user } - { name: dbuser_monitor ,roles: [pg_monitor] ,pgbouncer: true ,parameters: {log_min_duration_statement: 1000 } ,pool_mode: session ,pool_connlimit: 8 ,comment: pgsql monitor user } pg_default_privileges: # default privileges when created by admin user - GRANT USAGE ON SCHEMAS TO dbrole_readonly - GRANT SELECT ON TABLES TO dbrole_readonly - GRANT SELECT ON SEQUENCES TO dbrole_readonly - GRANT EXECUTE ON FUNCTIONS TO dbrole_readonly - GRANT USAGE ON SCHEMAS TO dbrole_offline - GRANT SELECT ON TABLES TO dbrole_offline - GRANT SELECT ON SEQUENCES TO dbrole_offline - GRANT EXECUTE ON FUNCTIONS TO dbrole_offline - GRANT INSERT ON TABLES TO dbrole_readwrite - GRANT UPDATE ON TABLES TO dbrole_readwrite - GRANT DELETE ON TABLES TO dbrole_readwrite - GRANT USAGE ON SEQUENCES TO dbrole_readwrite - GRANT UPDATE ON SEQUENCES TO dbrole_readwrite - GRANT TRUNCATE ON TABLES TO dbrole_admin - GRANT REFERENCES ON TABLES TO dbrole_admin - GRANT TRIGGER ON TABLES TO dbrole_admin - GRANT CREATE ON SCHEMAS TO dbrole_admin pg_default_schemas: [ monitor ] # default schemas to be created pg_default_extensions: # default extensions to be created - { name: pg_stat_statements ,schema: monitor } - { name: pgstattuple ,schema: monitor } - { name: pg_buffercache ,schema: monitor } - { name: pageinspect ,schema: monitor } - { name: pg_prewarm ,schema: monitor } - { name: pg_visibility ,schema: monitor } - { name: pg_freespacemap ,schema: monitor } - { name: postgres_fdw ,schema: public } - { name: file_fdw ,schema: public } - { name: btree_gist ,schema: public } - { name: btree_gin ,schema: public } - { name: pg_trgm ,schema: public } - { name: intagg ,schema: public } - { name: intarray ,schema: public } - { name: pg_repack } pg_reload: true # reload postgres after hba changes pg_default_hba_rules: # postgres default host-based authentication rules, order by `order` - {user: '${dbsu}' ,db: all ,addr: local ,auth: ident ,title: 'dbsu access via local os user ident' ,order: 100} - {user: '${dbsu}' ,db: replication ,addr: local ,auth: ident ,title: 'dbsu replication from local os ident' ,order: 150} - {user: '${repl}' ,db: replication ,addr: localhost ,auth: pwd ,title: 'replicator replication from localhost',order: 200} - {user: '${repl}' ,db: replication ,addr: intra ,auth: pwd ,title: 'replicator replication from intranet' ,order: 250} - {user: '${repl}' ,db: postgres ,addr: intra ,auth: pwd ,title: 'replicator postgres db from intranet' ,order: 300} - {user: '${monitor}' ,db: all ,addr: localhost ,auth: pwd ,title: 'monitor from localhost with password' ,order: 350} - {user: '${monitor}' ,db: all ,addr: infra ,auth: pwd ,title: 'monitor from infra host with password',order: 400} - {user: '${admin}' ,db: all ,addr: infra ,auth: ssl ,title: 'admin @ infra nodes with pwd \u0026 ssl' ,order: 450} - {user: '${admin}' ,db: all ,addr: world ,auth: ssl ,title: 'admin @ everywhere with ssl \u0026 pwd' ,order: 500} - {user: '+dbrole_readonly',db: all ,addr: localhost ,auth: pwd ,title: 'pgbouncer read/write via local socket',order: 550} - {user: '+dbrole_readonly',db: all ,addr: intra ,auth: pwd ,title: 'read/write biz user via password' ,order: 600} - {user: '+dbrole_offline' ,db: all ,addr: intra ,auth: pwd ,title: 'allow etl offline tasks from intranet',order: 650} pgb_default_hba_rules: # pgbouncer default host-based authentication rules, order by `order` - {user: '${dbsu}' ,db: pgbouncer ,addr: local ,auth: peer ,title: 'dbsu local admin access with os ident',order: 100} - {user: 'all' ,db: all ,addr: localhost ,auth: pwd ,title: 'allow all user local access with pwd' ,order: 150} - {user: '${monitor}' ,db: pgbouncer ,addr: intra ,auth: pwd ,title: 'monitor access via intranet with pwd' ,order: 200} - {user: '${monitor}' ,db: all ,addr: world ,auth: deny ,title: 'reject all other monitor access addr' ,order: 250} - {user: '${admin}' ,db: all ,addr: intra ,auth: pwd ,title: 'admin access via intranet with pwd' ,order: 300} - {user: '${admin}' ,db: all ,addr: world ,auth: deny ,title: 'reject all other admin access addr' ,order: 350} - {user: 'all' ,db: all ,addr: intra ,auth: pwd ,title: 'allow all user intra access with pwd' ,order: 400} #----------------------------------------------------------------- # PG_BACKUP #----------------------------------------------------------------- pgbackrest_enabled: true # enable pgbackrest on pgsql host? pgbackrest_log_dir: /pg/log/pgbackrest # pgbackrest log dir, `/pg/log/pgbackrest` by default pgbackrest_method: local # pgbackrest repo method: local,minio,[user-defined...] pgbackrest_init_backup: true # take a full backup after pgbackrest is initialized? pgbackrest_repo: # pgbackrest repo: https://pgbackrest.org/configuration.html#section-repository local: # default pgbackrest repo with local posix fs path: /pg/backup # local backup directory, `/pg/backup` by default retention_full_type: count # retention full backups by count retention_full: 2 # keep 2, at most 3 full backups when using local fs repo minio: # optional minio repo for pgbackrest type: s3 # minio is s3-compatible, so s3 is used s3_endpoint: sss.pigsty # minio endpoint domain name, `sss.pigsty` by default s3_region: us-east-1 # minio region, us-east-1 by default, useless for minio s3_bucket: pgsql # minio bucket name, `pgsql` by default s3_key: pgbackrest # minio user access key for pgbackrest s3_key_secret: S3User.Backup # minio user secret key for pgbackrest s3_uri_style: path # use path style uri for minio rather than host style path: /pgbackrest # minio backup path, default is `/pgbackrest` storage_port: 9000 # minio port, 9000 by default storage_ca_file: /etc/pki/ca.crt # minio ca file path, `/etc/pki/ca.crt` by default block: y # Enable block incremental backup bundle: y # bundle small files into a single file bundle_limit: 20MiB # Limit for file bundles, 20MiB for object storage bundle_size: 128MiB # Target size for file bundles, 128MiB for object storage cipher_type: aes-256-cbc # enable AES encryption for remote backup repo cipher_pass: pgBackRest # AES encryption password, default is 'pgBackRest' retention_full_type: time # retention full backup by time on minio repo retention_full: 14 # keep full backup for the the last 14 days #----------------------------------------------------------------- # PG_ACCESS #----------------------------------------------------------------- pgbouncer_enabled: true # if disabled, pgbouncer will not be launched on pgsql host pgbouncer_port: 6432 # pgbouncer listen port, 6432 by default pgbouncer_log_dir: /pg/log/pgbouncer # pgbouncer log dir, `/pg/log/pgbouncer` by default pgbouncer_auth_query: false # query postgres to retrieve unlisted business users? pgbouncer_poolmode: transaction # pooling mode: transaction,session,statement, transaction by default pgbouncer_sslmode: disable # pgbouncer client ssl mode, disable by default pgbouncer_ignore_param: [ extra_float_digits, application_name, TimeZone, DateStyle, IntervalStyle, search_path ] pg_weight: 100 #INSTANCE # relative load balance weight in service, 100 by default, 0-255 pg_service_provider: '' # dedicate haproxy node group name, or empty string for local nodes by default pg_default_service_dest: pgbouncer # default service destination if svc.dest='default' pg_default_services: # postgres default service definitions - { name: primary ,port: 5433 ,dest: default ,check: /primary ,selector: \"[]\" } - { name: replica ,port: 5434 ,dest: default ,check: /read-only ,selector: \"[]\" , backup: \"[? pg_role == `primary` || pg_role == `offline` ]\" } - { name: default ,port: 5436 ,dest: postgres ,check: /primary ,selector: \"[]\" } - { name: offline ,port: 5438 ,dest: postgres ,check: /replica ,selector: \"[? pg_role == `offline` || pg_offline_query ]\" , backup: \"[? pg_role == `replica` \u0026\u0026 !pg_offline_query]\"} pg_vip_enabled: false # enable a l2 vip for pgsql primary? false by default pg_vip_address: 127.0.0.1/24 # vip address in `\u003cipv4\u003e/\u003cmask\u003e` format, require if vip is enabled pg_vip_interface: eth0 # vip network interface to listen, eth0 by default pg_dns_suffix: '' # pgsql dns suffix, '' by default pg_dns_target: auto # auto, primary, vip, none, or ad hoc ip #----------------------------------------------------------------- # PG_MONITOR #----------------------------------------------------------------- pg_exporter_enabled: true # enable pg_exporter on pgsql hosts? pg_exporter_config: pg_exporter.yml # pg_exporter configuration file name pg_exporter_cache_ttls: '1,10,60,300' # pg_exporter collector ttl stage in seconds, '1,10,60,300' by default pg_exporter_port: 9630 # pg_exporter listen port, 9630 by default pg_exporter_params: 'sslmode=disable' # extra url parameters for pg_exporter dsn pg_exporter_url: '' # overwrite auto-generate pg dsn if specified pg_exporter_auto_discovery: true # enable auto database discovery? enabled by default pg_exporter_exclude_database: 'template0,template1,postgres' # csv of database that WILL NOT be monitored during auto-discovery pg_exporter_include_database: '' # csv of database that WILL BE monitored during auto-discovery pg_exporter_connect_timeout: 200 # pg_exporter connect timeout in ms, 200 by default pg_exporter_options: '' # overwrite extra options for pg_exporter pgbouncer_exporter_enabled: true # enable pgbouncer_exporter on pgsql hosts? pgbouncer_exporter_port: 9631 # pgbouncer_exporter listen port, 9631 by default pgbouncer_exporter_url: '' # overwrite auto-generate pgbouncer dsn if specified pgbouncer_exporter_options: '' # overwrite extra options for pgbouncer_exporter pgbackrest_exporter_enabled: true # enable pgbackrest_exporter on pgsql hosts? pgbackrest_exporter_port: 9854 # pgbackrest_exporter listen port, 9854 by default pgbackrest_exporter_options: \u003e --collect.interval=120 --log.level=info #----------------------------------------------------------------- # PG_REMOVE #----------------------------------------------------------------- pg_safeguard: false # stop pg_remove running if pg_safeguard is enabled, false by default pg_rm_data: true # remove postgres data during remove? true by default pg_rm_backup: true # remove pgbackrest backup during primary remove? true by default pg_rm_pkg: true # uninstall postgres packages during remove? true by default ... Explanation The demo/debian template is optimized for Debian and Ubuntu distributions.\nSupported Distributions:\nDebian 12 (Bookworm) Debian 13 (Trixie) Ubuntu 22.04 LTS (Jammy) Ubuntu 24.04 LTS (Noble) Key Features:\nUses PGDG APT repositories Optimized for APT package manager Supports Debian/Ubuntu-specific package names Use Cases:\nCloud servers (Ubuntu widely used) Container environments (Debian commonly used as base image) Development and testing environments ","categories":["Reference"],"description":"Configuration template optimized for Debian/Ubuntu","excerpt":"Configuration template optimized for Debian/Ubuntu","ref":"/docs/conf/debian/","tags":"","title":"demo/debian"},{"body":"The demo/demo configuration template is used by Pigsty’s public demo site, demonstrating how to expose services publicly, configure SSL certificates, and install all available extensions.\nIf you want to set up your own public service on a cloud server, you can use this template as a reference.\nOverview Config Name: demo/demo Node Count: Single node Description: Pigsty public demo site configuration OS Distro: el8, el9, el10, d12, d13, u22, u24 OS Arch: x86_64 Related: meta, rich Usage:\n./configure -c demo/demo [-i \u003cprimary_ip\u003e] Key Features This template enhances the meta template with:\nSSL certificate and custom domain configuration (e.g., pigsty.cc) Downloads and installs all available PostgreSQL 18 extensions Enables Docker with image acceleration Deploys MinIO object storage Pre-configures multiple business databases and users Adds Redis primary-replica instance examples Adds FerretDB MongoDB-compatible cluster Adds Kafka sample cluster Content Source: pigsty/conf/demo/demo.yml\n--- #==============================================================# # File : demo.yml # Desc : Pigsty Public Demo Configuration # Ctime : 2020-05-22 # Mtime : 2025-12-12 # Docs : https://doc.pgsty.com/config # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# all: children: # infra cluster for proxy, monitor, alert, etc.. infra: hosts: { 10.10.10.10: { infra_seq: 1 } } vars: nodename: pigsty.cc # overwrite the default hostname node_id_from_pg: false # do not use the pg identity as hostname docker_enabled: true # enable docker on this node docker_registry_mirrors: [\"https://mirror.ccs.tencentyun.com\", \"https://docker.1ms.run\"] # ./pgsql-monitor.yml -l infra # monitor 'external' PostgreSQL instance pg_exporters: # treat local postgres as RDS for demonstration purpose 20001: { pg_cluster: pg-foo, pg_seq: 1, pg_host: 10.10.10.10 } #20002: { pg_cluster: pg-bar, pg_seq: 1, pg_host: 10.10.10.11 , pg_port: 5432 } #20003: { pg_cluster: pg-bar, pg_seq: 2, pg_host: 10.10.10.12 , pg_exporter_url: 'postgres://dbuser_monitor:DBUser.Monitor@10.10.10.12:5432/postgres?sslmode=disable' } #20004: { pg_cluster: pg-bar, pg_seq: 3, pg_host: 10.10.10.13 , pg_monitor_username: dbuser_monitor, pg_monitor_password: DBUser.Monitor } # etcd cluster for ha postgres etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 } }, vars: { etcd_cluster: etcd } } # minio cluster, s3 compatible object storage minio: { hosts: { 10.10.10.10: { minio_seq: 1 } }, vars: { minio_cluster: minio } } # postgres example cluster: pg-meta pg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta pg_users: - {name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [dbrole_admin] ,comment: pigsty admin user } - {name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [dbrole_readonly] ,comment: read-only viewer for meta database } - {name: dbuser_grafana ,password: DBUser.Grafana ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for grafana database } - {name: dbuser_bytebase ,password: DBUser.Bytebase ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for bytebase database } - {name: dbuser_kong ,password: DBUser.Kong ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for kong api gateway } - {name: dbuser_gitea ,password: DBUser.Gitea ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for gitea service } - {name: dbuser_wiki ,password: DBUser.Wiki ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for wiki.js service } - {name: dbuser_noco ,password: DBUser.Noco ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for nocodb service } - {name: dbuser_odoo ,password: DBUser.Odoo ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for odoo service ,createdb: true } #,superuser: true} - {name: dbuser_mattermost ,password: DBUser.MatterMost ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for mattermost ,createdb: true } pg_databases: - {name: meta ,baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [pigsty] ,extensions: [{name: vector},{name: postgis},{name: timescaledb}]} - {name: grafana ,owner: dbuser_grafana ,revokeconn: true ,comment: grafana primary database } - {name: bytebase ,owner: dbuser_bytebase ,revokeconn: true ,comment: bytebase primary database } - {name: kong ,owner: dbuser_kong ,revokeconn: true ,comment: kong api gateway database } - {name: gitea ,owner: dbuser_gitea ,revokeconn: true ,comment: gitea meta database } - {name: wiki ,owner: dbuser_wiki ,revokeconn: true ,comment: wiki meta database } - {name: noco ,owner: dbuser_noco ,revokeconn: true ,comment: nocodb database } #- {name: odoo ,owner: dbuser_odoo ,revokeconn: true ,comment: odoo main database } - {name: mattermost ,owner: dbuser_mattermost ,revokeconn: true ,comment: mattermost main database } pg_hba_rules: - {user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes'} pg_libs: 'timescaledb,pg_stat_statements, auto_explain' # add timescaledb to shared_preload_libraries pg_extensions: # extensions to be installed on this cluster - timescaledb timescaledb_toolkit pg_timeseries periods temporal_tables emaj table_version pg_cron pg_task pg_later pg_background - postgis pgrouting pointcloud pg_h3 q3c ogr_fdw geoip pg_polyline pg_geohash #mobilitydb - pgvector vchord pgvectorscale pg_vectorize pg_similarity smlar pg_summarize pg_tiktoken pg4ml #pgml - pg_search pgroonga pg_bigm zhparser pg_bestmatch vchord_bm25 hunspell - citus hydra pg_analytics pg_duckdb pg_mooncake duckdb_fdw pg_parquet pg_fkpart pg_partman plproxy #pg_strom - age hll rum pg_graphql pg_jsonschema jsquery pg_hint_plan hypopg index_advisor pg_plan_filter imgsmlr pg_ivm pg_incremental pgmq pgq pg_cardano omnigres #rdkit - pg_tle plv8 pllua plprql pldebugger plpgsql_check plprofiler plsh pljava #plr #pgtap #faker #dbt2 - pg_prefix pg_semver pgunit pgpdf pglite_fusion md5hash asn1oid roaringbitmap pgfaceting pgsphere pg_country pg_xenophile pg_currency pg_collection pgmp numeral pg_rational pguint pg_uint128 hashtypes ip4r pg_uri pgemailaddr pg_acl timestamp9 chkpass #pg_duration #debversion #pg_rrule - pg_gzip pg_bzip pg_zstd pg_http pg_net pg_curl pgjq pgjwt pg_smtp_client pg_html5_email_address url_encode pgsql_tweaks pg_extra_time pgpcre icu_ext pgqr pg_protobuf envvar floatfile pg_readme ddl_historization data_historization pg_schedoc pg_hashlib pg_xxhash shacrypt cryptint pg_ecdsa pgsparql - pg_idkit pg_uuidv7 permuteseq pg_hashids sequential_uuids topn quantile lower_quantile count_distinct omnisketch ddsketch vasco pgxicor tdigest first_last_agg extra_window_functions floatvec aggs_for_vecs aggs_for_arrays pg_arraymath pg_math pg_random pg_base36 pg_base62 pg_base58 pg_financial - pg_repack pg_squeeze pg_dirtyread pgfincore pg_cooldown pg_ddlx pg_prioritize pg_checksums pg_readonly pg_upless pg_permissions pgautofailover pg_catcheck preprepare pgcozy pg_orphaned pg_crash pg_cheat_funcs pg_fio pg_savior safeupdate pg_drop_events table_log #pgagent #pgpool - pg_profile pg_tracing pg_show_plans pg_stat_kcache pg_stat_monitor pg_qualstats pg_store_plans pg_track_settings pg_wait_sampling system_stats pg_meta pgnodemx pg_sqlog bgw_replstatus pgmeminfo toastinfo pg_explain_ui pg_relusage pagevis powa - passwordcheck supautils pgsodium pg_vault pg_session_jwt pg_anon pg_tde pgsmcrypto pgaudit pgauditlogtofile pg_auth_mon credcheck pgcryptokey pg_jobmon logerrors login_hook set_user pg_snakeoil pgextwlist pg_auditor sslutils pg_noset - wrappers multicorn odbc_fdw jdbc_fdw mysql_fdw tds_fdw sqlite_fdw pgbouncer_fdw mongo_fdw redis_fdw pg_redis_pubsub kafka_fdw hdfs_fdw firebird_fdw aws_s3 log_fdw #oracle_fdw #db2_fdw - documentdb orafce pgtt session_variable pg_statement_rollback pg_dbms_metadata pg_dbms_lock pgmemcache #pg_dbms_job #wiltondb - pglogical pglogical_ticker pgl_ddl_deploy pg_failover_slots db_migrator wal2json wal2mongo decoderbufs decoder_raw mimeo pg_fact_loader pg_bulkload #repmgr redis-ms: # redis classic primary \u0026 replica hosts: { 10.10.10.10: { redis_node: 1 , redis_instances: { 6379: { }, 6380: { replica_of: '10.10.10.10 6379' }, 6381: { replica_of: '10.10.10.10 6379' } } } } vars: { redis_cluster: redis-ms ,redis_password: 'redis.ms' ,redis_max_memory: 64MB } # ./mongo.yml -l pg-mongo pg-mongo: hosts: { 10.10.10.10: { mongo_seq: 1 } } vars: mongo_cluster: pg-mongo mongo_pgurl: 'postgres://dbuser_meta:DBUser.Meta@10.10.10.10:5432/grafana' # ./kafka.yml -l kf-main kf-main: hosts: { 10.10.10.10: { kafka_seq: 1, kafka_role: controller } } vars: kafka_cluster: kf-main kafka_peer_port: 9093 vars: # global variables version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: china # upstream mirror region: default|china|europe infra_portal: # infra services exposed via portal home : { domain: i.pigsty } # default domain name cc : { domain: pigsty.cc ,path: \"/www/pigsty.cc\" ,cert: /etc/cert/pigsty.cc.crt ,key: /etc/cert/pigsty.cc.key } minio : { domain: m.pigsty.cc ,endpoint: \"${admin_ip}:9001\" ,scheme: https ,websocket: true } postgrest : { domain: api.pigsty.cc ,endpoint: \"127.0.0.1:8884\" } pgadmin : { domain: adm.pigsty.cc ,endpoint: \"127.0.0.1:8885\" } pgweb : { domain: cli.pigsty.cc ,endpoint: \"127.0.0.1:8886\" } bytebase : { domain: ddl.pigsty.cc ,endpoint: \"127.0.0.1:8887\" } jupyter : { domain: lab.pigsty.cc ,endpoint: \"127.0.0.1:8888\", websocket: true } gitea : { domain: git.pigsty.cc ,endpoint: \"127.0.0.1:8889\" } wiki : { domain: wiki.pigsty.cc ,endpoint: \"127.0.0.1:9002\" } noco : { domain: noco.pigsty.cc ,endpoint: \"127.0.0.1:9003\" } supa : { domain: supa.pigsty.cc ,endpoint: \"10.10.10.10:8000\" ,websocket: true } dify : { domain: dify.pigsty.cc ,endpoint: \"10.10.10.10:8001\" ,websocket: true } odoo : { domain: odoo.pigsty.cc ,endpoint: \"127.0.0.1:8069\" ,websocket: true } mm : { domain: mm.pigsty.cc ,endpoint: \"10.10.10.10:8065\" ,websocket: true } # scp -r ~/pgsty/cc/cert/* pj:/etc/cert/ # copy https certs # scp -r ~/dev/pigsty.cc/public pj:/www/pigsty.cc # copy pigsty.cc website node_etc_hosts: [ \"${admin_ip} sss.pigsty\" ] node_timezone: Asia/Hong_Kong node_ntp_servers: - pool cn.pool.ntp.org iburst - pool ${admin_ip} iburst # assume non-admin nodes does not have internet access pgbackrest_enabled: false # do not take backups since this is disposable demo env #prometheus_options: '--storage.tsdb.retention.time=15d' # prometheus extra server options prometheus_options: '--storage.tsdb.retention.size=3GB' # keep 3GB data at most on demo env # install all postgresql18 extensions pg_version: 18 # default postgres version repo_extra_packages: [ pg18-core ,pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-olap ,pg18-feat ,pg18-lang ,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl] pg_extensions: [pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-feat ,pg18-lang ,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl ] #,pg18-olap] #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty grafana_view_password: DBUser.Viewer pg_admin_password: DBUser.DBA pg_monitor_password: DBUser.Monitor pg_replication_password: DBUser.Replicator patroni_password: Patroni.API haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO etcd_root_password: Etcd.Root ... Explanation The demo/demo template is Pigsty’s public demo configuration, showcasing a complete production-grade deployment example.\nKey Features:\nHTTPS certificate and custom domain configuration All available PostgreSQL extensions installed Integration with Redis, FerretDB, Kafka, and other components Docker image acceleration configured Use Cases:\nSetting up public demo sites Scenarios requiring complete feature demonstration Learning Pigsty advanced configuration Notes:\nSSL certificate files must be prepared DNS resolution must be configured Some extensions are not available on ARM64 architecture ","categories":["Reference"],"description":"Pigsty public demo site configuration, showcasing SSL certificates, domain exposure, and full extension installation","excerpt":"Pigsty public demo site configuration, showcasing SSL certificates, …","ref":"/docs/conf/demo/","tags":"","title":"demo/demo"},{"body":"The demo/minio configuration template demonstrates how to deploy a four-node x four-drive, 16-disk total high-availability MinIO cluster, providing S3-compatible object storage services.\nFor more tutorials, see the MINIO module documentation.\nOverview Config Name: demo/minio Node Count: Four nodes Description: High-availability multi-node multi-disk MinIO cluster demo OS Distro: el8, el9, el10, d12, d13, u22, u24 OS Arch: x86_64, aarch64 Related: meta Usage:\n./configure -c demo/minio Note: This is a four-node template. You need to modify the IP addresses of the other three nodes after generating the configuration.\nContent Source: pigsty/conf/demo/minio.yml\n--- #==============================================================# # File : minio.yml # Desc : pigsty: 4 node x 4 disk MNMD minio clusters # Ctime : 2023-01-07 # Mtime : 2025-12-12 # Docs : https://doc.pgsty.com/config # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# # One pass installation with: # ./deploy.yml #==============================================================# # 1. minio-1 @ 10.10.10.10:9000 - - (9002) svc \u003c-x 10.10.10.9:9002 # 2. minio-2 @ 10.10.10.11:9000 -xx- (9002) svc \u003c-x \u003c---------------- # 3. minio-3 @ 10.10.10.12:9000 -xx- (9002) svc \u003c-x sss.pigsty:9002 # 4. minio-4 @ 10.10.10.12:9000 - - (9002) svc \u003c-x (intranet dns) #==============================================================# # use minio load balancer service (9002) instead of direct access (9000) # mcli alias set sss https://sss.pigsty:9002 minioadmin S3User.MinIO #==============================================================# # https://min.io/docs/minio/linux/operations/install-deploy-manage/deploy-minio-multi-node-multi-drive.html # MINIO_VOLUMES=\"https://minio-{1...4}.pigsty:9000/data{1...4}/minio\" all: children: # infra cluster for proxy, monitor, alert, etc.. infra: { hosts: { 10.10.10.10: { infra_seq: 1 } } } # minio cluster with 4 nodes and 4 drivers per node minio: hosts: 10.10.10.10: { minio_seq: 1 , nodename: minio-1 } 10.10.10.11: { minio_seq: 2 , nodename: minio-2 } 10.10.10.12: { minio_seq: 3 , nodename: minio-3 } 10.10.10.13: { minio_seq: 4 , nodename: minio-4 } vars: minio_cluster: minio minio_data: '/data{1...4}' minio_buckets: # list of minio bucket to be created - { name: pgsql } - { name: meta ,versioning: true } - { name: data } minio_users: # list of minio user to be created - { access_key: pgbackrest ,secret_key: S3User.Backup ,policy: pgsql } - { access_key: s3user_meta ,secret_key: S3User.Meta ,policy: meta } - { access_key: s3user_data ,secret_key: S3User.Data ,policy: data } # bind a node l2 vip (10.10.10.9) to minio cluster (optional) node_cluster: minio vip_enabled: true vip_vrid: 128 vip_address: 10.10.10.9 vip_interface: eth1 # expose minio service with haproxy on all nodes haproxy_services: - name: minio # [REQUIRED] service name, unique port: 9002 # [REQUIRED] service port, unique balance: leastconn # [OPTIONAL] load balancer algorithm options: # [OPTIONAL] minio health check - option httpchk - option http-keep-alive - http-check send meth OPTIONS uri /minio/health/live - http-check expect status 200 servers: - { name: minio-1 ,ip: 10.10.10.10 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-2 ,ip: 10.10.10.11 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-3 ,ip: 10.10.10.12 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-4 ,ip: 10.10.10.13 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } vars: version: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default|china|europe infra_portal: # infra services exposed via portal home : { domain: i.pigsty } # default domain name # domain names to access minio web console via nginx web portal (optional) minio : { domain: m.pigsty ,endpoint: \"10.10.10.10:9001\" ,scheme: https ,websocket: true } minio10 : { domain: m10.pigsty ,endpoint: \"10.10.10.10:9001\" ,scheme: https ,websocket: true } minio11 : { domain: m11.pigsty ,endpoint: \"10.10.10.11:9001\" ,scheme: https ,websocket: true } minio12 : { domain: m12.pigsty ,endpoint: \"10.10.10.12:9001\" ,scheme: https ,websocket: true } minio13 : { domain: m13.pigsty ,endpoint: \"10.10.10.13:9001\" ,scheme: https ,websocket: true } minio_endpoint: https://sss.pigsty:9002 # explicit overwrite minio endpoint with haproxy port node_etc_hosts: [\"10.10.10.9 sss.pigsty\"] # domain name to access minio from all nodes (required) #----------------------------------------------# # PASSWORD : https://doc.pgsty.com/config/security #----------------------------------------------# grafana_admin_password: pigsty haproxy_admin_password: pigsty minio_secret_key: S3User.MinIO ... Explanation The demo/minio template is a production-grade reference configuration for MinIO, showcasing Multi-Node Multi-Drive (MNMD) architecture.\nKey Features:\nMulti-Node Multi-Drive Architecture: 4 nodes × 4 drives = 16-drive erasure coding group L2 VIP High Availability: Virtual IP binding via Keepalived HAProxy Load Balancing: Unified access endpoint on port 9002 Fine-grained Permissions: Separate users and buckets for different applications Access:\n# Configure MinIO alias with mcli (via HAProxy load balancing) mcli alias set sss https://sss.pigsty:9002 minioadmin S3User.MinIO # List buckets mcli ls sss/ # Use console # Visit https://m.pigsty or https://m10-m13.pigsty Use Cases:\nEnvironments requiring S3-compatible object storage PostgreSQL backup storage (pgBackRest remote repository) Data lake for big data and AI workloads Production environments requiring high-availability object storage Notes:\nEach node requires 4 independent disks mounted at /data1 - /data4 Production environments recommend at least 4 nodes for erasure coding redundancy VIP requires proper network interface configuration (vip_interface) ","categories":["Reference"],"description":"Four-node x four-drive high-availability multi-node multi-disk MinIO cluster demo","excerpt":"Four-node x four-drive high-availability multi-node multi-disk MinIO …","ref":"/docs/conf/minio/","tags":"","title":"demo/minio"},{"body":"The build/oss configuration template is the build environment configuration for Pigsty open-source edition offline packages, used to batch-build offline installation packages across multiple operating systems.\nThis configuration is intended for developers and contributors only.\nOverview Config Name: build/oss Node Count: Six nodes (el9, el10, d12, d13, u22, u24) Description: Pigsty open-source edition offline package build environment OS Distro: el9, el10, d12, d13, u22, u24 OS Arch: x86_64 Usage:\ncp conf/build/oss.yml pigsty.yml Note: This is a build template with fixed IP addresses, intended for internal use only.\nContent Source: pigsty/conf/build/oss.yml\n--- #==============================================================# # File : oss.yml # Desc : Pigsty 3-node building env (PG18) # Ctime : 2024-10-22 # Mtime : 2025-12-12 # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# all: vars: version: v4.0.0 admin_ip: 10.10.10.24 region: china etcd_clean: true proxy_env: no_proxy: \"localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,*.pigsty,*.aliyun.com,mirrors.*,*.myqcloud.com,*.tsinghua.edu.cn,*.pigsty.cc\" # building spec pg_version: 18 cache_pkg_dir: 'dist/${version}' repo_modules: infra,node,pgsql repo_packages: [ node-bootstrap, infra-package, infra-addons, node-package1, node-package2, pgsql-utility, extra-modules ] repo_extra_packages: [pg18-core ,pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-olap ,pg18-feat ,pg18-lang ,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl] pg_extensions: [ pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-olap, pg18-feat ,pg18-lang ,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl] children: #el8: { hosts: { 10.10.10.8: { pg_cluster: el8 ,pg_seq: 1 ,pg_role: primary }}} el9: { hosts: { 10.10.10.9: { pg_cluster: el9 ,pg_seq: 1 ,pg_role: primary }}} el10: { hosts: { 10.10.10.10: { pg_cluster: el10 ,pg_seq: 1 ,pg_role: primary }}} d12: { hosts: { 10.10.10.12: { pg_cluster: d12 ,pg_seq: 1 ,pg_role: primary }}} d13: { hosts: { 10.10.10.13: { pg_cluster: d13 ,pg_seq: 1 ,pg_role: primary }}} u22: { hosts: { 10.10.10.22: { pg_cluster: u22 ,pg_seq: 1 ,pg_role: primary }}} u24: { hosts: { 10.10.10.24: { pg_cluster: u24 ,pg_seq: 1 ,pg_role: primary }}} etcd: { hosts: { 10.10.10.24: { etcd_seq: 1 }}, vars: { etcd_cluster: etcd }} infra: hosts: #10.10.10.8: { infra_seq: 1, admin_ip: 10.10.10.8 ,ansible_host: el8 } #, ansible_python_interpreter: /usr/bin/python3.12 } 10.10.10.9: { infra_seq: 2, admin_ip: 10.10.10.9 ,ansible_host: el9 } 10.10.10.10: { infra_seq: 3, admin_ip: 10.10.10.10 ,ansible_host: el10 } 10.10.10.12: { infra_seq: 4, admin_ip: 10.10.10.12 ,ansible_host: d12 } 10.10.10.13: { infra_seq: 5, admin_ip: 10.10.10.13 ,ansible_host: d13 } 10.10.10.22: { infra_seq: 6, admin_ip: 10.10.10.22 ,ansible_host: u22 } 10.10.10.24: { infra_seq: 7, admin_ip: 10.10.10.24 ,ansible_host: u24 } vars: { node_conf: oltp } ... Explanation The build/oss template is the build configuration for Pigsty open-source edition offline packages.\nBuild Contents:\nPostgreSQL 18 and all categorized extension packages Infrastructure packages (Prometheus, Grafana, Nginx, etc.) Node packages (monitoring agents, tools, etc.) Extra modules Supported Operating Systems:\nEL9 (Rocky/Alma/RHEL 9) EL10 (Rocky 10 / RHEL 10) Debian 12 (Bookworm) Debian 13 (Trixie) Ubuntu 22.04 (Jammy) Ubuntu 24.04 (Noble) Build Process:\n# 1. Prepare build environment cp conf/build/oss.yml pigsty.yml # 2. Download packages on each node ./infra.yml -t repo_build # 3. Package offline installation files make cache Use Cases:\nPigsty developers building new versions Contributors testing new extensions Enterprise users customizing offline packages ","categories":["Reference"],"description":"Pigsty open-source edition offline package build environment configuration","excerpt":"Pigsty open-source edition offline package build environment …","ref":"/docs/conf/oss/","tags":"","title":"build/oss"},{"body":"The build/pro configuration template is the build environment configuration for Pigsty professional edition offline packages, including PostgreSQL 13-18 all versions and additional commercial components.\nThis configuration is intended for developers and contributors only.\nOverview Config Name: build/pro Node Count: Six nodes (el9, el10, d12, d13, u22, u24) Description: Pigsty professional edition offline package build environment (multi-version) OS Distro: el9, el10, d12, d13, u22, u24 OS Arch: x86_64 Usage:\ncp conf/build/pro.yml pigsty.yml Note: This is a build template with fixed IP addresses, intended for internal use only.\nContent Source: pigsty/conf/build/pro.yml\n--- #==============================================================# # File : pro.yml # Desc : Pigsty 6-node pro building env (PG 13-18) # Ctime : 2024-10-22 # Mtime : 2025-12-15 # License : Apache-2.0 @ https://pigsty.io/docs/about/license/ # Copyright : 2018-2026 Ruohang Feng / Vonng (rh@vonng.com) #==============================================================# all: vars: version: v4.0.0 admin_ip: 10.10.10.24 region: china etcd_clean: true proxy_env: no_proxy: \"localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,*.pigsty,*.aliyun.com,mirrors.*,*.myqcloud.com,*.tsinghua.edu.cn,*.pigsty.cc\" # building spec pg_version: 18 cache_pkg_dir: 'dist/${version}/pro' repo_modules: infra,node,pgsql pg_extensions: [] repo_packages: [ node-bootstrap, infra-package, infra-addons, node-package1, node-package2, pgsql-utility, extra-modules, pg18-full,pg18-time,pg18-gis,pg18-rag,pg18-fts,pg18-olap,pg18-feat,pg18-lang,pg18-type,pg18-util,pg18-func,pg18-admin,pg18-stat,pg18-sec,pg18-fdw,pg18-sim,pg18-etl, pg17-full,pg17-time,pg17-gis,pg17-rag,pg17-fts,pg17-olap,pg17-feat,pg17-lang,pg17-type,pg17-util,pg17-func,pg17-admin,pg17-stat,pg17-sec,pg17-fdw,pg17-sim,pg17-etl, pg16-full,pg16-time,pg16-gis,pg16-rag,pg16-fts,pg16-olap,pg16-feat,pg16-lang,pg16-type,pg16-util,pg16-func,pg16-admin,pg16-stat,pg16-sec,pg16-fdw,pg16-sim,pg16-etl, pg15-full,pg15-time,pg15-gis,pg15-rag,pg15-fts,pg15-olap,pg15-feat,pg15-lang,pg15-type,pg15-util,pg15-func,pg15-admin,pg15-stat,pg15-sec,pg15-fdw,pg15-sim,pg15-etl, pg14-full,pg14-time,pg14-gis,pg14-rag,pg14-fts,pg14-olap,pg14-feat,pg14-lang,pg14-type,pg14-util,pg14-func,pg14-admin,pg14-stat,pg14-sec,pg14-fdw,pg14-sim,pg14-etl, pg13-full,pg13-time,pg13-gis,pg13-rag,pg13-fts,pg13-olap,pg13-feat,pg13-lang,pg13-type,pg13-util,pg13-func,pg13-admin,pg13-stat,pg13-sec,pg13-fdw,pg13-sim,pg13-etl, infra-extra, kafka, java-runtime, sealos, tigerbeetle, polardb, ivorysql ] children: #el8: { hosts: { 10.10.10.8: { pg_cluster: el8 ,pg_seq: 1 ,pg_role: primary }}} el9: { hosts: { 10.10.10.9: { pg_cluster: el9 ,pg_seq: 1 ,pg_role: primary }}} el10: { hosts: { 10.10.10.10: { pg_cluster: el10 ,pg_seq: 1 ,pg_role: primary }}} d12: { hosts: { 10.10.10.12: { pg_cluster: d12 ,pg_seq: 1 ,pg_role: primary }}} d13: { hosts: { 10.10.10.13: { pg_cluster: d13 ,pg_seq: 1 ,pg_role: primary }}} u22: { hosts: { 10.10.10.22: { pg_cluster: u22 ,pg_seq: 1 ,pg_role: primary }}} u24: { hosts: { 10.10.10.24: { pg_cluster: u24 ,pg_seq: 1 ,pg_role: primary }}} etcd: { hosts: { 10.10.10.24: { etcd_seq: 1 }}, vars: { etcd_cluster: etcd }} infra: hosts: #10.10.10.8: { infra_seq: 9, admin_ip: 10.10.10.8 ,ansible_host: el8 } #, ansible_python_interpreter: /usr/bin/python3.12 } 10.10.10.9: { infra_seq: 1, admin_ip: 10.10.10.9 ,ansible_host: el9 } 10.10.10.10: { infra_seq: 2, admin_ip: 10.10.10.10 ,ansible_host: el10 } 10.10.10.12: { infra_seq: 3, admin_ip: 10.10.10.12 ,ansible_host: d12 } 10.10.10.13: { infra_seq: 4, admin_ip: 10.10.10.13 ,ansible_host: d13 } 10.10.10.22: { infra_seq: 5, admin_ip: 10.10.10.22 ,ansible_host: u22 } 10.10.10.24: { infra_seq: 6, admin_ip: 10.10.10.24 ,ansible_host: u24 } vars: { node_conf: oltp } ... Explanation The build/pro template is the build configuration for Pigsty professional edition offline packages, containing more content than the open-source edition.\nDifferences from OSS Edition:\nIncludes all six major PostgreSQL versions 13-18 Includes additional commercial/enterprise components: Kafka, PolarDB, IvorySQL, etc. Includes Java runtime and Sealos tools Output directory is dist/${version}/pro/ Build Contents:\nPostgreSQL 13, 14, 15, 16, 17, 18 all versions All categorized extension packages for each version Kafka message queue PolarDB and IvorySQL kernels TigerBeetle distributed database Sealos container platform Use Cases:\nEnterprise customers requiring multi-version support Need for Oracle/MySQL compatible kernels Need for Kafka message queue integration Long-term support versions (LTS) requirements Build Process:\n# 1. Prepare build environment cp conf/build/pro.yml pigsty.yml # 2. Download packages on each node ./infra.yml -t repo_build # 3. Package offline installation files make cache-pro ","categories":["Reference"],"description":"Pigsty professional edition offline package build environment configuration (multi-version)","excerpt":"Pigsty professional edition offline package build environment …","ref":"/docs/conf/pro/","tags":"","title":"build/pro"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/_div_module/","tags":"","title":"Modules"},{"body":" The world’s most advanced open-source relational database!\nPigsty brings it to full potential: batteries-included, reliable, observable, maintainable, and scalable! Config | Admin | Playbooks | Dashboards | Parameters\nOverview Learn key topics and concepts about PostgreSQL.\nArchitecture Cluster Config Extensions Users/Roles Databases Services/Access Auth/HBA Access Control Admin SOP Backup \u0026 Recovery Monitoring Migration Dashboards Config Describe your desired PostgreSQL cluster\nIdentity Params: Define identity params for a PostgreSQL cluster Primary Instance: Create single-instance “cluster” with one primary Replica Instance: Create basic HA cluster with one primary and one replica Offline Instance: Create special read-only instance for OLAP/ETL/interactive queries Sync Standby: Enable sync commit to ensure zero data loss Quorum Commit: Use quorum sync commit for higher consistency level Standby Cluster: Clone existing cluster and keep in sync (DR cluster) Delayed Cluster: Clone existing cluster with delayed replay for emergency recovery Citus Cluster: Define and create Citus distributed database cluster Major Version: Deploy cluster with different PostgreSQL major version Admin Manage your PostgreSQL clusters.\nCheatsheet Create Cluster Create User Create Database Reload Service Reload HBA Config Cluster Append Replica Remove Replica Remove Cluster Switchover Backup Cluster Restore Cluster Troubleshooting Playbooks Use idempotent playbooks to materialize your config.\npgsql.yml: Init PostgreSQL cluster or add new replicas. pgsql-rm.yml: Remove PostgreSQL cluster or specific instance pgsql-user.yml: Add new biz user to existing PostgreSQL cluster pgsql-db.yml: Add new biz database to existing PostgreSQL cluster pgsql-monitor.yml: Monitor remote postgres instance pgsql-migration.yml: Generate migration manual and scripts Example: Install PGSQL Module Example: Remove PGSQL Module Monitoring Check PostgreSQL status via Grafana dashboards.\nPigsty has 26 PostgreSQL-related dashboards:\nOverview Cluster Instance Database PGSQL Overview PGSQL Cluster PGSQL Instance PGSQL Database PGSQL Alert PGRDS Cluster PGRDS Instance PGCAT Database PGSQL Shard PGSQL Activity PGCAT Instance PGSQL Tables PGSQL Replication PGSQL Persist PGSQL Table PGSQL Service PGSQL Proxy PGCAT Table PGSQL Databases PGSQL Pgbouncer PGSQL Query PGSQL Patroni PGSQL Session PGCAT Query PGSQL PITR PGSQL Xacts PGCAT Locks PGSQL Exporter PGCAT Schema Parameters Config params for the PGSQL module\nPG_ID: Calculate \u0026 validate PostgreSQL instance identity PG_BUSINESS: PostgreSQL biz object definitions PG_INSTALL: Install PostgreSQL kernel, pkgs \u0026 extensions PG_BOOTSTRAP: Init HA PostgreSQL cluster with Patroni PG_PROVISION: Create PostgreSQL users, databases \u0026 in-db objects PG_BACKUP: Setup backup repo with pgbackrest PG_ACCESS: Expose PostgreSQL services, bindVIP (optional), register DNS PG_MONITOR: Add monitoring for PostgreSQL instance and register to infra PG_REMOVE: Remove PostgreSQL cluster, instance and related resources Full Parameter List Parameter Section Type Level Description pg_mode PG_ID enum C pgsql cluster mode: pgsql,citus,gpsql pg_cluster PG_ID string C pgsql cluster name, REQUIRED identity param pg_seq PG_ID int I pgsql instance seq number, REQUIRED identity param pg_role PG_ID enum I pgsql role, REQUIRED, could be primary,replica,offline pg_instances PG_ID dict I define multiple pg instances on node in {port:ins_vars} format pg_upstream PG_ID ip I repl upstream ip for standby cluster or cascade replica pg_shard PG_ID string C pgsql shard name, optional identity for sharding clusters pg_group PG_ID int C pgsql shard index number, optional identity for sharding clusters gp_role PG_ID enum C greenplum role of this cluster, could be master or segment pg_exporters PG_ID dict C additional pg_exporters to monitor remote postgres instances pg_offline_query PG_ID bool I set true to enable offline query on this instance pg_users PG_BUSINESS user[] C postgres biz users pg_databases PG_BUSINESS database[] C postgres biz databases pg_services PG_BUSINESS service[] C postgres biz services pg_hba_rules PG_BUSINESS hba[] C biz hba rules for postgres pgb_hba_rules PG_BUSINESS hba[] C biz hba rules for pgbouncer pg_replication_username PG_BUSINESS username G postgres replication username, replicator by default pg_replication_password PG_BUSINESS password G postgres replication password, DBUser.Replicator by default pg_admin_username PG_BUSINESS username G postgres admin username, dbuser_dba by default pg_admin_password PG_BUSINESS password G postgres admin password in plain text, DBUser.DBA by default pg_monitor_username PG_BUSINESS username G postgres monitor username, dbuser_monitor by default pg_monitor_password PG_BUSINESS password G postgres monitor password, DBUser.Monitor by default pg_dbsu_password PG_BUSINESS password G/C dbsu password, empty string means no dbsu password by default pg_dbsu PG_INSTALL username C os dbsu name, postgres by default, better not change it pg_dbsu_uid PG_INSTALL int C os dbsu uid and gid, 26 for default postgres users and groups pg_dbsu_sudo PG_INSTALL enum C dbsu sudo privilege, none,limit,all,nopass. limit by default pg_dbsu_home PG_INSTALL path C postgresql home dir, /var/lib/pgsql by default pg_dbsu_ssh_exchange PG_INSTALL bool C exchange postgres dbsu ssh key among same pgsql cluster pg_version PG_INSTALL enum C postgres major version to install, 18 by default pg_bin_dir PG_INSTALL path C postgres binary dir, /usr/pgsql/bin by default pg_log_dir PG_INSTALL path C postgres log dir, /pg/log/postgres by default pg_packages PG_INSTALL string[] C pg pkgs to install, ${pg_version} will be replaced pg_extensions PG_INSTALL string[] C pg extensions to install, ${pg_version} will be replaced pg_clean PG_BOOTSTRAP bool G/C/A purge existing postgres during pgsql init? true by default pg_data PG_BOOTSTRAP path C postgres data dir, /pg/data by default pg_fs_main PG_BOOTSTRAP path C mountpoint/path for postgres main data, /data by default pg_fs_bkup PG_BOOTSTRAP path C mountpoint/path for pg backup data, /data/backup by default pg_storage_type PG_BOOTSTRAP enum C storage type for pg main data, SSD,HDD, SSD by default pg_dummy_filesize PG_BOOTSTRAP size C size of /pg/dummy, hold 64MB disk space for emergency use pg_listen PG_BOOTSTRAP ip(s) C/I postgres/pgbouncer listen addr, comma separated list pg_port PG_BOOTSTRAP port C postgres listen port, 5432 by default pg_localhost PG_BOOTSTRAP path C postgres unix socket dir for localhost connection pg_namespace PG_BOOTSTRAP path C top level key namespace in etcd, used by patroni \u0026 vip patroni_enabled PG_BOOTSTRAP bool C if disabled, no postgres cluster will be created during init patroni_mode PG_BOOTSTRAP enum C patroni working mode: default,pause,remove patroni_port PG_BOOTSTRAP port C patroni listen port, 8008 by default patroni_log_dir PG_BOOTSTRAP path C patroni log dir, /pg/log/patroni by default patroni_ssl_enabled PG_BOOTSTRAP bool G secure patroni RestAPI comms with SSL? patroni_watchdog_mode PG_BOOTSTRAP enum C patroni watchdog mode: automatic,required,off. off by default patroni_username PG_BOOTSTRAP username C patroni restapi username, postgres by default patroni_password PG_BOOTSTRAP password C patroni restapi password, Patroni.API by default pg_etcd_password PG_BOOTSTRAP password C etcd password for this pg cluster, empty to use pg_cluster pg_primary_db PG_BOOTSTRAP string C primary database in this cluster, optional, postgres by default pg_parameters PG_BOOTSTRAP dict C extra params in postgresql.auto.conf pg_files PG_BOOTSTRAP path[] C extra files to copy to postgres data dir pg_conf PG_BOOTSTRAP enum C config template: oltp,olap,crit,tiny. oltp.yml by default pg_max_conn PG_BOOTSTRAP int C postgres max connections, auto will use recommended value pg_shared_buffer_ratio PG_BOOTSTRAP float C postgres shared buffer mem ratio, 0.25 by default, 0.1~0.4 pg_io_method PG_BOOTSTRAP enum C io method for postgres: auto,sync,worker,io_uring, worker by default pg_rto PG_BOOTSTRAP int C recovery time objective in seconds, 30s by default pg_rpo PG_BOOTSTRAP int C recovery point objective in bytes, 1MiB at most by default pg_libs PG_BOOTSTRAP string C preloaded libs, timescaledb,pg_stat_statements,auto_explain by default pg_delay PG_BOOTSTRAP interval I replication apply delay for standby cluster leader pg_checksum PG_BOOTSTRAP bool C enable data checksum for postgres cluster? pg_pwd_enc PG_BOOTSTRAP enum C password encryption algo: md5,scram-sha-256 pg_encoding PG_BOOTSTRAP enum C database cluster encoding, UTF8 by default pg_locale PG_BOOTSTRAP enum C database cluster locale, C by default pg_lc_collate PG_BOOTSTRAP enum C database cluster collate, C by default pg_lc_ctype PG_BOOTSTRAP enum C database char type, C by default pgsodium_key PG_BOOTSTRAP string C pgsodium key, 64 hex digit, default to sha256(pg_cluster) pgsodium_getkey_script PG_BOOTSTRAP path C pgsodium getkey script path pgbouncer_enabled PG_ACCESS bool C if disabled, pgbouncer will not be launched on pgsql host pgbouncer_port PG_ACCESS port C pgbouncer listen port, 6432 by default pgbouncer_log_dir PG_ACCESS path C pgbouncer log dir, /pg/log/pgbouncer by default pgbouncer_auth_query PG_ACCESS bool C query postgres to retrieve unlisted biz users? pgbouncer_poolmode PG_ACCESS enum C pooling mode: transaction,session,statement, transaction by default pgbouncer_sslmode PG_ACCESS enum C pgbouncer client ssl mode, disable by default pgbouncer_ignore_param PG_ACCESS string[] C pgbouncer ignore_startup_parameters list pg_provision PG_PROVISION bool C provision postgres cluster after bootstrap pg_init PG_PROVISION string G/C provision init script for cluster template, pg-init by default pg_default_roles PG_PROVISION role[] G/C default roles and users in postgres cluster pg_default_privileges PG_PROVISION string[] G/C default privileges when created by admin user pg_default_schemas PG_PROVISION string[] G/C default schemas to be created pg_default_extensions PG_PROVISION extension[] G/C default extensions to be created pg_reload PG_PROVISION bool A reload postgres after hba changes pg_default_hba_rules PG_PROVISION hba[] G/C postgres default host-based auth rules pgb_default_hba_rules PG_PROVISION hba[] G/C pgbouncer default host-based auth rules pgbackrest_enabled PG_BACKUP bool C enable pgbackrest on pgsql host? pgbackrest_clean PG_BACKUP bool C remove pg backup data during init? pgbackrest_log_dir PG_BACKUP path C pgbackrest log dir, /pg/log/pgbackrest by default pgbackrest_method PG_BACKUP enum C pgbackrest repo method: local,minio,etc… pgbackrest_init_backup PG_BACKUP bool C take a full backup after pgbackrest init? pgbackrest_repo PG_BACKUP dict G/C pgbackrest repo: https://pgbackrest.org/configuration.html#section-repository pg_weight PG_ACCESS int I relative load balance weight in service, 100 by default, 0-255 pg_service_provider PG_ACCESS enum G/C dedicated haproxy node group name, or empty string for local nodes by default pg_default_service_dest PG_ACCESS enum G/C default service dest if svc.dest=‘default’ pg_default_services PG_ACCESS service[] G/C postgres default service definitions pg_vip_enabled PG_ACCESS bool C enable L2 VIP for pgsql primary? false by default pg_vip_address PG_ACCESS cidr4 C vip addr in \u003cipv4\u003e/\u003cmask\u003e format, required if vip is enabled pg_vip_interface PG_ACCESS string C/I vip network interface to listen, eth0 by default pg_dns_suffix PG_ACCESS string C pgsql dns suffix, ’’ by default pg_dns_target PG_ACCESS enum C auto, primary, vip, none, or ad hoc ip pg_exporter_enabled PG_MONITOR bool C enable pg_exporter on pgsql hosts? pg_exporter_config PG_MONITOR string C pg_exporter config file name pg_exporter_cache_ttls PG_MONITOR string C pg_exporter collector ttl stage in seconds, ‘1,10,60,300’ by default pg_exporter_port PG_MONITOR port C pg_exporter listen port, 9630 by default pg_exporter_params PG_MONITOR string C extra url params for pg_exporter dsn pg_exporter_url PG_MONITOR pgurl C overwrite auto-gen pg dsn if specified pg_exporter_auto_discovery PG_MONITOR bool C enable auto database discovery? enabled by default pg_exporter_exclude_database PG_MONITOR string C csv of database that WILL NOT be monitored during auto-discovery pg_exporter_include_database PG_MONITOR string C csv of database that WILL BE monitored during auto-discovery pg_exporter_connect_timeout PG_MONITOR int C pg_exporter connect timeout in ms, 200 by default pg_exporter_options PG_MONITOR arg C overwrite extra options for pg_exporter pgbouncer_exporter_enabled PG_MONITOR bool C enable pgbouncer_exporter on pgsql hosts? pgbouncer_exporter_port PG_MONITOR port C pgbouncer_exporter listen port, 9631 by default pgbouncer_exporter_url PG_MONITOR pgurl C overwrite auto-gen pgbouncer dsn if specified pgbouncer_exporter_options PG_MONITOR arg C overwrite extra options for pgbouncer_exporter pgbackrest_exporter_enabled PG_MONITOR bool C enable pgbackrest_exporter on pgsql hosts? pgbackrest_exporter_port PG_MONITOR port C pgbackrest_exporter listen port, 9854 by default pgbackrest_exporter_options PG_MONITOR arg C overwrite extra options for pgbackrest_exporter pg_safeguard PG_REMOVE bool G/C/A prevent purging running postgres instance? false by default pg_rm_data PG_REMOVE bool G/C/A remove postgres data during remove? true by default pg_rm_backup PG_REMOVE bool G/C/A remove pgbackrest backup during primary remove? true by default pg_rm_pkg PG_REMOVE bool G/C/A uninstall postgres pkgs during remove? true by default Tutorials Tutorials for using/managing PostgreSQL in Pigsty.\nClone an existing PostgreSQL cluster Create an online standby cluster of existing PostgreSQL cluster Create a delayed standby cluster of existing PostgreSQL cluster Monitor an existing postgres instance Migrate from external PostgreSQL to Pigsty-managed PostgreSQL using logical replication Use MinIO as centralized pgBackRest backup repo Use dedicated etcd cluster as PostgreSQL / Patroni DCS Use dedicated haproxy load balancer cluster to expose PostgreSQL services Use pg-meta CMDB instead of pigsty.yml as inventory source Use PostgreSQL as Grafana backend storage Use PostgreSQL as Prometheus backend storage ","categories":["Reference"],"description":"Deploy and manage world's most advanced open-source relational database — PostgreSQL, customizable and production-ready!\n","excerpt":"Deploy and manage world's most advanced open-source relational …","ref":"/docs/pgsql/","tags":"","title":"Module: PGSQL"},{"body":"Core concepts and architecture design\n","categories":["Reference"],"description":"","excerpt":"Core concepts and architecture design\n","ref":"/docs/pgsql/_div_concept/","tags":"","title":"Core Concepts"},{"body":"Pigsty is a “configuration-driven” PostgreSQL platform: all behaviors come from the combination of inventory files in ~/pigsty/conf/*.yml and PGSQL parameters. Once you’ve written the configuration, you can replicate a customized cluster with instances, users, databases, access control, extensions, and tuning policies in just a few minutes.\nConfiguration Entry Prepare Inventory: Copy a pigsty/conf/*.yml template or write an Ansible Inventory from scratch, placing cluster groups (all.children.\u003ccls\u003e.hosts) and global variables (all.vars) in the same file. Define Parameters: Override the required PGSQL parameters in the vars block. The override order from global → cluster → host determines the final value. Apply Configuration: Run ./configure -c \u003cconf\u003e or bin/pgsql-add \u003ccls\u003e and other playbooks to apply the configuration. Pigsty will generate the configuration files needed for Patroni/pgbouncer/pgbackrest based on the parameters. Pigsty’s default demo inventory conf/pgsql.yml is a minimal example: one pg-meta cluster, global pg_version: 18, and a few business user and database definitions. You can expand with more clusters from this base.\nFocus Areas \u0026 Documentation Index Pigsty’s PostgreSQL configuration can be organized from the following dimensions. Subsequent documentation will explain “how to configure” each:\nCluster \u0026 Instances: Define instance topology (standalone, primary-replica, standby cluster, delayed cluster, Citus, etc.) through pg_cluster / pg_role / pg_seq / pg_upstream. Kernel Version: Select the core version, flavor, and tuning templates using pg_version, pg_mode, pg_packages, pg_extensions, pg_conf, and other parameters. Users/Roles: Declare system roles, business accounts, password policies, and connection pool attributes in pg_default_roles and pg_users. Database Objects: Create databases as needed using pg_databases, baseline, schemas, extensions, pool_* fields and automatically integrate with pgbouncer/Grafana. Access Control (HBA): Maintain host-based authentication policies using pg_default_hba_rules and pg_hba_rules to ensure access boundaries for different roles/networks. Privilege Model (ACL): Converge object privileges through pg_default_privileges, pg_default_roles, pg_revoke_public parameters, providing an out-of-the-box layered role system. After understanding these parameters, you can write declarative inventory manifests as “configuration as infrastructure” for any business requirement. Pigsty will handle execution and ensure idempotency.\nA Typical Example The following snippet shows how to control instance topology, kernel version, extensions, users, and databases in the same configuration file:\nall: children: pg-analytics: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } 10.10.10.12: { pg_seq: 2, pg_role: replica, pg_offline_query: true } vars: pg_cluster: pg-analytics pg_conf: olap.yml pg_extensions: [ postgis, timescaledb, pgvector ] pg_databases: - { name: bi, owner: dbuser_bi, schemas: [mart], extensions: [timescaledb], pool_mode: session } pg_users: - { name: dbuser_bi, password: DBUser.BI, roles: [dbrole_admin], pgbouncer: true } vars: pg_version: 17 pg_packages: [ pgsql-main pgsql-common ] pg_hba_rules: - { user: dbuser_bi, db: bi, addr: intra, auth: ssl, title: 'BI only allows intranet SSL access' } The pg-analytics cluster contains one primary and one offline replica. Global settings specify pg_version: 17 with a set of extension examples and load olap.yml tuning. Declare business objects in pg_databases and pg_users, automatically generating schema/extension and connection pool entries. Additional pg_hba_rules restrict access sources and authentication methods. Modify and apply this inventory to get a customized PostgreSQL cluster without manual configuration.\n","categories":["Reference"],"description":"Choose the appropriate instance and cluster types based on your requirements to configure PostgreSQL database clusters that meet your needs.","excerpt":"Choose the appropriate instance and cluster types based on your …","ref":"/docs/pgsql/config/","tags":"","title":"Configuration"},{"body":" Choose the appropriate instance and cluster types based on your requirements to configure PostgreSQL database clusters that meet your needs.\nYou can define different types of instances and clusters. Here are several common PostgreSQL instance/cluster types in Pigsty:\nPrimary: Define a single instance cluster. Replica: Define a basic HA cluster with one primary and one replica. Offline: Define an instance dedicated to OLAP/ETL/interactive queries Sync Standby: Enable synchronous commit to ensure no data loss. Quorum Commit: Use quorum sync commit for a higher consistency level. Standby Cluster: Clone an existing cluster and follow it Delayed Cluster: Clone an existing cluster for emergency data recovery Citus Cluster: Define a Citus distributed database cluster Primary We start with the simplest case: a single instance cluster consisting of one primary:\npg-test: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } vars: pg_cluster: pg-test This configuration is concise and self-describing, consisting only of identity parameters. Note that the Ansible Group name should match pg_cluster.\nUse the following command to create this cluster:\nbin/pgsql-add pg-test For demos, development testing, hosting temporary requirements, or performing non-critical analytical tasks, a single database instance may not be a big problem. However, such a single-node cluster has no high availability. When hardware failures occur, you’ll need to use PITR or other recovery methods to ensure the cluster’s RTO/RPO. For this reason, you may consider adding several read-only replicas to the cluster.\nReplica To add a read-only replica instance, you can add a new node to pg-test and set its pg_role to replica.\npg-test: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } 10.10.10.12: { pg_seq: 2, pg_role: replica } # \u003c--- newly added replica vars: pg_cluster: pg-test If the entire cluster doesn’t exist, you can directly create the complete cluster. If the cluster primary has already been initialized, you can add a replica to the existing cluster:\nbin/pgsql-add pg-test # initialize the entire cluster at once bin/pgsql-add pg-test 10.10.10.12 # add replica to existing cluster When the cluster primary fails, the read-only instance (Replica) can take over the primary’s work with the help of the high availability system. Additionally, read-only instances can be used to execute read-only queries: many businesses have far more read requests than write requests, and most read-only query loads can be handled by replica instances.\nOffline Offline instances are dedicated read-only replicas specifically for serving slow queries, ETL, OLAP traffic, and interactive queries. Slow queries/long transactions have adverse effects on the performance and stability of online business, so it’s best to isolate them from online business.\nTo add an offline instance, assign it a new instance and set pg_role to offline.\npg-test: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } 10.10.10.12: { pg_seq: 2, pg_role: replica } 10.10.10.13: { pg_seq: 3, pg_role: offline } # \u003c--- newly added offline replica vars: pg_cluster: pg-test Dedicated offline instances work similarly to common replica instances, but they serve as backup servers in the pg-test-replica service. That is, only when all replica instances are down will the offline and primary instances provide this read-only service.\nIn many cases, database resources are limited, and using a separate server as an offline instance is not economical. As a compromise, you can select an existing replica instance and mark it with the pg_offline_query flag to indicate it can handle “offline queries”. In this case, this read-only replica will handle both online read-only requests and offline queries. You can use pg_default_hba_rules and pg_hba_rules for additional access control on offline instances.\nSync Standby When Sync Standby is enabled, PostgreSQL will select one replica as the sync standby, with all other replicas as candidates. The primary database will wait for the standby instance to flush to disk before confirming commits. The standby instance always has the latest data with no replication lag, and primary-standby switchover to the sync standby will have no data loss.\nPostgreSQL uses asynchronous streaming replication by default, which may have small replication lag (on the order of 10KB/10ms). When the primary fails, there may be a small data loss window (which can be controlled using pg_rpo), but this is acceptable for most scenarios.\nHowever, in some critical scenarios (e.g., financial transactions), data loss is completely unacceptable, or read replication lag is unacceptable. In such cases, you can use synchronous commit to solve this problem. To enable sync standby mode, you can simply use the crit.yml template in pg_conf.\npg-test: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } 10.10.10.12: { pg_seq: 2, pg_role: replica } 10.10.10.13: { pg_seq: 3, pg_role: replica } vars: pg_cluster: pg-test pg_conf: crit.yml # \u003c--- use crit template To enable sync standby on an existing cluster, configure the cluster and enable synchronous_mode:\n$ pg edit-config pg-test # run as admin user on admin node +++ -synchronous_mode: false # \u003c--- old value +synchronous_mode: true # \u003c--- new value synchronous_mode_strict: false Apply these changes? [y/N]: y In this case, the PostgreSQL configuration parameter synchronous_standby_names is automatically managed by Patroni. One replica will be elected as the sync standby, and its application_name will be written to the PostgreSQL primary configuration file and applied.\nQuorum Commit Quorum Commit provides more powerful control than sync standby: especially when you have multiple replicas, you can set criteria for successful commits, achieving higher/lower consistency levels (and trade-offs with availability).\nIf you want at least two replicas to confirm commits, you can adjust the synchronous_node_count parameter through Patroni cluster configuration and apply it:\nsynchronous_mode: true # ensure synchronous commit is enabled synchronous_node_count: 2 # specify \"at least\" how many replicas must successfully commit If you want to use more sync replicas, modify the synchronous_node_count value. When the cluster size changes, you should ensure this configuration is still valid to avoid service unavailability.\nIn this case, the PostgreSQL configuration parameter synchronous_standby_names is automatically managed by Patroni.\nsynchronous_standby_names = '2 (\"pg-test-3\",\"pg-test-2\")' Example: Using multiple sync standbys $ pg edit-config pg-test --- +synchronous_node_count: 2 Apply these changes? [y/N]: y After applying the configuration, two sync standbys appear.\n+ Cluster: pg-test (7080814403632534854) +---------+----+-----------+-----------------+ | Member | Host | Role | State | TL | Lag in MB | Tags | +-----------+-------------+--------------+---------+----+-----------+-----------------+ | pg-test-1 | 10.10.10.10 | Leader | running | 1 | | clonefrom: true | | pg-test-2 | 10.10.10.11 | Sync Standby | running | 1 | 0 | clonefrom: true | | pg-test-3 | 10.10.10.12 | Sync Standby | running | 1 | 0 | clonefrom: true | +-----------+-------------+--------------+---------+----+-----------+-----------------+ Another scenario is using any n replicas to confirm commits. In this case, the configuration is slightly different. For example, if we only need any one replica to confirm commits:\nsynchronous_mode: quorum # use quorum commit postgresql: parameters: # modify PostgreSQL's configuration parameter synchronous_standby_names, using `ANY n ()` syntax synchronous_standby_names: 'ANY 1 (*)' # you can specify a specific replica list or use * to wildcard all replicas. Example: Enable ANY quorum commit $ pg edit-config pg-test + synchronous_standby_names: 'ANY 1 (*)' # in ANY mode, this parameter is needed - synchronous_node_count: 2 # in ANY mode, this parameter is not needed Apply these changes? [y/N]: y After applying, the configuration takes effect, and all standbys become regular replicas in Patroni. However, in pg_stat_replication, you can see sync_state becomes quorum.\nStandby Cluster You can clone an existing cluster and create a standby cluster for data migration, horizontal splitting, multi-region deployment, or disaster recovery.\nUnder normal circumstances, the standby cluster will follow the upstream cluster and keep content synchronized. You can promote the standby cluster to become a truly independent cluster.\nThe standby cluster definition is basically the same as a normal cluster definition, except that the pg_upstream parameter is additionally defined on the primary. The primary of the standby cluster is called the Standby Leader.\nFor example, below defines a pg-test cluster and its standby cluster pg-test2. The configuration inventory might look like this:\n# pg-test is the original cluster pg-test: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } vars: { pg_cluster: pg-test } # pg-test2 is the standby cluster of pg-test pg-test2: hosts: 10.10.10.12: { pg_seq: 1, pg_role: primary , pg_upstream: 10.10.10.11 } # \u003c--- pg_upstream defined here 10.10.10.13: { pg_seq: 2, pg_role: replica } vars: { pg_cluster: pg-test2 } The primary node pg-test2-1 of the pg-test2 cluster will be a downstream replica of pg-test and serve as the Standby Leader in the pg-test2 cluster.\nJust ensure the pg_upstream parameter is configured on the standby cluster’s primary node to automatically pull backups from the original upstream.\nbin/pgsql-add pg-test # create original cluster bin/pgsql-add pg-test2 # create standby cluster Example: Change replication upstream If necessary (e.g., upstream primary-standby switchover/failover), you can change the standby cluster’s replication upstream through cluster configuration.\nTo do this, simply change standby_cluster.host to the new upstream IP address and apply.\n$ pg edit-config pg-test2 standby_cluster: create_replica_methods: - basebackup - host: 10.10.10.13 # \u003c--- old upstream + host: 10.10.10.12 # \u003c--- new upstream port: 5432 Apply these changes? [y/N]: y Example: Promote standby cluster You can promote the standby cluster to an independent cluster at any time, so the cluster can independently handle write requests and diverge from the original cluster.\nTo do this, you must configure the cluster and completely erase the standby_cluster section, then apply.\n$ pg edit-config pg-test2 -standby_cluster: - create_replica_methods: - - basebackup - host: 10.10.10.11 - port: 5432 Apply these changes? [y/N]: y Example: Cascade replication If you specify pg_upstream on a replica instead of the primary, you can configure cascade replication for the cluster.\nWhen configuring cascade replication, you must use the IP address of an instance in the cluster as the parameter value, otherwise initialization will fail. The replica performs streaming replication from a specific instance rather than the primary.\nThe instance acting as a WAL relay is called a Bridge Instance. Using a bridge instance can share the burden of sending WAL from the primary. When you have dozens of replicas, using bridge instance cascade replication is a good idea.\npg-test: hosts: # pg-test-1 ---\u003e pg-test-2 ---\u003e pg-test-3 10.10.10.11: { pg_seq: 1, pg_role: primary } 10.10.10.12: { pg_seq: 2, pg_role: replica } # \u003c--- bridge instance 10.10.10.13: { pg_seq: 3, pg_role: replica, pg_upstream: 10.10.10.12 } # ^--- replicate from pg-test-2 (bridge) instead of pg-test-1 (primary) vars: { pg_cluster: pg-test } Delayed Cluster A Delayed Cluster is a special type of standby cluster used to quickly recover “accidentally deleted” data.\nFor example, if you want a cluster named pg-testdelay whose data content is the same as the pg-test cluster from one hour ago:\n# pg-test is the original cluster pg-test: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } vars: { pg_cluster: pg-test } # pg-testdelay is the delayed cluster of pg-test pg-testdelay: hosts: 10.10.10.12: { pg_seq: 1, pg_role: primary , pg_upstream: 10.10.10.11, pg_delay: 1d } 10.10.10.13: { pg_seq: 2, pg_role: replica } vars: { pg_cluster: pg-testdelay } You can also configure a “replication delay” on an existing standby cluster.\n$ pg edit-config pg-testdelay standby_cluster: create_replica_methods: - basebackup host: 10.10.10.11 port: 5432 + recovery_min_apply_delay: 1h # \u003c--- add delay duration here, e.g. 1 hour Apply these changes? [y/N]: y When some tuples and tables are accidentally deleted, you can modify this parameter to advance this delayed cluster to an appropriate point in time, read data from it, and quickly fix the original cluster.\nDelayed clusters require additional resources, but are much faster than PITR and have much less impact on the system. For very critical clusters, consider setting up delayed clusters.\nCitus Cluster Pigsty natively supports Citus. You can refer to files/pigsty/citus.yml and prod.yml as examples.\nTo define a Citus cluster, you need to specify the following parameters:\npg_mode must be set to citus, not the default pgsql The shard name pg_shard and shard number pg_group must be defined on each shard cluster pg_primary_db must be defined to specify the database managed by Patroni. If you want to use pg_dbsu postgres instead of the default pg_admin_username to execute admin commands, then pg_dbsu_password must be set to a non-empty plaintext password Additionally, extra hba rules are needed to allow SSL access from localhost and other data nodes. As shown below:\nall: children: pg-citus0: # citus shard 0 hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: { pg_cluster: pg-citus0 , pg_group: 0 } pg-citus1: # citus shard 1 hosts: { 10.10.10.11: { pg_seq: 1, pg_role: primary } } vars: { pg_cluster: pg-citus1 , pg_group: 1 } pg-citus2: # citus shard 2 hosts: { 10.10.10.12: { pg_seq: 1, pg_role: primary } } vars: { pg_cluster: pg-citus2 , pg_group: 2 } pg-citus3: # citus shard 3 hosts: 10.10.10.13: { pg_seq: 1, pg_role: primary } 10.10.10.14: { pg_seq: 2, pg_role: replica } vars: { pg_cluster: pg-citus3 , pg_group: 3 } vars: # global parameters for all Citus clusters pg_mode: citus # pgsql cluster mode must be set to: citus pg_shard: pg-citus # citus horizontal shard name: pg-citus pg_primary_db: meta # citus database name: meta pg_dbsu_password: DBUser.Postgres # if using dbsu, need to configure a password for it pg_users: [ { name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [ dbrole_admin ] } ] pg_databases: [ { name: meta ,extensions: [ { name: citus }, { name: postgis }, { name: timescaledb } ] } ] pg_hba_rules: - { user: 'all' ,db: all ,addr: 127.0.0.1/32 ,auth: ssl ,title: 'all user ssl access from localhost' } - { user: 'all' ,db: all ,addr: intra ,auth: ssl ,title: 'all user ssl access from intranet' } On the coordinator node, you can create distributed tables and reference tables and query them from any data node. Starting from 11.2, any Citus database node can act as a coordinator.\nSELECT create_distributed_table('pgbench_accounts', 'aid'); SELECT truncate_local_data_after_distributing_table($$public.pgbench_accounts$$); SELECT create_reference_table('pgbench_branches') ; SELECT truncate_local_data_after_distributing_table($$public.pgbench_branches$$); SELECT create_reference_table('pgbench_history') ; SELECT truncate_local_data_after_distributing_table($$public.pgbench_history$$); SELECT create_reference_table('pgbench_tellers') ; SELECT truncate_local_data_after_distributing_table($$public.pgbench_tellers$$); ","categories":["Reference"],"description":"Choose the appropriate instance and cluster types based on your requirements to configure PostgreSQL database clusters that meet your needs.","excerpt":"Choose the appropriate instance and cluster types based on your …","ref":"/docs/pgsql/config/cluster/","tags":"","title":"Cluster / Instance"},{"body":" Choosing a “kernel” in Pigsty means determining the PostgreSQL major version, mode/distribution, packages to install, and tuning templates to load.\nPigsty supports PostgreSQL from version 10 onwards. The current version packages core software for versions 13-18 by default and provides a complete extension set for 17/18. The following content shows how to make these choices through configuration files.\nMajor Version and Packages pg_version: Specify the PostgreSQL major version (default 18). Pigsty will automatically map to the correct package name prefix based on the version. pg_packages: Define the core package set to install, supports using package aliases (default pgsql-main pgsql-common, includes kernel + patroni/pgbouncer/pgbackrest and other common tools). pg_extensions: List of additional extension packages to install, also supports aliases; defaults to empty meaning only core dependencies are installed. all: vars: pg_version: 17 pg_packages: [ pgsql-main pgsql-common ] pg_extensions: [ postgis, timescaledb, pgvector, pgml ] Effect: Ansible will pull packages corresponding to pg_version=17 during installation, pre-install extensions to the system, and database initialization scripts can then directly CREATE EXTENSION.\nExtension support varies across versions in Pigsty’s offline repository: 12/13 only provide core and tier-1 extensions, while 15/17/18 cover all extensions. If an extension is not pre-packaged, it can be added via repo_packages_extra.\nKernel Mode (pg_mode) pg_mode controls the kernel “flavor” to deploy. Default pgsql indicates standard PostgreSQL. Pigsty currently supports the following modes:\nMode Scenario pgsql Standard PostgreSQL, HA + replication citus Citus distributed cluster, requires additional pg_shard / pg_group gpsql Greenplum / MatrixDB mssql Babelfish for PostgreSQL mysql OpenGauss/HaloDB compatible with MySQL protocol polar Alibaba PolarDB (based on pg polar distribution) ivory IvorySQL (Oracle-compatible syntax) oriole OrioleDB storage engine oracle PostgreSQL + ora compatibility (pg_mode: oracle) After selecting a mode, Pigsty will automatically load corresponding templates, dependency packages, and Patroni configurations. For example, deploying Citus:\nall: children: pg-citus0: hosts: { 10.10.10.11: { pg_seq: 1, pg_role: primary } } vars: { pg_cluster: pg-citus0, pg_group: 0 } pg-citus1: hosts: { 10.10.10.12: { pg_seq: 1, pg_role: primary } } vars: { pg_cluster: pg-citus1, pg_group: 1 } vars: pg_mode: citus pg_shard: pg-citus patroni_citus_db: meta Effect: All members will install Citus-related packages, Patroni writes to etcd in shard mode, and automatically CREATE EXTENSION citus in the meta database.\nExtensions and Pre-installed Objects Besides system packages, you can control components automatically loaded after database startup through the following parameters:\npg_libs: List to write to shared_preload_libraries. For example: pg_libs: 'timescaledb, pg_stat_statements, auto_explain'. pg_default_extensions / pg_default_schemas: Control schemas and extensions pre-created in template1 and postgres by initialization scripts. pg_parameters: Append ALTER SYSTEM SET for all instances (written to postgresql.auto.conf). Example: Enable TimescaleDB, pgvector and customize some system parameters.\npg-analytics: vars: pg_cluster: pg-analytics pg_libs: 'timescaledb, pg_stat_statements, pgml' pg_default_extensions: - { name: timescaledb } - { name: pgvector } pg_parameters: timescaledb.max_background_workers: 8 shared_preload_libraries: \"'timescaledb,pg_stat_statements,pgml'\" Effect: During initialization, template1 creates extensions, Patroni’s postgresql.conf injects corresponding parameters, and all business databases inherit these settings.\nTuning Template (pg_conf) pg_conf points to Patroni templates in roles/pgsql/templates/*.yml. Pigsty includes four built-in general templates:\nTemplate Applicable Scenario oltp.yml Default template, for 4–128 core TP workload olap.yml Optimized for analytical scenarios crit.yml Emphasizes sync commit/minimal latency, suitable for zero-loss scenarios like finance tiny.yml Lightweight machines / edge scenarios / resource-constrained environments You can directly replace the template or customize a YAML file in templates/, then specify it in cluster vars.\npg-ledger: hosts: { 10.10.10.21: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-ledger pg_conf: crit.yml pg_parameters: synchronous_commit: 'remote_apply' max_wal_senders: 16 wal_keep_size: '2GB' Effect: Copy crit.yml as Patroni configuration, overlay pg_parameters written to postgresql.auto.conf, making instances run immediately in synchronous commit mode.\nCombined Instance: A Complete Example pg-rag: hosts: 10.10.10.31: { pg_seq: 1, pg_role: primary } 10.10.10.32: { pg_seq: 2, pg_role: replica } vars: pg_cluster: pg-rag pg_version: 18 pg_mode: pgsql pg_conf: olap.yml pg_packages: [ pgsql-main pgsql-common ] pg_extensions: [ pgvector, pgml, postgis ] pg_libs: 'pg_stat_statements, pgvector, pgml' pg_parameters: max_parallel_workers: 8 shared_buffers: '32GB' First primary + one replica, using olap.yml tuning. Install PG18 + RAG common extensions, automatically load pgvector/pgml at system level. Patroni/pgbouncer/pgbackrest generated by Pigsty, no manual intervention needed. Replace the above parameters according to business needs to complete all kernel-level customization.\n","categories":["Reference"],"description":"How to choose the appropriate PostgreSQL kernel and major version.","excerpt":"How to choose the appropriate PostgreSQL kernel and major version.","ref":"/docs/pgsql/config/kernel/","tags":"","title":"Kernel Version"},{"body":"PostgreSQL package naming conventions vary significantly across different operating systems:\nEL systems (RHEL/Rocky/Alma/…) use formats like pgvector_17, postgis36_17* Debian/Ubuntu systems use formats like postgresql-17-pgvector, postgresql-17-postgis-3 This difference adds cognitive burden to users: you need to remember different package name rules for different systems, and handle the embedding of PostgreSQL version numbers.\nPackage Alias Pigsty solves this problem through the Package Alias mechanism: you only need to use unified aliases, and Pigsty will handle all the details:\n# Using aliases - simple, unified, cross-platform pg_extensions: [ postgis, pgvector, timescaledb ] # Equivalent to actual package names on EL9 + PG17 pg_extensions: [ postgis36_17*, pgvector_17*, timescaledb-tsl_17* ] # Equivalent to actual package names on Ubuntu 24 + PG17 pg_extensions: [ postgresql-17-postgis-3, postgresql-17-pgvector, postgresql-17-timescaledb-tsl ] Alias Translation Aliases can also group a set of packages as a whole. For example, Pigsty’s default installed packages - the default value of pg_packages is:\npg_packages: # pg packages to be installed, alias can be used - pgsql-main pgsql-common Pigsty will query the current operating system alias list (assuming el10.x86_64) and translate it to PGSQL kernel, extensions, and toolkits:\npgsql-main: \"postgresql$v postgresql$v-server postgresql$v-libs postgresql$v-contrib postgresql$v-plperl postgresql$v-plpython3 postgresql$v-pltcl postgresql$v-llvmjit pg_repack_$v* wal2json_$v* pgvector_$v*\" pgsql-common: \"patroni patroni-etcd pgbouncer pgbackrest pg_exporter pgbackrest_exporter vip-manager\" Next, Pigsty further translates pgsql-main using the currently specified PG major version (assuming pg_version = 18):\npg18-main: \"postgresql18 postgresql18-server postgresql18-libs postgresql18-contrib postgresql18-plperl postgresql18-plpython3 postgresql18-pltcl postgresql18-llvmjit pg_repack_18* wal2json_18* pgvector_18*\" Through this approach, Pigsty shields the complexity of packages, allowing users to simply specify the functional components they want.\nWhich Variables Can Use Aliases? You can use package aliases in the following four parameters, and the aliases will be automatically converted to actual package names according to the translation process:\npg_extensions - PG extension packages pg_packages - PG kernel/base utility packages repo_packages - Package download parameter: packages to download to local repository repo_packages_extra - Extension installation parameter: additional packages to download to local repository Alias List You can find the alias mapping files for each operating system and architecture in the roles/node_id/vars/ directory of the Pigsty project source code:\nel10.x86_64 el10.aarch64 el9.x86_64 el9.aarch64 el8.x86_64 el8.aarch64 u24.x86_64 u24.aarch64 u22.x86_64 u22.aarch64 d13.x86_64 d13.aarch64 d12.x86_64 d12.aarch64 How It Works Alias Translation Process User config alias --\u003e Detect OS --\u003e Find alias mapping table ---\u003e Replace $v placeholder ---\u003e Install actual packages ↓ ↓ ↓ ↓ postgis el9.x86_64 postgis36_$v* postgis36_17* postgis u24.x86_64 postgresql-$v-postgis-3 postgresql-17-postgis-3 Version Placeholder Pigsty’s alias system uses $v as a placeholder for the PostgreSQL version number. When you specify a PostgreSQL version using pg_version, all $v in aliases will be replaced with the actual version number.\nFor example, when pg_version: 17:\nAlias Definition (EL) Expanded Result postgresql$v* postgresql17* pgvector_$v* pgvector_17* timescaledb-tsl_$v* timescaledb-tsl_17* Alias Definition (Debian/Ubuntu) Expanded Result postgresql-$v postgresql-17 postgresql-$v-pgvector postgresql-17-pgvector postgresql-$v-timescaledb-tsl postgresql-17-timescaledb-tsl Wildcard Matching On EL systems, many aliases use the * wildcard to match related subpackages. For example:\npostgis36_17* will match postgis36_17, postgis36_17-client, postgis36_17-utils, etc. postgresql17* will match postgresql17, postgresql17-server, postgresql17-libs, postgresql17-contrib, etc. This design ensures you don’t need to list each subpackage individually - one alias can install the complete extension.\n","categories":["Reference"],"description":"Pigsty provides a package alias translation mechanism that shields the differences in binary package details across operating systems, making installation easier.","excerpt":"Pigsty provides a package alias translation mechanism that shields the …","ref":"/docs/pgsql/config/alias/","tags":"","title":"Package Alias"},{"body":" In this document, “user” refers to a logical object within a database cluster created with CREATE USER/ROLE.\nIn PostgreSQL, users belong directly to the database cluster rather than a specific database. Therefore, when creating business databases and users, follow the principle of “users first, databases later”.\nPigsty defines roles and users through two config parameters:\npg_default_roles: Define globally shared roles and users pg_users: Define business users and roles at cluster level The former defines roles/users shared across the entire environment; the latter defines business roles/users specific to a single cluster. Both have the same format as arrays of user definition objects. Users/roles are created sequentially in array order, so later users can belong to roles defined earlier.\nBy default, all users marked with pgbouncer: true are added to the Pgbouncer connection pool user list.\nDefine Users Example from Pigsty demo pg-meta cluster:\npg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta pg_users: - {name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [dbrole_admin] ,comment: pigsty admin user } - {name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [dbrole_readonly] ,comment: read-only viewer for meta database } - {name: dbuser_grafana ,password: DBUser.Grafana ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for grafana database } - {name: dbuser_bytebase ,password: DBUser.Bytebase ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for bytebase database } - {name: dbuser_kong ,password: DBUser.Kong ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for kong api gateway } - {name: dbuser_gitea ,password: DBUser.Gitea ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for gitea service } - {name: dbuser_wiki ,password: DBUser.Wiki ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for wiki.js service } - {name: dbuser_noco ,password: DBUser.Noco ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for nocodb service } - {name: dbuser_remove ,state: absent } # use state: absent to delete user Each user/role definition is a complex object. Only name is required:\n- name: dbuser_meta # REQUIRED, `name` is the only mandatory field state: create # Optional, user state: create (default), absent password: DBUser.Meta # Optional, password, can be scram-sha-256 hash or plaintext login: true # Optional, can login, default true superuser: false # Optional, is superuser, default false createdb: false # Optional, can create databases, default false createrole: false # Optional, can create roles, default false inherit: true # Optional, inherit role privileges, default true replication: false # Optional, can replicate, default false bypassrls: false # Optional, bypass row-level security, default false connlimit: -1 # Optional, connection limit, default -1 (unlimited) expire_in: 3650 # Optional, expire N days from creation (priority over expire_at) expire_at: '2030-12-31' # Optional, expiration date in YYYY-MM-DD format comment: pigsty admin user # Optional, user comment roles: [dbrole_admin] # Optional, roles array parameters: # Optional, role-level config params search_path: public pgbouncer: true # Optional, add to connection pool user list, default false pool_mode: transaction # Optional, pgbouncer pool mode, default transaction pool_connlimit: -1 # Optional, user-level max pool connections, default -1 Parameter Overview The only required field is name - a valid, unique username within the cluster. All other params have sensible defaults.\nField Category Type Attr Description name Basic string Required Username, must be valid and unique state Basic enum Optional State: create (default), absent password Basic string Mutable User password, plaintext or hash comment Basic string Mutable User comment login Privilege bool Mutable Can login, default true superuser Privilege bool Mutable Is superuser, default false createdb Privilege bool Mutable Can create databases, default false createrole Privilege bool Mutable Can create roles, default false inherit Privilege bool Mutable Inherit role privileges, default true replication Privilege bool Mutable Can replicate, default false bypassrls Privilege bool Mutable Bypass RLS, default false connlimit Privilege int Mutable Connection limit, -1 unlimited expire_in Validity int Mutable Expire N days from now (priority) expire_at Validity string Mutable Expiration date, YYYY-MM-DD format roles Role array Additive Roles array, string or object format parameters Params object Mutable Role-level parameters pgbouncer Pool bool Mutable Add to connection pool, default false pool_mode Pool enum Mutable Pool mode: transaction (default) pool_connlimit Pool int Mutable Pool user max connections Parameter Details name String, required. Username - must be unique within the cluster.\nMust be a valid PostgreSQL identifier matching ^[a-z_][a-z0-9_]{0,62}$: starts with lowercase letter or underscore, contains only lowercase letters, digits, underscores, max 63 chars.\n- name: dbuser_app # Standard naming - name: app_readonly # Underscore separated - name: _internal # Underscore prefix (for internal roles) state Enum for user operation: create or absent. Default create.\nState Description create Default, create user, update if exists absent Delete user with DROP ROLE - name: dbuser_app # state defaults to create - name: dbuser_old state: absent # Delete user These system users cannot be deleted via state: absent (to prevent cluster failure):\npostgres: Database superuser replicator: Replication user (or pg_replication_username) dbuser_dba: Admin user (or pg_admin_username) dbuser_monitor: Monitor user (or pg_monitor_username) password String, mutable. User password - users without password can’t login via password auth.\nPassword can be:\nFormat Example Description Plaintext DBUser.Meta Not recommended, logged to config SCRAM-SHA-256 SCRAM-SHA-256$4096:xxx$yyy:zzz Recommended, PG10+ default MD5 hash md5... Legacy compatibility # Plaintext (not recommended, logged to config) - name: dbuser_app password: MySecretPassword # SCRAM-SHA-256 hash (recommended) - name: dbuser_app password: 'SCRAM-SHA-256$4096:xxx$yyy:zzz' When setting password, Pigsty temporarily disables logging to prevent leakage:\nSET log_statement TO 'none'; ALTER USER \"dbuser_app\" PASSWORD 'xxx'; SET log_statement TO DEFAULT; To generate SCRAM-SHA-256 hash:\n# Using PostgreSQL (requires pgcrypto extension) psql -c \"SELECT encode(digest('password' || 'username', 'sha256'), 'hex')\" comment String, mutable. User comment, defaults to business user {name}.\nSet via COMMENT ON ROLE, supports special chars (quotes auto-escaped).\n- name: dbuser_app comment: 'Main business application account' COMMENT ON ROLE \"dbuser_app\" IS 'Main business application account'; login Boolean, mutable. Can login, default true.\nSetting false creates a Role rather than User - typically for permission grouping.\nIn PostgreSQL, CREATE USER equals CREATE ROLE ... LOGIN.\n# Create login-able user - name: dbuser_app login: true # Create role (no login, for permission grouping) - name: dbrole_custom login: false comment: custom permission role CREATE USER \"dbuser_app\" LOGIN; CREATE USER \"dbrole_custom\" NOLOGIN; superuser Boolean, mutable. Is superuser, default false.\nSuperusers have full database privileges, bypassing all permission checks.\n- name: dbuser_admin superuser: true # Dangerous: full privileges ALTER USER \"dbuser_admin\" SUPERUSER; Pigsty provides default superuser via pg_admin_username (dbuser_dba). Don’t create additional superusers unless necessary.\ncreatedb Boolean, mutable. Can create databases, default false.\n- name: dbuser_dev createdb: true # Allow create database ALTER USER \"dbuser_dev\" CREATEDB; Some applications (Gitea, Odoo, etc.) may require CREATEDB privilege for their admin users.\ncreaterole Boolean, mutable. Can create other roles, default false.\nUsers with CREATEROLE can create, modify, delete other non-superuser roles.\n- name: dbuser_admin createrole: true # Allow manage other roles ALTER USER \"dbuser_admin\" CREATEROLE; inherit Boolean, mutable. Auto-inherit privileges from member roles, default true.\nSetting false requires explicit SET ROLE to use member role privileges.\n# Auto-inherit role privileges (default) - name: dbuser_app inherit: true roles: [dbrole_readwrite] # Requires explicit SET ROLE - name: dbuser_special inherit: false roles: [dbrole_admin] ALTER USER \"dbuser_special\" NOINHERIT; -- User must execute SET ROLE dbrole_admin to get privileges replication Boolean, mutable. Can initiate streaming replication, default false.\nUsually only replication users (replicator) need this. Normal users shouldn’t have it unless for logical decoding subscriptions.\n- name: replicator replication: true # Allow streaming replication roles: [pg_monitor, dbrole_readonly] ALTER USER \"replicator\" REPLICATION; bypassrls Boolean, mutable. Bypass row-level security (RLS) policies, default false.\nWhen enabled, user can access all rows even with RLS policies. Usually only for admins.\n- name: dbuser_myappadmin bypassrls: true # Bypass RLS policies ALTER USER \"dbuser_myappadmin\" BYPASSRLS; connlimit Integer, mutable. Max concurrent connections, default -1 (unlimited).\nPositive integer limits max simultaneous sessions for this user. Doesn’t affect superusers.\n- name: dbuser_app connlimit: 100 # Max 100 concurrent connections - name: dbuser_batch connlimit: 10 # Limit batch user connections ALTER USER \"dbuser_app\" CONNECTION LIMIT 100; expire_in Integer, mutable. Expire N days from current date.\nThis param has higher priority than expire_at. Expiration recalculated on each playbook run - good for temp users needing periodic renewal.\n- name: temp_user expire_in: 30 # Expire in 30 days - name: contractor_user expire_in: 90 # Expire in 90 days Generates SQL:\n-- expire_in: 30, assuming current date is 2025-01-01 ALTER USER \"temp_user\" VALID UNTIL '2025-01-31'; expire_at String, mutable. Expiration date in YYYY-MM-DD format, or special value infinity.\nLower priority than expire_in. Use infinity for never-expiring users.\n- name: contractor_user expire_at: '2024-12-31' # Expire on specific date - name: permanent_user expire_at: 'infinity' # Never expires ALTER USER \"contractor_user\" VALID UNTIL '2024-12-31'; ALTER USER \"permanent_user\" VALID UNTIL 'infinity'; roles Array, additive. Roles this user belongs to. Elements can be strings or objects.\nSimple format - strings for role names:\n- name: dbuser_app roles: - dbrole_readwrite - pg_read_all_data GRANT \"dbrole_readwrite\" TO \"dbuser_app\"; GRANT \"pg_read_all_data\" TO \"dbuser_app\"; Full format - objects for fine-grained control:\n- name: dbuser_app roles: - dbrole_readwrite # Simple string: GRANT role - { name: dbrole_admin, admin: true } # WITH ADMIN OPTION - { name: pg_monitor, set: false } # PG16+: disallow SET ROLE - { name: pg_signal_backend, inherit: false } # PG16+: don't auto-inherit - { name: old_role, state: absent } # Revoke role membership Object Format Parameters:\nParam Type Description name string Role name (required) state enum grant (default) or absent/revoke: control membership admin bool true: WITH ADMIN OPTION, false: REVOKE ADMIN set bool PG16+: true: WITH SET TRUE, false: REVOKE SET inherit bool PG16+: true: WITH INHERIT TRUE, false: REVOKE INHERIT PostgreSQL 16+ New Features:\nPostgreSQL 16 introduced finer-grained role membership control:\nADMIN OPTION: Allow granting role to other users SET OPTION: Allow using SET ROLE to switch to this role INHERIT OPTION: Auto-inherit this role’s privileges # PostgreSQL 16+ complete example - name: dbuser_app roles: # Normal membership - dbrole_readwrite # Can grant dbrole_admin to other users - { name: dbrole_admin, admin: true } # Cannot SET ROLE to pg_monitor (only inherit privileges) - { name: pg_monitor, set: false } # Don't auto-inherit pg_execute_server_program (need explicit SET ROLE) - { name: pg_execute_server_program, inherit: false } # Revoke old_role membership - { name: old_role, state: absent } set and inherit options only work in PG16+. On earlier versions they’re ignored with warning comments.\nparameters Object, mutable. Role-level config params via ALTER ROLE ... SET. Applies to all sessions for this user.\n- name: dbuser_analyst parameters: work_mem: '256MB' statement_timeout: '5min' search_path: 'analytics,public' log_statement: 'all' ALTER USER \"dbuser_analyst\" SET \"work_mem\" = '256MB'; ALTER USER \"dbuser_analyst\" SET \"statement_timeout\" = '5min'; ALTER USER \"dbuser_analyst\" SET \"search_path\" = 'analytics,public'; ALTER USER \"dbuser_analyst\" SET \"log_statement\" = 'all'; Use special value DEFAULT (case-insensitive) to reset to PostgreSQL default:\n- name: dbuser_app parameters: work_mem: DEFAULT # Reset to default statement_timeout: '30s' # Set new value ALTER USER \"dbuser_app\" SET \"work_mem\" = DEFAULT; ALTER USER \"dbuser_app\" SET \"statement_timeout\" = '30s'; Common role-level params:\nParameter Description Example work_mem Query work memory '64MB' statement_timeout Statement timeout '30s' lock_timeout Lock wait timeout '10s' idle_in_transaction_session_timeout Idle transaction timeout '10min' search_path Schema search path 'app,public' log_statement Log level 'ddl' temp_file_limit Temp file size limit '10GB' Query user-level params via pg_db_role_setting system view.\npgbouncer Boolean, mutable. Add user to Pgbouncer user list, default false.\nFor prod users needing connection pool access, must explicitly set pgbouncer: true. Default false prevents accidentally exposing internal users to the pool.\n# Prod user: needs connection pool - name: dbuser_app password: DBUser.App pgbouncer: true # Internal user: no connection pool needed - name: dbuser_internal password: DBUser.Internal pgbouncer: false # Default, can be omitted Users with pgbouncer: true are added to /etc/pgbouncer/userlist.txt.\npool_mode Enum, mutable. User-level pool mode: transaction, session, or statement. Default transaction.\nMode Description Use Case transaction Return connection after txn Most OLTP apps, default session Return connection after session Apps needing session state statement Return after each statement Simple stateless queries # DBA user: session mode (may need SET commands etc.) - name: dbuser_dba pgbouncer: true pool_mode: session # Normal business user: transaction mode - name: dbuser_app pgbouncer: true pool_mode: transaction User-level pool params are configured via /etc/pgbouncer/useropts.txt:\ndbuser_dba = pool_mode=session max_user_connections=16 dbuser_monitor = pool_mode=session max_user_connections=8 pool_connlimit Integer, mutable. User-level max pool connections, default -1 (unlimited).\n- name: dbuser_app pgbouncer: true pool_connlimit: 50 # Max 50 pool connections for this user ACL System Pigsty provides a built-in, out-of-the-box access control / ACL system. Just assign these four default roles to business users:\nRole Privileges Typical Use Case dbrole_readwrite Global read-write Primary business prod accounts dbrole_readonly Global read-only Other business read-only access dbrole_admin DDL privileges Business admins, table creation dbrole_offline Restricted read-only (offline only) Individual users, ETL/analytics # Typical business user configuration pg_users: - name: dbuser_app password: DBUser.App pgbouncer: true roles: [dbrole_readwrite] # Prod account, read-write - name: dbuser_readonly password: DBUser.Readonly pgbouncer: true roles: [dbrole_readonly] # Read-only account - name: dbuser_admin password: DBUser.Admin pgbouncer: true roles: [dbrole_admin] # Admin, can execute DDL - name: dbuser_etl password: DBUser.ETL roles: [dbrole_offline] # Offline analytics account To redesign your own ACL system, customize:\npg_default_roles: System-wide roles and global users pg_default_privileges: Default privileges for new objects pg-init-role.sql: Role creation SQL template pg-init-template.sql: Privilege SQL template Pgbouncer Users Pgbouncer is enabled by default as connection pool middleware. Pigsty adds all users in pg_users with explicit pgbouncer: true flag to the pgbouncer user list.\nUsers in connection pool are listed in /etc/pgbouncer/userlist.txt:\n\"postgres\" \"\" \"dbuser_wiki\" \"SCRAM-SHA-256$4096:+77dyhrPeFDT/TptHs7/7Q==$KeatuohpKIYzHPCt/tqBu85vI11o9mar/by0hHYM2W8=:X9gig4JtjoS8Y/o1vQsIX/gY1Fns8ynTXkbWOjUfbRQ=\" \"dbuser_view\" \"SCRAM-SHA-256$4096:DFoZHU/DXsHL8MJ8regdEw==$gx9sUGgpVpdSM4o6A2R9PKAUkAsRPLhLoBDLBUYtKS0=:MujSgKe6rxcIUMv4GnyXJmV0YNbf39uFRZv724+X1FE=\" \"dbuser_monitor\" \"SCRAM-SHA-256$4096:fwU97ZMO/KR0ScHO5+UuBg==$CrNsmGrx1DkIGrtrD1Wjexb/aygzqQdirTO1oBZROPY=:L8+dJ+fqlMQh7y4PmVR/gbAOvYWOr+KINjeMZ8LlFww=\" \"dbuser_meta\" \"SCRAM-SHA-256$4096:leB2RQPcw1OIiRnPnOMUEg==$eyC+NIMKeoTxshJu314+BmbMFpCcspzI3UFZ1RYfNyU=:fJgXcykVPvOfro2MWNkl5q38oz21nSl1dTtM65uYR1Q=\" User-level pool params are maintained in /etc/pgbouncer/useropts.txt:\ndbuser_dba = pool_mode=session max_user_connections=16 dbuser_monitor = pool_mode=session max_user_connections=8 When creating users, Pgbouncer user list is refreshed via online reload - doesn’t affect existing connections.\nPgbouncer runs as same dbsu as PostgreSQL (default postgres OS user). Use pgb alias to access pgbouncer admin functions.\npgbouncer_auth_query param allows dynamic query for pool user auth - convenient when you prefer not to manually manage pool users.\nRelated Resources For user management operations, see User Management.\nFor user access privileges, see ACL: Role Privileges.\n","categories":["Reference"],"description":"How to define and customize PostgreSQL users and roles through configuration?","excerpt":"How to define and customize PostgreSQL users and roles through …","ref":"/docs/pgsql/config/user/","tags":"","title":"User/Role"},{"body":" In this document, “database” refers to a logical object within a database cluster created with CREATE DATABASE.\nA PostgreSQL cluster can serve multiple databases simultaneously. In Pigsty, you can define required databases in cluster configuration.\nPigsty customizes the template1 template database - creating default schemas, installing default extensions, configuring default privileges. Newly created databases inherit these settings from template1. You can also specify other template databases via template for instant database cloning.\nBy default, all business databases are 1:1 added to Pgbouncer connection pool; pg_exporter auto-discovers all business databases for in-database object monitoring. All databases are also registered as PostgreSQL datasources in Grafana on all INFRA nodes for PGCAT dashboards.\nDefine Database Business databases are defined in cluster param pg_databases, an array of database definition objects. During cluster initialization, databases are created in definition order, so later databases can use earlier ones as templates.\nExample from Pigsty demo pg-meta cluster:\npg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta pg_databases: - { name: meta ,baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [pigsty] ,extensions: [{name: postgis, schema: public}, {name: timescaledb}]} - { name: grafana ,owner: dbuser_grafana ,revokeconn: true ,comment: grafana primary database } - { name: bytebase ,owner: dbuser_bytebase ,revokeconn: true ,comment: bytebase primary database } - { name: kong ,owner: dbuser_kong ,revokeconn: true ,comment: kong the api gateway database } - { name: gitea ,owner: dbuser_gitea ,revokeconn: true ,comment: gitea meta database } - { name: wiki ,owner: dbuser_wiki ,revokeconn: true ,comment: wiki meta database } - { name: noco ,owner: dbuser_noco ,revokeconn: true ,comment: nocodb database } Each database definition is a complex object with fields below. Only name is required:\n- name: meta # REQUIRED, `name` is the only mandatory field state: create # Optional, database state: create (default), absent, recreate baseline: cmdb.sql # Optional, SQL baseline file path (relative to Ansible search path, e.g., files/) pgbouncer: true # Optional, add to pgbouncer database list? default true schemas: [pigsty] # Optional, additional schemas to create, array of schema names extensions: # Optional, extensions to install: array of extension objects - { name: postgis , schema: public } # Can specify schema, or omit (installs to first schema in search_path) - { name: timescaledb } # Some extensions create and use fixed schemas comment: pigsty meta database # Optional, database comment/description owner: postgres # Optional, database owner, defaults to current user template: template1 # Optional, template to use, default template1 strategy: FILE_COPY # Optional, clone strategy: FILE_COPY or WAL_LOG (PG15+) encoding: UTF8 # Optional, inherits from template/cluster config (UTF8) locale: C # Optional, inherits from template/cluster config (C) lc_collate: C # Optional, inherits from template/cluster config (C) lc_ctype: C # Optional, inherits from template/cluster config (C) locale_provider: libc # Optional, locale provider: libc, icu, builtin (PG15+) icu_locale: en-US # Optional, ICU locale rules (PG15+) icu_rules: '' # Optional, ICU collation rules (PG16+) builtin_locale: C.UTF-8 # Optional, builtin locale provider rules (PG17+) tablespace: pg_default # Optional, default tablespace is_template: false # Optional, mark as template database allowconn: true # Optional, allow connections, default true revokeconn: false # Optional, revoke public CONNECT privilege, default false register_datasource: true # Optional, register to grafana datasource? default true connlimit: -1 # Optional, connection limit, -1 means unlimited parameters: # Optional, database-level params via ALTER DATABASE SET work_mem: '64MB' statement_timeout: '30s' pool_auth_user: dbuser_meta # Optional, auth user for pgbouncer auth_query pool_mode: transaction # Optional, database-level pgbouncer pool mode pool_size: 64 # Optional, database-level pgbouncer default pool size pool_reserve: 32 # Optional, database-level pgbouncer reserve pool pool_size_min: 0 # Optional, database-level pgbouncer min pool size pool_connlimit: 100 # Optional, database-level max database connections Parameter Overview The only required field is name - a valid, unique database name within the cluster. All other params have sensible defaults. Parameters marked “Immutable” only take effect at creation; changing them requires database recreation.\nField Category Type Attr Description name Basic string Required Database name, must be valid and unique state Basic enum Optional State: create (default), absent, recreate owner Basic string Mutable Database owner, defaults to postgres comment Basic string Mutable Database comment template Template string Immutable Template database, default template1 strategy Template enum Immutable Clone strategy: FILE_COPY or WAL_LOG (PG15+) encoding Encoding string Immutable Character encoding, default inherited (UTF8) locale Encoding string Immutable Locale setting, default inherited (C) lc_collate Encoding string Immutable Collation rule, default inherited (C) lc_ctype Encoding string Immutable Character classification, default inherited (C) locale_provider Encoding enum Immutable Locale provider: libc, icu, builtin (PG15+) icu_locale Encoding string Immutable ICU locale rules (PG15+) icu_rules Encoding string Immutable ICU collation customization (PG16+) builtin_locale Encoding string Immutable Builtin locale rules (PG17+) tablespace Storage string Mutable Default tablespace, change triggers data migration is_template Privilege bool Mutable Mark as template database allowconn Privilege bool Mutable Allow connections, default true revokeconn Privilege bool Mutable Revoke PUBLIC CONNECT privilege connlimit Privilege int Mutable Connection limit, -1 for unlimited baseline Init string Mutable SQL baseline file path, runs only on first create schemas Init (string|object)[] Mutable Schema definitions to create extensions Init (string|object)[] Mutable Extension definitions to install parameters Init object Mutable Database-level parameters pgbouncer Pool bool Mutable Add to connection pool, default true pool_mode Pool enum Mutable Pool mode: transaction (default) pool_size Pool int Mutable Default pool size, default 64 pool_size_min Pool int Mutable Min pool size, default 0 pool_reserve Pool int Mutable Reserve pool size, default 32 pool_connlimit Pool int Mutable Max database connections, default 100 pool_auth_user Pool string Mutable Auth query user register_datasource Monitor bool Mutable Register to Grafana datasource, default true Parameter Details name String, required. Database name - must be unique within the cluster.\nMust be a valid PostgreSQL identifier: max 63 chars, no SQL keywords, starts with letter or underscore, followed by letters, digits, or underscores. Must match: ^[A-Za-z_][A-Za-z0-9_$]{0,62}$\n- name: myapp # Simple naming - name: my_application # Underscore separated - name: app_v2 # Version included state Enum for database operation: create, absent, or recreate. Default create.\nState Description create Default, create or modify database, adjust mutable params if exists absent Delete database with DROP DATABASE WITH (FORCE) recreate Drop then create, for database reset - name: myapp # state defaults to create - name: olddb state: absent # Delete database - name: testdb state: recreate # Rebuild database owner String. Database owner, defaults to pg_dbsu (postgres) if not specified.\nTarget user must exist. Changing owner executes (old owner retains existing privileges):\nDatabase owner has full control including creating schemas, tables, extensions - useful for multi-tenant scenarios.\nALTER DATABASE \"myapp\" OWNER TO \"new_owner\"; GRANT ALL PRIVILEGES ON DATABASE \"myapp\" TO \"new_owner\"; comment String. Database comment, defaults to business database {name}.\nSet via COMMENT ON DATABASE, supports Chinese and special chars (Pigsty auto-escapes quotes). Stored in pg_database.datacl, viewable via \\l+.\nCOMMENT ON DATABASE \"myapp\" IS 'my main application database'; - name: myapp comment: my main application database template String, immutable. Template database for creation, default template1.\nPostgreSQL’s CREATE DATABASE clones the template - new database inherits all objects, extensions, schemas, permissions. Pigsty customizes template1 during cluster init, so new databases inherit these settings.\nTemplate Description template1 Default, includes Pigsty pre-configured extensions/schemas/perms template0 Clean template, required for non-default locale providers Custom database Use existing database as template for cloning When using icu or builtin locale provider, must specify template: template0 since template1 locale settings can’t be overridden.\n- name: myapp_icu template: template0 # Required for ICU locale_provider: icu icu_locale: zh-Hans Using template0 skips monitoring extensions/schemas and default privileges - allowing fully custom database.\nstrategy Enum, immutable. Clone strategy: FILE_COPY or WAL_LOG. Available PG15+.\nStrategy Description Use Case FILE_COPY Direct file copy, PG15+ default Large templates, general WAL_LOG Clone via WAL logging Small templates, non-blocking WAL_LOG doesn’t block template connections during clone but less efficient for large templates. Ignored on PG14 and earlier.\n- name: cloned_db template: source_db strategy: WAL_LOG # WAL-based cloning encoding String, immutable. Character encoding, inherits from template if unspecified (usually UTF8).\nStrongly recommend UTF8 unless special requirements. Cannot be changed after creation.\n- name: legacy_db template: template0 # Use template0 for non-default encoding encoding: LATIN1 locale String, immutable. Locale setting - sets both lc_collate and lc_ctype. Inherits from template (usually C).\nDetermines string sort order and character classification. Use C or POSIX for best performance and cross-platform consistency; use language-specific locales (e.g., zh_CN.UTF-8) for proper language sorting.\n- name: chinese_db template: template0 locale: zh_CN.UTF-8 # Chinese locale encoding: UTF8 lc_collate String, immutable. String collation rule. Inherits from template (usually C).\nDetermines ORDER BY and comparison results. Common values: C (byte order, fastest), C.UTF-8, en_US.UTF-8, zh_CN.UTF-8. Cannot be changed after creation.\n- name: myapp template: template0 lc_collate: en_US.UTF-8 # English collation lc_ctype: en_US.UTF-8 lc_ctype String, immutable. Character classification rule for upper/lower case, digits, letters. Inherits from template (usually C).\nAffects upper(), lower(), regex \\w, etc. Cannot be changed after creation.\nlocale_provider Enum, immutable. Locale implementation provider: libc, icu, or builtin. Available PG15+, default libc.\nProvider Version Description libc - OS C library, traditional default, varies by system icu PG15+ ICU library, cross-platform consistent, more langs builtin PG17+ PostgreSQL builtin, most efficient, C/C.UTF-8 only Using icu or builtin requires template: template0 with corresponding icu_locale or builtin_locale.\n- name: fast_db template: template0 locale_provider: builtin # Builtin provider, most efficient builtin_locale: C.UTF-8 icu_locale String, immutable. ICU locale identifier. Available PG15+ when locale_provider: icu.\nICU identifiers follow BCP 47. Common values:\nValue Description en-US US English en-GB British English zh-Hans Simplified Chinese zh-Hant Traditional Chinese ja-JP Japanese ko-KR Korean - name: chinese_app template: template0 locale_provider: icu icu_locale: zh-Hans # Simplified Chinese ICU collation encoding: UTF8 icu_rules String, immutable. Custom ICU collation rules. Available PG16+.\nAllows fine-tuning default sort behavior using ICU Collation Customization.\n- name: custom_sort_db template: template0 locale_provider: icu icu_locale: en-US icu_rules: '\u0026V \u003c\u003c w \u003c\u003c\u003c W' # Custom V/W sort order builtin_locale String, immutable. Builtin locale provider rules. Available PG17+ when locale_provider: builtin. Values: C or C.UTF-8.\nbuiltin provider is PG17’s new builtin implementation - faster than libc with consistent cross-platform behavior. Suitable for C/C.UTF-8 collation only.\n- name: fast_db template: template0 locale_provider: builtin builtin_locale: C.UTF-8 # Builtin UTF-8 support encoding: UTF8 tablespace String, mutable. Default tablespace, default pg_default.\nChanging tablespace triggers physical data migration - PostgreSQL moves all objects to new tablespace. Can take long time for large databases, use cautiously.\n- name: archive_db tablespace: slow_hdd # Archive data on slow storage ALTER DATABASE \"archive_db\" SET TABLESPACE \"slow_hdd\"; is_template Boolean, mutable. Mark database as template, default false.\nWhen true, any user with CREATEDB privilege can use this database as template for cloning. Template databases typically pre-install standard schemas, extensions, and data.\n- name: app_template is_template: true # Mark as template, allow user cloning schemas: [core, api] extensions: [postgis, pg_trgm] Deleting is_template: true databases: Pigsty first executes ALTER DATABASE ... IS_TEMPLATE false then drops.\nallowconn Boolean, mutable. Allow connections, default true.\nSetting false completely disables connections at database level - no user (including superuser) can connect. Used for maintenance or archival purposes.\n- name: archive_db allowconn: false # Disallow all connections ALTER DATABASE \"archive_db\" ALLOW_CONNECTIONS false; revokeconn Boolean, mutable. Revoke PUBLIC CONNECT privilege, default false.\nWhen true, Pigsty executes:\nRevoke PUBLIC CONNECT, regular users can’t connect Grant connect to replication user (replicator) and monitor user (dbuser_monitor) Grant connect to admin user (dbuser_dba) and owner with WITH GRANT OPTION Setting false restores PUBLIC CONNECT privilege.\n- name: secure_db owner: dbuser_secure revokeconn: true # Revoke public connect, only specified users connlimit Integer, mutable. Max concurrent connections, default -1 (unlimited).\nPositive integer limits max simultaneous sessions. Doesn’t affect superusers.\n- name: limited_db connlimit: 50 # Max 50 concurrent connections ALTER DATABASE \"limited_db\" CONNECTION LIMIT 50; baseline String, one-time. SQL baseline file path executed after database creation.\nBaseline files typically contain schema definitions, initial data, stored procedures. Path is relative to Ansible search path, usually in files/.\nBaseline runs only on first creation; skipped if database exists. state: recreate re-runs baseline.\n- name: myapp baseline: myapp_schema.sql # Looks for files/myapp_schema.sql schemas Array, mutable (add/remove). Schema definitions to create or drop. Elements can be strings or objects.\nSimple format - strings for schema names (create only):\nschemas: - app - api - core Full format - objects for owner and drop operations:\nschemas: - name: app # Schema name (required) owner: dbuser_app # Schema owner (optional), generates AUTHORIZATION clause - name: deprecated state: absent # Drop schema (CASCADE) Create uses IF NOT EXISTS; drop uses CASCADE (deletes all objects in schema).\nCREATE SCHEMA IF NOT EXISTS \"app\" AUTHORIZATION \"dbuser_app\"; DROP SCHEMA IF EXISTS \"deprecated\" CASCADE; extensions Array, mutable (add/remove). Extension definitions to install or uninstall. Elements can be strings or objects.\nSimple format - strings for extension names (install only):\nextensions: - postgis - pg_trgm - vector Full format - objects for schema, version, and uninstall:\nextensions: - name: vector # Extension name (required) schema: public # Install to schema (optional) version: '0.5.1' # Specific version (optional) - name: old_extension state: absent # Uninstall extension (CASCADE) Install uses CASCADE to auto-install dependencies; uninstall uses CASCADE (deletes dependent objects).\nCREATE EXTENSION IF NOT EXISTS \"vector\" WITH SCHEMA \"public\" VERSION '0.5.1' CASCADE; DROP EXTENSION IF EXISTS \"old_extension\" CASCADE; parameters Object, mutable. Database-level config params via ALTER DATABASE ... SET. Applies to all sessions connecting to this database.\n- name: analytics parameters: work_mem: '256MB' maintenance_work_mem: '512MB' statement_timeout: '5min' search_path: 'analytics,public' Use special value DEFAULT (case-insensitive) to reset to PostgreSQL default:\nparameters: work_mem: DEFAULT # Reset to default statement_timeout: '30s' # Set new value ALTER DATABASE \"myapp\" SET \"work_mem\" = DEFAULT; ALTER DATABASE \"myapp\" SET \"statement_timeout\" = '30s'; pgbouncer Boolean, mutable. Add database to Pgbouncer pool list, default true.\nSetting false excludes database from Pgbouncer - clients can’t access via connection pool. For internal management databases or direct-connect scenarios.\n- name: internal_db pgbouncer: false # No connection pool access pool_mode Enum, mutable. Pgbouncer pool mode: transaction, session, or statement. Default transaction.\nMode Description Use Case transaction Return connection after txn Most OLTP apps, default session Return connection after session Apps needing session state statement Return after each statement Simple stateless queries - name: session_app pool_mode: session # Session-level pooling pool_size Integer, mutable. Pgbouncer default pool size, default 64.\nPool size determines backend connections reserved for this database. Adjust based on workload.\n- name: high_load_db pool_size: 128 # Larger pool for high load pool_size_min Integer, mutable. Pgbouncer minimum pool size, default 0.\nValues \u003e 0 pre-create specified backend connections for connection warming, reducing first-request latency.\n- name: latency_sensitive pool_size_min: 10 # Pre-warm 10 connections pool_reserve Integer, mutable. Pgbouncer reserve pool size, default 32.\nWhen default pool exhausted, Pgbouncer can allocate up to pool_reserve additional connections for burst traffic.\n- name: bursty_db pool_size: 64 pool_reserve: 64 # Allow burst to 128 connections pool_connlimit Integer, mutable. Max connections via Pgbouncer pool, default 100.\nThis is Pgbouncer-level limit, independent of database’s connlimit param.\n- name: limited_pool_db pool_connlimit: 50 # Pool max 50 connections pool_auth_user String, mutable. User for Pgbouncer auth query.\nRequires pgbouncer_auth_query enabled. When set, all Pgbouncer connections to this database use specified user for auth query password verification.\n- name: myapp pool_auth_user: dbuser_monitor # Use monitor user for auth query register_datasource Boolean, mutable. Register database to Grafana as PostgreSQL datasource, default true.\nSet false to skip Grafana registration. For temp databases, test databases, or internal databases not needed in monitoring.\n- name: temp_db register_datasource: false # Don't register to Grafana Template Inheritance Many parameters inherit from template database if not explicitly specified. Default template is template1, whose encoding settings are determined by cluster init params:\nCluster Param Default Description pg_encoding UTF8 Cluster encoding pg_locale C / C-UTF-8 (if supported) Cluster locale pg_lc_collate C / C-UTF-8 (if supported) Cluster collation pg_lc_ctype C / C-UTF-8 (if supported) Cluster ctype New databases fork from template1, which is customized during PG_PROVISION with extensions, schemas, and default privileges. Unless you explicitly use another template.\nDeep Customization Pigsty provides rich customization params. To customize template database, refer to:\npg_default_roles: Default predefined roles and system users pg_default_privileges: Default privileges for objects created by admin user pg_default_schemas: Default schemas to create pg_default_extensions: Default extensions to create pg_default_hba_rules: Default PostgreSQL HBA rules pgb_default_hba_rules: Default Pgbouncer HBA rules If above configurations don’t meet your needs, use pg_init to specify custom cluster init scripts:\npg-init: Cluster init script pg-init-template.sql: Template customization SQL pg-init-roles.sql: Default roles SQL Locale Providers PostgreSQL 15+ introduced locale_provider for different locale implementations. These are immutable after creation.\nPigsty’s configure wizard selects builtin C.UTF-8/C locale provider based on PG and OS versions. Databases inherit cluster locale by default. To specify different locale provider, you must use template0.\nUsing ICU provider (PG15+):\n- name: myapp_icu template: template0 # ICU requires template0 locale_provider: icu icu_locale: en-US # ICU locale rules encoding: UTF8 Using builtin provider (PG17+):\n- name: myapp_builtin template: template0 locale_provider: builtin builtin_locale: C.UTF-8 # Builtin locale rules encoding: UTF8 Provider comparison: libc (traditional, OS-dependent), icu (PG15+, cross-platform, feature-rich), builtin (PG17+, most efficient C/C.UTF-8).\nConnection Pool Pgbouncer connection pool optimizes short-connection performance, reduces contention, prevents excessive connections from overwhelming database, and provides flexibility during migrations.\nPigsty configures 1:1 connection pool for each PostgreSQL instance, running as same pg_dbsu (default postgres OS user). Pool communicates with database via /var/run/postgresql Unix socket.\nPigsty adds all databases in pg_databases to pgbouncer by default. Set pgbouncer: false to exclude specific databases. Pgbouncer database list and config params are defined in /etc/pgbouncer/database.txt:\nmeta = host=/var/run/postgresql mode=session grafana = host=/var/run/postgresql mode=transaction bytebase = host=/var/run/postgresql auth_user=dbuser_meta kong = host=/var/run/postgresql pool_size=32 reserve_pool=64 gitea = host=/var/run/postgresql min_pool_size=10 wiki = host=/var/run/postgresql noco = host=/var/run/postgresql mongo = host=/var/run/postgresql When creating databases, Pgbouncer database list is refreshed via online reload - doesn’t affect existing connections.\n","categories":["Reference"],"description":"How to define and customize PostgreSQL databases through configuration?","excerpt":"How to define and customize PostgreSQL databases through …","ref":"/docs/pgsql/config/db/","tags":"","title":"Database"},{"body":"Overview HBA (Host-Based Authentication) controls “who can connect to the database from where and how”. Pigsty manages HBA rules declaratively through pg_default_hba_rules and pg_hba_rules.\nPigsty renders the following config files during cluster init or HBA refresh:\nConfig File Path Description PostgreSQL HBA /pg/data/pg_hba.conf PostgreSQL server HBA rules Pgbouncer HBA /etc/pgbouncer/pgb_hba.conf Connection pool HBA rules HBA rules are controlled by these parameters:\nParameter Level Description pg_default_hba_rules G PostgreSQL global default HBA pg_hba_rules G/C/I PostgreSQL cluster/instance add pgb_default_hba_rules G Pgbouncer global default HBA pgb_hba_rules G/C/I Pgbouncer cluster/instance add Rule features:\nRole filtering: Rules support role field, auto-filter based on instance’s pg_role Order sorting: Rules support order field, controls position in final config file Two syntaxes: Supports alias form (simplified) and raw form (direct HBA text) Refresh HBA After modifying config, re-render config files and reload services:\nbin/pgsql-hba \u003ccls\u003e # Refresh entire cluster HBA (recommended) bin/pgsql-hba \u003ccls\u003e \u003cip\u003e... # Refresh specific instances in cluster Script executes the following playbook:\n./pgsql.yml -l \u003ccls\u003e -t pg_hba,pg_reload,pgbouncer_hba,pgbouncer_reload -e pg_reload=true PostgreSQL only: ./pgsql.yml -l \u003ccls\u003e -t pg_hba,pg_reload -e pg_reload=true\nPgbouncer only: ./pgsql.yml -l \u003ccls\u003e -t pgbouncer_hba,pgbouncer_reload\nDon't edit config files directly Don’t directly edit /pg/data/pg_hba.conf or /etc/pgbouncer/pgb_hba.conf - they’ll be overwritten on next playbook run. All changes should be made in pigsty.yml, then execute bin/pgsql-hba to refresh.\nParameter Details pg_default_hba_rules\nPostgreSQL global default HBA rule list, usually defined in all.vars, provides base access control for all clusters.\nType: rule[], Level: Global (G) pg_default_hba_rules: - {user: '${dbsu}' ,db: all ,addr: local ,auth: ident ,title: 'dbsu access via local os user ident' ,order: 100} - {user: '${dbsu}' ,db: replication ,addr: local ,auth: ident ,title: 'dbsu replication from local os ident' ,order: 150} - {user: '${repl}' ,db: replication ,addr: localhost ,auth: pwd ,title: 'replicator replication from localhost',order: 200} - {user: '${repl}' ,db: replication ,addr: intra ,auth: pwd ,title: 'replicator replication from intranet' ,order: 250} - {user: '${repl}' ,db: postgres ,addr: intra ,auth: pwd ,title: 'replicator postgres db from intranet' ,order: 300} - {user: '${monitor}' ,db: all ,addr: localhost ,auth: pwd ,title: 'monitor from localhost with password' ,order: 350} - {user: '${monitor}' ,db: all ,addr: infra ,auth: pwd ,title: 'monitor from infra host with password',order: 400} - {user: '${admin}' ,db: all ,addr: infra ,auth: ssl ,title: 'admin @ infra nodes with pwd \u0026 ssl' ,order: 450} - {user: '${admin}' ,db: all ,addr: world ,auth: ssl ,title: 'admin @ everywhere with ssl \u0026 pwd' ,order: 500} - {user: '+dbrole_readonly',db: all ,addr: localhost ,auth: pwd ,title: 'pgbouncer read/write via local socket',order: 550} - {user: '+dbrole_readonly',db: all ,addr: intra ,auth: pwd ,title: 'read/write biz user via password' ,order: 600} - {user: '+dbrole_offline' ,db: all ,addr: intra ,auth: pwd ,title: 'allow etl offline tasks from intranet',order: 650} pg_hba_rules\nPostgreSQL cluster/instance-level additional HBA rules, can override at cluster or instance level, merged with default rules and sorted by order.\nType: rule[], Level: Global/Cluster/Instance (G/C/I), Default: [] pg_hba_rules: - {user: app_user, db: app_db, addr: intra, auth: pwd, title: 'app user access'} pgb_default_hba_rules\nPgbouncer global default HBA rule list, usually defined in all.vars.\nType: rule[], Level: Global (G) pgb_default_hba_rules: - {user: '${dbsu}' ,db: pgbouncer ,addr: local ,auth: peer ,title: 'dbsu local admin access with os ident',order: 100} - {user: 'all' ,db: all ,addr: localhost ,auth: pwd ,title: 'allow all user local access with pwd' ,order: 150} - {user: '${monitor}' ,db: pgbouncer ,addr: intra ,auth: pwd ,title: 'monitor access via intranet with pwd' ,order: 200} - {user: '${monitor}' ,db: all ,addr: world ,auth: deny ,title: 'reject all other monitor access addr' ,order: 250} - {user: '${admin}' ,db: all ,addr: intra ,auth: pwd ,title: 'admin access via intranet with pwd' ,order: 300} - {user: '${admin}' ,db: all ,addr: world ,auth: deny ,title: 'reject all other admin access addr' ,order: 350} - {user: 'all' ,db: all ,addr: intra ,auth: pwd ,title: 'allow all user intra access with pwd' ,order: 400} pgb_hba_rules\nPgbouncer cluster/instance-level additional HBA rules.\nType: rule[], Level: Global/Cluster/Instance (G/C/I), Default: [] Note: Pgbouncer HBA does not support db: replication.\nRule Fields Each HBA rule is a YAML dict supporting these fields:\nField Type Required Default Description user string No all Username, supports all, placeholders, +rolename db string No all Database name, supports all, replication, db name addr string Yes* - Address alias or CIDR, see Address Aliases auth string No pwd Auth method alias, see Auth Methods title string No - Rule description, rendered as comment in config role string No common Instance role filter, see Role Filtering order int No 1000 Sort weight, lower first, see Order Sorting rules list Yes* - Raw HBA text lines, mutually exclusive with addr Either addr or rules must be specified. Use rules to write raw HBA format directly.\nAddress Aliases Pigsty provides address aliases to simplify HBA rule writing:\nAlias Expands To Description local Unix socket Local Unix socket localhost Unix socket + 127.0.0.1/32 + ::1/128 Loopback addresses admin ${admin_ip}/32 Admin IP address infra All infra group node IPs Infrastructure nodes cluster All current cluster member IPs Same cluster instances intra / intranet 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16 Intranet CIDRs world / all 0.0.0.0/0 + ::/0 Any address (IPv4 + IPv6) \u003cCIDR\u003e Direct use e.g., 192.168.1.0/24 Intranet CIDRs can be customized via node_firewall_intranet:\nnode_firewall_intranet: - 10.0.0.0/8 - 172.16.0.0/12 - 192.168.0.0/16 Auth Methods Pigsty provides auth method aliases for simplified config:\nAlias Actual Method Connection Type Description pwd scram-sha-256 or md5 host Auto-select based on pg_pwd_enc ssl scram-sha-256 or md5 hostssl Force SSL + password ssl-sha scram-sha-256 hostssl Force SSL + SCRAM-SHA-256 ssl-md5 md5 hostssl Force SSL + MD5 cert cert hostssl Client certificate auth trust trust host Unconditional trust (dangerous) deny / reject reject host Reject connection ident ident host OS user mapping (PostgreSQL) peer peer local OS user mapping (Pgbouncer/local) pg_pwd_enc defaults to scram-sha-256, can be set to md5 for legacy client compatibility.\nUser Variables HBA rules support these user placeholders, auto-replaced with actual usernames during rendering:\nPlaceholder Default Corresponding Param ${dbsu} postgres pg_dbsu ${repl} replicator pg_replication_username ${monitor} dbuser_monitor pg_monitor_username ${admin} dbuser_dba pg_admin_username Role Filtering The role field in HBA rules controls which instances the rule applies to:\nRole Description common Default, applies to all instances primary Primary instance only replica Replica instance only offline Offline instance only (pg_role: offline or pg_offline_query: true) standby Standby instance delayed Delayed replica instance Role filtering matches based on instance’s pg_role variable. Non-matching rules are commented out (prefixed with #).\npg_hba_rules: # Only applies on primary: writer can only connect to primary - {user: writer, db: all, addr: intra, auth: pwd, role: primary, title: 'writer only on primary'} # Only applies on offline instances: ETL dedicated network - {user: '+dbrole_offline', db: all, addr: '172.20.0.0/16', auth: ssl, role: offline, title: 'offline dedicated'} Order Sorting PostgreSQL HBA is first-match-wins, rule order is critical. Pigsty controls rule rendering order via the order field.\nOrder Interval Convention\nInterval Usage 0 - 99 User high-priority rules (before all defaults) 100 - 650 Default rule zone (spaced by 50 for insertion) 1000+ User rule default (rules without order) PostgreSQL Default Rules Order\nOrder Rule Description 100 dbsu local ident 150 dbsu replication local 200 replicator localhost 250 replicator intra replication 300 replicator intra postgres 350 monitor localhost 400 monitor infra 450 admin infra ssl 500 admin world ssl 550 dbrole_readonly localhost 600 dbrole_readonly intra 650 dbrole_offline intra Pgbouncer Default Rules Order\nOrder Rule Description 100 dbsu local peer 150 all localhost pwd 200 monitor pgbouncer intra 250 monitor world deny 300 admin intra pwd 350 admin world deny 400 all intra pwd Syntax Examples Alias Form: Using Pigsty simplified syntax\npg_hba_rules: - title: allow grafana view access role: primary user: dbuser_view db: meta addr: infra auth: ssl Rendered result:\n# allow grafana view access [primary] hostssl meta dbuser_view 10.10.10.10/32 scram-sha-256 Raw Form: Using PostgreSQL HBA syntax directly\npg_hba_rules: - title: allow intranet password access role: common rules: - host all all 10.0.0.0/8 scram-sha-256 - host all all 172.16.0.0/12 scram-sha-256 - host all all 192.168.0.0/16 scram-sha-256 Rendered result:\n# allow intranet password access [common] host all all 10.0.0.0/8 scram-sha-256 host all all 172.16.0.0/12 scram-sha-256 host all all 192.168.0.0/16 scram-sha-256 Common Scenarios Blacklist IP: Use order: 0 to ensure first match\npg_hba_rules: - {user: all, db: all, addr: '10.1.1.100/32', auth: deny, order: 0, title: 'block bad ip'} Whitelist App Server: High priority for specific IP\npg_hba_rules: - {user: app_user, db: app_db, addr: '192.168.1.10/32', auth: ssl, order: 50, title: 'app server'} Admin Force Certificate: Override default SSL password auth\npg_hba_rules: - {user: '${admin}', db: all, addr: world, auth: cert, order: 10, title: 'admin cert only'} Offline Instance Dedicated Network: Only on offline instances\npg_hba_rules: - {user: '+dbrole_offline', db: all, addr: '172.20.0.0/16', auth: ssl-sha, role: offline, title: 'etl network'} Restrict Access by Database: Sensitive databases limited to specific networks\npg_hba_rules: - {user: fin_user, db: finance_db, addr: '10.20.0.0/16', auth: ssl, title: 'finance only'} - {user: hr_user, db: hr_db, addr: '10.30.0.0/16', auth: ssl, title: 'hr only'} Pgbouncer Dedicated Rules: Note no db: replication support\npgb_hba_rules: - {user: '+dbrole_readwrite', db: all, addr: world, auth: ssl, title: 'app via pgbouncer'} Complete Cluster Example pg-prod: hosts: 10.10.10.11: {pg_seq: 1, pg_role: primary} 10.10.10.12: {pg_seq: 2, pg_role: replica} 10.10.10.13: {pg_seq: 3, pg_role: offline} vars: pg_cluster: pg-prod pg_hba_rules: # Blacklist: known malicious IP (highest priority) - {user: all, db: all, addr: '10.1.1.100/32', auth: deny, order: 0, title: 'blacklist'} # App server whitelist (high priority) - {user: app_user, db: app_db, addr: '192.168.1.0/24', auth: ssl, order: 50, title: 'app servers'} # ETL tasks: offline instances only - {user: etl_user, db: all, addr: '172.20.0.0/16', auth: pwd, role: offline, title: 'etl tasks'} # Cluster internal monitoring - {user: '${monitor}', db: all, addr: cluster, auth: pwd, order: 380, title: 'cluster monitor'} pgb_hba_rules: # App via connection pool - {user: '+dbrole_readwrite', db: all, addr: '192.168.1.0/24', auth: ssl, title: 'app via pgbouncer'} Verification \u0026 Troubleshooting View Current HBA Rules\npsql -c \"TABLE pg_hba_file_rules\" # View via SQL (recommended) cat /pg/data/pg_hba.conf # View PostgreSQL HBA file cat /etc/pgbouncer/pgb_hba.conf # View Pgbouncer HBA file grep '^#' /pg/data/pg_hba.conf | head -20 # View rule titles (verify order) Test Connection Auth\npsql -h \u003chost\u003e -p 5432 -U \u003cuser\u003e -d \u003cdb\u003e -c \"SELECT 1\" Common Issues\nError Message Possible Cause Solution no pg_hba.conf entry for host... No matching HBA rule Add corresponding rule and refresh password authentication failed Wrong password or enc Check password and pg_pwd_enc Rule not taking effect Not refreshed or order Run bin/pgsql-hba, check order Important Notes Order sensitive: PostgreSQL HBA is first-match-wins, use order wisely Role matching: Ensure role field matches target instance’s pg_role Address format: CIDR must be correct, e.g., 10.0.0.0/8 not 10.0.0.0/255.0.0.0 Pgbouncer limitation: Does not support db: replication SSL prerequisite: Ensure SSL is configured before using ssl, cert auth Test first: Validate in test environment before modifying HBA Refresh on scale: Rules using addr: cluster need refresh after cluster membership changes Related Documentation HBA Management: Daily HBA rule management operations User Config: User and role configuration Access Control: Role system and permission model Security \u0026 Compliance: PostgreSQL cluster security features ","categories":["Reference"],"description":"Detailed explanation of PostgreSQL and Pgbouncer Host-Based Authentication (HBA) rules configuration in Pigsty.","excerpt":"Detailed explanation of PostgreSQL and Pgbouncer Host-Based …","ref":"/docs/pgsql/config/hba/","tags":"","title":"HBA Rules"},{"body":" Access control is determined by the combination of “role system + privilege templates + HBA”. This section focuses on how to declare roles and object privileges through configuration parameters.\nPigsty provides a streamlined ACL model, fully described by the following parameters:\npg_default_roles: System roles and system users. pg_users: Business users and roles. pg_default_privileges: Default privileges for objects created by administrators/owners. pg_revoke_public, pg_default_schemas, pg_default_extensions: Control the default behavior of template1. After understanding these parameters, you can write fully reproducible privilege configurations.\nDefault Role System (pg_default_roles) By default, it includes 4 business roles + 4 system users:\nName Type Description dbrole_readonly NOLOGIN Shared by all business, has SELECT/USAGE dbrole_readwrite NOLOGIN Inherits read-only role, with INSERT/UPDATE/DELETE dbrole_admin NOLOGIN Inherits pg_monitor + read-write role, can create objects and triggers dbrole_offline NOLOGIN Restricted read-only role, only allowed to access offline instances postgres User System superuser, same as pg_dbsu replicator User Used for streaming replication and backup, inherits monitoring and read-only privileges dbuser_dba User Primary admin account, also synced to pgbouncer dbuser_monitor User Monitoring account, has pg_monitor privilege, records slow SQL by default These definitions are in pg_default_roles. They can theoretically be customized, but if you replace names, you must synchronize updates in HBA/ACL/script references.\nExample: Add an additional dbrole_etl for offline tasks:\npg_default_roles: - { name: dbrole_etl, login: false, roles: [dbrole_offline], comment: 'etl read-only role' } - { name: dbrole_admin, login: false, roles: [pg_monitor, dbrole_readwrite, dbrole_etl] } Effect: All users inheriting dbrole_admin automatically have dbrole_etl privileges, can access offline instances and execute ETL.\nDefault Users and Credential Parameters System user usernames/passwords are controlled by the following parameters:\nParameter Default Value Purpose pg_dbsu postgres Database/system superuser pg_dbsu_password Empty string dbsu password (disabled by default) pg_replication_username replicator Replication username pg_replication_password DBUser.Replicator Replication user password pg_admin_username dbuser_dba Admin username pg_admin_password DBUser.DBA Admin password pg_monitor_username dbuser_monitor Monitoring user pg_monitor_password DBUser.Monitor Monitoring user password If you modify these parameters, please synchronize updates to the corresponding user definitions in pg_default_roles to avoid role attribute inconsistencies.\nBusiness Roles and Authorization (pg_users) Business users are declared through pg_users (see User Configuration for detailed fields), where the roles field controls the granted business roles.\nExample: Create one read-only and one read-write user:\npg_users: - { name: app_reader, password: DBUser.Reader, roles: [dbrole_readonly], pgbouncer: true } - { name: app_writer, password: DBUser.Writer, roles: [dbrole_readwrite], pgbouncer: true } By inheriting dbrole_* to control access privileges, no need to GRANT for each database separately. Combined with pg_hba_rules, you can distinguish access sources.\nFor finer-grained ACL, you can use standard GRANT/REVOKE in baseline SQL or subsequent playbooks. Pigsty won’t prevent you from granting additional privileges.\nDefault Privilege Templates (pg_default_privileges) pg_default_privileges will set DEFAULT PRIVILEGE on postgres, dbuser_dba, dbrole_admin (after business admin SET ROLE). The default template is as follows:\npg_default_privileges: - GRANT USAGE ON SCHEMAS TO dbrole_readonly - GRANT SELECT ON TABLES TO dbrole_readonly - GRANT SELECT ON SEQUENCES TO dbrole_readonly - GRANT EXECUTE ON FUNCTIONS TO dbrole_readonly - GRANT USAGE ON SCHEMAS TO dbrole_offline - GRANT SELECT ON TABLES TO dbrole_offline - GRANT SELECT ON SEQUENCES TO dbrole_offline - GRANT EXECUTE ON FUNCTIONS TO dbrole_offline - GRANT INSERT ON TABLES TO dbrole_readwrite - GRANT UPDATE ON TABLES TO dbrole_readwrite - GRANT DELETE ON TABLES TO dbrole_readwrite - GRANT USAGE ON SEQUENCES TO dbrole_readwrite - GRANT UPDATE ON SEQUENCES TO dbrole_readwrite - GRANT TRUNCATE ON TABLES TO dbrole_admin - GRANT REFERENCES ON TABLES TO dbrole_admin - GRANT TRIGGER ON TABLES TO dbrole_admin - GRANT CREATE ON SCHEMAS TO dbrole_admin As long as objects are created by the above administrators, they will automatically carry the corresponding privileges without manual GRANT. If business needs a custom template, simply replace this array.\nAdditional notes:\npg_revoke_public defaults to true, meaning automatic revocation of PUBLIC’s CREATE privilege on databases and the public schema. pg_default_schemas and pg_default_extensions control pre-created schemas/extensions in template1/postgres, typically used for monitoring objects (monitor schema, pg_stat_statements, etc.). Common Configuration Scenarios Provide Read-Only Account for Partners pg_users: - name: partner_ro password: Partner.Read roles: [dbrole_readonly] pg_hba_rules: - { user: partner_ro, db: analytics, addr: 203.0.113.0/24, auth: ssl } Effect: Partner account only has default read-only privileges after login, and can only access the analytics database via TLS from the specified network segment.\nGrant DDL Capability to Business Administrators pg_users: - name: app_admin password: DBUser.AppAdmin roles: [dbrole_admin] Business administrators can inherit the default DDL privilege template by SET ROLE dbrole_admin or logging in directly as app_admin.\nCustomize Default Privileges pg_default_privileges: - GRANT INSERT,UPDATE,DELETE ON TABLES TO dbrole_admin - GRANT SELECT,UPDATE ON SEQUENCES TO dbrole_admin - GRANT SELECT ON TABLES TO reporting_group After replacing the default template, all objects created by administrators will carry the new privilege definitions, avoiding per-object authorization.\nCoordination with Other Components HBA Rules: Use pg_hba_rules to bind roles with sources (e.g., only allow dbrole_offline to access offline instances). Pgbouncer: Users with pgbouncer: true will be written to userlist.txt, and pool_mode/pool_connlimit can control connection pool-level quotas. Grafana/Monitoring: dbuser_monitor’s privileges come from pg_default_roles. If you add a new monitoring user, remember to grant pg_monitor + access to the monitor schema. Through these parameters, you can version the privilege system along with code, truly achieving “configuration as policy”.\n","categories":["Reference"],"description":"Default role system and privilege model provided by Pigsty","excerpt":"Default role system and privilege model provided by Pigsty","ref":"/docs/pgsql/config/acl/","tags":["Security"],"title":"Access Control"},{"body":" Split read and write operations, route traffic correctly, and reliably deliver PostgreSQL cluster capabilities.\nService is an abstraction: it is the form in which database clusters provide capabilities externally, encapsulating the details of the underlying cluster.\nService is critical for stable access in production environments, showing its value during high availability cluster automatic failovers. Personal users typically don’t need to worry about this concept.\nPersonal User The concept of “service” is for production environments. Personal users/single-machine clusters can skip the complexity and directly access the database using instance names/IP addresses.\nFor example, Pigsty’s default single-node pg-meta.meta database can be directly connected using three different users:\npsql postgres://dbuser_dba:DBUser.DBA@10.10.10.10/meta # Direct connection with DBA superuser psql postgres://dbuser_meta:DBUser.Meta@10.10.10.10/meta # Connect with default business admin user psql postgres://dbuser_view:DBUser.View@pg-meta/meta # Connect with default read-only user via instance domain name Service Overview In real-world production environments, we use primary-replica database clusters based on replication. Within the cluster, there is one and only one instance as the leader (primary) that can accept writes. Other instances (replicas) continuously fetch change logs from the cluster leader to stay synchronized. Additionally, replicas can handle read-only requests, significantly offloading the primary in read-heavy, write-light scenarios. Therefore, distinguishing between write requests and read-only requests to the cluster is a very common practice.\nMoreover, for production environments with high-frequency short connections, we pool requests through connection pooling middleware (Pgbouncer) to reduce the overhead of connection and backend process creation. But for scenarios like ETL and change execution, we need to bypass the connection pool and directly access the database. At the same time, high-availability clusters may experience failover during failures, which causes a change in the cluster leader. Therefore, high-availability database solutions require write traffic to automatically adapt to cluster leader changes. These different access requirements (read-write separation, pooling vs. direct connection, automatic adaptation to failovers) ultimately abstract the concept of Service.\nTypically, database clusters must provide this most basic service:\nRead-write service (primary): Can read and write to the database For production database clusters, at least these two services should be provided:\nRead-write service (primary): Write data: Only carried by the primary. Read-only service (replica): Read data: Can be carried by replicas, but can also be carried by the primary if no replicas are available Additionally, depending on specific business scenarios, there might be other services, such as:\nDefault direct access service (default): Service that allows (admin) users to bypass the connection pool and directly access the database Offline replica service (offline): Dedicated replica that doesn’t handle online read-only traffic, used for ETL and analytical queries Synchronous replica service (standby): Read-only service with no replication delay, handled by synchronous standby/primary for read-only queries Delayed replica service (delayed): Access older data from the same cluster from a certain time ago, handled by delayed replicas Default Service Pigsty provides four different services by default for each PostgreSQL database cluster. Here are the default services and their definitions:\nService Port Description primary 5433 Production read-write, connect to primary pool (6432) replica 5434 Production read-only, connect to replica pool (6432) default 5436 Admin, ETL writes, direct access to primary (5432) offline 5438 OLAP, ETL, personal users, interactive queries Taking the default pg-meta cluster as an example, it provides four default services:\npsql postgres://dbuser_meta:DBUser.Meta@pg-meta:5433/meta # pg-meta-primary : production read-write via primary pgbouncer(6432) psql postgres://dbuser_meta:DBUser.Meta@pg-meta:5434/meta # pg-meta-replica : production read-only via replica pgbouncer(6432) psql postgres://dbuser_dba:DBUser.DBA@pg-meta:5436/meta # pg-meta-default : direct connection via primary postgres(5432) psql postgres://dbuser_stats:DBUser.Stats@pg-meta:5438/meta # pg-meta-offline : direct connection via offline postgres(5432) From the sample cluster architecture diagram, you can see how these four services work:\nNote that the pg-meta domain name points to the cluster’s L2 VIP, which in turn points to the haproxy load balancer on the cluster primary, responsible for routing traffic to different instances. See Access Service for details.\nService Implementation In Pigsty, services are implemented using haproxy on nodes, differentiated by different ports on the host node.\nHaproxy is enabled by default on every node managed by Pigsty to expose services, and database nodes are no exception. Although nodes in the cluster have primary-replica distinctions from the database perspective, from the service perspective, all nodes are the same: This means even if you access a replica node, as long as you use the correct service port, you can still use the primary’s read-write service. This design seals the complexity: as long as you can access any instance on the PostgreSQL cluster, you can fully access all services.\nThis design is similar to the NodePort service in Kubernetes. Similarly, in Pigsty, every service includes these two core elements:\nAccess endpoints exposed via NodePort (port number, from where to access?) Target instances chosen through Selectors (list of instances, who will handle it?) The boundary of Pigsty’s service delivery stops at the cluster’s HAProxy. Users can access these load balancers in various ways. Please refer to Access Service.\nAll services are declared through configuration files. For instance, the default PostgreSQL service is defined by the pg_default_services parameter:\npg_default_services: - { name: primary ,port: 5433 ,dest: default ,check: /primary ,selector: \"[]\" } - { name: replica ,port: 5434 ,dest: default ,check: /read-only ,selector: \"[]\" , backup: \"[? pg_role == `primary` || pg_role == `offline` ]\" } - { name: default ,port: 5436 ,dest: postgres ,check: /primary ,selector: \"[]\" } - { name: offline ,port: 5438 ,dest: postgres ,check: /replica ,selector: \"[? pg_role == `offline` || pg_offline_query ]\" , backup: \"[? pg_role == `replica` \u0026\u0026 !pg_offline_query]\"} You can also define additional services in pg_services. Both pg_default_services and pg_services are arrays of Service Definition objects.\nDefine Service Pigsty allows you to define your own services:\npg_default_services: Services uniformly exposed by all PostgreSQL clusters, with four by default. pg_services: Additional PostgreSQL services, can be defined at global or cluster level as needed. haproxy_services: Directly customize HAProxy service content, can be used for other component access For PostgreSQL clusters, you typically only need to focus on the first two. Each service definition will generate a new configuration file in the configuration directory of all related HAProxy instances: /etc/haproxy/\u003csvcname\u003e.cfg Here’s a custom service example standby: When you want to provide a read-only service with no replication delay, you can add this record in pg_services:\n- name: standby # required, service name, the actual svc name will be prefixed with `pg_cluster`, e.g: pg-meta-standby port: 5435 # required, service exposed port (work as kubernetes service node port mode) ip: \"*\" # optional, service bind ip address, `*` for all ip by default selector: \"[]\" # required, service member selector, use JMESPath to filter inventory backup: \"[? pg_role == `primary`]\" # optional, backup server selector, these instances will only be used when default selector instances are all down dest: default # optional, destination port, default|postgres|pgbouncer|\u003cport_number\u003e, 'default' by default, which means use pg_default_service_dest value check: /sync # optional, health check url path, / by default, here using Patroni API: /sync, only sync standby and primary will return 200 healthy status maxconn: 5000 # optional, max allowed front-end connection, default 5000 balance: roundrobin # optional, haproxy load balance algorithm (roundrobin by default, other options: leastconn) options: 'inter 3s fastinter 1s downinter 5s rise 3 fall 3 on-marked-down shutdown-sessions slowstart 30s maxconn 3000 maxqueue 128 weight 100' The service definition above will be translated to a haproxy config file /etc/haproxy/pg-test-standby.conf on the sample three-node pg-test:\n#--------------------------------------------------------------------- # service: pg-test-standby @ 10.10.10.11:5435 #--------------------------------------------------------------------- # service instances 10.10.10.11, 10.10.10.13, 10.10.10.12 # service backups 10.10.10.11 listen pg-test-standby bind *:5435 # \u003c--- Binds to port 5435 on all IP addresses mode tcp # \u003c--- Load balancer works on TCP protocol maxconn 5000 # \u003c--- Max connections 5000, can be increased as needed balance roundrobin # \u003c--- Load balance algorithm is rr round-robin, can also use leastconn option httpchk # \u003c--- Enable HTTP health check option http-keep-alive # \u003c--- Keep HTTP connections http-check send meth OPTIONS uri /sync # \u003c---- Using /sync here, Patroni health check API, only sync standby and primary will return 200 healthy status http-check expect status 200 # \u003c---- Health check return code 200 means healthy default-server inter 3s fastinter 1s downinter 5s rise 3 fall 3 on-marked-down shutdown-sessions slowstart 30s maxconn 3000 maxqueue 128 weight 100 # servers: All three instances of pg-test cluster are selected by selector: \"[]\", as there are no filtering conditions, they will all be backend servers for pg-test-replica service. But due to /sync health check, only primary and sync standby can actually serve requests. server pg-test-1 10.10.10.11:6432 check port 8008 weight 100 backup # \u003c----- Only primary satisfies condition pg_role == `primary`, selected by backup selector. server pg-test-3 10.10.10.13:6432 check port 8008 weight 100 # Therefore acts as fallback instance: normally doesn't serve requests, only serves read-only requests after all other replicas are down, maximizing avoidance of read-write service being affected by read-only service server pg-test-2 10.10.10.12:6432 check port 8008 weight 100 # Here, all three instances of the pg-test cluster are selected by selector: \"[]\" and rendered into the backend server list of the pg-test-replica service. But due to the /sync health check, the Patroni Rest API only returns HTTP 200 status code representing healthy on the primary and synchronous standby, so only the primary and sync standby can actually serve requests. Additionally, the primary satisfies the condition pg_role == primary and is selected by the backup selector, marked as a backup server, and will only be used when no other instances (i.e., sync standby) can satisfy the requirement.\nPrimary Service The Primary service is probably the most critical service in production environments. It provides read-write capability to the database cluster on port 5433, with the service definition as follows:\n- { name: primary ,port: 5433 ,dest: default ,check: /primary ,selector: \"[]\" } The selector parameter selector: \"[]\" means all cluster members will be included in the Primary service But only the primary can pass the health check (check: /primary), actually serving Primary service traffic. The destination parameter dest: default means the Primary service destination is affected by the pg_default_service_dest parameter The default value of dest is default which will be replaced with the value of pg_default_service_dest, defaulting to pgbouncer. By default, the Primary service destination is the connection pool on the primary, i.e., the port specified by pgbouncer_port, defaulting to 6432 If the value of pg_default_service_dest is postgres, then the primary service destination will bypass the connection pool and directly use the PostgreSQL database port (pg_port, default value 5432), which is very useful for scenarios where you don’t want to use a connection pool.\nExample: pg-test-primary haproxy configuration listen pg-test-primary bind *:5433 # \u003c--- primary service defaults to port 5433 mode tcp maxconn 5000 balance roundrobin option httpchk option http-keep-alive http-check send meth OPTIONS uri /primary # \u003c--- primary service defaults to using Patroni RestAPI /primary health check http-check expect status 200 default-server inter 3s fastinter 1s downinter 5s rise 3 fall 3 on-marked-down shutdown-sessions slowstart 30s maxconn 3000 maxqueue 128 weight 100 # servers server pg-test-1 10.10.10.11:6432 check port 8008 weight 100 server pg-test-3 10.10.10.13:6432 check port 8008 weight 100 server pg-test-2 10.10.10.12:6432 check port 8008 weight 100 Patroni’s high availability mechanism ensures that at most one instance’s /primary health check is true at any time, so the Primary service will always route traffic to the primary instance.\nOne benefit of using the Primary service instead of directly connecting to the database is that if the cluster experiences a split-brain situation (for example, killing the primary Patroni with kill -9 without watchdog), Haproxy can still avoid split-brain in this situation, because it only distributes traffic when Patroni is alive and returns primary status.\nReplica Service The Replica service is second only to the Primary service in importance in production environments. It provides read-only capability to the database cluster on port 5434, with the service definition as follows:\n- { name: replica ,port: 5434 ,dest: default ,check: /read-only ,selector: \"[]\" , backup: \"[? pg_role == `primary` || pg_role == `offline` ]\" } The selector parameter selector: \"[]\" means all cluster members will be included in the Replica service All instances can pass the health check (check: /read-only), serving Replica service traffic. Backup selector: [? pg_role == 'primary' || pg_role == 'offline' ] marks the primary and offline replicas as backup servers. Only when all regular replicas are down will the Replica service be served by the primary or offline replicas. The destination parameter dest: default means the Replica service destination is also affected by the pg_default_service_dest parameter The default value of dest is default which will be replaced with the value of pg_default_service_dest, defaulting to pgbouncer, same as the Primary service By default, the Replica service destination is the connection pool on replicas, i.e., the port specified by pgbouncer_port, defaulting to 6432 Example: pg-test-replica haproxy configuration listen pg-test-replica bind *:5434 mode tcp maxconn 5000 balance roundrobin option httpchk option http-keep-alive http-check send meth OPTIONS uri /read-only http-check expect status 200 default-server inter 3s fastinter 1s downinter 5s rise 3 fall 3 on-marked-down shutdown-sessions slowstart 30s maxconn 3000 maxqueue 128 weight 100 # servers server pg-test-1 10.10.10.11:6432 check port 8008 weight 100 backup server pg-test-3 10.10.10.13:6432 check port 8008 weight 100 server pg-test-2 10.10.10.12:6432 check port 8008 weight 100 The Replica service is very flexible: If there are living dedicated Replica instances, it will prioritize using these instances to serve read-only requests. Only when all replica instances are down will the primary serve as a fallback for read-only requests. For the common one-primary-one-replica two-node cluster: use the replica as long as it’s alive, use the primary only when the replica is down.\nAdditionally, unless all dedicated read-only instances are down, the Replica service will not use dedicated Offline instances, thus avoiding mixing online fast queries with offline slow queries and their mutual interference.\nDefault Service The Default service provides service on port 5436, and it’s a variant of the Primary service.\nThe Default service always bypasses the connection pool and directly connects to PostgreSQL on the primary, which is useful for admin connections, ETL writes, CDC change data capture, etc.\n- { name: primary ,port: 5433 ,dest: default ,check: /primary ,selector: \"[]\" } If pg_default_service_dest is changed to postgres, then the Default service is completely equivalent to the Primary service except for port and name. In this case, you can consider removing Default from default services.\nExample: pg-test-default haproxy configuration listen pg-test-default bind *:5436 # \u003c--- Except for listening port/target port and service name, other configurations are the same as primary service mode tcp maxconn 5000 balance roundrobin option httpchk option http-keep-alive http-check send meth OPTIONS uri /primary http-check expect status 200 default-server inter 3s fastinter 1s downinter 5s rise 3 fall 3 on-marked-down shutdown-sessions slowstart 30s maxconn 3000 maxqueue 128 weight 100 # servers server pg-test-1 10.10.10.11:5432 check port 8008 weight 100 server pg-test-3 10.10.10.13:5432 check port 8008 weight 100 server pg-test-2 10.10.10.12:5432 check port 8008 weight 100 Offline Service The Offline service provides service on port 5438, and it also bypasses the connection pool to directly access PostgreSQL database, typically used for slow queries/analytical queries/ETL reads/personal user interactive queries, with service definition as follows:\n- { name: offline ,port: 5438 ,dest: postgres ,check: /replica ,selector: \"[? pg_role == `offline` || pg_offline_query ]\" , backup: \"[? pg_role == `replica` \u0026\u0026 !pg_offline_query]\"} The Offline service routes traffic directly to dedicated offline replicas, or regular read-only instances marked with pg_offline_query.\nThe selector parameter filters two types of instances from the cluster: offline replicas with pg_role = offline, or regular read-only instances marked with pg_offline_query = true The main difference between dedicated offline replicas and marked regular replicas is: the former doesn’t serve Replica service requests by default, avoiding mixing fast and slow queries, while the latter does serve by default. The backup selector parameter filters one type of instance from the cluster: regular replicas without the offline mark, which means if offline instances or marked regular replicas are down, other regular replicas can be used to serve Offline service. Health check /replica only returns 200 for replicas, primary returns error, so Offline service will never distribute traffic to the primary instance, even if only the primary remains in the cluster. At the same time, the primary instance is neither selected by the selector nor by the backup selector, so it will never serve Offline service. Therefore, Offline service can always avoid users accessing the primary, thus avoiding impact on the primary. Example: pg-test-offline haproxy configuration listen pg-test-offline bind *:5438 mode tcp maxconn 5000 balance roundrobin option httpchk option http-keep-alive http-check send meth OPTIONS uri /replica http-check expect status 200 default-server inter 3s fastinter 1s downinter 5s rise 3 fall 3 on-marked-down shutdown-sessions slowstart 30s maxconn 3000 maxqueue 128 weight 100 # servers server pg-test-3 10.10.10.13:5432 check port 8008 weight 100 server pg-test-2 10.10.10.12:5432 check port 8008 weight 100 backup The Offline service provides restricted read-only service, typically used for two types of queries: interactive queries (personal users), slow queries and long transactions (analytics/ETL).\nThe Offline service requires extra maintenance care: When the cluster undergoes primary-replica switchover or automatic failover, the instance roles will change, but Haproxy configuration won’t automatically change. For clusters with multiple replicas, this is usually not a problem. However, for streamlined small clusters with one-primary-one-replica where the replica runs Offline queries, primary-replica switchover means the replica becomes primary (health check fails), and the original primary becomes replica (not in Offline backend list), so no instance can serve Offline service, requiring manual reload service to make changes effective.\nIf your business model is relatively simple, you can consider removing Default service and Offline service, using Primary service and Replica service to directly connect to the database.\nReload Service When cluster membership changes, such as adding/removing replicas, switchover/failover, or adjusting relative weights, you need to reload service to make the changes take effect.\nbin/pgsql-svc \u003ccls\u003e [ip...] # reload service for lb cluster or lb instance # ./pgsql.yml -t pg_service # the actual ansible task to reload service Access Service The boundary of Pigsty’s service delivery stops at the cluster’s HAProxy. Users can access these load balancers in various ways.\nThe typical approach is to use DNS or VIP access, binding to all or any number of load balancers in the cluster.\nYou can use different host \u0026 port combinations, which provide PostgreSQL services in different ways.\nHost\nType Example Description Cluster Domain Name pg-test Access via cluster domain name (resolved by dnsmasq @ infra nodes) Cluster VIP Address 10.10.10.3 Access via L2 VIP address managed by vip-manager, bound to primary Instance Hostname pg-test-1 Access via any instance hostname (resolved by dnsmasq @ infra nodes) Instance IP Address 10.10.10.11 Access any instance IP address Port\nPigsty uses different ports to distinguish pg services\nPort Service Type Description 5432 postgres database Direct access to postgres server 6432 pgbouncer middleware Go through connection pool middleware before postgres 5433 primary service Access primary pgbouncer (or postgres) 5434 replica service Access replica pgbouncer (or postgres) 5436 default service Access primary postgres 5438 offline service Access offline postgres Combinations\n# Access via cluster domain postgres://test@pg-test:5432/test # DNS -\u003e L2 VIP -\u003e primary direct connection postgres://test@pg-test:6432/test # DNS -\u003e L2 VIP -\u003e primary connection pool -\u003e primary postgres://test@pg-test:5433/test # DNS -\u003e L2 VIP -\u003e HAProxy -\u003e Primary Connection Pool -\u003e Primary postgres://test@pg-test:5434/test # DNS -\u003e L2 VIP -\u003e HAProxy -\u003e Replica Connection Pool -\u003e Replica postgres://dbuser_dba@pg-test:5436/test # DNS -\u003e L2 VIP -\u003e HAProxy -\u003e Primary direct connection (for Admin) postgres://dbuser_stats@pg-test:5438/test # DNS -\u003e L2 VIP -\u003e HAProxy -\u003e offline direct connection (for ETL/personal queries) # Direct access via cluster VIP postgres://test@10.10.10.3:5432/test # L2 VIP -\u003e Primary direct access postgres://test@10.10.10.3:6432/test # L2 VIP -\u003e Primary Connection Pool -\u003e Primary postgres://test@10.10.10.3:5433/test # L2 VIP -\u003e HAProxy -\u003e Primary Connection Pool -\u003e Primary postgres://test@10.10.10.3:5434/test # L2 VIP -\u003e HAProxy -\u003e Replica Connection Pool -\u003e Replica postgres://dbuser_dba@10.10.10.3:5436/test # L2 VIP -\u003e HAProxy -\u003e Primary direct connection (for Admin) postgres://dbuser_stats@10.10.10.3::5438/test # L2 VIP -\u003e HAProxy -\u003e offline direct connect (for ETL/personal queries) # Specify any cluster instance name directly postgres://test@pg-test-1:5432/test # DNS -\u003e Database Instance Direct Connect (singleton access) postgres://test@pg-test-1:6432/test # DNS -\u003e connection pool -\u003e database postgres://test@pg-test-1:5433/test # DNS -\u003e HAProxy -\u003e connection pool -\u003e database read/write postgres://test@pg-test-1:5434/test # DNS -\u003e HAProxy -\u003e connection pool -\u003e database read-only postgres://dbuser_dba@pg-test-1:5436/test # DNS -\u003e HAProxy -\u003e database direct connect postgres://dbuser_stats@pg-test-1:5438/test # DNS -\u003e HAProxy -\u003e database offline read/write # Directly specify any cluster instance IP access postgres://test@10.10.10.11:5432/test # Database instance direct connection (directly specify instance, no automatic traffic distribution) postgres://test@10.10.10.11:6432/test # Connection Pool -\u003e Database postgres://test@10.10.10.11:5433/test # HAProxy -\u003e connection pool -\u003e database read/write postgres://test@10.10.10.11:5434/test # HAProxy -\u003e connection pool -\u003e database read-only postgres://dbuser_dba@10.10.10.11:5436/test # HAProxy -\u003e Database Direct Connections postgres://dbuser_stats@10.10.10.11:5438/test # HAProxy -\u003e database offline read-write # Smart client automatic read/write separation postgres://test@10.10.10.11:6432,10.10.10.12:6432,10.10.10.13:6432/test?target_session_attrs=primary postgres://test@10.10.10.11:6432,10.10.10.12:6432,10.10.10.13:6432/test?target_session_attrs=prefer-standby Override Service You can override the default service configuration in several ways. A common requirement is to have Primary service and Replica service bypass Pgbouncer connection pool and directly access PostgreSQL database.\nTo achieve this, you can change pg_default_service_dest to postgres, so all services with svc.dest='default' in the service definition will use postgres instead of the default pgbouncer as the target.\nIf you’ve already pointed Primary service to PostgreSQL, then the default service becomes redundant and can be removed.\nIf you don’t need to distinguish between personal interactive queries and analytics/ETL slow queries, you can consider removing the Offline service from the default service list pg_default_services.\nIf you don’t need read-only replicas to share online read-only traffic, you can also remove Replica service from the default service list.\nDelegate Service Pigsty exposes PostgreSQL services with haproxy on nodes. All haproxy instances in the cluster are configured with the same service definition.\nHowever, you can delegate pg service to a specific node group (e.g., dedicated haproxy lb cluster) rather than haproxy on PostgreSQL cluster members.\nTo do so, you need to override the default service definition with pg_default_services and set pg_service_provider to the proxy group name.\nFor example, this configuration will expose pg cluster primary service on haproxy node group proxy with port 10013.\npg_service_provider: proxy # use load balancer on group `proxy` with port 10013 pg_default_services: [{ name: primary ,port: 10013 ,dest: postgres ,check: /primary ,selector: \"[]\" }] It’s user’s responsibility to make sure each delegate service port is unique among the proxy cluster.\nA dedicated load balancer cluster example is provided in the 43-node production environment simulation sandbox: prod.yml\n","categories":["Reference"],"description":"Split read and write operations, route traffic correctly, and reliably deliver PostgreSQL cluster capabilities.","excerpt":"Split read and write operations, route traffic correctly, and reliably …","ref":"/docs/pgsql/service/","tags":"","title":"Service/Access"},{"body":" Pigsty provides a battery-included access control model based on role system and privilege system.\nAccess control is crucial, yet many users struggle to implement it properly. Therefore, Pigsty provides a streamlined, battery-included access control model to provide a safety net for your cluster security.\nRole System Pigsty’s default role system includes four default roles and four default users:\nRole Name Attributes Member of Description dbrole_readonly NOLOGIN role for global read-only access dbrole_readwrite NOLOGIN dbrole_readonly role for global read-write access dbrole_admin NOLOGIN pg_monitor,dbrole_readwrite role for admin/object creation dbrole_offline NOLOGIN role for restricted read-only access postgres SUPERUSER system superuser replicator REPLICATION pg_monitor,dbrole_readonly system replicator dbuser_dba SUPERUSER dbrole_admin pgsql admin user dbuser_monitor pg_monitor pgsql monitor user The detailed definitions of these roles and users are as follows:\npg_default_roles: # default roles and users in postgres cluster - { name: dbrole_readonly ,login: false ,comment: role for global read-only access } - { name: dbrole_offline ,login: false ,comment: role for restricted read-only access } - { name: dbrole_readwrite ,login: false ,roles: [dbrole_readonly] ,comment: role for global read-write access } - { name: dbrole_admin ,login: false ,roles: [pg_monitor, dbrole_readwrite] ,comment: role for object creation } - { name: postgres ,superuser: true ,comment: system superuser } - { name: replicator ,replication: true ,roles: [pg_monitor, dbrole_readonly] ,comment: system replicator } - { name: dbuser_dba ,superuser: true ,roles: [dbrole_admin] ,pgbouncer: true ,pool_mode: session, pool_connlimit: 16 ,comment: pgsql admin user } - { name: dbuser_monitor ,roles: [pg_monitor] ,pgbouncer: true ,parameters: {log_min_duration_statement: 1000 } ,pool_mode: session ,pool_connlimit: 8 ,comment: pgsql monitor user } Default Roles There are four default roles in Pigsty:\nRead-Only (dbrole_readonly): Role for global read-only access. If other business applications need read-only access to this database, they can use this role. Read-Write (dbrole_readwrite): Role for global read-write access, the primary business production account should have database read-write privileges. Admin (dbrole_admin): Role with DDL privileges, typically used for business administrators or scenarios requiring table creation in applications (such as various business software). Offline (dbrole_offline): Restricted read-only access role (can only access offline instances, typically for personal users and ETL tool accounts). Default roles are defined in pg_default_roles. Unless you really know what you’re doing, it’s recommended not to change the default role names.\n- { name: dbrole_readonly , login: false , comment: role for global read-only access } # production read-only role - { name: dbrole_offline , login: false , comment: role for restricted read-only access (offline instance) } # restricted read-only role - { name: dbrole_readwrite , login: false , roles: [dbrole_readonly], comment: role for global read-write access } # production read-write role - { name: dbrole_admin , login: false , roles: [pg_monitor, dbrole_readwrite] , comment: role for object creation } # production DDL change role Default Users Pigsty also has four default users (system users):\nSuperuser (postgres), the owner and creator of the cluster, same name as the OS dbsu. Replication user (replicator), the system user used for primary-replica replication. Monitor user (dbuser_monitor), a user used to monitor database and connection pool metrics. Admin user (dbuser_dba), the admin user who performs daily operations and database changes. The usernames/passwords for these 4 default users are defined through 4 pairs of dedicated parameters, referenced in many places:\npg_dbsu: OS dbsu name, defaults to postgres, better not to change it pg_dbsu_password: dbsu password, empty string by default means no password is set for dbsu, best not to set it. pg_replication_username: postgres replication username, defaults to replicator pg_replication_password: postgres replication password, defaults to DBUser.Replicator pg_admin_username: postgres admin username, defaults to dbuser_dba pg_admin_password: postgres admin password in plain text, defaults to DBUser.DBA pg_monitor_username: postgres monitor username, defaults to dbuser_monitor pg_monitor_password: postgres monitor password, defaults to DBUser.Monitor Remember to change these passwords in production deployment! Do not use the default values!\npg_dbsu: postgres # database superuser name, better not to change this username. pg_dbsu_password: '' # database superuser password, it's recommended to leave this empty! Disable dbsu password login. pg_replication_username: replicator # system replication username pg_replication_password: DBUser.Replicator # system replication password, must change this password! pg_monitor_username: dbuser_monitor # system monitor username pg_monitor_password: DBUser.Monitor # system monitor password, must change this password! pg_admin_username: dbuser_dba # system admin username pg_admin_password: DBUser.DBA # system admin password, must change this password! If you modify the default user parameters, modify the corresponding role definitions in pg_default_roles:\n- { name: postgres ,superuser: true ,comment: system superuser } - { name: replicator ,replication: true ,roles: [pg_monitor, dbrole_readonly] ,comment: system replicator } - { name: dbuser_dba ,superuser: true ,roles: [dbrole_admin] ,pgbouncer: true ,pool_mode: session, pool_connlimit: 16 , comment: pgsql admin user } - { name: dbuser_monitor ,roles: [pg_monitor, dbrole_readonly] ,pgbouncer: true ,parameters: {log_min_duration_statement: 1000 } ,pool_mode: session ,pool_connlimit: 8 ,comment: pgsql monitor user } Privilege System Pigsty has a battery-included privilege model that works with default roles.\nAll users have access to all schemas. Read-Only users (dbrole_readonly) can read from all tables. (SELECT, EXECUTE) Read-Write users (dbrole_readwrite) can write to all tables and run DML. (INSERT, UPDATE, DELETE). Admin users (dbrole_admin) can create objects and run DDL (CREATE, USAGE, TRUNCATE, REFERENCES, TRIGGER). Offline users (dbrole_offline) are similar to read-only users but with restricted access, only allowed to access offline instances (pg_role = 'offline' or pg_offline_query = true) Objects created by admin users will have correct privileges. Default privileges are configured on all databases, including template databases. Database connect privileges are managed by database definitions. The CREATE privilege on database and public schema is revoked from PUBLIC by default. Object Privileges Default privileges for newly created objects in the database are controlled by the parameter pg_default_privileges:\n- GRANT USAGE ON SCHEMAS TO dbrole_readonly - GRANT SELECT ON TABLES TO dbrole_readonly - GRANT SELECT ON SEQUENCES TO dbrole_readonly - GRANT EXECUTE ON FUNCTIONS TO dbrole_readonly - GRANT USAGE ON SCHEMAS TO dbrole_offline - GRANT SELECT ON TABLES TO dbrole_offline - GRANT SELECT ON SEQUENCES TO dbrole_offline - GRANT EXECUTE ON FUNCTIONS TO dbrole_offline - GRANT INSERT ON TABLES TO dbrole_readwrite - GRANT UPDATE ON TABLES TO dbrole_readwrite - GRANT DELETE ON TABLES TO dbrole_readwrite - GRANT USAGE ON SEQUENCES TO dbrole_readwrite - GRANT UPDATE ON SEQUENCES TO dbrole_readwrite - GRANT TRUNCATE ON TABLES TO dbrole_admin - GRANT REFERENCES ON TABLES TO dbrole_admin - GRANT TRIGGER ON TABLES TO dbrole_admin - GRANT CREATE ON SCHEMAS TO dbrole_admin Objects newly created by admin users will have the above privileges by default. Use \\ddp+ to view these default privileges:\nType Access privileges function =X dbrole_readonly=X dbrole_offline=X dbrole_admin=X schema dbrole_readonly=U dbrole_offline=U dbrole_admin=UC sequence dbrole_readonly=r dbrole_offline=r dbrole_readwrite=wU dbrole_admin=rwU table dbrole_readonly=r dbrole_offline=r dbrole_readwrite=awd dbrole_admin=arwdDxt Default Privileges ALTER DEFAULT PRIVILEGES allows you to set the privileges that will be applied to objects created in the future. It does not affect privileges assigned to already-existing objects, nor objects created by non-admin users.\nIn Pigsty, default privileges are defined for three roles:\n{% for priv in pg_default_privileges %} ALTER DEFAULT PRIVILEGES FOR ROLE {{ pg_dbsu }} {{ priv }}; {% endfor %} {% for priv in pg_default_privileges %} ALTER DEFAULT PRIVILEGES FOR ROLE {{ pg_admin_username }} {{ priv }}; {% endfor %} -- For other business administrators, they should execute SET ROLE dbrole_admin before running DDL to use the corresponding default privilege configuration. {% for priv in pg_default_privileges %} ALTER DEFAULT PRIVILEGES FOR ROLE \"dbrole_admin\" {{ priv }}; {% endfor %} These contents will be used by the PG cluster initialization template pg-init-template.sql, rendered and output to /pg/tmp/pg-init-template.sql during cluster initialization. This command will be executed on template1 and postgres databases, and newly created databases will inherit these default privilege configurations through template template1.\nThat is to say, to maintain correct object privileges, you must run DDL with admin users, which could be:\n{{ pg_dbsu }}, defaults to postgres {{ pg_admin_username }}, defaults to dbuser_dba Business admin users granted with dbrole_admin role (switch to dbrole_admin identity via SET ROLE) It’s wise to use postgres as the global object owner. If you wish to create objects with business admin user, you must use SET ROLE dbrole_admin before running DDL to maintain correct privileges.\nOf course, you can also explicitly grant default privileges to business admins in the database with ALTER DEFAULT PRIVILEGE FOR ROLE \u003csome_biz_admin\u003e XXX.\nDatabase Privileges In Pigsty, database-level privileges are covered in database definitions.\nThere are 3 database-level privileges: CONNECT, CREATE, TEMP, and a special ‘privilege’: OWNERSHIP.\n- name: meta # required, `name` is the only mandatory field in database definition owner: postgres # optional, database owner, defaults to postgres allowconn: true # optional, allow connection, true by default. false will completely disable connection to this database revokeconn: false # optional, revoke public connection privilege. false by default, when set to true, CONNECT privilege will be revoked from users other than owner and admin If owner parameter exists, it will be used as the database owner instead of the default {{ pg_dbsu }} (usually postgres) If revokeconn is false, all users have the database’s CONNECT privilege, this is the default behavior. If revokeconn is explicitly set to true: The database’s CONNECT privilege will be revoked from PUBLIC: ordinary users cannot connect to this database CONNECT privilege will be explicitly granted to {{ pg_replication_username }}, {{ pg_monitor_username }} and {{ pg_admin_username }} CONNECT privilege will be granted to the database owner with GRANT OPTION, the database owner can then grant connection privileges to other users. The revokeconn option can be used to isolate cross-database access within the same cluster. You can create different business users as owners for each database and set the revokeconn option for them. Example: Database Isolation pg-infra: hosts: 10.10.10.40: { pg_seq: 1, pg_role: primary } 10.10.10.41: { pg_seq: 2, pg_role: replica , pg_offline_query: true } vars: pg_cluster: pg-infra pg_users: - { name: dbuser_confluence, password: mc2iohos , pgbouncer: true, roles: [ dbrole_admin ] } - { name: dbuser_gitlab, password: sdf23g22sfdd , pgbouncer: true, roles: [ dbrole_readwrite ] } - { name: dbuser_jira, password: sdpijfsfdsfdfs , pgbouncer: true, roles: [ dbrole_admin ] } pg_databases: - { name: confluence , revokeconn: true, owner: dbuser_confluence , connlimit: 100 } - { name: gitlab , revokeconn: true, owner: dbuser_gitlab, connlimit: 100 } - { name: jira , revokeconn: true, owner: dbuser_jira , connlimit: 100 } CREATE Privileges For security considerations, Pigsty revokes the CREATE privilege on database from PUBLIC by default, and this has been the default behavior since PostgreSQL 15.\nThe database owner can always adjust CREATE privileges as needed based on actual requirements.\n","categories":["Reference"],"description":"Default role system and privilege model provided by Pigsty","excerpt":"Default role system and privilege model provided by Pigsty","ref":"/docs/pgsql/security/","tags":["Security"],"title":"Access Control"},{"body":" Separate read and write operations, route traffic correctly, and deliver PostgreSQL cluster capabilities reliably.\nService is an abstraction: it is the form in which database clusters provide capabilities to the outside world and encapsulates the details of the underlying cluster.\nServices are critical for stable access in production environments and show their value when high availability clusters automatically fail over. Single-node users typically don’t need to worry about this concept.\nSingle-Node Users The concept of “service” is for production environments. Personal users/single-node clusters can simply access the database directly using instance name/IP address.\nFor example, Pigsty’s default single-node pg-meta.meta database can be connected directly using three different users:\npsql postgres://dbuser_dba:DBUser.DBA@10.10.10.10/meta # Connect directly with DBA superuser psql postgres://dbuser_meta:DBUser.Meta@10.10.10.10/meta # Connect with default business admin user psql postgres://dbuser_view:DBUser.View@pg-meta/meta # Connect with default read-only user via instance domain name Service Overview In real-world production environments, we use replication-based primary-replica database clusters. In a cluster, there is one and only one instance as the leader (primary) that can accept writes. Other instances (replicas) continuously fetch change logs from the cluster leader and stay consistent with it. At the same time, replicas can also handle read-only requests, significantly reducing the load on the primary in read-heavy scenarios. Therefore, separating write requests and read-only requests to the cluster is a very common practice.\nIn addition, for production environments with high-frequency short connections, we also pool requests through a connection pool middleware (Pgbouncer) to reduce the overhead of creating connections and backend processes. But for scenarios such as ETL and change execution, we need to bypass the connection pool and access the database directly. At the same time, high-availability clusters will experience failover when failures occur, and failover will cause changes to the cluster’s leader. Therefore, high-availability database solutions require that write traffic can automatically adapt to changes in the cluster’s leader. These different access requirements (read-write separation, pooling and direct connection, automatic failover adaptation) ultimately abstract the concept of Service.\nTypically, database clusters must provide this most basic service:\nRead-Write Service (primary): Can read and write to the database For production database clusters, at least these two services should be provided:\nRead-Write Service (primary): Write data: can only be carried by the primary. Read-Only Service (replica): Read data: can be carried by replicas, or by the primary if there are no replicas In addition, depending on specific business scenarios, there may be other services, such as:\nDefault Direct Service (default): Allows (admin) users to access the database directly, bypassing the connection pool Offline Replica Service (offline): Dedicated replicas that do not handle online read-only traffic, used for ETL and analytical queries Standby Replica Service (standby): Read-only service without replication lag, handled by sync standby/primary for read-only queries Delayed Replica Service (delayed): Access old data from the same cluster at a previous point in time, handled by delayed replica Default Services Pigsty provides four different services by default for each PostgreSQL database cluster. Here are the default services and their definitions:\nService Port Description primary 5433 Production read-write, connects to primary connection pool (6432) replica 5434 Production read-only, connects to replica connection pool (6432) default 5436 Admin, ETL writes, direct access to primary (5432) offline 5438 OLAP, ETL, personal users, interactive queries Taking the default pg-meta cluster as an example, it provides four default services:\npsql postgres://dbuser_meta:DBUser.Meta@pg-meta:5433/meta # pg-meta-primary : production read-write via primary pgbouncer(6432) psql postgres://dbuser_meta:DBUser.Meta@pg-meta:5434/meta # pg-meta-replica : production read-only via replica pgbouncer(6432) psql postgres://dbuser_dba:DBUser.DBA@pg-meta:5436/meta # pg-meta-default : direct connection via primary postgres(5432) psql postgres://dbuser_stats:DBUser.Stats@pg-meta:5438/meta # pg-meta-offline : direct connection via offline postgres(5432) You can see how these four services work from the sample cluster architecture diagram:\nNote that the pg-meta domain name points to the cluster’s L2 VIP, which in turn points to the haproxy load balancer on the cluster primary, which routes traffic to different instances. See Accessing Services for details.\nService Implementation In Pigsty, services are implemented using haproxy on nodes, differentiated by different ports on host nodes.\nHaproxy is enabled by default on each node managed by Pigsty to expose services, and database nodes are no exception. Although nodes in a cluster have primary-replica distinctions from the database perspective, from the service perspective, each node is the same: This means that even if you access a replica node, as long as you use the correct service port, you can still use the primary’s read-write service. This design can hide complexity: so as long as you can access any instance on a PostgreSQL cluster, you can completely access all services.\nThis design is similar to NodePort services in Kubernetes. Similarly, in Pigsty, each service includes the following two core elements:\nAccess endpoints exposed through NodePort (port number, where to access?) Target instances selected through Selectors (instance list, who carries the load?) Pigsty’s service delivery boundary stops at the cluster’s HAProxy, and users can access these load balancers in various ways. See Accessing Services.\nAll services are declared through configuration files. For example, the PostgreSQL default services are defined by the pg_default_services parameter:\npg_default_services: - { name: primary ,port: 5433 ,dest: default ,check: /primary ,selector: \"[]\" } - { name: replica ,port: 5434 ,dest: default ,check: /read-only ,selector: \"[]\" , backup: \"[? pg_role == `primary` || pg_role == `offline` ]\" } - { name: default ,port: 5436 ,dest: postgres ,check: /primary ,selector: \"[]\" } - { name: offline ,port: 5438 ,dest: postgres ,check: /replica ,selector: \"[? pg_role == `offline` || pg_offline_query ]\" , backup: \"[? pg_role == `replica` \u0026\u0026 !pg_offline_query]\"} You can also define additional services in pg_services. Both pg_default_services and pg_services are arrays of service definition objects.\nDefining Services Pigsty allows you to define your own services:\npg_default_services: Services uniformly exposed by all PostgreSQL clusters, four by default. pg_services: Additional PostgreSQL services, can be defined at global or cluster level as needed. haproxy_services: Directly customize HAProxy service content, can be used for accessing other components For PostgreSQL clusters, you typically only need to focus on the first two. Each service definition generates a new configuration file in the configuration directory of all related HAProxy instances: /etc/haproxy/\u003csvcname\u003e.cfg Here’s a custom service example standby: when you want to provide a read-only service without replication lag, you can add this record to pg_services:\n- name: standby # Required, service name, final svc name uses `pg_cluster` as prefix, e.g.: pg-meta-standby port: 5435 # Required, exposed service port (as kubernetes service node port mode) ip: \"*\" # Optional, IP address the service binds to, all IP addresses by default selector: \"[]\" # Required, service member selector, uses JMESPath to filter configuration manifest backup: \"[? pg_role == `primary`]\" # Optional, service member selector (backup), instances selected here only carry the service when all default selector instances are down dest: default # Optional, target port, default|postgres|pgbouncer|\u003cport_number\u003e, defaults to 'default', Default means using pg_default_service_dest value to ultimately decide check: /sync # Optional, health check URL path, defaults to /, here uses Patroni API: /sync, only sync standby and primary return 200 healthy status code maxconn: 5000 # Optional, maximum number of allowed frontend connections, defaults to 5000 balance: roundrobin # Optional, haproxy load balancing algorithm (defaults to roundrobin, other options: leastconn) options: 'inter 3s fastinter 1s downinter 5s rise 3 fall 3 on-marked-down shutdown-sessions slowstart 30s maxconn 3000 maxqueue 128 weight 100' The above service definition will be converted to haproxy configuration file /etc/haproxy/pg-test-standby.conf on the sample three-node pg-test:\n#--------------------------------------------------------------------- # service: pg-test-standby @ 10.10.10.11:5435 #--------------------------------------------------------------------- # service instances 10.10.10.11, 10.10.10.13, 10.10.10.12 # service backups 10.10.10.11 listen pg-test-standby bind *:5435 # \u003c--- Binds port 5435 on all IP addresses mode tcp # \u003c--- Load balancer works on TCP protocol maxconn 5000 # \u003c--- Maximum connections 5000, can be increased as needed balance roundrobin # \u003c--- Load balancing algorithm is rr round-robin, can also use leastconn option httpchk # \u003c--- Enable HTTP health check option http-keep-alive # \u003c--- Keep HTTP connection http-check send meth OPTIONS uri /sync # \u003c---- Here uses /sync, Patroni health check API, only sync standby and primary return 200 healthy status code http-check expect status 200 # \u003c---- Health check return code 200 means normal default-server inter 3s fastinter 1s downinter 5s rise 3 fall 3 on-marked-down shutdown-sessions slowstart 30s maxconn 3000 maxqueue 128 weight 100 # servers: # All three instances of pg-test cluster are selected by selector: \"[]\", since there are no filter conditions, they all become backend servers for pg-test-replica service. But due to /sync health check, only primary and sync standby can actually handle requests server pg-test-1 10.10.10.11:6432 check port 8008 weight 100 backup # \u003c----- Only primary satisfies condition pg_role == `primary`, selected by backup selector server pg-test-3 10.10.10.13:6432 check port 8008 weight 100 # Therefore serves as service fallback instance: normally doesn't handle requests, only handles read-only requests when all other replicas fail, thus maximally avoiding read-write service being affected by read-only service server pg-test-2 10.10.10.12:6432 check port 8008 weight 100 # Here, all three instances of the pg-test cluster are selected by selector: \"[]\", rendered into the backend server list of the pg-test-replica service. But due to the /sync health check, Patroni Rest API only returns healthy HTTP 200 status code on the primary and sync standby, so only the primary and sync standby can actually handle requests. Additionally, the primary satisfies the condition pg_role == primary, is selected by the backup selector, and is marked as a backup server, only used when no other instances (i.e., sync standby) can meet the demand.\nPrimary Service The Primary service is perhaps the most critical service in production environments. It provides read-write capability to the database cluster on port 5433. The service definition is as follows:\n- { name: primary ,port: 5433 ,dest: default ,check: /primary ,selector: \"[]\" } The selector parameter selector: \"[]\" means all cluster members will be included in the Primary service But only the primary can pass the health check (check: /primary) and actually carry Primary service traffic. The destination parameter dest: default means the Primary service destination is affected by the pg_default_service_dest parameter The default value default of dest will be replaced by the value of pg_default_service_dest, which defaults to pgbouncer. By default, the Primary service destination is the connection pool on the primary, which is the port specified by pgbouncer_port, defaulting to 6432 If the value of pg_default_service_dest is postgres, then the primary service destination will bypass the connection pool and use the PostgreSQL database port directly (pg_port, default 5432). This parameter is very useful for scenarios that don’t want to use a connection pool.\nExample: haproxy configuration for pg-test-primary listen pg-test-primary bind *:5433 # \u003c--- primary service defaults to port 5433 mode tcp maxconn 5000 balance roundrobin option httpchk option http-keep-alive http-check send meth OPTIONS uri /primary # \u003c--- primary service defaults to Patroni RestAPI /primary health check http-check expect status 200 default-server inter 3s fastinter 1s downinter 5s rise 3 fall 3 on-marked-down shutdown-sessions slowstart 30s maxconn 3000 maxqueue 128 weight 100 # servers server pg-test-1 10.10.10.11:6432 check port 8008 weight 100 server pg-test-3 10.10.10.13:6432 check port 8008 weight 100 server pg-test-2 10.10.10.12:6432 check port 8008 weight 100 Patroni’s high availability mechanism ensures that at most one instance’s /primary health check is true at any time, so the Primary service will always route traffic to the primary instance.\nOne benefit of using the Primary service instead of direct database connection is that if the cluster has a split-brain situation for some reason (e.g., kill -9 killing the primary Patroni without watchdog), Haproxy can still avoid split-brain in this case, because it will only distribute traffic when Patroni is alive and returns primary status.\nReplica Service The Replica service is second only to the Primary service in importance in production environments. It provides read-only capability to the database cluster on port 5434. The service definition is as follows:\n- { name: replica ,port: 5434 ,dest: default ,check: /read-only ,selector: \"[]\" , backup: \"[? pg_role == `primary` || pg_role == `offline` ]\" } The selector parameter selector: \"[]\" means all cluster members will be included in the Replica service All instances can pass the health check (check: /read-only) and carry Replica service traffic. Backup selector: [? pg_role == 'primary' || pg_role == 'offline' ] marks the primary and offline replicas as backup servers. Only when all normal replicas are down will the Replica service be carried by the primary or offline replicas. The destination parameter dest: default means the Replica service destination is also affected by the pg_default_service_dest parameter The default value default of dest will be replaced by the value of pg_default_service_dest, which defaults to pgbouncer, same as the Primary service By default, the Replica service destination is the connection pool on the replicas, which is the port specified by pgbouncer_port, defaulting to 6432 Example: haproxy configuration for pg-test-replica listen pg-test-replica bind *:5434 mode tcp maxconn 5000 balance roundrobin option httpchk option http-keep-alive http-check send meth OPTIONS uri /read-only http-check expect status 200 default-server inter 3s fastinter 1s downinter 5s rise 3 fall 3 on-marked-down shutdown-sessions slowstart 30s maxconn 3000 maxqueue 128 weight 100 # servers server pg-test-1 10.10.10.11:6432 check port 8008 weight 100 backup server pg-test-3 10.10.10.13:6432 check port 8008 weight 100 server pg-test-2 10.10.10.12:6432 check port 8008 weight 100 The Replica service is very flexible: if there are surviving dedicated Replica instances, it will prioritize using these instances to handle read-only requests. Only when all replica instances are down will the primary handle read-only requests. For the common one-primary-one-replica two-node cluster, this means: use the replica as long as it’s alive, use the primary when the replica is down.\nAdditionally, unless all dedicated read-only instances are down, the Replica service will not use dedicated Offline instances, thus avoiding mixing online fast queries and offline slow queries together, interfering with each other.\nDefault Service The Default service provides services on port 5436. It is a variant of the Primary service.\nThe Default service always bypasses the connection pool and connects directly to PostgreSQL on the primary. This is useful for admin connections, ETL writes, CDC data change capture, etc.\n- { name: default ,port: 5436 ,dest: postgres ,check: /primary ,selector: \"[]\" } If pg_default_service_dest is changed to postgres, then the Default service is completely equivalent to the Primary service except for port and name. In this case, you can consider removing Default from default services.\nExample: haproxy configuration for pg-test-default listen pg-test-default bind *:5436 # \u003c--- Except for listening port/target port and service name, other configurations are exactly the same as primary service mode tcp maxconn 5000 balance roundrobin option httpchk option http-keep-alive http-check send meth OPTIONS uri /primary http-check expect status 200 default-server inter 3s fastinter 1s downinter 5s rise 3 fall 3 on-marked-down shutdown-sessions slowstart 30s maxconn 3000 maxqueue 128 weight 100 # servers server pg-test-1 10.10.10.11:5432 check port 8008 weight 100 server pg-test-3 10.10.10.13:5432 check port 8008 weight 100 server pg-test-2 10.10.10.12:5432 check port 8008 weight 100 Offline Service The Offline service provides services on port 5438. It also bypasses the connection pool to directly access the PostgreSQL database, typically used for slow queries/analytical queries/ETL reads/personal user interactive queries. Its service definition is as follows:\n- { name: offline ,port: 5438 ,dest: postgres ,check: /replica ,selector: \"[? pg_role == `offline` || pg_offline_query ]\" , backup: \"[? pg_role == `replica` \u0026\u0026 !pg_offline_query]\"} The Offline service routes traffic directly to dedicated offline replicas, or normal read-only instances with the pg_offline_query flag.\nThe selector parameter filters two types of instances from the cluster: offline replicas with pg_role = offline, or normal read-only instances with pg_offline_query = true The main difference between dedicated offline replicas and flagged normal replicas is: the former does not handle Replica service requests by default, avoiding mixing fast and slow requests together, while the latter does by default. The backup selector parameter filters one type of instance from the cluster: normal replicas without offline flag. This means if offline instances or flagged normal replicas fail, other normal replicas can be used to carry the Offline service. The health check /replica only returns 200 for replicas, the primary returns an error, so the Offline service will never distribute traffic to the primary instance, even if only this primary is left in the cluster. At the same time, the primary instance is neither selected by the selector nor by the backup selector, so it will never carry the Offline service. Therefore, the Offline service can always avoid user access to the primary, thus avoiding impact on the primary. Example: haproxy configuration for pg-test-offline listen pg-test-offline bind *:5438 mode tcp maxconn 5000 balance roundrobin option httpchk option http-keep-alive http-check send meth OPTIONS uri /replica http-check expect status 200 default-server inter 3s fastinter 1s downinter 5s rise 3 fall 3 on-marked-down shutdown-sessions slowstart 30s maxconn 3000 maxqueue 128 weight 100 # servers server pg-test-3 10.10.10.13:5432 check port 8008 weight 100 server pg-test-2 10.10.10.12:5432 check port 8008 weight 100 backup The Offline service provides limited read-only service, typically used for two types of queries: interactive queries (personal users), slow queries and long transactions (analytics/ETL).\nThe Offline service requires extra maintenance care: when the cluster experiences primary-replica switchover or automatic failover, the cluster’s instance roles change, but Haproxy’s configuration does not automatically change. For clusters with multiple replicas, this is usually not a problem. However, for simplified small clusters with one primary and one replica running Offline queries, primary-replica switchover means the replica becomes the primary (health check fails), and the original primary becomes a replica (not in the Offline backend list), so no instance can carry the Offline service. Therefore, you need to manually reload services to make the changes effective.\nIf your business model is relatively simple, you can consider removing the Default service and Offline service, and use the Primary service and Replica service to connect directly to the database.\nReload Services When cluster members change, such as adding/removing replicas, primary-replica switchover, or adjusting relative weights, you need to reload services to make the changes effective.\nbin/pgsql-svc \u003ccls\u003e [ip...] # Reload services for lb cluster or lb instance # ./pgsql.yml -t pg_service # Actual ansible task for reloading services Accessing Services Pigsty’s service delivery boundary stops at the cluster’s HAProxy. Users can access these load balancers in various ways.\nThe typical approach is to use DNS or VIP access, binding them to all or any number of load balancers in the cluster.\nYou can use different host \u0026 port combinations, which provide PostgreSQL services in different ways.\nHost\nType Example Description Cluster Domain pg-test Access via cluster domain name (resolved by dnsmasq @ infra node) Cluster VIP Address 10.10.10.3 Access via L2 VIP address managed by vip-manager, bound to primary node Instance Hostname pg-test-1 Access via any instance hostname (resolved by dnsmasq @ infra node) Instance IP Address 10.10.10.11 Access any instance’s IP address Port\nPigsty uses different ports to distinguish pg services\nPort Service Type Description 5432 postgres Database Direct access to postgres server 6432 pgbouncer Middleware Access postgres via connection pool middleware 5433 primary Service Access primary pgbouncer (or postgres) 5434 replica Service Access replica pgbouncer (or postgres) 5436 default Service Access primary postgres 5438 offline Service Access offline postgres Combinations\n# Access via cluster domain name postgres://test@pg-test:5432/test # DNS -\u003e L2 VIP -\u003e Primary direct connection postgres://test@pg-test:6432/test # DNS -\u003e L2 VIP -\u003e Primary connection pool -\u003e Primary postgres://test@pg-test:5433/test # DNS -\u003e L2 VIP -\u003e HAProxy -\u003e Primary connection pool -\u003e Primary postgres://test@pg-test:5434/test # DNS -\u003e L2 VIP -\u003e HAProxy -\u003e Replica connection pool -\u003e Replica postgres://dbuser_dba@pg-test:5436/test # DNS -\u003e L2 VIP -\u003e HAProxy -\u003e Primary direct connection (for admin) postgres://dbuser_stats@pg-test:5438/test # DNS -\u003e L2 VIP -\u003e HAProxy -\u003e Offline direct connection (for ETL/personal queries) # Direct access via cluster VIP postgres://test@10.10.10.3:5432/test # L2 VIP -\u003e Primary direct access postgres://test@10.10.10.3:6432/test # L2 VIP -\u003e Primary connection pool -\u003e Primary postgres://test@10.10.10.3:5433/test # L2 VIP -\u003e HAProxy -\u003e Primary connection pool -\u003e Primary postgres://test@10.10.10.3:5434/test # L2 VIP -\u003e HAProxy -\u003e Replica connection pool -\u003e Replica postgres://dbuser_dba@10.10.10.3:5436/test # L2 VIP -\u003e HAProxy -\u003e Primary direct connection (for admin) postgres://dbuser_stats@10.10.10.3::5438/test # L2 VIP -\u003e HAProxy -\u003e Offline direct connection (for ETL/personal queries) # Specify any cluster instance name directly postgres://test@pg-test-1:5432/test # DNS -\u003e Database instance direct connection (single instance access) postgres://test@pg-test-1:6432/test # DNS -\u003e Connection pool -\u003e Database postgres://test@pg-test-1:5433/test # DNS -\u003e HAProxy -\u003e Connection pool -\u003e Database read/write postgres://test@pg-test-1:5434/test # DNS -\u003e HAProxy -\u003e Connection pool -\u003e Database read-only postgres://dbuser_dba@pg-test-1:5436/test # DNS -\u003e HAProxy -\u003e Database direct connection postgres://dbuser_stats@pg-test-1:5438/test # DNS -\u003e HAProxy -\u003e Database offline read/write # Specify any cluster instance IP directly postgres://test@10.10.10.11:5432/test # Database instance direct connection (direct instance specification, no automatic traffic distribution) postgres://test@10.10.10.11:6432/test # Connection pool -\u003e Database postgres://test@10.10.10.11:5433/test # HAProxy -\u003e Connection pool -\u003e Database read/write postgres://test@10.10.10.11:5434/test # HAProxy -\u003e Connection pool -\u003e Database read-only postgres://dbuser_dba@10.10.10.11:5436/test # HAProxy -\u003e Database direct connection postgres://dbuser_stats@10.10.10.11:5438/test # HAProxy -\u003e Database offline read-write # Smart client: automatic read-write separation postgres://test@10.10.10.11:6432,10.10.10.12:6432,10.10.10.13:6432/test?target_session_attrs=primary postgres://test@10.10.10.11:6432,10.10.10.12:6432,10.10.10.13:6432/test?target_session_attrs=prefer-standby Overriding Services You can override default service configuration in multiple ways. A common requirement is to have Primary service and Replica service bypass the Pgbouncer connection pool and access the PostgreSQL database directly.\nTo achieve this, you can change pg_default_service_dest to postgres, so all services with svc.dest='default' in their service definitions will use postgres instead of the default pgbouncer as the target.\nIf you have already pointed Primary service to PostgreSQL, then default service becomes redundant and can be considered for removal.\nIf you don’t need to distinguish between personal interactive queries and analytical/ETL slow queries, you can consider removing Offline service from the default service list pg_default_services.\nIf you don’t need read-only replicas to share online read-only traffic, you can also remove Replica service from the default service list.\nDelegating Services Pigsty exposes PostgreSQL services through haproxy on nodes. All haproxy instances in the entire cluster are configured with the same service definitions.\nHowever, you can delegate pg services to specific node groups (e.g., dedicated haproxy load balancer cluster) instead of haproxy on PostgreSQL cluster members.\nTo do this, you need to override the default service definitions using pg_default_services and set pg_service_provider to the proxy group name.\nFor example, this configuration will expose the pg cluster’s primary service on the proxy haproxy node group on port 10013.\npg_service_provider: proxy # Use load balancer from `proxy` group on port 10013 pg_default_services: [{ name: primary ,port: 10013 ,dest: postgres ,check: /primary ,selector: \"[]\" }] Users need to ensure that the port for each delegated service is unique in the proxy cluster.\nAn example of using a dedicated load balancer cluster is provided in the 43-node production environment simulation sandbox: prod.yml\n","categories":["Reference"],"description":"Separate read and write operations, route traffic correctly, and deliver PostgreSQL cluster capabilities reliably.","excerpt":"Separate read and write operations, route traffic correctly, and …","ref":"/docs/pgsql/misc/svc/","tags":"","title":"Service / Access"},{"body":"Database administration and operation tasks\n","categories":["Reference"],"description":"","excerpt":"Database administration and operation tasks\n","ref":"/docs/pgsql/_div_admin/","tags":"","title":"Administration"},{"body":"","categories":["Task"],"description":"Standard Operating Procedures (SOP) for database administration tasks","excerpt":"Standard Operating Procedures (SOP) for database administration tasks","ref":"/docs/pgsql/admin/","tags":"","title":"Administration"},{"body":" When: Backup schedule Where: Backup repository How: Backup method When to Backup The first question is when to backup your database - this is a tradeoff between backup frequency and recovery time. Since you need to replay WAL logs from the last backup to the recovery target point, the more frequent the backups, the less WAL logs need to be replayed, and the faster the recovery.\nDaily Full Backup For production databases, it’s recommended to start with the simplest daily full backup strategy. This is also Pigsty’s default backup strategy, implemented via crontab.\nnode_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] pgbackrest_method: local # Choose backup repository method: `local`, `minio`, or other custom repository pgbackrest_repo: # pgbackrest repository configuration: https://pgbackrest.org/configuration.html#section-repository local: # Default pgbackrest repository using local POSIX filesystem path: /pg/backup # Local backup directory, defaults to `/pg/backup` retention_full_type: count # Retain full backups by count retention_full: 2 # Keep 2, up to 3 full backups when using local filesystem repository When used with the default local local filesystem backup repository, this provides a 24~48 hour recovery window.\nAssuming your database size is 100GB and writes 10GB of data per day, the backup size is as follows:\nThis will consume 2~3 times the database size in space, plus 2 days of WAL logs. Therefore, in practice, you may need to prepare at least 3~5 times the database size for backup disk to use the default backup strategy.\nFull + Incremental Backup You can optimize backup space usage by adjusting these parameters.\nIf using MinIO / S3 as a centralized backup repository, you can use storage space beyond local disk limitations. In this case, consider using full + incremental backup with a 2-week retention policy:\nnode_crontab: # Full backup at 1 AM on Monday, incremental backups on weekdays - '00 01 * * 1 postgres /pg/bin/pg-backup full' - '00 01 * * 2,3,4,5,6,7 postgres /pg/bin/pg-backup' pgbackrest_method: minio pgbackrest_repo: # pgbackrest repository configuration: https://pgbackrest.org/configuration.html#section-repository minio: # Optional minio repository type: s3 # minio is S3 compatible s3_endpoint: sss.pigsty # minio endpoint domain, defaults to `sss.pigsty` s3_region: us-east-1 # minio region, defaults to us-east-1, meaningless for minio s3_bucket: pgsql # minio bucket name, defaults to `pgsql` s3_key: pgbackrest # minio user access key for pgbackrest s3_key_secret: S3User.Backup # minio user secret for pgbackrest s3_uri_style: path # minio uses path-style URIs instead of host-style path: /pgbackrest # minio backup path, defaults to `/pgbackrest` storage_port: 9000 # minio port, defaults to 9000 storage_ca_file: /etc/pki/ca.crt # minio CA certificate path, defaults to `/etc/pki/ca.crt` block: y # Enable block-level incremental backup bundle: y # Bundle small files into a single file bundle_limit: 20MiB # Bundle size limit, recommended 20MiB for object storage bundle_size: 128MiB # Bundle target size, recommended 128MiB for object storage cipher_type: aes-256-cbc # Enable AES encryption for remote backup repository cipher_pass: pgBackRest # AES encryption password, defaults to 'pgBackRest' retention_full_type: time # Retain full backups by time retention_full: 14 # Keep full backups from the last 14 days When used with the built-in minio backup repository, this provides a guaranteed 1-week PITR recovery window.\nAssuming your database size is 100GB and writes 10GB of data per day, the backup size is as follows:\nBackup Location By default, Pigsty provides two default backup repository definitions: local and minio backup repositories.\nlocal: Default option, uses local /pg/backup directory (symlink to pg_fs_backup: /data/backups) minio: Uses SNSD single-node MinIO cluster (supported by Pigsty, but not enabled by default) pgbackrest_method: local # Choose backup repository method: `local`, `minio`, or other custom repository pgbackrest_repo: # pgbackrest repository configuration: https://pgbackrest.org/configuration.html#section-repository local: # Default pgbackrest repository using local POSIX filesystem path: /pg/backup # Local backup directory, defaults to `/pg/backup` retention_full_type: count # Retain full backups by count retention_full: 2 # Keep 2, up to 3 full backups when using local filesystem repository minio: # Optional minio repository type: s3 # minio is S3 compatible s3_endpoint: sss.pigsty # minio endpoint domain, defaults to `sss.pigsty` s3_region: us-east-1 # minio region, defaults to us-east-1, meaningless for minio s3_bucket: pgsql # minio bucket name, defaults to `pgsql` s3_key: pgbackrest # minio user access key for pgbackrest s3_key_secret: S3User.Backup # minio user secret for pgbackrest s3_uri_style: path # minio uses path-style URIs instead of host-style path: /pgbackrest # minio backup path, defaults to `/pgbackrest` storage_port: 9000 # minio port, defaults to 9000 storage_ca_file: /etc/pki/ca.crt # minio CA certificate path, defaults to `/etc/pki/ca.crt` block: y # Enable block-level incremental backup bundle: y # Bundle small files into a single file bundle_limit: 20MiB # Bundle size limit, recommended 20MiB for object storage bundle_size: 128MiB # Bundle target size, recommended 128MiB for object storage cipher_type: aes-256-cbc # Enable AES encryption for remote backup repository cipher_pass: pgBackRest # AES encryption password, defaults to 'pgBackRest' retention_full_type: time # Retain full backups by time retention_full: 14 # Keep full backups from the last 14 days ","categories":["Task"],"description":"Design backup policies according to your needs","excerpt":"Design backup policies according to your needs","ref":"/docs/pgsql/backup/policy/","tags":"","title":"Backup Policy"},{"body":"Backups can be invoked via built-in scripts, scheduled using node crontab, managed by pgbackrest, and stored in backup repositories, which can be local disk filesystems or MinIO / S3, supporting different retention policies.\nScripts You can create backups using the pg_dbsu user (defaults to postgres) to execute pgbackrest commands:\nBackup Commands backup full diff incr info pgbackrest --stanza=pg-meta --type=full backup # Create full backup for cluster pg-meta $ pgbackrest --stanza=pg-meta --type=full backup 2025-07-15 01:36:57.007 P00 INFO: backup command begin 2.54.2: --annotation=pg_cluster=pg-meta ... 2025-07-15 01:36:57.030 P00 INFO: execute non-exclusive backup start: backup begins after the requested immediate checkpoint completes 2025-07-15 01:36:57.105 P00 INFO: backup start archive = 000000010000000000000006, lsn = 0/6000028 2025-07-15 01:36:58.540 P00 INFO: new backup label = 20250715-013657F 2025-07-15 01:36:58.588 P00 INFO: full backup size = 44.5MB, file total = 1437 2025-07-15 01:36:58.589 P00 INFO: backup command end: completed successfully (1584ms) $ pgbackrest --stanza=pg-meta --type=diff backup 2025-07-15 01:37:24.952 P00 INFO: backup command begin 2.54.2: ... 2025-07-15 01:37:24.985 P00 INFO: last backup label = 20250715-013657F, version = 2.54.2 2025-07-15 01:37:26.337 P00 INFO: new backup label = 20250715-013657F_20250715-013724D 2025-07-15 01:37:26.381 P00 INFO: diff backup size = 424.3KB, file total = 1437 2025-07-15 01:37:26.381 P00 INFO: backup command end: completed successfully (1431ms) $ pgbackrest --stanza=pg-meta --type=incr backup 2025-07-15 01:37:30.305 P00 INFO: backup command begin 2.54.2: ... 2025-07-15 01:37:30.337 P00 INFO: last backup label = 20250715-013657F_20250715-013724D, version = 2.54.2 2025-07-15 01:37:31.356 P00 INFO: new backup label = 20250715-013657F_20250715-013730I 2025-07-15 01:37:31.403 P00 INFO: incr backup size = 8.3KB, file total = 1437 2025-07-15 01:37:31.403 P00 INFO: backup command end: completed successfully (1099ms) $ pgbackrest --stanza=pg-meta info stanza: pg-meta status: ok cipher: aes-256-cbc db (current) wal archive min/max (17): 000000010000000000000001/00000001000000000000000A full backup: 20250715-013657F timestamp start/stop: 2025-07-15 01:36:57+00 / 2025-07-15 01:36:58+00 wal start/stop: 000000010000000000000006 / 000000010000000000000006 database size: 44.5MB, database backup size: 44.5MB repo1: backup size: 8.7MB diff backup: 20250715-013657F_20250715-013724D timestamp start/stop: 2025-07-15 01:37:24+00 / 2025-07-15 01:37:26+00 database size: 44.5MB, database backup size: 424.3KB repo1: backup size: 94KB backup reference total: 1 full incr backup: 20250715-013657F_20250715-013730I timestamp start/stop: 2025-07-15 01:37:30+00 / 2025-07-15 01:37:31+00 database size: 44.5MB, database backup size: 8.3KB repo1: backup size: 504B backup reference total: 1 full, 1 diff Here the stanza is the database cluster name: pg_cluster, which is pg-meta in the default configuration.\nPigsty provides the pb alias and pg-backup wrapper script, which automatically fills in the current cluster name as the stanza:\nfunction pb() { local stanza=$(grep -o '\\[[^][]*]' /etc/pgbackrest/pgbackrest.conf | head -n1 | sed 's/.*\\[\\([^]]*\\)].*/\\1/') pgbackrest --stanza=$stanza $@ } pb ... # pgbackrest --stanza=pg-meta ... pb info # pgbackrest --stanza=pg-meta info pb backup # pgbackrest --stanza=pg-meta backup pg-backup full # Perform full backup = pgbackrest --stanza=pg-meta --type=full backup pg-backup incr # Perform incremental backup = pgbackrest --stanza=pg-meta --type=incr backup pg-backup diff # Perform differential backup = pgbackrest --stanza=pg-meta --type=diff backup Scheduled Backups Pigsty uses Linux crontab to schedule backup tasks. You can use it to define backup policies.\nFor example, most single-node configuration templates have the following node_crontab for backups:\nnode_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] You can design more complex backup strategies using crontab and the pg-backup script, for example:\nnode_crontab: # Full backup at 1 AM on Monday, incremental backups on weekdays - '00 01 * * 1 postgres /pg/bin/pg-backup full' - '00 01 * * 2,3,4,5,6,7 postgres /pg/bin/pg-backup' To apply crontab changes, use node.yml to update crontab on all nodes:\n./node.yml -t node_crontab -l pg-meta # Apply crontab changes to pg-meta group pgbackrest Here are the configuration details for pgbackrest in Pigsty:\npgbackrest backup tool is enabled and configured by default (pgbackrest_enabled) Installed in the pg_install task of the pgsql.yml playbook, defined in pg_packages Configured in the pg_backup task of the pgsql.yml playbook, see Parameters: PG_BACKUP Backup repository initialized in the pgbackrest_init task, which will fail if the repository already exists (error can be ignored) Initial backup created in the pgbackrest_backup task, controlled by pgbackrest_init_backup File Hierarchy bin: /usr/bin/pgbackrest, from PGDG’s pgbackrest package, in group alias pgsql-common. conf: /etc/pgbackrest, main configuration file is /etc/pgbackrest/pgbackrest.conf. logs: /pg/log/pgbackrest/*, controlled by pgbackrest_log_dir tmp: /pg/spool used as temporary spool directory for pgbackrest data: /pg/backup used to store data (when using the default local filesystem backup repository) Additionally, during PITR recovery, Pigsty creates a temporary /pg/conf/pitr.conf pgbackrest configuration file, and writes postgres recovery logs to the /pg/tmp/recovery.log file.\nMonitoring There is a pgbackrest_exporter service running on pgbackrest_exporter_port (9854) port for exporting pgbackrest metrics. You can customize it via pgbackrest_exporter_options, or set pgbackrest_exporter_enabled to false to disable it.\nInitial Backup When creating a postgres cluster, Pigsty automatically creates an initial backup. Since the new cluster is almost empty, this is a very small backup. It leaves a /etc/pgbackrest/initial.done marker file to avoid recreating the initial backup. If you don’t want an initial backup, set pgbackrest_init_backup to false.\nManagement Enable Backup If pgbackrest_enabled is set to true when the database cluster is created, backups will be automatically enabled.\nIf this value was false at creation time, you can enable the pgbackrest component with the following command:\n./pgsql.yml -t pg_backup # Run pgbackrest subtask Remove Backup When removing the primary instance (pg_role = primary), Pigsty will delete the pgbackrest backup stanza.\n./pgsql-rm.yml ./pgsql-rm.yml -e pg_rm_backup=false # Keep backups ./pgsql-rm.yml -t pg_backup # Remove backups only Use the pg_backup subtask to remove backups only, and the pg_rm_backup parameter (set to false) to preserve backups.\nIf your backup repository is locked (e.g., S3 / MinIO has locking options), this operation will fail.\nBackup Deletion Deleting backups may result in permanent data loss. This is a dangerous operation, please proceed with caution.\nList Backups This command will list all backups in the pgbackrest repository (shared across all clusters)\npgbackrest info Manual Backup Pigsty provides a built-in script /pg/bin/pg-backup that wraps the pgbackrest backup command.\npg-backup # Perform incremental backup pg-backup full # Perform full backup pg-backup incr # Perform incremental backup pg-backup diff # Perform differential backup Base Backup Pigsty provides an alternative backup script /pg/bin/pg-basebackup that does not depend on pgbackrest and directly provides a physical copy of the database cluster. The default backup directory is /pg/backup.\npg-basebackup help backup NAME pg-basebackup -- make base backup from PostgreSQL instance SYNOPSIS pg-basebackup -sdfeukr pg-basebackup --src postgres:/// --dst . --file backup.tar.lz4 DESCRIPTION -s, --src, --url Backup source URL, optional, defaults to \"postgres:///\", password should be provided in url, ENV, or .pgpass if required -d, --dst, --dir Location to store backup file, defaults to \"/pg/backup\" -f, --file Override default backup filename, \"backup_${tag}_${date}.tar.lz4\" -r, --remove Remove .lz4 files older than n minutes, defaults to 1200 (20 hours) -t, --tag Backup file tag, uses target cluster name or local IP address if not set, also used for default filename -k, --key Encryption key when --encrypt is specified, defaults to ${tag} -u, --upload Upload backup file to cloud storage (needs to be implemented by yourself) -e, --encryption Use OpenSSL RC4 encryption, uses tag as key if not specified -h, --help Print this help information postgres@pg-meta-1:~$ pg-basebackup [2025-07-13 06:16:05][INFO] ================================================================ [2025-07-13 06:16:05][INFO] [INIT] pg-basebackup begin, checking parameters [2025-07-13 06:16:05][DEBUG] [INIT] filename (-f) : backup_pg-meta_20250713.tar.lz4 [2025-07-13 06:16:05][DEBUG] [INIT] src (-s) : postgres:/// [2025-07-13 06:16:05][DEBUG] [INIT] dst (-d) : /pg/backup [2025-07-13 06:16:05][INFO] [LOCK] lock acquired success on /tmp/backup.lock, pid=107417 [2025-07-13 06:16:05][INFO] [BKUP] backup begin, from postgres:/// to /pg/backup/backup_pg-meta_20250713.tar.lz4 pg_basebackup: initiating base backup, waiting for checkpoint to complete pg_basebackup: checkpoint completed pg_basebackup: write-ahead log start point: 0/7000028 on timeline 1 pg_basebackup: write-ahead log end point: 0/7000FD8 pg_basebackup: syncing data to disk ... pg_basebackup: base backup completed [2025-07-13 06:16:06][INFO] [BKUP] backup complete! [2025-07-13 06:16:06][INFO] [DONE] backup procedure complete! [2025-07-13 06:16:06][INFO] ================================================================ The backup uses lz4 compression. You can decompress and extract the tarball with the following command:\nmkdir -p /tmp/data # Extract backup to this directory cat /pg/backup/backup_pg-meta_20250713.tar.lz4 | unlz4 -d -c | tar -xC /tmp/data Logical Backup You can also perform logical backups using the pg_dump command.\nLogical backups cannot be used for PITR (Point-in-Time Recovery), but are very useful for migrating data between different major versions or implementing flexible data export logic.\nBootstrap from Repository Suppose you have an existing cluster pg-meta and want to clone it as pg-meta2:\nYou need to create a new pg-meta2 cluster branch and then run pitr on it.\n","categories":["Task","Concept"],"description":"Backup scripts, cron jobs, backup repository and infrastructure","excerpt":"Backup scripts, cron jobs, backup repository and infrastructure","ref":"/docs/pgsql/backup/mechanism/","tags":"","title":"Backup Mechanism"},{"body":"You can configure the backup storage location by specifying the pgbackrest_repo parameter. You can define multiple repositories here, and Pigsty will choose which one to use based on the value of pgbackrest_method.\nDefault Repositories By default, Pigsty provides two default backup repository definitions: local and minio backup repositories.\nlocal: Default option, uses local /pg/backup directory (symlink to pg_fs_backup: /data/backups) minio: Uses SNSD single-node MinIO cluster (supported by Pigsty, but not enabled by default) pgbackrest_method: local # Choose backup repository method: `local`, `minio`, or other custom repository pgbackrest_repo: # pgbackrest repository configuration: https://pgbackrest.org/configuration.html#section-repository local: # Default pgbackrest repository using local POSIX filesystem path: /pg/backup # Local backup directory, defaults to `/pg/backup` retention_full_type: count # Retain full backups by count retention_full: 2 # Keep 2, up to 3 full backups when using local filesystem repository minio: # Optional minio repository type: s3 # minio is S3 compatible s3_endpoint: sss.pigsty # minio endpoint domain, defaults to `sss.pigsty` s3_region: us-east-1 # minio region, defaults to us-east-1, meaningless for minio s3_bucket: pgsql # minio bucket name, defaults to `pgsql` s3_key: pgbackrest # minio user access key for pgbackrest s3_key_secret: S3User.Backup # minio user secret for pgbackrest s3_uri_style: path # minio uses path-style URIs instead of host-style path: /pgbackrest # minio backup path, defaults to `/pgbackrest` storage_port: 9000 # minio port, defaults to 9000 storage_ca_file: /etc/pki/ca.crt # minio CA certificate path, defaults to `/etc/pki/ca.crt` block: y # Enable block-level incremental backup bundle: y # Bundle small files into a single file bundle_limit: 20MiB # Bundle size limit, recommended 20MiB for object storage bundle_size: 128MiB # Bundle target size, recommended 128MiB for object storage cipher_type: aes-256-cbc # Enable AES encryption for remote backup repository cipher_pass: pgBackRest # AES encryption password, defaults to 'pgBackRest' retention_full_type: time # Retain full backups by time retention_full: 14 # Keep full backups from the last 14 days Repository Retention Policy If you backup daily but don’t delete old backups, the backup repository will grow indefinitely and exhaust disk space. You need to define a retention policy to keep only a limited number of backups.\nThe default backup policy is defined in the pgbackrest_repo parameter and can be adjusted as needed.\nlocal: Keep the latest 2 full backups, allowing up to 3 during backup minio: Keep all full backups from the last 14 days Space Planning Object storage provides almost unlimited storage capacity, so there’s no need to worry about disk space. You can use a hybrid full + differential backup strategy to optimize space usage.\nFor local disk backup repositories, Pigsty recommends using a policy that keeps the latest 2 full backups, meaning the disk will retain the two most recent full backups (there may be a third copy while running a new backup).\nThis guarantees at least a 24-hour recovery window. See Backup Policy for details.\nOther Repository Options You can also use other services as backup repositories, refer to the pgbackrest documentation for details:\nS3 Compatible Object Storage Azure Compatible Object Storage GCS Compatible Object Storage SFTP Support Repository Versioning You can even specify repo target time to get snapshots of object storage.\nYou can enable MinIO versioning by adding the versioning flag in minio_buckets:\nminio_buckets: - { name: pgsql ,versioning: true } - { name: meta ,versioning: true } - { name: data } Repository Locking Some object storage services (S3, MinIO, etc.) support locking functionality, which can prevent backups from being deleted, even by the DBA.\nMinIO Object Locking AWS S3: Locking Objects with Object Lock You can enable MinIO locking by adding the lock flag in minio_buckets:\nminio_buckets: - { name: pgsql , lock: true } - { name: meta ,versioning: true } - { name: data } Using Object Storage Object storage services provide almost unlimited storage capacity and provide remote disaster recovery capability for your system. If you don’t have an object storage service, Pigsty has built-in MinIO support.\nMinIO You can enable the MinIO backup repository by uncommenting the following settings. Note that pgbackrest only supports HTTPS / domain names, so you must run MinIO with domain names and HTTPS endpoints.\nall: vars: pgbackrest_method: minio # Use minio as default backup repository children: # Define a single-node minio SNSD cluster minio: { hosts: { 10.10.10.10: { minio_seq: 1 }} ,vars: { minio_cluster: minio }} S3 If you only have one node, a meaningful backup strategy would be to use cloud provider object storage services like AWS S3, Alibaba Cloud OSS, or Google Cloud, etc. To do this, you can define a new repository:\npgbackrest_method: s3 # Use 'pgbackrest_repo.s3' as backup repository pgbackrest_repo: # pgbackrest repository configuration: https://pgbackrest.org/configuration.html#section-repository s3: # Alibaba Cloud OSS (S3 compatible) object storage service type: s3 # oss is S3 compatible s3_endpoint: oss-cn-beijing-internal.aliyuncs.com s3_region: oss-cn-beijing s3_bucket: \u003cyour_bucket_name\u003e s3_key: \u003cyour_access_key\u003e s3_key_secret: \u003cyour_secret_key\u003e s3_uri_style: host path: /pgbackrest bundle: y # Bundle small files into a single file bundle_limit: 20MiB # Bundle size limit, recommended 20MiB for object storage bundle_size: 128MiB # Bundle target size, recommended 128MiB for object storage cipher_type: aes-256-cbc # Enable AES encryption for remote backup repository cipher_pass: pgBackRest # AES encryption password, defaults to 'pgBackRest' retention_full_type: time # Retain full backups by time retention_full: 14 # Keep full backups from the last 14 days local: # Default pgbackrest repository using local POSIX filesystem path: /pg/backup # Local backup directory, defaults to `/pg/backup` retention_full_type: count # Retain full backups by count retention_full: 2 # Keep 2, up to 3 full backups when using local filesystem repository Managing Backups Enable Backup If pgbackrest_enabled is set to true when the database cluster is created, backups will be automatically enabled.\nIf this value was false at creation time, you can enable the pgbackrest component with the following command:\n./pgsql.yml -t pg_backup # Run pgbackrest subtask Remove Backup When removing the primary instance (pg_role = primary), Pigsty will delete the pgbackrest backup stanza.\n./pgsql-rm.yml ./pgsql-rm.yml -e pg_rm_backup=false # Keep backups ./pgsql-rm.yml -t pg_backup # Remove backups only Use the pg_backup subtask to remove backups only, and the pg_rm_backup parameter (set to false) to preserve backups.\nIf your backup repository is locked (e.g., S3 / MinIO has locking options), this operation will fail.\nBackup Deletion Deleting backups may result in permanent data loss. This is a dangerous operation, please proceed with caution.\nList Backups This command will list all backups in the pgbackrest repository (shared across all clusters)\npgbackrest info Manual Backup Pigsty provides a built-in script /pg/bin/pg-backup that wraps the pgbackrest backup command.\npg-backup # Perform incremental backup pg-backup full # Perform full backup pg-backup incr # Perform incremental backup pg-backup diff # Perform differential backup Base Backup Pigsty provides an alternative backup script /pg/bin/pg-basebackup that does not depend on pgbackrest and directly provides a physical copy of the database cluster. The default backup directory is /pg/backup.\npg-basebackup help backup NAME pg-basebackup -- make base backup from PostgreSQL instance SYNOPSIS pg-basebackup -sdfeukr pg-basebackup --src postgres:/// --dst . --file backup.tar.lz4 DESCRIPTION -s, --src, --url Backup source URL, optional, defaults to \"postgres:///\", password should be provided in url, ENV, or .pgpass if required -d, --dst, --dir Location to store backup file, defaults to \"/pg/backup\" -f, --file Override default backup filename, \"backup_${tag}_${date}.tar.lz4\" -r, --remove Remove .lz4 files older than n minutes, defaults to 1200 (20 hours) -t, --tag Backup file tag, uses target cluster name or local IP address if not set, also used for default filename -k, --key Encryption key when --encrypt is specified, defaults to ${tag} -u, --upload Upload backup file to cloud storage (needs to be implemented by yourself) -e, --encryption Use OpenSSL RC4 encryption, uses tag as key if not specified -h, --help Print this help information postgres@pg-meta-1:~$ pg-basebackup [2025-07-13 06:16:05][INFO] ================================================================ [2025-07-13 06:16:05][INFO] [INIT] pg-basebackup begin, checking parameters [2025-07-13 06:16:05][DEBUG] [INIT] filename (-f) : backup_pg-meta_20250713.tar.lz4 [2025-07-13 06:16:05][DEBUG] [INIT] src (-s) : postgres:/// [2025-07-13 06:16:05][DEBUG] [INIT] dst (-d) : /pg/backup [2025-07-13 06:16:05][INFO] [LOCK] lock acquired success on /tmp/backup.lock, pid=107417 [2025-07-13 06:16:05][INFO] [BKUP] backup begin, from postgres:/// to /pg/backup/backup_pg-meta_20250713.tar.lz4 pg_basebackup: initiating base backup, waiting for checkpoint to complete pg_basebackup: checkpoint completed pg_basebackup: write-ahead log start point: 0/7000028 on timeline 1 pg_basebackup: write-ahead log end point: 0/7000FD8 pg_basebackup: syncing data to disk ... pg_basebackup: base backup completed [2025-07-13 06:16:06][INFO] [BKUP] backup complete! [2025-07-13 06:16:06][INFO] [DONE] backup procedure complete! [2025-07-13 06:16:06][INFO] ================================================================ The backup uses lz4 compression. You can decompress and extract the tarball with the following command:\nmkdir -p /tmp/data # Extract backup to this directory cat /pg/backup/backup_pg-meta_20250713.tar.lz4 | unlz4 -d -c | tar -xC /tmp/data Logical Backup You can also perform logical backups using the pg_dump command.\nLogical backups cannot be used for PITR (Point-in-Time Recovery), but are very useful for migrating data between different major versions or implementing flexible data export logic.\nBootstrap from Repository Suppose you have an existing cluster pg-meta and want to clone it as pg-meta2:\nYou need to create a new pg-meta2 cluster branch and then run pitr on it.\n","categories":["Task"],"description":"PostgreSQL backup storage repository configuration","excerpt":"PostgreSQL backup storage repository configuration","ref":"/docs/pgsql/backup/repository/","tags":"","title":"Backup Repository"},{"body":"Enable Backup If pgbackrest_enabled is set to true when the database cluster is created, backups will be automatically enabled.\nIf this value was false at creation time, you can enable the pgbackrest component with the following command:\n./pgsql.yml -t pg_backup # Run pgbackrest subtask Remove Backup When removing the primary instance (pg_role = primary), Pigsty will delete the pgbackrest backup stanza.\n./pgsql-rm.yml ./pgsql-rm.yml -e pg_rm_backup=false # Keep backups ./pgsql-rm.yml -t pg_backup # Remove backups only Use the pg_backup subtask to remove backups only, and the pg_rm_backup parameter (set to false) to preserve backups.\nIf your backup repository is locked (e.g., S3 / MinIO has locking options), this operation will fail.\nBackup Deletion Deleting backups may result in permanent data loss. This is a dangerous operation, please proceed with caution.\nList Backups This command will list all backups in the pgbackrest repository (shared across all clusters)\npgbackrest info Manual Backup Pigsty provides a built-in script /pg/bin/pg-backup that wraps the pgbackrest backup command.\npg-backup # Perform incremental backup pg-backup full # Perform full backup pg-backup incr # Perform incremental backup pg-backup diff # Perform differential backup Base Backup Pigsty provides an alternative backup script /pg/bin/pg-basebackup that does not depend on pgbackrest and directly provides a physical copy of the database cluster. The default backup directory is /pg/backup.\npg-basebackup help backup NAME pg-basebackup -- make base backup from PostgreSQL instance SYNOPSIS pg-basebackup -sdfeukr pg-basebackup --src postgres:/// --dst . --file backup.tar.lz4 DESCRIPTION -s, --src, --url Backup source URL, optional, defaults to \"postgres:///\", password should be provided in url, ENV, or .pgpass if required -d, --dst, --dir Location to store backup file, defaults to \"/pg/backup\" -f, --file Override default backup filename, \"backup_${tag}_${date}.tar.lz4\" -r, --remove Remove .lz4 files older than n minutes, defaults to 1200 (20 hours) -t, --tag Backup file tag, uses target cluster name or local IP address if not set, also used for default filename -k, --key Encryption key when --encrypt is specified, defaults to ${tag} -u, --upload Upload backup file to cloud storage (needs to be implemented by yourself) -e, --encryption Use OpenSSL RC4 encryption, uses tag as key if not specified -h, --help Print this help information postgres@pg-meta-1:~$ pg-basebackup [2025-07-13 06:16:05][INFO] ================================================================ [2025-07-13 06:16:05][INFO] [INIT] pg-basebackup begin, checking parameters [2025-07-13 06:16:05][DEBUG] [INIT] filename (-f) : backup_pg-meta_20250713.tar.lz4 [2025-07-13 06:16:05][DEBUG] [INIT] src (-s) : postgres:/// [2025-07-13 06:16:05][DEBUG] [INIT] dst (-d) : /pg/backup [2025-07-13 06:16:05][INFO] [LOCK] lock acquired success on /tmp/backup.lock, pid=107417 [2025-07-13 06:16:05][INFO] [BKUP] backup begin, from postgres:/// to /pg/backup/backup_pg-meta_20250713.tar.lz4 pg_basebackup: initiating base backup, waiting for checkpoint to complete pg_basebackup: checkpoint completed pg_basebackup: write-ahead log start point: 0/7000028 on timeline 1 pg_basebackup: write-ahead log end point: 0/7000FD8 pg_basebackup: syncing data to disk ... pg_basebackup: base backup completed [2025-07-13 06:16:06][INFO] [BKUP] backup complete! [2025-07-13 06:16:06][INFO] [DONE] backup procedure complete! [2025-07-13 06:16:06][INFO] ================================================================ The backup uses lz4 compression. You can decompress and extract the tarball with the following command:\nmkdir -p /tmp/data # Extract backup to this directory cat /pg/backup/backup_pg-meta_20250713.tar.lz4 | unlz4 -d -c | tar -xC /tmp/data Logical Backup You can also perform logical backups using the pg_dump command.\nLogical backups cannot be used for PITR (Point-in-Time Recovery), but are very useful for migrating data between different major versions or implementing flexible data export logic.\nBootstrap from Repository Suppose you have an existing cluster pg-meta and want to clone it as pg-meta2:\nYou need to create a new pg-meta2 cluster branch and then run pitr on it.\n","categories":["Task"],"description":"Managing backup repositories and backups","excerpt":"Managing backup repositories and backups","ref":"/docs/pgsql/backup/admin/","tags":"","title":"Admin Commands"},{"body":"You can perform Point-in-Time Recovery (PITR) in Pigsty using pre-configured pgbackrest.\nManual Approach: Manually execute PITR using pg-pitr prompt scripts, more flexible but more complex. Playbook Approach: Automatically execute PITR using pgsql-pitr.yml playbook, highly automated but less flexible and error-prone. If you are very familiar with the configuration, you can use the fully automated playbook, otherwise manual step-by-step operation is recommended.\nQuick Start If you want to roll back the pg-meta cluster to a previous point in time, add the pg_pitr parameter:\npg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta2 pg_pitr: { time: '2025-07-13 10:00:00+00' } # Recover from latest backup Then run the pgsql-pitr.yml playbook, which will roll back the pg-meta cluster to the specified point in time.\n./pgsql-pitr.yml -l pg-meta Post-Recovery The recovered cluster will have archive_mode disabled to prevent accidental WAL writes. If the recovered database state is normal, you can enable archive_mode and perform a full backup.\npsql -c 'ALTER SYSTEM RESET archive_mode; SELECT pg_reload_conf();' pg-backup full # Perform new full backup Recovery Target You can specify different types of recovery targets in pg_pitr, but they are mutually exclusive:\ntime: To which point in time to recover? name: Recover to a named restore point (created by pg_create_restore_point) xid: Recover to a specific transaction ID (TXID/XID) lsn: Recover to a specific LSN (Log Sequence Number) point If any of the above parameters are specified, the recovery type will be set accordingly, otherwise it will be set to latest (end of WAL archive stream). The special immediate type can be used to instruct pgbackrest to minimize recovery time by stopping at the first consistent point.\nTarget Types Recovery Target Types latest time lsn xid name immediate pg_pitr: { } # Recover to latest state (end of WAL archive stream) pg_pitr: { time: \"2025-07-13 10:00:00+00\" } pg_pitr: { lsn: \"0/4001C80\" } pg_pitr: { xid: \"250000\" } pg_pitr: { name: \"some_restore_point\" } pg_pitr: { type: \"immediate\" } Recover by Time The most commonly used target is a point in time; you can specify the time point to recover to:\n./pgsql-pitr.yml -e '{\"pg_pitr\": { \"time\": \"2025-07-13 10:00:00+00\" }}' Time should be in valid PostgreSQL TIMESTAMP format, YYYY-MM-DD HH:MM:SS+TZ is recommended.\nRecover by Name You can create named restore points using pg_create_restore_point:\nSELECT pg_create_restore_point('shit_incoming'); Then use that named restore point in PITR:\n./pgsql-pitr.yml -e '{\"pg_pitr\": { \"name\": \"shit_incoming\" }}' Recover by XID If you have a transaction that accidentally deleted some data, the best way to recover is to restore the database to the state before that transaction.\n./pgsql-pitr.yml -e '{\"pg_pitr\": { \"xid\": \"250000\", exclusive: true }}' You can find the exact transaction ID from monitoring dashboards or from the TXID field in CSVLOG.\nInclusive vs Exclusive Target parameters are “inclusive” by default, meaning recovery will include the target point. The exclusive flag will exclude that exact target, e.g., xid 24999 will be the last transaction replayed.\nThis only applies to time, xid, lsn recovery targets, see recovery_target_inclusive for details.\nRecover by LSN PostgreSQL uses LSN (Log Sequence Number) to identify the location of WAL records. You can find it in many places, such as the PG LSN panel in Pigsty dashboards.\n./pgsql-pitr.yml -e '{\"pg_pitr\": { \"lsn\": \"0/4001C80\", timeline: \"1\" }}' To recover to an exact position in the WAL stream, you can also specify the timeline parameter (defaults to latest)\nRecovery Source cluster: From which cluster to recover? Defaults to current pg_cluster, you can use any other cluster in the same pgbackrest repository repo: Override backup repository, uses same format as pgbackrest_repo set: Defaults to latest backup set, but you can specify a specific pgbackrest backup by label Pigsty will recover from the pgbackrest backup repository. If you use a centralized backup repository (like MinIO/S3), you can specify another “stanza” (another cluster’s backup directory) as the recovery source.\npg-meta2: hosts: { 10.10.10.11: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta2 pg_pitr: { cluster: pg-meta } # Recover from pg-meta cluster backup The above configuration will mark the PITR process to use the pg-meta stanza. You can also pass the pg_pitr parameter via CLI arguments:\n./pgsql-pitr.yml -l pg-meta2 -e '{\"pg_pitr\": { \"cluster\": \"pg-meta\" }}' You can also use these targets when PITR from another cluster:\n./pgsql-pitr.yml -l pg-meta2 -e '{\"pg_pitr\": { \"cluster\": \"pg-meta\", \"time\": \"2025-07-14 08:00:00+00\" }}' Step-by-Step Execution This approach is semi-automatic, you will participate in the PITR process to make critical decisions.\nFor example, this configuration will restore the pg-meta cluster itself to the specified point in time:\npg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta2 pg_pitr: { time: '2025-07-13 10:00:00+00' } # Recover from latest backup Let’s execute step by step:\n./pgsql-pitr.yml -l pg-meta -t down # Pause patroni high availability ./pgsql-pitr.yml -l pg-meta -t pitr # Run pitr process ./pgsql-pitr.yml -l pg-meta -t up # Generate pgbackrest config and recovery script # down : # Stop high availability and shutdown patroni and postgres # - pause : # Pause patroni auto-failover # - stop : # Stop patroni and postgres services # - stop_patroni : # Stop patroni service # - stop_postgres : # Stop postgres service # pitr : # Perform PITR process # - config : # Generate pgbackrest config and recovery script # - restore : # Run pgbackrest restore command # - recovery : # Start postgres and complete recovery # - verify : # Verify recovered cluster control data # up: : # Start postgres / patroni and restore high availability # - etcd : # Clean etcd metadata before starting # - start : # Start patroni and postgres services # - start_postgres : # Start postgres service # - start_patroni : # Start patroni service # - resume : # Resume patroni auto-failover PITR Parameter Definition The pg_pitr parameter has more options available:\npg_pitr: # Define PITR task cluster: \"some_pg_cls_name\" # Source cluster name type: latest # Recovery target type: time, xid, name, lsn, immediate, latest time: \"2025-01-01 10:00:00+00\" # Recovery target: time, mutually exclusive with xid, name, lsn name: \"some_restore_point\" # Recovery target: named restore point, mutually exclusive with time, xid, lsn xid: \"100000\" # Recovery target: transaction ID, mutually exclusive with time, name, lsn lsn: \"0/3000000\" # Recovery target: log sequence number, mutually exclusive with time, name, xid timeline: latest # Target timeline, can be integer, defaults to latest exclusive: false # Whether to exclude target point, defaults to false action: pause # Post-recovery action: pause, promote, shutdown archive: false # Whether to keep archive settings? Defaults to false db_exclude: [ template0, template1 ] db_include: [] link_map: pg_wal: '/data/wal' pg_xact: '/data/pg_xact' process: 4 # Number of parallel recovery processes repo: {} # Recovery source repository data: /pg/data # Data recovery location port: 5432 # Listening port for recovered instance ","categories":["Task"],"description":"Restore PostgreSQL from backups","excerpt":"Restore PostgreSQL from backups","ref":"/docs/pgsql/backup/restore/","tags":"","title":"Restore Operations"},{"body":"Quick Start Create an online replica of an existing cluster using Standby Cluster Create a point-in-time snapshot of an existing cluster using PITR Perform post-PITR cleanup to ensure the new cluster’s backup process works properly You can use the PG PITR mechanism to clone an entire database cluster.\nReset a Cluster’s State You can also consider creating a brand new empty cluster, then use PITR to reset it to a specific state of the pg-meta cluster.\nUsing this technique, you can clone any point-in-time (within backup retention period) state of the existing cluster pg-meta to a new cluster.\nUsing the Pigsty 4-node sandbox environment as an example, use the following command to reset the pg-test cluster to the latest state of the pg-meta cluster:\n./pgsql-pitr.yml -l pg-test -e '{\"pg_pitr\": { \"cluster\": \"pg-meta\" }}' Post-PITR Cleanup When you restore a cluster using PITR, the new cluster’s PITR functionality is disabled. This is because if it also tries to generate backups and archive WAL, it could dirty the backup repository of the previous cluster.\nTherefore, after confirming that the state of this PITR-restored new cluster meets expectations, you need to perform the following cleanup:\nUpgrade the backup repository Stanza to accept new backups from different clusters (only when restoring from another cluster) Enable archive_mode to allow the new cluster to archive WAL logs (requires cluster restart) Perform a new full backup to ensure the new cluster’s data is included (optional, can also wait for crontab scheduled execution) pb stanza-upgrade psql -c 'ALTER SYSTEM RESET archive_mode;' pg-backup full Through these operations, your new cluster will have its own backup history starting from the first full backup. If you skip these steps, the new cluster’s backups will not work, and WAL archiving will not take effect, meaning you cannot perform any backup or PITR operations on the new cluster.\nConsequences of Not Cleaning Up Suppose you performed PITR recovery on the pg-test cluster using data from another cluster pg-meta, but did not perform cleanup.\nThen at the next routine backup, you will see the following error:\npostgres@pg-test-1:~$ pb backup 2025-12-27 10:20:29.336 P00 INFO: backup command begin... 2025-12-27 10:20:29.357 P00 ERROR: [051]: PostgreSQL version 18, system-id 7588470953413201282 do not match stanza version 18, system-id 7588470974940466058 HINT: is this the correct stanza? Clone a New Cluster For example, suppose you have a cluster pg-meta, and now you want to clone a new cluster pg-meta2 from pg-meta.\nYou can consider using the Standby Cluster method to create a new cluster pg-meta2.\npgBackrest supports incremental backup/restore, so if you have already pulled pg-meta’s data through physical replication, the incremental PITR restore is usually very fast.\npb stop --force pb stanza-delete --force pb start pb stanza-create If you want to reset the pg-test cluster to the state of pg-meta cluster at 15:30 on December 26, 2025, you can use the following command:\n./pgsql-pitr.yml -l pg-test -e '{\"pg_pitr\": { \"cluster\": \"pg-meta\", \"time\": \"2025-12-27 17:50:00+08\" ,archive: true }}' Using this technique, you can not only clone the latest state of the pg-meta cluster, but also clone to any point in time.\n","categories":["Task"],"description":"How to use PITR to create a new PostgreSQL cluster and restore to a specified point in time?","excerpt":"How to use PITR to create a new PostgreSQL cluster and restore to a …","ref":"/docs/pgsql/backup/cluster/","tags":"","title":"Clone PG Cluster"},{"body":"Pigsty provides two utility scripts for quickly cloning instances and performing point-in-time recovery on the same machine:\npg-fork: Quickly clone a new PostgreSQL instance on the same machine pg-pitr: Manually perform point-in-time recovery using pgbackrest These two scripts can be used together: first use pg-fork to clone the instance, then use pg-pitr to restore the cloned instance to a specified point in time.\npg-fork pg-fork can quickly clone a new PostgreSQL instance on the same machine.\nQuick Start Execute the following command as the postgres user (dbsu) to create a new instance:\npg-fork 1 # Clone from /pg/data to /pg/data1, port 15432 pg-fork 2 -d /pg/data1 # Clone from /pg/data1 to /pg/data2, port 25432 pg-fork 3 -D /tmp/test -P 5555 # Clone to custom directory and port After cloning, start and access the new instance:\npg_ctl -D /pg/data1 start # Start cloned instance psql -p 15432 # Connect to cloned instance Command Syntax pg-fork \u003cFORK_ID\u003e [options] Required Parameters:\nParameter Description \u003cFORK_ID\u003e Clone instance number (1-9), determines default port and data directory Optional Parameters:\nParameter Description Default -d, --data \u003cdatadir\u003e Source instance data directory /pg/data or $PG_DATA -D, --dst \u003cdst_dir\u003e Target data directory /pg/data\u003cFORK_ID\u003e -p, --port \u003cport\u003e Source instance port 5432 or $PG_PORT -P, --dst-port \u003cport\u003e Target instance port \u003cFORK_ID\u003e5432 -s, --skip Skip backup API, use cold copy mode - -y, --yes Skip confirmation prompts - -h, --help Show help information - How It Works pg-fork supports two working modes:\nHot Backup Mode (default, source instance running):\nCall pg_backup_start() to start backup Use cp --reflink=auto to copy data directory Call pg_backup_stop() to end backup Modify configuration files to avoid conflicts with source instance Cold Copy Mode (using -s parameter or source instance not running):\nDirectly use cp --reflink=auto to copy data directory Modify configuration files If you use XFS (with reflink enabled), Btrfs, or ZFS file systems, pg-fork will leverage Copy-on-Write features. The data directory copy completes in a few hundred milliseconds and takes almost no additional storage space.\npg-pitr pg-pitr is a script for manually performing point-in-time recovery, based on pgbackrest.\nQuick Start pg-pitr -d # Restore to latest state pg-pitr -i # Restore to backup completion time pg-pitr -t \"2025-01-01 12:00:00+08\" # Restore to specified time point pg-pitr -n my-savepoint # Restore to named restore point pg-pitr -l \"0/7C82CB8\" # Restore to specified LSN pg-pitr -x 12345678 -X # Restore to before transaction pg-pitr -b 20251225-120000F # Restore to specified backup set Command Syntax pg-pitr [options] [recovery_target] Recovery Target (choose one):\nParameter Description -d, --default Restore to end of WAL archive stream (latest state) -i, --immediate Restore to database consistency point (fastest recovery) -t, --time \u003ctimestamp\u003e Restore to specified time point -n, --name \u003crestore_point\u003e Restore to named restore point -l, --lsn \u003clsn\u003e Restore to specified LSN -x, --xid \u003cxid\u003e Restore to specified transaction ID -b, --backup \u003clabel\u003e Restore to specified backup set Optional Parameters:\nParameter Description Default -D, --data \u003cpath\u003e Recovery target data directory /pg/data -s, --stanza \u003cname\u003e pgbackrest stanza name Auto-detect -X, --exclusive Exclude target point (restore to before target) - -P, --promote Auto-promote after recovery (default pauses) - -c, --check Dry run mode, only print commands - -y, --yes Skip confirmation and countdown - Post-Recovery Processing After recovery completes, the instance will be in recovery paused state (unless -P parameter is used). You need to:\nStart instance: pg_ctl -D /pg/data start Verify data: Check if data meets expectations Promote instance: pg_ctl -D /pg/data promote Enable archiving: psql -c \"ALTER SYSTEM SET archive_mode = on;\" Restart instance: pg_ctl -D /pg/data restart Execute backup: pg-backup full Combined Usage pg-fork and pg-pitr can be combined for a safe PITR verification workflow:\n# 1. Clone current instance pg-fork 1 -y # 2. Execute PITR on cloned instance (doesn't affect production) pg-pitr -D /pg/data1 -t \"2025-12-27 10:00:00+08\" # 3. Start cloned instance pg_ctl -D /pg/data1 start # 4. Verify recovery results psql -p 15432 -c \"SELECT count(*) FROM orders WHERE created_at \u003c '2025-12-27 10:00:00';\" # 5. After confirmation, you can choose: # - Option A: Execute the same PITR on production instance # - Option B: Promote cloned instance as new production instance # 6. Clean up test instance pg_ctl -D /pg/data1 stop rm -rf /pg/data1 Notes Runtime Requirements Must be executed as postgres user (or postgres group member) pg-pitr requires stopping target instance’s PostgreSQL before execution pg-fork hot backup mode requires source instance to be running File System XFS (with reflink enabled) or Btrfs file system recommended Cloning on CoW file systems is almost instant and takes no extra space Non-CoW file systems will perform full copy, taking longer Port Planning FORK_ID Default Port Default Data Directory 1 15432 /pg/data1 2 25432 /pg/data2 3 35432 /pg/data3 … … … 9 95432 /pg/data9 ","categories":["Task"],"description":"Clone instances and perform point-in-time recovery on the same machine","excerpt":"Clone instances and perform point-in-time recovery on the same machine","ref":"/docs/pgsql/tutorial/instance/","tags":"","title":"Instance Recovery"},{"body":"Clone Database You can copy a PostgreSQL database through the template mechanism, but no active connections to the template database are allowed during this period.\nIf you want to clone the postgres database, you must execute the following two statements at the same time. Ensure all connections to the postgres database are cleaned up before executing Clone:\nSELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = 'postgres'; CREATE DATABASE pgcopy TEMPLATE postgres STRATEGY FILE_COPY; Instant Clone If you are using PostgreSQL 18 or higher, Pigsty sets file_copy_method by default. This parameter allows you to clone a database in O(1) (~200ms) time complexity without copying data files.\nHowever, you must explicitly use the FILE_COPY strategy to create the database. Since the STRATEGY parameter of CREATE DATABASE was introduced in PostgreSQL 15, the default value has been WAL_LOG. You need to explicitly specify FILE_COPY for instant cloning.\nSELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = 'meta'; CREATE DATABASE pgcopy TEMPLATE meta STRATEGY FILE_COPY; For example, cloning a 30 GB database: normal clone (WAL_LOG) takes 18 seconds, while instant clone (FILE_COPY) only needs constant time of 200 milliseconds.\nHowever, you still need to ensure no active connections to the template database during cloning, but this time can be very short, making it practical for production environments.\nIf you need a new database copy for testing or development, instant cloning is an excellent choice. It doesn’t introduce additional storage overhead because it uses the file system’s CoW (Copy on Write) mechanism.\nSince Pigsty v4.0, you can use strategy: FILE_COPY in the pg_databases parameter to achieve instant database cloning.\npg-meta: hosts: 10.10.10.10: { pg_seq: 1, pg_role: primary } vars: pg_cluster: pg-meta pg_version: 18 pg_databases: - name: meta - name: meta_dev template: meta strategy: FILE_COPY # \u003c---- Introduced in PG 15, instant in PG18 #comment: \"meta clone\" # \u003c---- Database comment #pgbouncer: false # \u003c---- Not added to connection pool? #register_datasource: false # \u003c---- Not added to Grafana datasource? After configuration, use the standard database creation SOP to create the database:\nbin/pgsql-db pg-meta meta_dev Limitations and Notes This feature is only available on supported file systems (xfs, btrfs, zfs, apfs). If the file system doesn’t support it, PostgreSQL will fail with an error.\nBy default, mainstream OS distributions’ xfs have reflink=1 enabled by default, so you don’t need to worry about this in most cases.\nOpenZFS requires explicit configuration to support CoW, but due to prior data corruption incidents, it’s not recommended for production use.\nIf your PostgreSQL version is below 15, specifying strategy will have no effect.\nPlease don’t use the postgres database as a template database for cloning, as management connections typically connect to the postgres database, which prevents the cloning operation.\nUse instant cloning with caution in extremely high concurrency/throughput production environments, as it requires clearing all connections to the template database within the cloning window (200ms), otherwise the clone will fail.\n","categories":["Task"],"description":"How to clone an existing database within a PostgreSQL cluster using instant XFS cloning","excerpt":"How to clone an existing database within a PostgreSQL cluster using …","ref":"/docs/pgsql/backup/database/","tags":"","title":"Clone Database"},{"body":"Pigsty uses pgBackRest to manage PostgreSQL backups, arguably the most powerful open-source backup tool in the ecosystem. It supports incremental/parallel backup and restore, encryption, MinIO/S3, and many other features. Pigsty configures backup functionality by default for each PGSQL cluster.\nSection Content Mechanism Backup scripts, cron jobs, pgbackrest, repository and management Policy Backup strategy, disk planning, recovery window tradeoffs Repository Configuring backup repositories: local, MinIO, S3 Admin Common backup management commands Restore Restore to a specific point in time using playbooks Example Sandbox example: performing restore operations manually Disclaimer Pigsty makes every effort to provide a reliable PITR solution, but we accept no responsibility for data loss resulting from PITR operations. Use at your own risk. If you need professional support, please consider our professional services.\nQuick Start Backup Policy: Schedule base backups using Crontab WAL Archiving: Continuously record write activity Restore \u0026 Recovery: Recover from backups and WAL archives node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] ./pgsql-pitr.yml -e '{\"pg_pitr\": { \"time\": \"2025-07-13 10:00:00+00\" }}' ","categories":["Task","Reference"],"description":"Point-in-Time Recovery (PITR) Backup and Restore","excerpt":"Point-in-Time Recovery (PITR) Backup and Restore","ref":"/docs/pgsql/backup/","tags":"","title":"Backup \u0026 Restore"},{"body":" In this context, users refer to logical objects within a database cluster created using the SQL commands CREATE USER/ROLE.\nIn PostgreSQL, users belong directly to the database cluster rather than to a specific database. Therefore, when creating business databases and business users, you should follow the principle of “users first, then databases.”\nDefining Users Pigsty defines roles and users in database clusters through two configuration parameters:\npg_default_roles: Defines globally unified roles and users pg_users: Defines business users and roles at the database cluster level The former defines roles and users shared across the entire environment, while the latter defines business roles and users specific to individual clusters. Both have the same format and are arrays of user definition objects.\nYou can define multiple users/roles, and they will be created sequentially—first global, then cluster-level, and finally in array order—so later users can belong to roles defined earlier.\nHere is the business user definition for the default cluster pg-meta in the Pigsty demo environment:\npg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta pg_users: - {name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [dbrole_admin] ,comment: pigsty admin user } - {name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [dbrole_readonly] ,comment: read-only viewer for meta database } - {name: dbuser_grafana ,password: DBUser.Grafana ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for grafana database } - {name: dbuser_bytebase ,password: DBUser.Bytebase ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for bytebase database } - {name: dbuser_kong ,password: DBUser.Kong ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for kong api gateway } - {name: dbuser_gitea ,password: DBUser.Gitea ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for gitea service } - {name: dbuser_wiki ,password: DBUser.Wiki ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for wiki.js service } - {name: dbuser_noco ,password: DBUser.Noco ,pgbouncer: true ,roles: [dbrole_admin] ,comment: admin user for nocodb service } Each user/role definition is an object that may include the following fields. Using dbuser_meta as an example:\n- name: dbuser_meta # Required, `name` is the only mandatory field in user definition password: DBUser.Meta # Optional, password can be scram-sha-256 hash string or plaintext login: true # Optional, can login by default superuser: false # Optional, default is false, is this a superuser? createdb: false # Optional, default is false, can create databases? createrole: false # Optional, default is false, can create roles? inherit: true # Optional, by default this role can use inherited privileges? replication: false # Optional, default is false, can this role perform replication? bypassrls: false # Optional, default is false, can this role bypass row-level security? pgbouncer: true # Optional, default is false, add this user to pgbouncer user list? (production users using connection pool should explicitly set to true) connlimit: -1 # Optional, user connection limit, default -1 disables limit expire_in: 3650 # Optional, this role expires: calculated from creation + n days (higher priority than expire_at) expire_at: '2030-12-31' # Optional, when this role expires, use YYYY-MM-DD format string to specify a date (lower priority than expire_in) comment: pigsty admin user # Optional, description and comment string for this user/role roles: [dbrole_admin] # Optional, default roles are: dbrole_{admin,readonly,readwrite,offline} parameters: {} # Optional, use `ALTER ROLE SET` to configure role-level database parameters for this role pool_mode: transaction # Optional, pgbouncer pool mode defaulting to transaction, user level pool_connlimit: -1 # Optional, user-level maximum database connections, default -1 disables limit search_path: public # Optional, key-value configuration parameters per postgresql documentation (e.g., use pigsty as default search_path) The only required field is name, which should be a valid and unique username in the PostgreSQL cluster. Roles don’t need a password, but for loginable business users, a password is usually required. password can be plaintext or scram-sha-256 / md5 hash string; please avoid using plaintext passwords. Users/roles are created one by one in array order, so ensure roles/groups are defined before their members. login, superuser, createdb, createrole, inherit, replication, bypassrls are boolean flags. pgbouncer is disabled by default: to add business users to the pgbouncer user list, you should explicitly set it to true. ACL System\nPigsty has a built-in, out-of-the-box access control / ACL system. You can easily use it by simply assigning the following four default roles to business users:\ndbrole_readwrite: Role with global read-write access (production accounts primarily used by business should have database read-write privileges) dbrole_readonly: Role with global read-only access (if other businesses need read-only access, use this role) dbrole_admin: Role with DDL privileges (business administrators, scenarios requiring table creation in applications) dbrole_offline: Restricted read-only access role (can only access offline instances, typically for individual users) If you want to redesign your own ACL system, consider customizing the following parameters and templates:\npg_default_roles: System-wide roles and global users pg_default_privileges: Default privileges for newly created objects roles/pgsql/templates/pg-init-role.sql: Role creation SQL template roles/pgsql/templates/pg-init-template.sql: Privilege SQL template Creating Users Users and roles defined in pg_default_roles and pg_users are automatically created one by one during the cluster initialization PROVISION phase. If you want to create users on an existing cluster, you can use the bin/pgsql-user tool. Add the new user/role definition to all.children.\u003ccls\u003e.pg_users and use the following method to create the user:\nbin/pgsql-user \u003ccls\u003e \u003cusername\u003e # pgsql-user.yml -l \u003ccls\u003e -e username=\u003cusername\u003e Unlike databases, the user creation playbook is always idempotent. When the target user already exists, Pigsty will modify the target user’s attributes to match the configuration. So running it repeatedly on existing clusters is usually not a problem.\nPlease Use Playbooks to Create Users We don’t recommend manually creating new business users, especially when you want the user to use the default pgbouncer connection pool: unless you’re willing to manually maintain the user list in Pgbouncer and keep it consistent with PostgreSQL. When creating new users with bin/pgsql-user tool or pgsql-user.yml playbook, the user will also be added to the Pgbouncer Users list.\nModifying Users The method for modifying PostgreSQL user attributes is the same as Creating Users.\nFirst, adjust your user definition, modify the attributes that need adjustment, then execute the following command to apply:\nbin/pgsql-user \u003ccls\u003e \u003cusername\u003e # pgsql-user.yml -l \u003ccls\u003e -e username=\u003cusername\u003e Note that modifying users will not delete users, but modify user attributes through the ALTER USER command; it also won’t revoke user privileges and groups, and will use the GRANT command to grant new roles.\nPgbouncer Users Pgbouncer is enabled by default and serves as a connection pool middleware, with its users managed by default.\nPigsty adds all users in pg_users that explicitly have the pgbouncer: true flag to the pgbouncer user list.\nUsers in the Pgbouncer connection pool are listed in /etc/pgbouncer/userlist.txt:\n\"postgres\" \"\" \"dbuser_wiki\" \"SCRAM-SHA-256$4096:+77dyhrPeFDT/TptHs7/7Q==$KeatuohpKIYzHPCt/tqBu85vI11o9mar/by0hHYM2W8=:X9gig4JtjoS8Y/o1vQsIX/gY1Fns8ynTXkbWOjUfbRQ=\" \"dbuser_view\" \"SCRAM-SHA-256$4096:DFoZHU/DXsHL8MJ8regdEw==$gx9sUGgpVpdSM4o6A2R9PKAUkAsRPLhLoBDLBUYtKS0=:MujSgKe6rxcIUMv4GnyXJmV0YNbf39uFRZv724+X1FE=\" \"dbuser_monitor\" \"SCRAM-SHA-256$4096:fwU97ZMO/KR0ScHO5+UuBg==$CrNsmGrx1DkIGrtrD1Wjexb/aygzqQdirTO1oBZROPY=:L8+dJ+fqlMQh7y4PmVR/gbAOvYWOr+KINjeMZ8LlFww=\" \"dbuser_meta\" \"SCRAM-SHA-256$4096:leB2RQPcw1OIiRnPnOMUEg==$eyC+NIMKeoTxshJu314+BmbMFpCcspzI3UFZ1RYfNyU=:fJgXcykVPvOfro2MWNkl5q38oz21nSl1dTtM65uYR1Q=\" \"dbuser_kong\" \"SCRAM-SHA-256$4096:bK8sLXIieMwFDz67/0dqXQ==$P/tCRgyKx9MC9LH3ErnKsnlOqgNd/nn2RyvThyiK6e4=:CDM8QZNHBdPf97ztusgnE7olaKDNHBN0WeAbP/nzu5A=\" \"dbuser_grafana\" \"SCRAM-SHA-256$4096:HjLdGaGmeIAGdWyn2gDt/Q==$jgoyOB8ugoce+Wqjr0EwFf8NaIEMtiTuQTg1iEJs9BM=:ed4HUFqLyB4YpRr+y25FBT7KnlFDnan6JPVT9imxzA4=\" \"dbuser_gitea\" \"SCRAM-SHA-256$4096:l1DBGCc4dtircZ8O8Fbzkw==$tpmGwgLuWPDog8IEKdsaDGtiPAxD16z09slvu+rHE74=:pYuFOSDuWSofpD9OZhG7oWvyAR0PQjJBffgHZLpLHds=\" \"dbuser_dba\" \"SCRAM-SHA-256$4096:zH8niABU7xmtblVUo2QFew==$Zj7/pq+ICZx7fDcXikiN7GLqkKFA+X5NsvAX6CMshF0=:pqevR2WpizjRecPIQjMZOm+Ap+x0kgPL2Iv5zHZs0+g=\" \"dbuser_bytebase\" \"SCRAM-SHA-256$4096:OMoTM9Zf8QcCCMD0svK5gg==$kMchqbf4iLK1U67pVOfGrERa/fY818AwqfBPhsTShNQ=:6HqWteN+AadrUnrgC0byr5A72noqnPugItQjOLFw0Wk=\" User-level connection pool parameters are maintained in a separate file: /etc/pgbouncer/useropts.txt, for example:\ndbuser_dba = pool_mode=session max_user_connections=16 dbuser_monitor = pool_mode=session max_user_connections=8 When you create a database, the Pgbouncer database list definition file will be refreshed and take effect through online configuration reload, without affecting existing connections.\nPgbouncer runs with the same dbsu as PostgreSQL, which defaults to the postgres operating system user. You can use the pgb alias to access pgbouncer management functions using the dbsu.\nPigsty also provides a utility function pgb-route that can quickly switch pgbouncer database traffic to other nodes in the cluster, useful for zero-downtime migration:\nThe connection pool user configuration files userlist.txt and useropts.txt are automatically refreshed when you create users, and take effect through online configuration reload, normally without affecting existing connections.\nNote that the pgbouncer_auth_query parameter allows you to use dynamic queries to complete connection pool user authentication—this is a compromise when you don’t want to manage users in the connection pool.\n","categories":["Reference"],"description":"Users/roles refer to logical objects within a database cluster created using the SQL commands `CREATE USER/ROLE`.","excerpt":"Users/roles refer to logical objects within a database cluster created …","ref":"/docs/pgsql/misc/user/","tags":["User"],"title":"User / Role"},{"body":" In this context, Database refers to the logical object created using the SQL command CREATE DATABASE within a database cluster.\nA PostgreSQL server can serve multiple databases simultaneously. In Pigsty, you can define the required databases in the cluster configuration.\nPigsty will modify and customize the default template database template1, creating default schemas, installing default extensions, and configuring default privileges. Newly created databases will inherit these settings from template1 by default.\nBy default, all business databases will be added to the Pgbouncer connection pool in a 1:1 manner; pg_exporter will use an auto-discovery mechanism to find all business databases and monitor objects within them.\nDefine Database Business databases are defined in the database cluster parameter pg_databases, which is an array of database definition objects. Databases in the array are created sequentially according to the definition order, so later defined databases can use previously defined databases as templates.\nBelow is the database definition for the default pg-meta cluster in the Pigsty demo environment:\npg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta pg_databases: - { name: meta ,baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [pigsty] ,extensions: [{name: postgis, schema: public}, {name: timescaledb}]} - { name: grafana ,owner: dbuser_grafana ,revokeconn: true ,comment: grafana primary database } - { name: bytebase ,owner: dbuser_bytebase ,revokeconn: true ,comment: bytebase primary database } - { name: kong ,owner: dbuser_kong ,revokeconn: true ,comment: kong the api gateway database } - { name: gitea ,owner: dbuser_gitea ,revokeconn: true ,comment: gitea meta database } - { name: wiki ,owner: dbuser_wiki ,revokeconn: true ,comment: wiki meta database } - { name: noco ,owner: dbuser_noco ,revokeconn: true ,comment: nocodb database } Each database definition is an object that may include the following fields, using the meta database as an example:\n- name: meta # REQUIRED, `name` is the only mandatory field of a database definition baseline: cmdb.sql # optional, database sql baseline path (relative path among ansible search path, e.g. files/) pgbouncer: true # optional, add this database to pgbouncer database list? true by default schemas: [pigsty] # optional, additional schemas to be created, array of schema names extensions: # optional, additional extensions to be installed: array of extension objects - { name: postgis , schema: public } # can specify which schema to install the extension in, or leave it unspecified (will install in the first schema of search_path) - { name: timescaledb } # for example, some extensions create and use fixed schemas, so no schema specification is needed. comment: pigsty meta database # optional, comment string for this database owner: postgres # optional, database owner, postgres by default template: template1 # optional, which template to use, template1 by default, target must be a template database encoding: UTF8 # optional, database encoding, UTF8 by default (MUST same as template database) locale: C # optional, database locale, C by default (MUST same as template database) lc_collate: C # optional, database collate, C by default (MUST same as template database), no reason not to recommend changing. lc_ctype: C # optional, database ctype, C by default (MUST same as template database) tablespace: pg_default # optional, default tablespace, 'pg_default' by default allowconn: true # optional, allow connection, true by default. false will disable connect at all revokeconn: false # optional, revoke public connection privilege. false by default, when set to true, CONNECT privilege will be revoked from users other than owner and admin register_datasource: true # optional, register this database to grafana datasources? true by default, explicitly set to false to skip registration connlimit: -1 # optional, database connection limit, default -1 disable limit, set to positive integer will limit connections pool_auth_user: dbuser_meta # optional, all connections to this pgbouncer database will be authenticated using this user (only useful when pgbouncer_auth_query is enabled) pool_mode: transaction # optional, pgbouncer pool mode at database level, default transaction pool_size: 64 # optional, pgbouncer pool size at database level, default 64 pool_size_reserve: 32 # optional, pgbouncer pool size reserve at database level, default 32, when default pool is insufficient, can request at most this many burst connections pool_size_min: 0 # optional, pgbouncer pool size min at database level, default 0 pool_max_db_conn: 100 # optional, max database connections at database level, default 100 The only required field is name, which should be a valid and unique database name in the current PostgreSQL cluster, other parameters have reasonable defaults.\nname: Database name, required. baseline: SQL file path (Ansible search path, usually in files), used to initialize database content. owner: Database owner, default is postgres template: Template used when creating the database, default is template1 encoding: Database default character encoding, default is UTF8, default is consistent with the instance. It is recommended not to configure and modify. locale: Database default locale, default is C, it is recommended not to configure, keep consistent with the instance. lc_collate: Database default locale string collation, default is same as instance setting, it is recommended not to modify, must be consistent with template database. It is strongly recommended not to configure, or configure to C. lc_ctype: Database default LOCALE, default is same as instance setting, it is recommended not to modify or set, must be consistent with template database. It is recommended to configure to C or en_US.UTF8. allowconn: Whether to allow connection to the database, default is true, not recommended to modify. revokeconn: Whether to revoke connection privilege to the database? Default is false. If true, PUBLIC CONNECT privilege on the database will be revoked. Only default users (dbsu|monitor|admin|replicator|owner) can connect. In addition, admin|owner will have GRANT OPTION, can grant connection privileges to other users. tablespace: Tablespace associated with the database, default is pg_default. connlimit: Database connection limit, default is -1, meaning no limit. extensions: Object array, each object defines an extension in the database, and the schema in which it is installed. parameters: KV object, each KV defines a parameter that needs to be modified for the database through ALTER DATABASE. pgbouncer: Boolean option, whether to add this database to Pgbouncer. All databases will be added to Pgbouncer list unless explicitly specified as pgbouncer: false. comment: Database comment information. pool_auth_user: When pgbouncer_auth_query is enabled, all connections to this pgbouncer database will use the user specified here to execute authentication queries. You need to use a user with access to the pg_shadow table. pool_mode: Database level pgbouncer pool mode, default is transaction, i.e., transaction pooling. If left empty, will use pgbouncer_poolmode parameter as default value. pool_size: Database level pgbouncer default pool size, default is 64 pool_size_reserve: Database level pgbouncer pool size reserve, default is 32, when default pool is insufficient, can request at most this many burst connections. pool_size_min: Database level pgbouncer pool size min, default is 0 pool_max_db_conn: Database level pgbouncer connection pool max database connections, default is 100 Newly created databases are forked from the template1 database by default. This template database will be customized during the PG_PROVISION phase: configured with extensions, schemas, and default privileges, so newly created databases will also inherit these configurations unless you explicitly use another database as a template.\nFor database access privileges, refer to ACL: Database Privilege section.\nCreate Database Databases defined in pg_databases will be automatically created during cluster initialization. If you wish to create database on an existing cluster, you can use the bin/pgsql-db wrapper script. Add new database definition to all.children.\u003ccls\u003e.pg_databases, and create that database with the following command:\nbin/pgsql-db \u003ccls\u003e \u003cdbname\u003e # pgsql-db.yml -l \u003ccls\u003e -e dbname=\u003cdbname\u003e Here are some considerations when creating a new database:\nThe create database playbook is idempotent by default, however when you use baseline scripts, it may not be: in this case, it’s usually not recommended to re-run this on existing databases unless you’re sure the provided baseline SQL is also idempotent.\nWe don’t recommend manually creating new databases, especially when you’re using the default pgbouncer connection pool: unless you’re willing to manually maintain the Pgbouncer database list and keep it consistent with PostgreSQL. When creating new databases using the pgsql-db tool or pgsql-db.yml playbook, this database will also be added to the Pgbouncer Database list.\nIf your database definition has a non-trivial owner (default is dbsu postgres), make sure the owner user exists before creating the database. Best practice is always to create users before creating databases.\nPgbouncer Database Pigsty will configure and enable a Pgbouncer connection pool for PostgreSQL instances in a 1:1 manner by default, communicating via /var/run/postgresql Unix Socket.\nConnection pools can optimize short connection performance, reduce concurrency contention, avoid overwhelming the database with too many connections, and provide additional flexibility during database migration.\nPigsty adds all databases in pg_databases to pgbouncer’s database list by default. You can disable pgbouncer connection pool support for a specific database by explicitly setting pgbouncer: false in the database definition.\nThe Pgbouncer database list is defined in /etc/pgbouncer/database.txt, and connection pool parameters from the database definition are reflected here:\nmeta = host=/var/run/postgresql mode=session grafana = host=/var/run/postgresql mode=transaction bytebase = host=/var/run/postgresql auth_user=dbuser_meta kong = host=/var/run/postgresql pool_size=32 reserve_pool=64 gitea = host=/var/run/postgresql min_pool_size=10 wiki = host=/var/run/postgresql noco = host=/var/run/postgresql mongo = host=/var/run/postgresql When you create databases, the Pgbouncer database list definition file will be refreshed and take effect through online configuration reload, normally without affecting existing connections.\nPgbouncer runs with the same dbsu as PostgreSQL, defaulting to the postgres os user. You can use the pgb alias to access pgbouncer management functions using dbsu.\nPigsty also provides a utility function pgb-route, which can quickly switch pgbouncer database traffic to other nodes in the cluster for zero-downtime migration:\n# route pgbouncer traffic to another cluster member function pgb-route(){ local ip=${1-'\\/var\\/run\\/postgresql'} sed -ie \"s/host=[^[:space:]]\\+/host=${ip}/g\" /etc/pgbouncer/pgbouncer.ini cat /etc/pgbouncer/pgbouncer.ini } ","categories":["Reference"],"description":"Database refers to the logical object created using the SQL command `CREATE DATABASE` within a database cluster.","excerpt":"Database refers to the logical object created using the SQL command …","ref":"/docs/pgsql/misc/db/","tags":["Database"],"title":"Database"},{"body":" Detailed explanation of Host-Based Authentication (HBA) in Pigsty.\nAuthentication is the foundation of Access Control and the Privilege System. PostgreSQL has multiple authentication methods.\nHere we mainly introduce HBA: Host Based Authentication. HBA rules define which users can access which databases from which locations and in which ways.\nClient Authentication To connect to a PostgreSQL database, users must first be authenticated (password is used by default).\nYou can provide the password in the connection string (not secure), or pass it using the PGPASSWORD environment variable or .pgpass file. Refer to the psql documentation and PostgreSQL Connection Strings for more details.\npsql 'host=\u003chost\u003e port=\u003cport\u003e dbname=\u003cdbname\u003e user=\u003cusername\u003e password=\u003cpassword\u003e' psql postgres://\u003cusername\u003e:\u003cpassword\u003e@\u003chost\u003e:\u003cport\u003e/\u003cdbname\u003e PGPASSWORD=\u003cpassword\u003e; psql -U \u003cusername\u003e -h \u003chost\u003e -p \u003cport\u003e -d \u003cdbname\u003e For example, to connect to Pigsty’s default meta database, you can use the following connection strings:\npsql 'host=10.10.10.10 port=5432 dbname=meta user=dbuser_dba password=DBUser.DBA' psql postgres://dbuser_dba:DBUser.DBA@10.10.10.10:5432/meta PGPASSWORD=DBUser.DBA; psql -U dbuser_dba -h 10.10.10.10 -p 5432 -d meta By default, Pigsty enables server-side SSL encryption but does not verify client SSL certificates. To connect using client SSL certificates, you can provide client parameters using the PGSSLCERT and PGSSLKEY environment variables or sslkey and sslcert parameters.\npsql 'postgres://dbuser_dba:DBUser.DBA@10.10.10.10:5432/meta?sslkey=/path/to/dbuser_dba.key\u0026sslcert=/path/to/dbuser_dba.crt' Client certificates (CN = username) can be signed using the local CA with the cert.yml playbook.\nDefining HBA In Pigsty, there are four parameters related to HBA rules:\npg_hba_rules: postgres HBA rules pg_default_hba_rules: postgres global default HBA rules pgb_hba_rules: pgbouncer HBA rules pgb_default_hba_rules: pgbouncer global default HBA rules These are all arrays of HBA rule objects. Each HBA rule is an object in one of the following two forms:\n1. Raw Form The raw form of HBA is almost identical to the PostgreSQL pg_hba.conf format:\n- title: allow intranet password access role: common rules: - host all all 10.0.0.0/8 md5 - host all all 172.16.0.0/12 md5 - host all all 192.168.0.0/16 md5 In this form, the rules field is an array of strings, where each line is a raw HBA rule. The title field is rendered as a comment explaining what the rules below do.\nThe role field specifies which instance roles the rule applies to. When an instance’s pg_role matches the role, the HBA rule will be added to that instance’s HBA.\nHBA rules with role: common will be added to all instances. HBA rules with role: primary will only be added to primary instances. HBA rules with role: replica will only be added to replica instances. HBA rules with role: offline will be added to offline instances (pg_role = offline or pg_offline_query = true) 2. Alias Form The alias form allows you to maintain HBA rules in a simpler, clearer, and more convenient way: it replaces the rules field with addr, auth, user, and db fields. The title and role fields still apply.\n- addr: 'intra' # world|intra|infra|admin|local|localhost|cluster|\u003ccidr\u003e auth: 'pwd' # trust|pwd|ssl|cert|deny|\u003cofficial auth method\u003e user: 'all' # all|${dbsu}|${repl}|${admin}|${monitor}|\u003cuser\u003e|\u003cgroup\u003e db: 'all' # all|replication|.... rules: [] # raw hba string precedence over above all title: allow intranet password access addr: where - Which IP address ranges are affected by this rule? world: All IP addresses intra: All intranet IP address ranges: '10.0.0.0/8', '172.16.0.0/12', '192.168.0.0/16' infra: IP addresses of Infra nodes admin: IP addresses of admin_ip management nodes local: Local Unix Socket localhost: Local Unix Socket and TCP 127.0.0.1/32 loopback address cluster: IP addresses of all members in the same PostgreSQL cluster \u003ccidr\u003e: A specific CIDR address block or IP address auth: how - What authentication method does this rule specify? deny: Deny access trust: Trust directly, no authentication required pwd: Password authentication, uses md5 or scram-sha-256 authentication based on the pg_pwd_enc parameter sha/scram-sha-256: Force use of scram-sha-256 password authentication. md5: md5 password authentication, but can also be compatible with scram-sha-256 authentication, not recommended. ssl: On top of password authentication pwd, require SSL to be enabled ssl-md5: On top of password authentication md5, require SSL to be enabled ssl-sha: On top of password authentication sha, require SSL to be enabled os/ident: Use ident authentication with the operating system user identity peer: Use peer authentication method, similar to os ident cert: Use client SSL certificate-based authentication, certificate CN is the username user: who: Which users are affected by this rule? all: All users ${dbsu}: Default database superuser pg_dbsu ${repl}: Default database replication user pg_replication_username ${admin}: Default database admin user pg_admin_username ${monitor}: Default database monitor user pg_monitor_username Other specific users or roles db: which: Which databases are affected by this rule? all: All databases replication: Allow replication connections (not specifying a specific database) A specific database 3. Definition Location Typically, global HBA is defined in all.vars. If you want to modify the global default HBA rules, you can copy one from the full.yml template to all.vars and modify it.\npg_default_hba_rules: postgres global default HBA rules pgb_default_hba_rules: pgbouncer global default HBA rules Cluster-specific HBA rules are defined in the database cluster-level configuration:\npg_hba_rules: postgres HBA rules pgb_hba_rules: pgbouncer HBA rules Here are some examples of cluster HBA rule definitions:\npg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta pg_hba_rules: - { user: dbuser_view ,db: all ,addr: infra ,auth: pwd ,title: 'Allow dbuser_view password access to all databases from infrastructure nodes'} - { user: all ,db: all ,addr: 100.0.0.0/8 ,auth: pwd ,title: 'Allow all users password access to all databases from K8S network' } - { user: '${admin}' ,db: world ,addr: 0.0.0.0/0 ,auth: cert ,title: 'Allow admin user to login from anywhere with client certificate' } Reloading HBA HBA is a static rule configuration file that needs to be reloaded to take effect after modification. The default HBA rule set typically doesn’t need to be reloaded because it doesn’t involve Role or cluster members.\nIf your HBA design uses specific instance role restrictions or cluster member restrictions, then when cluster instance members change (add/remove/failover), some HBA rules’ effective conditions/scope change, and you typically also need to reload HBA to reflect the latest changes.\nTo reload postgres/pgbouncer hba rules:\nbin/pgsql-hba \u003ccls\u003e # Reload hba rules for cluster `\u003ccls\u003e` bin/pgsql-hba \u003ccls\u003e ip1 ip2... # Reload hba rules for specific instances The underlying Ansible playbook commands actually executed are:\n./pgsql.yml -l \u003ccls\u003e -e pg_reload=true -t pg_hba,pg_reload ./pgsql.yml -l \u003ccls\u003e -e pg_reload=true -t pgbouncer_hba,pgbouncer_reload Default HBA Pigsty has a default set of HBA rules that are secure enough for most scenarios. These rules use the alias form, so they are basically self-explanatory.\npg_default_hba_rules: # postgres global default HBA rules - {user: '${dbsu}' ,db: all ,addr: local ,auth: ident ,title: 'dbsu access via local os user ident' } - {user: '${dbsu}' ,db: replication ,addr: local ,auth: ident ,title: 'dbsu replication from local os ident' } - {user: '${repl}' ,db: replication ,addr: localhost ,auth: pwd ,title: 'replicator replication from localhost'} - {user: '${repl}' ,db: replication ,addr: intra ,auth: pwd ,title: 'replicator replication from intranet' } - {user: '${repl}' ,db: postgres ,addr: intra ,auth: pwd ,title: 'replicator postgres db from intranet' } - {user: '${monitor}' ,db: all ,addr: localhost ,auth: pwd ,title: 'monitor from localhost with password' } - {user: '${monitor}' ,db: all ,addr: infra ,auth: pwd ,title: 'monitor from infra host with password'} - {user: '${admin}' ,db: all ,addr: infra ,auth: ssl ,title: 'admin @ infra nodes with pwd \u0026 ssl' } - {user: '${admin}' ,db: all ,addr: world ,auth: ssl ,title: 'admin @ everywhere with ssl \u0026 pwd' } - {user: '+dbrole_readonly',db: all ,addr: localhost ,auth: pwd ,title: 'pgbouncer read/write via local socket'} - {user: '+dbrole_readonly',db: all ,addr: intra ,auth: pwd ,title: 'read/write biz user via password' } - {user: '+dbrole_offline' ,db: all ,addr: intra ,auth: pwd ,title: 'allow etl offline tasks from intranet'} pgb_default_hba_rules: # pgbouncer global default HBA rules - {user: '${dbsu}' ,db: pgbouncer ,addr: local ,auth: peer ,title: 'dbsu local admin access with os ident'} - {user: 'all' ,db: all ,addr: localhost ,auth: pwd ,title: 'allow all user local access with pwd' } - {user: '${monitor}' ,db: pgbouncer ,addr: intra ,auth: pwd ,title: 'monitor access via intranet with pwd' } - {user: '${monitor}' ,db: all ,addr: world ,auth: deny ,title: 'reject all other monitor access addr' } - {user: '${admin}' ,db: all ,addr: intra ,auth: pwd ,title: 'admin access via intranet with pwd' } - {user: '${admin}' ,db: all ,addr: world ,auth: deny ,title: 'reject all other admin access addr' } - {user: 'all' ,db: all ,addr: intra ,auth: pwd ,title: 'allow all user intra access with pwd' } Example: Rendered pg_hba.conf #==============================================================# # File : pg_hba.conf # Desc : Postgres HBA Rules for pg-meta-1 [primary] # Time : 2023-01-11 15:19 # Host : pg-meta-1 @ 10.10.10.10:5432 # Path : /pg/data/pg_hba.conf # Note : ANSIBLE MANAGED, DO NOT CHANGE! # Author : Ruohang Feng (rh@vonng.com) # License : AGPLv3 #==============================================================# # addr alias # local : /var/run/postgresql # admin : 10.10.10.10 # infra : 10.10.10.10 # intra : 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16 # user alias # dbsu : postgres # repl : replicator # monitor : dbuser_monitor # admin : dbuser_dba # dbsu access via local os user ident [default] local all postgres ident # dbsu replication from local os ident [default] local replication postgres ident # replicator replication from localhost [default] local replication replicator scram-sha-256 host replication replicator 127.0.0.1/32 scram-sha-256 # replicator replication from intranet [default] host replication replicator 10.0.0.0/8 scram-sha-256 host replication replicator 172.16.0.0/12 scram-sha-256 host replication replicator 192.168.0.0/16 scram-sha-256 # replicator postgres db from intranet [default] host postgres replicator 10.0.0.0/8 scram-sha-256 host postgres replicator 172.16.0.0/12 scram-sha-256 host postgres replicator 192.168.0.0/16 scram-sha-256 # monitor from localhost with password [default] local all dbuser_monitor scram-sha-256 host all dbuser_monitor 127.0.0.1/32 scram-sha-256 # monitor from infra host with password [default] host all dbuser_monitor 10.10.10.10/32 scram-sha-256 # admin @ infra nodes with pwd \u0026 ssl [default] hostssl all dbuser_dba 10.10.10.10/32 scram-sha-256 # admin @ everywhere with ssl \u0026 pwd [default] hostssl all dbuser_dba 0.0.0.0/0 scram-sha-256 # pgbouncer read/write via local socket [default] local all +dbrole_readonly scram-sha-256 host all +dbrole_readonly 127.0.0.1/32 scram-sha-256 # read/write biz user via password [default] host all +dbrole_readonly 10.0.0.0/8 scram-sha-256 host all +dbrole_readonly 172.16.0.0/12 scram-sha-256 host all +dbrole_readonly 192.168.0.0/16 scram-sha-256 # allow etl offline tasks from intranet [default] host all +dbrole_offline 10.0.0.0/8 scram-sha-256 host all +dbrole_offline 172.16.0.0/12 scram-sha-256 host all +dbrole_offline 192.168.0.0/16 scram-sha-256 # allow application database intranet access [common] [DISABLED] #host kong dbuser_kong 10.0.0.0/8 md5 #host bytebase dbuser_bytebase 10.0.0.0/8 md5 #host grafana dbuser_grafana 10.0.0.0/8 md5 Example: Rendered pgb_hba.conf #==============================================================# # File : pgb_hba.conf # Desc : Pgbouncer HBA Rules for pg-meta-1 [primary] # Time : 2023-01-11 15:28 # Host : pg-meta-1 @ 10.10.10.10:5432 # Path : /etc/pgbouncer/pgb_hba.conf # Note : ANSIBLE MANAGED, DO NOT CHANGE! # Author : Ruohang Feng (rh@vonng.com) # License : AGPLv3 #==============================================================# # PGBOUNCER HBA RULES FOR pg-meta-1 @ 10.10.10.10:6432 # ansible managed: 2023-01-11 14:30:58 # addr alias # local : /var/run/postgresql # admin : 10.10.10.10 # infra : 10.10.10.10 # intra : 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16 # user alias # dbsu : postgres # repl : replicator # monitor : dbuser_monitor # admin : dbuser_dba # dbsu local admin access with os ident [default] local pgbouncer postgres peer # allow all user local access with pwd [default] local all all scram-sha-256 host all all 127.0.0.1/32 scram-sha-256 # monitor access via intranet with pwd [default] host pgbouncer dbuser_monitor 10.0.0.0/8 scram-sha-256 host pgbouncer dbuser_monitor 172.16.0.0/12 scram-sha-256 host pgbouncer dbuser_monitor 192.168.0.0/16 scram-sha-256 # reject all other monitor access addr [default] host all dbuser_monitor 0.0.0.0/0 reject # admin access via intranet with pwd [default] host all dbuser_dba 10.0.0.0/8 scram-sha-256 host all dbuser_dba 172.16.0.0/12 scram-sha-256 host all dbuser_dba 192.168.0.0/16 scram-sha-256 # reject all other admin access addr [default] host all dbuser_dba 0.0.0.0/0 reject # allow all user intra access with pwd [default] host all all 10.0.0.0/8 scram-sha-256 host all all 172.16.0.0/12 scram-sha-256 host all all 192.168.0.0/16 scram-sha-256 Security Hardening For scenarios requiring higher security, we provide a security hardening configuration template security.yml, which uses the following default HBA rule set:\npg_default_hba_rules: # postgres host-based auth rules by default - {user: '${dbsu}' ,db: all ,addr: local ,auth: ident ,title: 'dbsu access via local os user ident' } - {user: '${dbsu}' ,db: replication ,addr: local ,auth: ident ,title: 'dbsu replication from local os ident' } - {user: '${repl}' ,db: replication ,addr: localhost ,auth: ssl ,title: 'replicator replication from localhost'} - {user: '${repl}' ,db: replication ,addr: intra ,auth: ssl ,title: 'replicator replication from intranet' } - {user: '${repl}' ,db: postgres ,addr: intra ,auth: ssl ,title: 'replicator postgres db from intranet' } - {user: '${monitor}' ,db: all ,addr: localhost ,auth: pwd ,title: 'monitor from localhost with password' } - {user: '${monitor}' ,db: all ,addr: infra ,auth: ssl ,title: 'monitor from infra host with password'} - {user: '${admin}' ,db: all ,addr: infra ,auth: ssl ,title: 'admin @ infra nodes with pwd \u0026 ssl' } - {user: '${admin}' ,db: all ,addr: world ,auth: cert ,title: 'admin @ everywhere with ssl \u0026 cert' } - {user: '+dbrole_readonly',db: all ,addr: localhost ,auth: ssl ,title: 'pgbouncer read/write via local socket'} - {user: '+dbrole_readonly',db: all ,addr: intra ,auth: ssl ,title: 'read/write biz user via password' } - {user: '+dbrole_offline' ,db: all ,addr: intra ,auth: ssl ,title: 'allow etl offline tasks from intranet'} pgb_default_hba_rules: # pgbouncer host-based authentication rules - {user: '${dbsu}' ,db: pgbouncer ,addr: local ,auth: peer ,title: 'dbsu local admin access with os ident'} - {user: 'all' ,db: all ,addr: localhost ,auth: pwd ,title: 'allow all user local access with pwd' } - {user: '${monitor}' ,db: pgbouncer ,addr: intra ,auth: ssl ,title: 'monitor access via intranet with pwd' } - {user: '${monitor}' ,db: all ,addr: world ,auth: deny ,title: 'reject all other monitor access addr' } - {user: '${admin}' ,db: all ,addr: intra ,auth: ssl ,title: 'admin access via intranet with pwd' } - {user: '${admin}' ,db: all ,addr: world ,auth: deny ,title: 'reject all other admin access addr' } - {user: 'all' ,db: all ,addr: intra ,auth: ssl ,title: 'allow all user intra access with pwd' } For more information, refer to the Security Hardening section.\n","categories":["Reference"],"description":"Detailed explanation of Host-Based Authentication (HBA) in Pigsty.","excerpt":"Detailed explanation of Host-Based Authentication (HBA) in Pigsty.","ref":"/docs/pgsql/misc/hba/","tags":["HBA","Auth"],"title":"Authentication / HBA"},{"body":" Pigsty provides a battery-included access control model based on a role system and privilege system.\nAccess control is important, but many users don’t do it well. Therefore, Pigsty provides a simplified, ready-to-use access control model to provide a security baseline for your cluster.\nRole System Pigsty’s default role system includes four default roles and four default users:\nRole Name Attributes Member of Description dbrole_readonly NOLOGIN role for global read-only access dbrole_readwrite NOLOGIN dbrole_readonly role for global read-write access dbrole_admin NOLOGIN pg_monitor,dbrole_readwrite role for object creation dbrole_offline NOLOGIN role for restricted read-only access postgres SUPERUSER system superuser replicator REPLICATION pg_monitor,dbrole_readonly system replicator dbuser_dba SUPERUSER dbrole_admin pgsql admin user dbuser_monitor pg_monitor pgsql monitor user The detailed definitions of these roles and users are as follows:\npg_default_roles: # default roles and users in postgres cluster - { name: dbrole_readonly ,login: false ,comment: role for global read-only access } - { name: dbrole_offline ,login: false ,comment: role for restricted read-only access } - { name: dbrole_readwrite ,login: false ,roles: [dbrole_readonly] ,comment: role for global read-write access } - { name: dbrole_admin ,login: false ,roles: [pg_monitor, dbrole_readwrite] ,comment: role for object creation } - { name: postgres ,superuser: true ,comment: system superuser } - { name: replicator ,replication: true ,roles: [pg_monitor, dbrole_readonly] ,comment: system replicator } - { name: dbuser_dba ,superuser: true ,roles: [dbrole_admin] ,pgbouncer: true ,pool_mode: session, pool_connlimit: 16 ,comment: pgsql admin user } - { name: dbuser_monitor ,roles: [pg_monitor] ,pgbouncer: true ,parameters: {log_min_duration_statement: 1000 } ,pool_mode: session ,pool_connlimit: 8 ,comment: pgsql monitor user } Default Roles There are four default roles in Pigsty:\nBusiness Read-Only (dbrole_readonly): Role for global read-only access. If other businesses need read-only access to this database, they can use this role. Business Read-Write (dbrole_readwrite): Role for global read-write access. Production accounts used by primary business should have database read-write privileges. Business Admin (dbrole_admin): Role with DDL permissions, typically used for business administrators or scenarios requiring table creation in applications (such as various business software). Offline Read-Only (dbrole_offline): Restricted read-only access role (can only access offline instances, typically for personal users and ETL tool accounts). Default roles are defined in pg_default_roles. Unless you really know what you’re doing, it’s recommended not to change the default role names.\n- { name: dbrole_readonly , login: false , comment: role for global read-only access } # production read-only role - { name: dbrole_offline , login: false , comment: role for restricted read-only access (offline instance) } # restricted-read-only role - { name: dbrole_readwrite , login: false , roles: [dbrole_readonly], comment: role for global read-write access } # production read-write role - { name: dbrole_admin , login: false , roles: [pg_monitor, dbrole_readwrite] , comment: role for object creation } # production DDL change role Default Users Pigsty also has four default users (system users):\nSuperuser (postgres), the owner and creator of the cluster, same as the OS dbsu. Replication user (replicator), the system user used for primary-replica replication. Monitor user (dbuser_monitor), a user used to monitor database and connection pool metrics. Admin user (dbuser_dba), the admin user who performs daily operations and database changes. These four default users’ username/password are defined with four pairs of dedicated parameters, referenced in many places:\npg_dbsu: os dbsu name, postgres by default, better not change it pg_dbsu_password: dbsu password, empty string by default means no password is set for dbsu, best not to set it. pg_replication_username: postgres replication username, replicator by default pg_replication_password: postgres replication password, DBUser.Replicator by default pg_admin_username: postgres admin username, dbuser_dba by default pg_admin_password: postgres admin password in plain text, DBUser.DBA by default pg_monitor_username: postgres monitor username, dbuser_monitor by default pg_monitor_password: postgres monitor password, DBUser.Monitor by default Remember to change these passwords in production deployment! Don’t use default values!\npg_dbsu: postgres # database superuser name, it's recommended not to modify this username. pg_dbsu_password: '' # database superuser password, it's recommended to leave this empty! Prohibit dbsu password login. pg_replication_username: replicator # system replication username pg_replication_password: DBUser.Replicator # system replication password, be sure to modify this password! pg_monitor_username: dbuser_monitor # system monitor username pg_monitor_password: DBUser.Monitor # system monitor password, be sure to modify this password! pg_admin_username: dbuser_dba # system admin username pg_admin_password: DBUser.DBA # system admin password, be sure to modify this password! If you modify the default user parameters, update the corresponding role definition in pg_default_roles:\n- { name: postgres ,superuser: true ,comment: system superuser } - { name: replicator ,replication: true ,roles: [pg_monitor, dbrole_readonly] ,comment: system replicator } - { name: dbuser_dba ,superuser: true ,roles: [dbrole_admin] ,pgbouncer: true ,pool_mode: session, pool_connlimit: 16 , comment: pgsql admin user } - { name: dbuser_monitor ,roles: [pg_monitor, dbrole_readonly] ,pgbouncer: true ,parameters: {log_min_duration_statement: 1000 } ,pool_mode: session ,pool_connlimit: 8 ,comment: pgsql monitor user } Privilege System Pigsty has a battery-included privilege model that works with default roles.\nAll users have access to all schemas. Read-Only users (dbrole_readonly) can read from all tables. (SELECT, EXECUTE) Read-Write users (dbrole_readwrite) can write to all tables and run DML. (INSERT, UPDATE, DELETE). Admin users (dbrole_admin) can create objects and run DDL (CREATE, USAGE, TRUNCATE, REFERENCES, TRIGGER). Offline users (dbrole_offline) are like Read-Only users, but with limited access, only allowed to access offline instances (pg_role = 'offline' or pg_offline_query = true) Objects created by admin users will have correct privileges. Default privileges are installed on all databases, including template databases. Database connect privilege is covered by database definition. CREATE privileges of database \u0026 public schema are revoked from PUBLIC by default. Object Privilege Default object privileges for newly created objects in the database are controlled by the pg_default_privileges parameter:\n- GRANT USAGE ON SCHEMAS TO dbrole_readonly - GRANT SELECT ON TABLES TO dbrole_readonly - GRANT SELECT ON SEQUENCES TO dbrole_readonly - GRANT EXECUTE ON FUNCTIONS TO dbrole_readonly - GRANT USAGE ON SCHEMAS TO dbrole_offline - GRANT SELECT ON TABLES TO dbrole_offline - GRANT SELECT ON SEQUENCES TO dbrole_offline - GRANT EXECUTE ON FUNCTIONS TO dbrole_offline - GRANT INSERT ON TABLES TO dbrole_readwrite - GRANT UPDATE ON TABLES TO dbrole_readwrite - GRANT DELETE ON TABLES TO dbrole_readwrite - GRANT USAGE ON SEQUENCES TO dbrole_readwrite - GRANT UPDATE ON SEQUENCES TO dbrole_readwrite - GRANT TRUNCATE ON TABLES TO dbrole_admin - GRANT REFERENCES ON TABLES TO dbrole_admin - GRANT TRIGGER ON TABLES TO dbrole_admin - GRANT CREATE ON SCHEMAS TO dbrole_admin Newly created objects by admin users will have these privileges by default. Use \\ddp+ to view these default privileges:\nType Access privileges function =X dbrole_readonly=X dbrole_offline=X dbrole_admin=X schema dbrole_readonly=U dbrole_offline=U dbrole_admin=UC sequence dbrole_readonly=r dbrole_offline=r dbrole_readwrite=wU dbrole_admin=rwU table dbrole_readonly=r dbrole_offline=r dbrole_readwrite=awd dbrole_admin=arwdDxt Default Privilege ALTER DEFAULT PRIVILEGES allows you to set the privileges that will be applied to objects created in the future. It does not affect privileges assigned to already-existing objects, nor does it affect objects created by non-admin users.\nIn Pigsty, default privileges are defined for three roles:\n{% for priv in pg_default_privileges %} ALTER DEFAULT PRIVILEGES FOR ROLE {{ pg_dbsu }} {{ priv }}; {% endfor %} {% for priv in pg_default_privileges %} ALTER DEFAULT PRIVILEGES FOR ROLE {{ pg_admin_username }} {{ priv }}; {% endfor %} -- for additional business admin, they should SET ROLE dbrole_admin before executing DDL to use the corresponding default privilege configuration. {% for priv in pg_default_privileges %} ALTER DEFAULT PRIVILEGES FOR ROLE \"dbrole_admin\" {{ priv }}; {% endfor %} This content will be used by the PG cluster initialization template pg-init-template.sql, rendered during cluster initialization and output to /pg/tmp/pg-init-template.sql. These commands will be executed on template1 and postgres databases, and newly created databases will inherit these default privilege configurations from template1.\nThat is to say, to maintain correct object privileges, you must execute DDL with admin users, which could be:\n{{ pg_dbsu }}, postgres by default {{ pg_admin_username }}, dbuser_dba by default Business admin users granted with dbrole_admin role (by switching to dbrole_admin identity using SET ROLE) It’s wise to use postgres as the global object owner. If you wish to create objects as business admin user, you MUST USE SET ROLE dbrole_admin before running that DDL to maintain the correct privileges.\nYou can also explicitly grant default privileges to business admin users in the database through ALTER DEFAULT PRIVILEGE FOR ROLE \u003csome_biz_admin\u003e XXX.\nDatabase Privilege In Pigsty, database-level privileges are covered in the database definition.\nThere are three database level privileges: CONNECT, CREATE, TEMP, and a special ‘privilege’: OWNERSHIP.\n- name: meta # required, `name` is the only mandatory field of a database definition owner: postgres # optional, specify a database owner, postgres by default allowconn: true # optional, allow connection, true by default. false will disable connect at all revokeconn: false # optional, revoke public connection privilege. false by default. when set to true, CONNECT privilege will be revoked from users other than owner and admin If owner exists, it will be used as the database owner instead of default {{ pg_dbsu }} (which is usually postgres) If revokeconn is false, all users have the CONNECT privilege of the database, this is the default behavior. If revokeconn is explicitly set to true: CONNECT privilege of the database will be revoked from PUBLIC: regular users cannot connect to this database CONNECT privilege will be explicitly granted to {{ pg_replication_username }}, {{ pg_monitor_username }} and {{ pg_admin_username }} CONNECT privilege will be granted to the database owner with GRANT OPTION, the database owner can then grant connection privileges to other users. revokeconn flag can be used for database access isolation. You can create different business users as owners for each database and set the revokeconn option for them. Example: Database Isolation pg-infra: hosts: 10.10.10.40: { pg_seq: 1, pg_role: primary } 10.10.10.41: { pg_seq: 2, pg_role: replica , pg_offline_query: true } vars: pg_cluster: pg-infra pg_users: - { name: dbuser_confluence, password: mc2iohos , pgbouncer: true, roles: [ dbrole_admin ] } - { name: dbuser_gitlab, password: sdf23g22sfdd , pgbouncer: true, roles: [ dbrole_readwrite ] } - { name: dbuser_jira, password: sdpijfsfdsfdfs , pgbouncer: true, roles: [ dbrole_admin ] } pg_databases: - { name: confluence , revokeconn: true, owner: dbuser_confluence , connlimit: 100 } - { name: gitlab , revokeconn: true, owner: dbuser_gitlab, connlimit: 100 } - { name: jira , revokeconn: true, owner: dbuser_jira , connlimit: 100 } CREATE Privilege For security reasons, Pigsty revokes the CREATE privilege on databases from PUBLIC by default, which is also the default behavior since PostgreSQL 15.\nThe database owner has the full ability to adjust CREATE privileges as they see fit.\n","categories":["Reference"],"description":"Default role system and privilege model provided by Pigsty","excerpt":"Default role system and privilege model provided by Pigsty","ref":"/docs/pgsql/misc/acl/","tags":["Security"],"title":"Access Control"},{"body":"This document lists potential failures in PostgreSQL and Pigsty, as well as SOPs for locating, handling, and analyzing issues.\nDisk Space Exhausted Disk space exhaustion is the most common type of failure.\nSymptoms When the disk space where the database resides is exhausted, PostgreSQL will not work normally and may exhibit the following symptoms: database logs repeatedly report “no space left on device” errors, new data cannot be written, and PostgreSQL may even trigger a PANIC and force shutdown.\nPigsty includes a NodeFsSpaceFull alert rule that triggers when filesystem available space is less than 10%. Use the monitoring system’s NODE Instance panel to review the FS metrics panel to locate the issue.\nDiagnosis You can also log into the database node and use df -h to view the usage of each mounted partition to determine which partition is full. For database nodes, focus on checking the following directories and their sizes to determine which category of files has filled up the space:\nData directory (/pg/data/base): Stores data files for tables and indexes; pay attention to heavy writes and temporary files WAL directory (e.g., pg/data/pg_wal): Stores PG WAL; WAL accumulation/replication slot retention is a common cause of disk exhaustion. Database log directory (e.g., pg/log): If PG logs are not rotated in time and large amounts of errors are written, they may also consume significant space. Local backup directory (e.g., data/backups): When using pgBackRest or similar tools to save backups locally, this may also fill up the disk. If the issue occurs on the Pigsty admin node or monitoring node, also consider:\nMonitoring data: VictoriaMetrics time-series metrics and VictoriaLogs log storage both consume disk space; check retention policies. Object storage data: Pigsty’s integrated MinIO object storage may be used for PG backup storage. After identifying the directory consuming the most space, you can further use du -sh \u003cdirectory\u003e to drill down and find specific large files or subdirectories.\nResolution Disk exhaustion is an emergency issue requiring immediate action to free up space and ensure the database continues to operate. When the data disk is not separated from the system disk, a full disk may prevent shell commands from executing. In this case, you can delete the /pg/dummy placeholder file to free up a small amount of emergency space so shell commands can work again. If the database has crashed due to pg_wal filling up, you need to restart the database service after clearing space and carefully check data integrity.\nTransaction ID Wraparound PostgreSQL cyclically uses 32-bit transaction IDs (XIDs), and when exhausted, a “transaction ID wraparound” failure occurs (XID Wraparound).\nSymptoms The typical sign in the first phase is when the age saturation in the PGSQL Persist - Age Usage panel enters the warning zone. Database logs begin to show messages like: WARNING: database \"postgres\" must be vacuumed within xxxxxxxx transactions.\nIf the problem continues to worsen, PostgreSQL enters protection mode: when remaining transaction IDs drop to about 1 million, the database switches to read-only mode; when reaching the limit of about 2.1 billion (2^31), it refuses any new transactions and forces the server to shut down to avoid data corruption.\nDiagnosis PostgreSQL and Pigsty enable automatic garbage collection (AutoVacuum) by default, so the occurrence of this type of failure usually has deeper root causes. Common causes include: very long transactions (SAGE), misconfigured Autovacuum, replication slot blockage, insufficient resources, storage engine/extension bugs, disk bad blocks.\nFirst identify the database with the highest age, then use the Pigsty PGCAT Database - Tables panel to confirm the age distribution of tables. Also review the database error logs, which usually contain clues to locate the root cause.\nResolution Immediately freeze old transactions: If the database has not yet entered read-only protection mode, immediately execute a manual VACUUM FREEZE on the affected database. You can start by freezing the most severely aged tables one by one rather than doing the entire database at once to accelerate the effect. Connect to the database as a superuser and run VACUUM FREEZE table_name; on tables identified with the largest relfrozenxid, prioritizing tables with the highest XID age. This can quickly reclaim large amounts of transaction ID space. Single-user mode rescue: If the database is already refusing writes or has crashed for protection, you need to start the database in single-user mode to perform freeze operations. In single-user mode, run VACUUM FREEZE database_name; to freeze and clean the entire database. After completion, restart the database in multi-user mode. This can lift the wraparound lock and make the database writable again. Be very careful when operating in single-user mode and ensure sufficient transaction ID margin to complete the freeze. Standby node takeover: In some complex scenarios (e.g., when hardware issues prevent vacuum from completing), consider promoting a read-only standby node in the cluster to primary to obtain a relatively clean environment for handling the freeze. For example, if the primary cannot vacuum due to bad blocks, you can manually failover to promote the standby to the new primary, then perform emergency vacuum freeze on it. After ensuring the new primary has frozen old transactions, switch the load back. Connection Exhaustion PostgreSQL has a maximum connections configuration (max_connections). When client connections exceed this limit, new connection requests will be rejected. The typical symptom is that applications cannot connect to the database and report errors like FATAL: remaining connection slots are reserved for non-replication superuser connections or too many clients already. This indicates that regular connections are exhausted, leaving only slots reserved for superusers or replication.\nDiagnosis Connection exhaustion is usually caused by a large number of concurrent client requests. You can directly review the database’s current active sessions through PGCAT Instance / PGCAT Database / PGCAT Locks. Determine what types of queries are filling the system and proceed with further handling. Pay special attention to whether there are many connections in the “Idle in Transaction” state and long-running transactions (as well as slow queries).\nResolution Kill queries: For situations where exhaustion has already blocked business operations, typically use pg_terminate_backend(pid) immediately for emergency pressure relief. For cases using connection pooling, you can adjust the connection pool size parameters and execute a reload to reduce the number of connections at the database level.\nYou can also modify the max_connections parameter to a larger value, but this parameter requires a database restart to take effect.\netcd Quota Exhausted An exhausted etcd quota will cause the PG high availability control plane to fail and prevent configuration changes.\nDiagnosis Pigsty uses etcd as the distributed configuration store (DCS) when implementing high availability. etcd itself has a storage quota (default is about 2GB). When etcd storage usage reaches the quota limit, etcd will refuse write operations and report “etcdserver: mvcc: database space exceeded”. In this case, Patroni cannot write heartbeats or update configuration to etcd, causing cluster management functions to fail.\nResolution Versions between Pigsty v2.0.0 and v2.5.1 are affected by this issue by default. Pigsty v2.6.0 added auto-compaction configuration for deployed etcd. If you only use it for PG high availability leases, this issue will no longer occur in regular use cases.\nDefective Storage Engine Currently, TimescaleDB’s experimental storage engine Hypercore has been proven to have defects, with cases of VACUUM being unable to reclaim leading to XID wraparound failures. Users using this feature should migrate to PostgreSQL native tables or TimescaleDB’s default engine promptly.\nDetailed introduction: PG New Storage Engine Failure Case (Chinese)\n","categories":["Task"],"description":"Common failures and analysis troubleshooting approaches","excerpt":"Common failures and analysis troubleshooting approaches","ref":"/docs/pgsql/tutorial/failure/","tags":"","title":"Troubleshooting"},{"body":"Pigsty includes a built-in playbook pgsql-migration.yml that implements online database migration based on logical replication.\nWith pre-generated automation scripts, application downtime can be reduced to just a few seconds. However, note that logical replication requires PostgreSQL 10 or later to work.\nOf course, if you have sufficient downtime budget, you can always use the pg_dump | psql approach for offline migration.\nDefining Migration Tasks To use Pigsty’s online migration playbook, you need to create a definition file that describes the migration task details.\nRefer to the task definition file example: files/migration/pg-meta.yml.\nThis migration task will online migrate pg-meta.meta to pg-test.test, where the former is called the Source Cluster (SRC) and the latter is called the Destination Cluster (DST).\npg-meta-1\t10.10.10.10 --\u003e pg-test-1\t10.10.10.11 (10.10.10.12,10.10.10.13) Logical replication-based migration works on a per-database basis. You need to specify the database name to migrate, as well as the IP addresses of the source and destination cluster primary nodes and superuser connection information.\n--- #----------------------------------------------------------------- # PG_MIGRATION #----------------------------------------------------------------- context_dir: ~/migration # Directory for migration manual \u0026 scripts #----------------------------------------------------------------- # SRC Cluster (Old Cluster) #----------------------------------------------------------------- src_cls: pg-meta # Source cluster name \u003cRequired\u003e src_db: meta # Source database name \u003cRequired\u003e src_ip: 10.10.10.10 # Source cluster primary IP \u003cRequired\u003e #src_pg: '' # If defined, use this as source dbsu pgurl instead of: # # postgres://{{ pg_admin_username }}@{{ src_ip }}/{{ src_db }} # # e.g.: 'postgres://dbuser_dba:DBUser.DBA@10.10.10.10:5432/meta' #sub_conn: '' # If defined, use this as subscription connection string instead of: # # host={{ src_ip }} dbname={{ src_db }} user={{ pg_replication_username }}' # # e.g.: 'host=10.10.10.10 dbname=meta user=replicator password=DBUser.Replicator' #----------------------------------------------------------------- # DST Cluster (New Cluster) #----------------------------------------------------------------- dst_cls: pg-test # Destination cluster name \u003cRequired\u003e dst_db: test # Destination database name \u003cRequired\u003e dst_ip: 10.10.10.11 # Destination cluster primary IP \u003cRequired\u003e #dst_pg: '' # If defined, use this as destination dbsu pgurl instead of: # # postgres://{{ pg_admin_username }}@{{ dst_ip }}/{{ dst_db }} # # e.g.: 'postgres://dbuser_dba:DBUser.DBA@10.10.10.11:5432/test' #----------------------------------------------------------------- # PGSQL #----------------------------------------------------------------- pg_dbsu: postgres pg_replication_username: replicator pg_replication_password: DBUser.Replicator pg_admin_username: dbuser_dba pg_admin_password: DBUser.DBA pg_monitor_username: dbuser_monitor pg_monitor_password: DBUser.Monitor #----------------------------------------------------------------- ... By default, the superuser connection strings on both source and destination sides are constructed using the global admin user and the respective primary IP addresses, but you can always override these defaults through the src_pg and dst_pg parameters. Similarly, you can override the subscription connection string default through the sub_conn parameter.\nGenerating Migration Plan This playbook does not actively perform cluster migration, but it generates the operation manual and automation scripts needed for migration.\nBy default, you will find the migration context directory at ~/migration/pg-meta.meta. Follow the instructions in README.md and execute these scripts in sequence to complete the database migration!\n# Activate migration context: enable related environment variables . ~/migration/pg-meta.meta/activate # These scripts check src cluster status and help generate new cluster definitions in pigsty ./check-user # Check src users ./check-db # Check src databases ./check-hba # Check src hba rules ./check-repl # Check src replication identity ./check-misc # Check src special objects # These scripts establish logical replication between existing src cluster and pigsty-managed dst cluster, data except sequences will sync in real-time ./copy-schema # Copy schema to destination ./create-pub # Create publication on src ./create-sub # Create subscription on dst ./copy-progress # Print logical replication progress ./copy-diff # Quick compare src and dst differences by counting tables # These scripts run during online migration, which stops src cluster and copies sequence numbers (logical replication doesn't replicate sequences!) ./copy-seq [n] # Sync sequence numbers, if n is given, apply additional offset # You must switch application traffic to the new cluster based on your access method (dns,vip,haproxy,pgbouncer,etc.)! #./disable-src # Restrict src cluster access to admin nodes and new cluster (your implementation) #./re-routing # Re-route application traffic from SRC to DST! (your implementation) # Then cleanup to remove subscription and publication ./drop-sub # Drop subscription on dst after migration ./drop-pub # Drop publication on src after migration Notes\nIf you’re worried about primary key conflicts when copying sequence numbers, you can advance all sequences forward by some distance when copying, for example +1000. You can use ./copy-seq with a parameter 1000 to achieve this.\nYou must implement your own ./re-routing script to route your application traffic from src to dst. Because we don’t know how your traffic is routed (e.g., dns, VIP, haproxy, or pgbouncer). Of course, you can also do this manually…\nYou can implement a ./disable-src script to restrict application access to the src cluster—this is optional: if you can ensure all application traffic is cleanly switched in ./re-routing, you don’t really need this step.\nBut if you have various access from unknown sources that can’t be cleanly sorted out, it’s better to use more thorough methods: change HBA rules and reload to implement (recommended), or simply stop the postgres, pgbouncer, or haproxy processes on the source primary.\n","categories":["Task","Reference"],"description":"How to migrate an existing PostgreSQL cluster to a new Pigsty-managed PostgreSQL cluster with minimal downtime?","excerpt":"How to migrate an existing PostgreSQL cluster to a new Pigsty-managed …","ref":"/docs/pgsql/migration/","tags":["Migration"],"title":"Data Migration"},{"body":"You can use the pgsql-pitr.yml playbook to perform PITR, but in some cases, you may want to manually execute PITR using pgbackrest primitives directly for fine-grained control. We will use a four-node sandbox cluster with MinIO backup repository to demonstrate the process.\nInitialize Sandbox Use vagrant or terraform to prepare a four-node sandbox environment, then:\ncurl https://repo.pigsty.io/get | bash; cd ~/pigsty/ ./configure -c full ./install Now operate as the admin user (or dbsu) on the admin node.\nCheck Backup To check backup status, you need to switch to the postgres user and use the pb command:\nsudo su - postgres # Switch to dbsu: postgres user pb info # Print pgbackrest backup info pb is an alias for pgbackrest that automatically retrieves the stanza name from pgbackrest configuration.\nfunction pb() { local stanza=$(grep -o '\\[[^][]*]' /etc/pgbackrest/pgbackrest.conf | head -n1 | sed 's/.*\\[\\([^]]*\\)].*/\\1/') pgbackrest --stanza=$stanza $@ } You can see the initial backup information, which is a full backup:\nroot@pg-meta-1:~# pb info stanza: pg-meta status: ok cipher: aes-256-cbc db (current) wal archive min/max (17): 000000010000000000000001/000000010000000000000007 full backup: 20250713-022731F timestamp start/stop: 2025-07-13 02:27:31+00 / 2025-07-13 02:27:33+00 wal start/stop: 000000010000000000000004 / 000000010000000000000004 database size: 44MB, database backup size: 44MB repo1: backup size: 8.4MB The backup completed at 2025-07-13 02:27:33+00, which is the earliest time you can restore to. Since WAL archiving is active, you can restore to any point in time after the backup, up to the end of WAL (i.e., now).\nGenerate Heartbeats You can generate some heartbeats to simulate workload. /pg-bin/pg-heartbeat is for this purpose, it writes a heartbeat timestamp to the monitor.heartbeat table every second.\nHeartbeat Generation alias pgbench output make rh # Run heartbeat: ssh 10.10.10.10 'sudo -iu postgres /pg/bin/pg-heartbeat' ssh 10.10.10.10 'sudo -iu postgres /pg/bin/pg-heartbeat' cls | ts | lsn | lsn_int | txid | status | now | elapse ---------+-------------------------------+------------+-----------+------+---------+-----------------+---------- pg-meta | 2025-07-13 03:01:20.318234+00 | 0/115BF5C0 | 291239360 | 4812 | leading | 03:01:20.318234 | 00:00:00 You can even add more workload to the cluster. Let’s use pgbench to generate some random writes:\npgbench Workload alias pgbench output make ri # Initialize pgbench make rw # Run pgbench read-write workload pgbench -is10 postgres://dbuser_meta:DBUser.Meta@10.10.10.10:5433/meta while true; do pgbench -nv -P1 -c4 --rate=64 -T10 postgres://dbuser_meta:DBUser.Meta@10.10.10.10:5433/meta; done while true; do pgbench -nv -P1 -c4 --rate=64 -T10 postgres://dbuser_meta:DBUser.Meta@10.10.10.10:5433/meta; done pgbench (17.5 (Homebrew), server 17.4 (Ubuntu 17.4-1.pgdg24.04+2)) progress: 1.0 s, 60.9 tps, lat 7.295 ms stddev 4.219, 0 failed, lag 1.818 ms progress: 2.0 s, 69.1 tps, lat 6.296 ms stddev 1.983, 0 failed, lag 1.397 ms ... PITR Manual Now let’s choose a recovery point in time, such as 2025-07-13 03:03:03+00, which is a point after the initial backup (and heartbeat). To perform manual PITR, use the pg-pitr tool:\n$ pg-pitr -t \"2025-07-13 03:03:00+00\" It will generate instructions for performing the recovery, typically requiring four steps:\nPerform time PITR on pg-meta [1. Stop PostgreSQL] =========================================== 1.1 Pause Patroni (if there are any replicas) $ pg pause \u003ccls\u003e # Pause patroni auto-failover 1.2 Shutdown Patroni $ pt-stop # sudo systemctl stop patroni 1.3 Shutdown Postgres $ pg-stop # pg_ctl -D /pg/data stop -m fast [2. Perform PITR] =========================================== 2.1 Restore Backup $ pgbackrest --stanza=pg-meta --type=time --target='2025-07-13 03:03:00+00' restore 2.2 Start PG to Replay WAL $ pg-start # pg_ctl -D /pg/data start 2.3 Validate and Promote - If database content is ok, promote it to finish recovery, otherwise goto 2.1 $ pg-promote # pg_ctl -D /pg/data promote [3. Restore Primary] =========================================== 3.1 Enable Archive Mode (Restart Required) $ psql -c 'ALTER SYSTEM SET archive_mode = on;' 3.1 Restart Postgres to Apply Changes $ pg-restart # pg_ctl -D /pg/data restart 3.3 Restart Patroni $ pt-restart # sudo systemctl restart patroni [4. Restore Cluster] =========================================== 4.1 Re-Init All [**REPLICAS**] (if any) - 4.1.1 option 1: restore replicas with same pgbackrest cmd (require central backup repo) $ pgbackrest --stanza=pg-meta --type=time --target='2025-07-13 03:03:00+00' restore - 4.1.2 option 2: nuke the replica data dir and restart patroni (may take long time to restore) $ rm -rf /pg/data/*; pt-restart - 4.1.3 option 3: reinit with patroni, which may fail if primary lsn \u003c replica lsn $ pg reinit pg-meta 4.2 Resume Patroni $ pg resume pg-meta 4.3 Full Backup (optional) $ pg-backup full # Recommended to perform new full backup after PITR Single Node Example Let’s start with the simple single-node pg-meta cluster as a simpler example.\nShutdown Database Shutdown Services shutdown patroni shutdown postgres pt-stop # sudo systemctl stop patroni, shutdown patroni (and postgres) # Optional, because postgres will be shutdown by patroni if patroni is not paused $ pg_stop # pg_ctl -D /pg/data stop -m fast, shutdown postgres pg_ctl: PID file \"/pg/data/postmaster.pid\" does not exist Is server running? $ pg-ps # Print postgres related processes UID PID PPID C STIME TTY STAT TIME CMD postgres 31048 1 0 02:27 ? Ssl 0:19 /usr/sbin/pgbouncer /etc/pgbouncer/pgbouncer.ini postgres 32026 1 0 02:28 ? Ssl 0:03 /usr/bin/pg_exporter ... postgres 35510 35480 0 03:01 pts/2 S+ 0:00 /bin/bash /pg/bin/pg-heartbeat Make sure local postgres is not running, then execute the recovery commands given in the manual:\nRestore Backup Restore Backup restore output pgbackrest --stanza=pg-meta --type=time --target='2025-07-13 03:03:00+00' restore postgres@pg-meta-1:~$ pgbackrest --stanza=pg-meta --type=time --target='2025-07-13 03:03:00+00' restore 2025-07-13 03:17:07.443 P00 INFO: restore command begin 2.54.2: ... 2025-07-13 03:17:07.470 P00 INFO: repo1: restore backup set 20250713-022731F, recovery will start at 2025-07-13 02:27:31 2025-07-13 03:17:07.471 P00 INFO: remove invalid files/links/paths from '/pg/data' 2025-07-13 03:17:08.523 P00 INFO: write updated /pg/data/postgresql.auto.conf 2025-07-13 03:17:08.527 P00 INFO: restore size = 44MB, file total = 1436 2025-07-13 03:17:08.527 P00 INFO: restore command end: completed successfully (1087ms) Verify Data We don’t want patroni HA to take over until we’re sure the data is correct, so start postgres manually:\nVerify Data start postgres output pg-start waiting for server to start....2025-07-13 03:19:33.133 UTC [39294] LOG: redirecting log output to logging collector process 2025-07-13 03:19:33.133 UTC [39294] HINT: Future log output will appear in directory \"/pg/log/postgres\". done server started Now you can check the data to see if it’s at the point in time you want. You can verify by checking the latest timestamp in business tables, or in this case, check via the heartbeat table.\npostgres@pg-meta-1:~$ psql -c 'table monitor.heartbeat' id | ts | lsn | txid ---------+-------------------------------+-----------+------ pg-meta | 2025-07-13 03:02:59.214104+00 | 302005504 | 4912 The timestamp is just before our specified point in time! (2025-07-13 03:03:00+00). If this is not the point in time you want, you can repeat the recovery with a different time point. Since recovery is performed incrementally and in parallel, it’s very fast. You can retry until you find the correct point in time.\nPromote Primary The recovered postgres cluster is in recovery mode, so it will reject any write operations until promoted to primary. These recovery parameters are generated by pgBackRest in the configuration file.\npostgres@pg-meta-1:~$ cat /pg/data/postgresql.auto.conf # Do not edit this file or use ALTER SYSTEM manually! # It is managed by Pigsty \u0026 Ansible automatically! # Recovery settings generated by pgBackRest restore on 2025-07-13 03:17:08 archive_mode = 'off' restore_command = 'pgbackrest --stanza=pg-meta archive-get %f \"%p\"' recovery_target_time = '2025-07-13 03:03:00+00' If the data is correct, you can promote it to primary, marking it as the new leader and ready to accept writes.\nPromote Primary promote check pg-promote waiting for server to promote.... done server promoted psql -c 'SELECT pg_is_in_recovery()' # 'f' means promoted to primary pg_is_in_recovery ------------------- f (1 row) New Timeline and Split Brain Once promoted, the database cluster will enter a new timeline (leader epoch). If there is any write traffic, it will be written to the new timeline.\nRestore Cluster Finally, not only do you need to restore data, but also restore cluster state, such as:\npatroni takeover archive mode backup set replicas Patroni Takeover Your postgres was started directly. To restore HA takeover, you need to start the patroni service:\nPatroni Takeover launch patroni resume patroni pt-start # sudo systemctl start patroni pg resume pg-meta # Resume patroni auto-failover (if previously paused) Archive Mode archive_mode is disabled during recovery by pgbackrest. If you want new leader writes to be archived to the backup repository, you also need to enable the archive_mode configuration.\nArchive Mode check archive_mode reset archive_mode edit directly psql -c 'show archive_mode' archive_mode -------------- off psql -c 'ALTER SYSTEM RESET archive_mode;' psql -c 'SELECT pg_reload_conf();' psql -c 'show archive_mode' # You can also directly edit postgresql.auto.conf and reload with pg_ctl sed -i '/archive_mode/d' /pg/data/postgresql.auto.conf pg_ctl -D /pg/data reload Backup Set It’s generally recommended to perform a new full backup after PITR, but this is optional.\nReplicas If your postgres cluster has replicas, you also need to perform PITR on each replica. Alternatively, a simpler approach is to remove the replica data directory and restart patroni, which will reinitialize the replica from the primary. We’ll cover this scenario in the next multi-node cluster example.\nMulti-Node Example Now let’s use the three-node pg-test cluster as a PITR example.\n","categories":["Task"],"description":"Manually perform PITR following prompt scripts in sandbox environment","excerpt":"Manually perform PITR following prompt scripts in sandbox environment","ref":"/docs/pgsql/tutorial/example/","tags":"","title":"Manual Recovery"},{"body":" Use node_hugepage_count and node_hugepage_ratio or /pg/bin/pg-tune-hugepage\nIf you plan to enable HugePages, consider using node_hugepage_count and node_hugepage_ratio, and apply with ./node.yml -t node_tune.\nHugePages have pros and cons for databases. The advantage is that memory is managed exclusively, eliminating concerns about being reallocated and reducing database OOM risk. The disadvantage is that it may negatively impact performance in certain scenarios.\nBefore PostgreSQL starts, you need to allocate enough huge pages. The wasted portion can be reclaimed using the pg-tune-hugepage script, but this script is only available for PostgreSQL 15+.\nIf your PostgreSQL is already running, you can enable huge pages using the following method (PG15+ only):\nsync; echo 3 \u003e /proc/sys/vm/drop_caches # Flush disk, release system cache (be prepared for database perf impact) sudo /pg/bin/pg-tune-hugepage # Write nr_hugepages to /etc/sysctl.d/hugepage.conf pg restart \u003ccls\u003e # Restart postgres to use hugepage ","categories":["Task"],"description":"Enabling HugePage for PostgreSQL to reduce memory fragmentation and improve performance.","excerpt":"Enabling HugePage for PostgreSQL to reduce memory fragmentation and …","ref":"/docs/pgsql/tutorial/hugepage/","tags":"","title":"Enabling HugePage for PostgreSQL"},{"body":"This section provides step-by-step tutorials for common PostgreSQL tasks and scenarios.\nCitus Cluster: Deploy and manage Citus distributed clusters Disaster Drill: Emergency recovery when 2 of 3 nodes fail PG VIP: Configure L2 VIP for PostgreSQL clusters ","categories":["Tutorial"],"description":"Step-by-step guides for common PostgreSQL tasks and scenarios.","excerpt":"Step-by-step guides for common PostgreSQL tasks and scenarios.","ref":"/docs/pgsql/tutorial/","tags":"","title":"Tutorials"},{"body":"Parameters and reference documentation\n","categories":["Reference"],"description":"","excerpt":"Parameters and reference documentation\n","ref":"/docs/pgsql/_div_reference/","tags":"","title":"Reference"},{"body":"This document introduces Pigsty’s monitoring system architecture, including metrics, logs, and target management. It also covers how to monitor existing PG clusters and remote RDS services.\nMonitoring Overview Pigsty uses a modern observability stack for PostgreSQL monitoring:\nGrafana for metrics visualization and PostgreSQL datasource VictoriaMetrics for collecting metrics from PostgreSQL / Pgbouncer / Patroni / HAProxy / Node VictoriaLogs for logging PostgreSQL / Pgbouncer / Patroni / pgBackRest and host component logs Battery-included Grafana dashboards showcasing all aspects of PostgreSQL Metrics\nPostgreSQL monitoring metrics are fully defined by the pg_exporter configuration file: pg_exporter.yml They are further processed by Prometheus recording rules and alert rules: files/prometheus/rules/pgsql.yml.\nPigsty uses three identity labels: cls, ins, ip, which are attached to all metrics and logs. Additionally, metrics from Pgbouncer, host nodes (NODE), and load balancers are also used by Pigsty, with the same labels used whenever possible for correlation analysis.\n{ cls: pg-meta, ins: pg-meta-1, ip: 10.10.10.10 } { cls: pg-meta, ins: pg-test-1, ip: 10.10.10.11 } { cls: pg-meta, ins: pg-test-2, ip: 10.10.10.12 } { cls: pg-meta, ins: pg-test-3, ip: 10.10.10.13 } Logs\nPostgreSQL-related logs are collected by Vector and sent to the VictoriaLogs log storage/query service on infra nodes.\npg_log_dir: postgres log directory, defaults to /pg/log/postgres pgbouncer_log_dir: pgbouncer log directory, defaults to /pg/log/pgbouncer patroni_log_dir: patroni log directory, defaults to /pg/log/patroni pgbackrest_log_dir: pgbackrest log directory, defaults to /pg/log/pgbackrest Target Management\nPrometheus monitoring targets are defined in static files under /etc/prometheus/targets/pgsql/, with each instance having a corresponding file. Taking pg-meta-1 as an example:\n# pg-meta-1 [primary] @ 10.10.10.10 - labels: { cls: pg-meta, ins: pg-meta-1, ip: 10.10.10.10 } targets: - 10.10.10.10:9630 # \u003c--- pg_exporter for PostgreSQL metrics - 10.10.10.10:9631 # \u003c--- pg_exporter for pgbouncer metrics - 10.10.10.10:8008 # \u003c--- patroni metrics (when API SSL is not enabled) When the global flag patroni_ssl_enabled is set, patroni targets will be moved to a separate file /etc/prometheus/targets/patroni/\u003cins\u003e.yml, as it uses the https scrape endpoint. When monitoring RDS instances, monitoring targets are placed separately in the /etc/prometheus/targets/pgrds/ directory and managed by cluster.\nWhen removing a cluster using bin/pgsql-rm or pgsql-rm.yml, the Prometheus monitoring targets will be removed. You can also remove them manually or use subtasks from the playbook:\nbin/pgmon-rm \u003ccls|ins\u003e # Remove prometheus monitoring targets from all infra nodes Remote RDS monitoring targets are placed in /etc/prometheus/targets/pgrds/\u003ccls\u003e.yml, created by the pgsql-monitor.yml playbook or bin/pgmon-add script.\nMonitoring Modes Pigsty provides three monitoring modes to suit different monitoring needs.\nItem \\ Level L1 L2 L3 Name Basic Managed Standard Abbr RDS MANAGED FULL Scenario Connection string only, e.g., RDS Existing DB, nodes manageable Instances created by Pigsty PGCAT Features ✅ Fully Available ✅ Fully Available ✅ Fully Available PGSQL Features ✅ PG metrics only ✅ PG \u0026 node metrics only ✅ Full Features Connection Pool Metrics ❌ Not Available ⚠️ Optional ✅ Pre-installed Load Balancer Metrics ❌ Not Available ⚠️ Optional ✅ Pre-installed PGLOG Features ❌ Not Available ⚠️ Optional ✅ Pre-installed PG Exporter ⚠️ On infra nodes ✅ On DB nodes ✅ On DB nodes Node Exporter ❌ Not deployed ✅ On DB nodes ✅ On DB nodes Intrusiveness ✅ Non-intrusive ⚠️ Install Exporter ⚠️ Fully managed by Pigsty Monitor Existing Instances ✅ Supported ✅ Supported ❌ For Pigsty-managed only Monitoring Users \u0026 Views Manual setup Manual setup Auto-created by Pigsty Deployment Playbook bin/pgmon-add \u003ccls\u003e Partial pgsql.yml/node.yml pgsql.yml Required Permissions Connectable PGURL from infra SSH \u0026 sudo on DB nodes SSH \u0026 sudo on DB nodes Feature Summary PGCAT + PGRDS Most features Full features Databases fully managed by Pigsty are automatically monitored with the best support and typically require no configuration. For existing PostgreSQL clusters or RDS services, if the target DB nodes can be managed by Pigsty (ssh accessible, sudo available), you can consider managed deployment for a monitoring experience similar to native Pigsty. If you can only access the target database via PGURL (database connection string), such as remote RDS services, you can use basic mode to monitor the target database.\nMonitor Existing Cluster If the target DB nodes can be managed by Pigsty (ssh accessible and sudo available), you can use the pg_exporter task in the pgsql.yml playbook to deploy monitoring components (PG Exporter) on target nodes in the same way as standard deployments. You can also use the pgbouncer and pgbouncer_exporter tasks from that playbook to deploy connection pools and their monitoring on existing instance nodes. Additionally, you can use node_exporter, haproxy, and vector from node.yml to deploy host monitoring, load balancing, and log collection components, achieving an experience identical to native Pigsty database instances.\nThe definition method for existing clusters is exactly the same as for clusters managed by Pigsty. You selectively execute partial tasks from the pgsql.yml playbook instead of running the entire playbook.\n./node.yml -l \u003ccls\u003e -t node_repo,node_pkg # Add YUM repos from INFRA nodes and install packages on host nodes ./node.yml -l \u003ccls\u003e -t node_exporter,node_register # Configure host monitoring and add to VictoriaMetrics ./node.yml -l \u003ccls\u003e -t vector # Configure host log collection and send to VictoriaLogs ./pgsql.yml -l \u003ccls\u003e -t pg_exporter,pg_register # Configure PostgreSQL monitoring and register with VictoriaMetrics/Grafana Since the target database cluster already exists, you need to manually create monitoring users, schemas, and extensions on the target database cluster.\nMonitor RDS If you can only access the target database via PGURL (database connection string), you can configure according to the instructions here. In this mode, Pigsty deploys corresponding PG Exporters on INFRA nodes to scrape remote database metrics, as shown below:\n------ infra ------ | | | prometheus | v---- pg-foo-1 ----v | ^ | metrics | ^ | | pg_exporter \u003c-|------------|---- postgres | | (port: 20001) | | 10.10.10.10:5432 | | ^ | ^------------------^ | ^ | ^ | ^ | v---- pg-foo-2 ----v | ^ | metrics | ^ | | pg_exporter \u003c-|------------|---- postgres | | (port: 20002) | | 10.10.10.11:5433 | ------------------- ^------------------^ In this mode, the monitoring system will not have metrics from hosts, connection pools, load balancers, or high availability components, but the database itself and real-time status information from the data catalog are still available. Pigsty provides two dedicated monitoring dashboards focused on PostgreSQL metrics: PGRDS Cluster and PGRDS Instance, while overview and database-level monitoring reuses existing dashboards. Since Pigsty cannot manage your RDS, users need to configure monitoring objects on the target database in advance.\nLimitations when monitoring external Postgres instances pgBouncer connection pool metrics are not available Patroni high availability component metrics are not available Host node monitoring metrics are not available, including node HAProxy and Keepalived metrics Log collection and log-derived metrics are not available Here we use the sandbox environment as an example: suppose the pg-meta cluster is an RDS instance pg-foo-1 to be monitored, and the pg-test cluster is an RDS cluster pg-bar to be monitored:\nCreate monitoring schemas, users, and permissions on the target. Refer to Monitor Setup for details\nDeclare the cluster in the configuration inventory. For example, if we want to monitor “remote” pg-meta \u0026 pg-test clusters:\ninfra: # Infra cluster for proxies, monitoring, alerts, etc. hosts: { 10.10.10.10: { infra_seq: 1 } } vars: # Install pg_exporter on group 'infra' for remote postgres RDS pg_exporters: # List all remote instances here, assign a unique unused local port for k 20001: { pg_cluster: pg-foo, pg_seq: 1, pg_host: 10.10.10.10 , pg_databases: [{ name: meta }] } # Register meta database as Grafana datasource 20002: { pg_cluster: pg-bar, pg_seq: 1, pg_host: 10.10.10.11 , pg_port: 5432 } # Different connection string methods 20003: { pg_cluster: pg-bar, pg_seq: 2, pg_host: 10.10.10.12 , pg_exporter_url: 'postgres://dbuser_monitor:DBUser.Monitor@10.10.10.12:5432/postgres?sslmode=disable'} 20004: { pg_cluster: pg-bar, pg_seq: 3, pg_host: 10.10.10.13 , pg_monitor_username: dbuser_monitor, pg_monitor_password: DBUser.Monitor } Databases listed in the pg_databases field will be registered in Grafana as PostgreSQL datasources, providing data support for PGCAT monitoring dashboards. If you don’t want to use PGCAT and register databases in Grafana, simply set pg_databases to an empty array or leave it blank.\nExecute the add monitoring command: bin/pgmon-add \u003cclsname\u003e\nbin/pgmon-add pg-foo # Bring pg-foo cluster into monitoring bin/pgmon-add pg-bar # Bring pg-bar cluster into monitoring To remove remote cluster monitoring targets, use bin/pgmon-rm \u003cclsname\u003e\nbin/pgmon-rm pg-foo # Remove pg-foo from Pigsty monitoring bin/pgmon-rm pg-bar # Remove pg-bar from Pigsty monitoring You can use more parameters to override default pg_exporter options. Here’s an example configuration for monitoring Aliyun RDS for PostgreSQL and PolarDB with Pigsty:\nExample: Monitoring Aliyun RDS for PostgreSQL and PolarDB For details, refer to: remote.yml\ninfra: # Infra cluster for proxies, monitoring, alerts, etc. hosts: { 10.10.10.10: { infra_seq: 1 } } vars: pg_exporters: # List all remote RDS PG instances to be monitored here 20001: # Assign a unique unused local port for local monitoring agent, this is a PolarDB primary pg_cluster: pg-polar # RDS cluster name (identity parameter, manually assigned name in monitoring system) pg_seq: 1 # RDS instance number (identity parameter, manually assigned name in monitoring system) pg_host: pc-2ze379wb1d4irc18x.polardbpg.rds.aliyuncs.com # RDS host address pg_port: 1921 # RDS port (from console connection info) pg_exporter_auto_discovery: true # Disable new database auto-discovery feature pg_exporter_include_database: 'test' # Only monitor databases in this list (comma-separated) pg_monitor_username: dbuser_monitor # Monitoring username, overrides global config pg_monitor_password: DBUser_Monitor # Monitoring password, overrides global config pg_databases: [{ name: test }] # List of databases to enable PGCAT for, only name field needed, set register_datasource to false to not register 20002: # This is a PolarDB standby pg_cluster: pg-polar # RDS cluster name (identity parameter, manually assigned name in monitoring system) pg_seq: 2 # RDS instance number (identity parameter, manually assigned name in monitoring system) pg_host: pe-2ze7tg620e317ufj4.polarpgmxs.rds.aliyuncs.com # RDS host address pg_port: 1521 # RDS port (from console connection info) pg_exporter_auto_discovery: true # Disable new database auto-discovery feature pg_exporter_include_database: 'test,postgres' # Only monitor databases in this list (comma-separated) pg_monitor_username: dbuser_monitor # Monitoring username pg_monitor_password: DBUser_Monitor # Monitoring password pg_databases: [ { name: test } ] # List of databases to enable PGCAT for, only name field needed, set register_datasource to false to not register 20004: # This is a basic single-node RDS for PostgreSQL instance pg_cluster: pg-rds # RDS cluster name (identity parameter, manually assigned name in monitoring system) pg_seq: 1 # RDS instance number (identity parameter, manually assigned name in monitoring system) pg_host: pgm-2zern3d323fe9ewk.pg.rds.aliyuncs.com # RDS host address pg_port: 5432 # RDS port (from console connection info) pg_exporter_auto_discovery: true # Disable new database auto-discovery feature pg_exporter_include_database: 'rds' # Only monitor databases in this list (comma-separated) pg_monitor_username: dbuser_monitor # Monitoring username pg_monitor_password: DBUser_Monitor # Monitoring password pg_databases: [ { name: rds } ] # List of databases to enable PGCAT for, only name field needed, set register_datasource to false to not register 20005: # This is a high-availability RDS for PostgreSQL cluster primary pg_cluster: pg-rdsha # RDS cluster name (identity parameter, manually assigned name in monitoring system) pg_seq: 1 # RDS instance number (identity parameter, manually assigned name in monitoring system) pg_host: pgm-2ze3d35d27bq08wu.pg.rds.aliyuncs.com # RDS host address pg_port: 5432 # RDS port (from console connection info) pg_exporter_include_database: 'rds' # Only monitor databases in this list (comma-separated) pg_databases: [ { name: rds }, {name : test} ] # Include these two databases in PGCAT management, register as Grafana datasources 20006: # This is a high-availability RDS for PostgreSQL cluster read-only instance (standby) pg_cluster: pg-rdsha # RDS cluster name (identity parameter, manually assigned name in monitoring system) pg_seq: 2 # RDS instance number (identity parameter, manually assigned name in monitoring system) pg_host: pgr-2zexqxalk7d37edt.pg.rds.aliyuncs.com # RDS host address pg_port: 5432 # RDS port (from console connection info) pg_exporter_include_database: 'rds' # Only monitor databases in this list (comma-separated) pg_databases: [ { name: rds }, {name : test} ] # Include these two databases in PGCAT management, register as Grafana datasources Monitor Setup When you want to monitor existing instances, whether RDS or self-built PostgreSQL instances, you need to configure the target database so that Pigsty can access them.\nTo monitor an external existing PostgreSQL instance, you need a connection string that can access that instance/cluster. Any accessible connection string (business user, superuser) can be used, but we recommend using a dedicated monitoring user to avoid permission leaks.\nMonitor User: The default username is dbuser_monitor, which should belong to the pg_monitor role group or have access to relevant views Monitor Authentication: Default password authentication is used; ensure HBA policies allow the monitoring user to access databases from the admin node or DB node locally Monitor Schema: Fixed schema name monitor is used for installing additional monitoring views and extension plugins; optional but recommended Monitor Extension: Strongly recommended to enable the built-in monitoring extension pg_stat_statements Monitor Views: Monitoring views are optional but can provide additional metric support Monitor User Using the default monitoring user dbuser_monitor as an example, create the following user on the target database cluster.\nCREATE USER dbuser_monitor; -- Create monitoring user COMMENT ON ROLE dbuser_monitor IS 'system monitor user'; -- Comment on monitoring user GRANT pg_monitor TO dbuser_monitor; -- Grant pg_monitor privilege to monitoring user, otherwise some metrics cannot be collected ALTER USER dbuser_monitor PASSWORD 'DBUser.Monitor'; -- Modify monitoring user password as needed (strongly recommended! but keep consistent with Pigsty config) ALTER USER dbuser_monitor SET log_min_duration_statement = 1000; -- Recommended to avoid logs filling up with monitoring slow queries ALTER USER dbuser_monitor SET search_path = monitor,public; -- Recommended to ensure pg_stat_statements extension works properly Please note that the monitoring user and password created here should be consistent with pg_monitor_username and pg_monitor_password.\nMonitor Authentication Configure the database pg_hba.conf file, adding the following rules to allow the monitoring user to access all databases from localhost and the admin machine using password authentication.\n# allow local role monitor with password local all dbuser_monitor md5 host all dbuser_monitor 127.0.0.1/32 md5 host all dbuser_monitor \u003cadmin_machine_IP\u003e/32 md5 If your RDS doesn’t support defining HBA, simply whitelist the internal IP address of the machine running Pigsty.\nMonitor Schema The monitoring schema is optional; even without it, the main functionality of Pigsty’s monitoring system can work properly, but we strongly recommend creating this schema.\nCREATE SCHEMA IF NOT EXISTS monitor; -- Create dedicated monitoring schema GRANT USAGE ON SCHEMA monitor TO dbuser_monitor; -- Allow monitoring user to use it Monitor Extension The monitoring extension is optional, but we strongly recommend enabling the pg_stat_statements extension, which provides important data about query performance.\nNote: This extension must be listed in the database parameter shared_preload_libraries to take effect, and modifying that parameter requires a database restart.\nCREATE EXTENSION IF NOT EXISTS \"pg_stat_statements\" WITH SCHEMA \"monitor\"; Please note that you should install this extension in the default admin database postgres. Sometimes RDS doesn’t allow you to create a monitoring schema in the postgres database. In such cases, you can install the pg_stat_statements plugin in the default public schema, as long as you ensure the monitoring user’s search_path is configured as above so it can find the pg_stat_statements view.\nCREATE EXTENSION IF NOT EXISTS \"pg_stat_statements\"; ALTER USER dbuser_monitor SET search_path = monitor,public; -- Recommended to ensure pg_stat_statements extension works properly Monitor Views Monitoring views provide several commonly used pre-processed results and encapsulate permissions for monitoring metrics that require high privileges (such as shared memory allocation), making them convenient for querying and use. Strongly recommended to create in all databases requiring monitoring.\nMonitoring schema and monitoring view definitions ---------------------------------------------------------------------- -- Table bloat estimate : monitor.pg_table_bloat ---------------------------------------------------------------------- DROP VIEW IF EXISTS monitor.pg_table_bloat CASCADE; CREATE OR REPLACE VIEW monitor.pg_table_bloat AS SELECT CURRENT_CATALOG AS datname, nspname, relname , tblid , bs * tblpages AS size, CASE WHEN tblpages - est_tblpages_ff \u003e 0 THEN (tblpages - est_tblpages_ff)/tblpages::FLOAT ELSE 0 END AS ratio FROM ( SELECT ceil( reltuples / ( (bs-page_hdr)*fillfactor/(tpl_size*100) ) ) + ceil( toasttuples / 4 ) AS est_tblpages_ff, tblpages, fillfactor, bs, tblid, nspname, relname, is_na FROM ( SELECT ( 4 + tpl_hdr_size + tpl_data_size + (2 * ma) - CASE WHEN tpl_hdr_size % ma = 0 THEN ma ELSE tpl_hdr_size % ma END - CASE WHEN ceil(tpl_data_size)::INT % ma = 0 THEN ma ELSE ceil(tpl_data_size)::INT % ma END ) AS tpl_size, (heappages + toastpages) AS tblpages, heappages, toastpages, reltuples, toasttuples, bs, page_hdr, tblid, nspname, relname, fillfactor, is_na FROM ( SELECT tbl.oid AS tblid, ns.nspname , tbl.relname, tbl.reltuples, tbl.relpages AS heappages, coalesce(toast.relpages, 0) AS toastpages, coalesce(toast.reltuples, 0) AS toasttuples, coalesce(substring(array_to_string(tbl.reloptions, ' ') FROM 'fillfactor=([0-9]+)')::smallint, 100) AS fillfactor, current_setting('block_size')::numeric AS bs, CASE WHEN version()~'mingw32' OR version()~'64-bit|x86_64|ppc64|ia64|amd64' THEN 8 ELSE 4 END AS ma, 24 AS page_hdr, 23 + CASE WHEN MAX(coalesce(s.null_frac,0)) \u003e 0 THEN ( 7 + count(s.attname) ) / 8 ELSE 0::int END + CASE WHEN bool_or(att.attname = 'oid' and att.attnum \u003c 0) THEN 4 ELSE 0 END AS tpl_hdr_size, sum( (1-coalesce(s.null_frac, 0)) * coalesce(s.avg_width, 0) ) AS tpl_data_size, bool_or(att.atttypid = 'pg_catalog.name'::regtype) OR sum(CASE WHEN att.attnum \u003e 0 THEN 1 ELSE 0 END) \u003c\u003e count(s.attname) AS is_na FROM pg_attribute AS att JOIN pg_class AS tbl ON att.attrelid = tbl.oid JOIN pg_namespace AS ns ON ns.oid = tbl.relnamespace LEFT JOIN pg_stats AS s ON s.schemaname=ns.nspname AND s.tablename = tbl.relname AND s.inherited=false AND s.attname=att.attname LEFT JOIN pg_class AS toast ON tbl.reltoastrelid = toast.oid WHERE NOT att.attisdropped AND tbl.relkind = 'r' AND nspname NOT IN ('pg_catalog','information_schema') GROUP BY 1,2,3,4,5,6,7,8,9,10 ) AS s ) AS s2 ) AS s3 WHERE NOT is_na; COMMENT ON VIEW monitor.pg_table_bloat IS 'postgres table bloat estimate'; GRANT SELECT ON monitor.pg_table_bloat TO pg_monitor; ---------------------------------------------------------------------- -- Index bloat estimate : monitor.pg_index_bloat ---------------------------------------------------------------------- DROP VIEW IF EXISTS monitor.pg_index_bloat CASCADE; CREATE OR REPLACE VIEW monitor.pg_index_bloat AS SELECT CURRENT_CATALOG AS datname, nspname, idxname AS relname, tblid, idxid, relpages::BIGINT * bs AS size, COALESCE((relpages - ( reltuples * (6 + ma - (CASE WHEN index_tuple_hdr % ma = 0 THEN ma ELSE index_tuple_hdr % ma END) + nulldatawidth + ma - (CASE WHEN nulldatawidth % ma = 0 THEN ma ELSE nulldatawidth % ma END)) / (bs - pagehdr)::FLOAT + 1 )), 0) / relpages::FLOAT AS ratio FROM ( SELECT nspname,idxname,indrelid AS tblid,indexrelid AS idxid, reltuples,relpages, current_setting('block_size')::INTEGER AS bs, (CASE WHEN version() ~ 'mingw32' OR version() ~ '64-bit|x86_64|ppc64|ia64|amd64' THEN 8 ELSE 4 END) AS ma, 24 AS pagehdr, (CASE WHEN max(COALESCE(pg_stats.null_frac, 0)) = 0 THEN 2 ELSE 6 END) AS index_tuple_hdr, sum((1.0 - COALESCE(pg_stats.null_frac, 0.0)) * COALESCE(pg_stats.avg_width, 1024))::INTEGER AS nulldatawidth FROM pg_attribute JOIN ( SELECT pg_namespace.nspname, ic.relname AS idxname, ic.reltuples, ic.relpages, pg_index.indrelid, pg_index.indexrelid, tc.relname AS tablename, regexp_split_to_table(pg_index.indkey::TEXT, ' ') :: INTEGER AS attnum, pg_index.indexrelid AS index_oid FROM pg_index JOIN pg_class ic ON pg_index.indexrelid = ic.oid JOIN pg_class tc ON pg_index.indrelid = tc.oid JOIN pg_namespace ON pg_namespace.oid = ic.relnamespace JOIN pg_am ON ic.relam = pg_am.oid WHERE pg_am.amname = 'btree' AND ic.relpages \u003e 0 AND nspname NOT IN ('pg_catalog', 'information_schema') ) ind_atts ON pg_attribute.attrelid = ind_atts.indexrelid AND pg_attribute.attnum = ind_atts.attnum JOIN pg_stats ON pg_stats.schemaname = ind_atts.nspname AND ((pg_stats.tablename = ind_atts.tablename AND pg_stats.attname = pg_get_indexdef(pg_attribute.attrelid, pg_attribute.attnum, TRUE)) OR (pg_stats.tablename = ind_atts.idxname AND pg_stats.attname = pg_attribute.attname)) WHERE pg_attribute.attnum \u003e 0 GROUP BY 1, 2, 3, 4, 5, 6 ) est; COMMENT ON VIEW monitor.pg_index_bloat IS 'postgres index bloat estimate (btree-only)'; GRANT SELECT ON monitor.pg_index_bloat TO pg_monitor; ---------------------------------------------------------------------- -- Relation Bloat : monitor.pg_bloat ---------------------------------------------------------------------- DROP VIEW IF EXISTS monitor.pg_bloat CASCADE; CREATE OR REPLACE VIEW monitor.pg_bloat AS SELECT coalesce(ib.datname, tb.datname) AS datname, coalesce(ib.nspname, tb.nspname) AS nspname, coalesce(ib.tblid, tb.tblid) AS tblid, coalesce(tb.nspname || '.' || tb.relname, ib.nspname || '.' || ib.tblid::RegClass) AS tblname, tb.size AS tbl_size, CASE WHEN tb.ratio \u003c 0 THEN 0 ELSE round(tb.ratio::NUMERIC, 6) END AS tbl_ratio, (tb.size * (CASE WHEN tb.ratio \u003c 0 THEN 0 ELSE tb.ratio::NUMERIC END)) ::BIGINT AS tbl_wasted, ib.idxid, ib.nspname || '.' || ib.relname AS idxname, ib.size AS idx_size, CASE WHEN ib.ratio \u003c 0 THEN 0 ELSE round(ib.ratio::NUMERIC, 5) END AS idx_ratio, (ib.size * (CASE WHEN ib.ratio \u003c 0 THEN 0 ELSE ib.ratio::NUMERIC END)) ::BIGINT AS idx_wasted FROM monitor.pg_index_bloat ib FULL OUTER JOIN monitor.pg_table_bloat tb ON ib.tblid = tb.tblid; COMMENT ON VIEW monitor.pg_bloat IS 'postgres relation bloat detail'; GRANT SELECT ON monitor.pg_bloat TO pg_monitor; ---------------------------------------------------------------------- -- monitor.pg_index_bloat_human ---------------------------------------------------------------------- DROP VIEW IF EXISTS monitor.pg_index_bloat_human CASCADE; CREATE OR REPLACE VIEW monitor.pg_index_bloat_human AS SELECT idxname AS name, tblname, idx_wasted AS wasted, pg_size_pretty(idx_size) AS idx_size, round(100 * idx_ratio::NUMERIC, 2) AS idx_ratio, pg_size_pretty(idx_wasted) AS idx_wasted, pg_size_pretty(tbl_size) AS tbl_size, round(100 * tbl_ratio::NUMERIC, 2) AS tbl_ratio, pg_size_pretty(tbl_wasted) AS tbl_wasted FROM monitor.pg_bloat WHERE idxname IS NOT NULL; COMMENT ON VIEW monitor.pg_index_bloat_human IS 'postgres index bloat info in human-readable format'; GRANT SELECT ON monitor.pg_index_bloat_human TO pg_monitor; ---------------------------------------------------------------------- -- monitor.pg_table_bloat_human ---------------------------------------------------------------------- DROP VIEW IF EXISTS monitor.pg_table_bloat_human CASCADE; CREATE OR REPLACE VIEW monitor.pg_table_bloat_human AS SELECT tblname AS name, idx_wasted + tbl_wasted AS wasted, pg_size_pretty(idx_wasted + tbl_wasted) AS all_wasted, pg_size_pretty(tbl_wasted) AS tbl_wasted, pg_size_pretty(tbl_size) AS tbl_size, tbl_ratio, pg_size_pretty(idx_wasted) AS idx_wasted, pg_size_pretty(idx_size) AS idx_size, round(idx_wasted::NUMERIC * 100.0 / idx_size, 2) AS idx_ratio FROM (SELECT datname, nspname, tblname, coalesce(max(tbl_wasted), 0) AS tbl_wasted, coalesce(max(tbl_size), 1) AS tbl_size, round(100 * coalesce(max(tbl_ratio), 0)::NUMERIC, 2) AS tbl_ratio, coalesce(sum(idx_wasted), 0) AS idx_wasted, coalesce(sum(idx_size), 1) AS idx_size FROM monitor.pg_bloat WHERE tblname IS NOT NULL GROUP BY 1, 2, 3 ) d; COMMENT ON VIEW monitor.pg_table_bloat_human IS 'postgres table bloat info in human-readable format'; GRANT SELECT ON monitor.pg_table_bloat_human TO pg_monitor; ---------------------------------------------------------------------- -- Activity Overview: monitor.pg_session ---------------------------------------------------------------------- DROP VIEW IF EXISTS monitor.pg_session CASCADE; CREATE OR REPLACE VIEW monitor.pg_session AS SELECT coalesce(datname, 'all') AS datname, numbackends, active, idle, ixact, max_duration, max_tx_duration, max_conn_duration FROM ( SELECT datname, count(*) AS numbackends, count(*) FILTER ( WHERE state = 'active' ) AS active, count(*) FILTER ( WHERE state = 'idle' ) AS idle, count(*) FILTER ( WHERE state = 'idle in transaction' OR state = 'idle in transaction (aborted)' ) AS ixact, max(extract(epoch from now() - state_change)) FILTER ( WHERE state = 'active' ) AS max_duration, max(extract(epoch from now() - xact_start)) AS max_tx_duration, max(extract(epoch from now() - backend_start)) AS max_conn_duration FROM pg_stat_activity WHERE backend_type = 'client backend' AND pid \u003c\u003e pg_backend_pid() GROUP BY ROLLUP (1) ORDER BY 1 NULLS FIRST ) t; COMMENT ON VIEW monitor.pg_session IS 'postgres activity group by session'; GRANT SELECT ON monitor.pg_session TO pg_monitor; ---------------------------------------------------------------------- -- Sequential Scan: monitor.pg_seq_scan ---------------------------------------------------------------------- DROP VIEW IF EXISTS monitor.pg_seq_scan CASCADE; CREATE OR REPLACE VIEW monitor.pg_seq_scan AS SELECT schemaname AS nspname, relname, seq_scan, seq_tup_read, seq_tup_read / seq_scan AS seq_tup_avg, idx_scan, n_live_tup + n_dead_tup AS tuples, round(n_live_tup * 100.0::NUMERIC / (n_live_tup + n_dead_tup), 2) AS live_ratio FROM pg_stat_user_tables WHERE seq_scan \u003e 0 and (n_live_tup + n_dead_tup) \u003e 0 ORDER BY seq_scan DESC; COMMENT ON VIEW monitor.pg_seq_scan IS 'table that have seq scan'; GRANT SELECT ON monitor.pg_seq_scan TO pg_monitor; Function for viewing shared memory allocation (PG13 and above) DROP FUNCTION IF EXISTS monitor.pg_shmem() CASCADE; CREATE OR REPLACE FUNCTION monitor.pg_shmem() RETURNS SETOF pg_shmem_allocations AS $$ SELECT * FROM pg_shmem_allocations;$$ LANGUAGE SQL SECURITY DEFINER; COMMENT ON FUNCTION monitor.pg_shmem() IS 'security wrapper for system view pg_shmem'; REVOKE ALL ON FUNCTION monitor.pg_shmem() FROM PUBLIC; GRANT EXECUTE ON FUNCTION monitor.pg_shmem() TO pg_monitor; ","categories":["Reference"],"description":"Overview of Pigsty's monitoring system architecture and how to monitor existing PostgreSQL instances","excerpt":"Overview of Pigsty's monitoring system architecture and how to monitor …","ref":"/docs/pgsql/monitor/","tags":"","title":"Monitoring"},{"body":" Pigsty provides numerous out-of-the-box Grafana monitoring dashboards for PostgreSQL: Demo \u0026 Gallery.\nPigsty has 26 PostgreSQL-related monitoring dashboards, organized by hierarchy into Overview, Cluster, Instance, and Database categories, and by data source into PGSQL, PGCAT, and PGLOG categories.\nOverview Overview Cluster Instance Database PGSQL Overview PGSQL Cluster PGSQL Instance PGSQL Database PGSQL Alert PGRDS Cluster PGRDS Instance PGCAT Database PGSQL Shard PGSQL Activity PGCAT Instance PGSQL Tables PGSQL Replication PGSQL Persist PGSQL Table PGSQL Service PGSQL Proxy PGCAT Table PGSQL Databases PGSQL Pgbouncer PGSQL Query PGSQL Patroni PGSQL Session PGCAT Query PGSQL PITR PGSQL Xacts PGCAT Locks PGSQL Exporter PGCAT Schema Overview\npgsql-overview: Main dashboard for the PGSQL module pgsql-alert: Global key metrics and alert events for PGSQL pgsql-shard: Overview of horizontally sharded PGSQL clusters (e.g., Citus/GPSQL) Cluster\npgsql-cluster: Main dashboard for a PGSQL cluster pgrds-cluster: RDS version of PGSQL Cluster, focusing on PostgreSQL-native metrics pgsql-activity: Session/load/QPS/TPS/locks for PGSQL cluster pgsql-replication: Replication, slots, and pub/sub for PGSQL cluster pgsql-service: Service, proxy, routing, and load balancing for PGSQL cluster pgsql-databases: Database CRUD, slow queries, and table statistics across all instances pgsql-patroni: HA status and Patroni component status for cluster pgsql-pitr: PITR context for point-in-time recovery assistance Instance\npgsql-instance: Main dashboard for a single PGSQL instance pgrds-instance: RDS version of PGSQL Instance, focusing on PostgreSQL-native metrics pgcat-instance: Instance info retrieved directly from database catalog pgsql-proxy: Detailed metrics for a single HAProxy load balancer pgsql-pgbouncer: Metrics overview for a single Pgbouncer connection pooler pgsql-persist: Persistence metrics: WAL, XID, checkpoint, archive, IO pgsql-session: Session and active/idle time metrics for a single instance pgsql-xacts: Transaction, lock, TPS/QPS related metrics pgsql-exporter: Self-monitoring metrics for Postgres and Pgbouncer exporters Database\npgsql-database: Main dashboard for a single PGSQL database pgcat-database: Database info retrieved directly from database catalog pgsql-tables: Table/index access metrics within a single database pgsql-table: Detailed info for a single table (QPS/RT/index/sequence…) pgcat-table: Detailed table info from database catalog (stats/bloat…) pgsql-query: Detailed info for a query type (QPS/RT) pgcat-query: Query details from database catalog (SQL/stats) pgcat-schema: Schema info from database catalog (tables/indexes/sequences…) pgcat-locks: Activity and lock wait info from database catalog Overview PGSQL Overview: Main dashboard for the PGSQL module\nPGSQL Overview PGSQL Alert: Global core metrics overview and alert events\nPGSQL Alert PGSQL Shard: Cross-shard metric comparison for horizontally sharded PGSQL clusters (e.g., CITUS/GPSQL)\nPGSQL Shard Cluster PGSQL Cluster: Main dashboard for a PGSQL cluster\nPGSQL Cluster PGRDS Cluster: RDS version of PGSQL Cluster, focusing on PostgreSQL-native metrics\nPGRDS Cluster PGSQL Service: Service, proxy, routing, and load balancing for PGSQL cluster\nPGSQL Service PGSQL Activity: Session/load/QPS/TPS/locks for PGSQL cluster\nPGSQL Activity PGSQL Replication: Replication, slots, and pub/sub for PGSQL cluster\nPGSQL Replication PGSQL Databases: Database CRUD, slow queries, and table statistics across all instances\nPGSQL Databases PGSQL Patroni: HA status and Patroni component status for cluster\nPGSQL Patroni PGSQL PITR: PITR context for point-in-time recovery assistance\nPGSQL PITR Instance PGSQL Instance: Main dashboard for a single PGSQL instance\nPGSQL Instance PGRDS Instance: RDS version of PGSQL Instance, focusing on PostgreSQL-native metrics\nPGRDS Instance PGSQL Proxy: Detailed metrics for a single HAProxy load balancer\nPGSQL Proxy PGSQL Pgbouncer: Metrics overview for a single Pgbouncer connection pooler\nPGSQL Pgbouncer PGSQL Persist: Persistence metrics: WAL, XID, checkpoint, archive, IO\nPGSQL Persist PGSQL Xacts: Transaction, lock, TPS/QPS related metrics\nPGSQL Xacts PGSQL Session: Session and active/idle time metrics for a single instance\nPGSQL Session PGSQL Exporter: Self-monitoring metrics for Postgres/Pgbouncer exporters\nPGSQL Exporter Database PGSQL Database: Main dashboard for a single PGSQL database\nPGSQL Database PGSQL Tables: Table/index access metrics within a single database\nPGSQL Tables PGSQL Table: Detailed info for a single table (QPS/RT/index/sequence…)\nPGSQL Table PGSQL Query: Detailed info for a query type (QPS/RT)\nPGSQL Query PGCAT PGCAT Instance: Instance info retrieved directly from database catalog\nPGCAT Instance PGCAT Database: Database info retrieved directly from database catalog\nPGCAT Database PGCAT Schema: Schema info from database catalog (tables/indexes/sequences…)\nPGCAT Schema PGCAT Table: Detailed table info from database catalog (stats/bloat…)\nPGCAT Table PGCAT Query: Query details from database catalog (SQL/stats)\nPGCAT Query PGCAT Locks: Activity and lock wait info from database catalog\nPGCAT Locks PGLOG PGLOG Overview: Overview of CSV log samples in Pigsty CMDB\nPGLOG Overview PGLOG Session: Log details for a single session in CSV log samples\nPGLOG Session Gallery See pigsty/wiki/gallery for details.\nPGSQL Overview PGSQL Shard PGSQL Cluster PGSQL Service PGSQL Activity PGSQL Replication PGSQL Databases PGSQL Instance PGSQL Proxy PGSQL Pgbouncer PGSQL Session PGSQL Xacts PGSQL Persist PGSQL Database PGSQL Tables PGSQL Table PGSQL Query PGCAT Instance PGCAT Database PGCAT Schema PGCAT Table PGCAT Lock PGCAT Query PGLOG Overview PGLOG Session ","categories":["Reference"],"description":"Pigsty provides numerous out-of-the-box Grafana monitoring dashboards for PostgreSQL","excerpt":"Pigsty provides numerous out-of-the-box Grafana monitoring dashboards …","ref":"/docs/pgsql/dashboard/","tags":"","title":"Dashboard"},{"body":" Pigsty provides many out-of-the-box Grafana monitoring dashboards for PostgreSQL: Demo \u0026 Gallery.\nThere are 26 PostgreSQL-related monitoring dashboards in Pigsty, organized hierarchically into Overview, Cluster, Instance, and Database categories, and by data source into PGSQL, PGCAT, and PGLOG categories.\nOverview Overview Cluster Instance Database PGSQL Overview PGSQL Cluster PGSQL Instance PGSQL Database PGSQL Alert PGRDS Cluster PGRDS Instance PGCAT Database PGSQL Shard PGSQL Activity PGCAT Instance PGSQL Tables PGSQL Replication PGSQL Persist PGSQL Table PGSQL Service PGSQL Proxy PGCAT Table PGSQL Databases PGSQL Pgbouncer PGSQL Query PGSQL Patroni PGSQL Session PGCAT Query PGSQL PITR PGSQL Xacts PGCAT Locks PGSQL Exporter PGCAT Schema Overview\npgsql-overview: Main dashboard for the PGSQL module pgsql-alert: Global critical metrics and alert events for PGSQL pgsql-shard: Overview of horizontally sharded PGSQL clusters, such as Citus / GPSQL clusters Cluster\npgsql-cluster: Main dashboard for a PGSQL cluster pgrds-cluster: RDS version of PGSQL Cluster, focused on all PostgreSQL-specific metrics pgsql-activity: Focus on PGSQL cluster sessions/load/QPS/TPS/locks pgsql-replication: Focus on PGSQL cluster replication, slots, and pub/sub pgsql-service: Focus on PGSQL cluster services, proxies, routing, and load balancing pgsql-databases: Focus on database CRUD, slow queries, and table statistics across all instances pgsql-patroni: Focus on cluster high availability status and Patroni component status pgsql-pitr: Focus on cluster PITR process context for point-in-time recovery assistance Instance\npgsql-instance: Main dashboard for a single PGSQL instance pgrds-instance: RDS version of PGSQL Instance, focused on all PostgreSQL-specific metrics pgcat-instance: Instance information retrieved directly from the database catalog pgsql-proxy: Detailed metrics for a single HAProxy load balancer pgsql-pgbouncer: Metrics overview in a single Pgbouncer connection pool instance pgsql-persist: Persistence metrics: WAL, XID, checkpoints, archiving, IO pgsql-session: Session and active/idle time metrics in a single instance pgsql-xacts: Metrics related to transactions, locks, TPS/QPS pgsql-exporter: Self-monitoring metrics for Postgres and Pgbouncer monitoring components Database\npgsql-database: Main dashboard for a single PGSQL database pgcat-database: Database information retrieved directly from the database catalog pgsql-tables: Table/index access metrics within a single database pgsql-table: Details of a single table (QPS/RT/index/sequences…) pgcat-table: Details of a single table retrieved directly from the database catalog (stats/bloat…) pgsql-query: Details of a single query (QPS/RT) pgcat-query: Details of a single query retrieved directly from the database catalog (SQL/stats) pgcat-schema: Information about schemas retrieved directly from the database catalog (tables/indexes/sequences…) pgcat-locks: Information about activities and lock waits retrieved directly from the database catalog Overview PGSQL Overview: Main dashboard for the PGSQL module\nPGSQL Overview PGSQL Alert: Global critical metrics overview and alert event listing for PGSQL\nPGSQL Alert PGSQL Shard: Shows horizontal metric comparisons within a PGSQL horizontally sharded cluster, such as CITUS / GPSQL clusters\nPGSQL Shard Cluster PGSQL Cluster: Main dashboard for a PGSQL cluster\nPGSQL Cluster PGRDS Cluster: RDS version of PGSQL Cluster, focused on all PostgreSQL-specific metrics\nPGRDS Cluster PGSQL Service: Focus on PGSQL cluster services, proxies, routing, and load balancing\nPGSQL Service PGSQL Activity: Focus on PGSQL cluster sessions/load/QPS/TPS/locks\nPGSQL Activity PGSQL Replication: Focus on PGSQL cluster replication, slots, and pub/sub\nPGSQL Replication PGSQL Databases: Focus on database CRUD, slow queries, and table statistics across all instances\nPGSQL Databases PGSQL Patroni: Focus on cluster high availability status and Patroni component status\nPGSQL Patroni PGSQL PITR: Focus on cluster PITR process context for point-in-time recovery assistance\nPGSQL PITR Instance PGSQL Instance: Main dashboard for a single PGSQL instance\nPGSQL Instance PGRDS Instance: RDS version of PGSQL Instance, focused on all PostgreSQL-specific metrics\nPGRDS Instance PGSQL Proxy: Detailed metrics for a single HAProxy load balancer\nPGSQL Proxy PGSQL Pgbouncer: Metrics overview in a single Pgbouncer connection pool instance\nPGSQL Pgbouncer PGSQL Persist: Persistence metrics: WAL, XID, checkpoints, archiving, IO\nPGSQL Persist PGSQL Xacts: Metrics related to transactions, locks, TPS/QPS\nPGSQL Xacts PGSQL Session: Session and active/idle time metrics in a single instance\nPGSQL Session PGSQL Exporter: Self-monitoring metrics for Postgres/Pgbouncer monitoring components\nPGSQL Exporter Database PGSQL Database: Main dashboard for a single PGSQL database\nPGSQL Database PGSQL Tables: Table/index access metrics within a single database\nPGSQL Tables PGSQL Table: Details of a single table (QPS/RT/index/sequences…)\nPGSQL Table PGSQL Query: Details of a single query (QPS/RT)\nPGSQL Query PGCAT PGCAT Instance: Instance information retrieved directly from the database catalog\nPGCAT Instance PGCAT Database: Database information retrieved directly from the database catalog\nPGCAT Database PGCAT Schema: Information about schemas retrieved directly from the database catalog (tables/indexes/sequences…)\nPGCAT Schema PGCAT Table: Details of a single table retrieved directly from the database catalog (stats/bloat…)\nPGCAT Table PGCAT Query: Details of a single query retrieved directly from the database catalog (SQL/stats)\nPGCAT Query PGCAT Locks: Information about activities and lock waits retrieved directly from the database catalog\nPGCAT Locks PGLOG PGLOG Overview: Overview of CSV log samples in Pigsty CMDB\nPGLOG Overview PGLOG Session: Log details of a session in CSV log samples in Pigsty CMDB\nPGLOG Session Gallery For details, refer to pigsty/wiki/gallery.\nPGSQL Overview PGSQL Shard PGSQL Cluster PGSQL Service PGSQL Activity PGSQL Replication PGSQL Databases PGSQL Instance PGSQL Proxy PGSQL Pgbouncer PGSQL Session PGSQL Xacts PGSQL Persist PGSQL Database PGSQL Tables PGSQL Table PGSQL Query PGCAT Instance PGCAT Database PGCAT Schema PGCAT Table PGCAT Lock PGCAT Query PGLOG Overview PGLOG Session ","categories":["Reference"],"description":"Pigsty provides many out-of-the-box Grafana monitoring dashboards for PostgreSQL","excerpt":"Pigsty provides many out-of-the-box Grafana monitoring dashboards for …","ref":"/docs/pgsql/monitor/dashboard/","tags":"","title":"Dashboards"},{"body":"The PGSQL module provides 638 available monitoring metrics.\nMetric Name Type Labels Description ALERTS Unknown category, job, level, ins, severity, ip, alertname, alertstate, instance, cls N/A ALERTS_FOR_STATE Unknown category, job, level, ins, severity, ip, alertname, instance, cls N/A cls:pressure1 Unknown job, cls N/A cls:pressure15 Unknown job, cls N/A cls:pressure5 Unknown job, cls N/A go_gc_duration_seconds summary job, ins, ip, instance, quantile, cls A summary of the pause duration of garbage collection cycles. go_gc_duration_seconds_count Unknown job, ins, ip, instance, cls N/A go_gc_duration_seconds_sum Unknown job, ins, ip, instance, cls N/A go_goroutines gauge job, ins, ip, instance, cls Number of goroutines that currently exist. go_info gauge version, job, ins, ip, instance, cls Information about the Go environment. go_memstats_alloc_bytes gauge job, ins, ip, instance, cls Number of bytes allocated and still in use. go_memstats_alloc_bytes_total counter job, ins, ip, instance, cls Total number of bytes allocated, even if freed. go_memstats_buck_hash_sys_bytes gauge job, ins, ip, instance, cls Number of bytes used by the profiling bucket hash table. go_memstats_frees_total counter job, ins, ip, instance, cls Total number of frees. go_memstats_gc_sys_bytes gauge job, ins, ip, instance, cls Number of bytes used for garbage collection system metadata. go_memstats_heap_alloc_bytes gauge job, ins, ip, instance, cls Number of heap bytes allocated and still in use. go_memstats_heap_idle_bytes gauge job, ins, ip, instance, cls Number of heap bytes waiting to be used. go_memstats_heap_inuse_bytes gauge job, ins, ip, instance, cls Number of heap bytes that are in use. go_memstats_heap_objects gauge job, ins, ip, instance, cls Number of allocated objects. go_memstats_heap_released_bytes gauge job, ins, ip, instance, cls Number of heap bytes released to OS. go_memstats_heap_sys_bytes gauge job, ins, ip, instance, cls Number of heap bytes obtained from system. go_memstats_last_gc_time_seconds gauge job, ins, ip, instance, cls Number of seconds since 1970 of last garbage collection. go_memstats_lookups_total counter job, ins, ip, instance, cls Total number of pointer lookups. go_memstats_mallocs_total counter job, ins, ip, instance, cls Total number of mallocs. go_memstats_mcache_inuse_bytes gauge job, ins, ip, instance, cls Number of bytes in use by mcache structures. go_memstats_mcache_sys_bytes gauge job, ins, ip, instance, cls Number of bytes used for mcache structures obtained from system. go_memstats_mspan_inuse_bytes gauge job, ins, ip, instance, cls Number of bytes in use by mspan structures. go_memstats_mspan_sys_bytes gauge job, ins, ip, instance, cls Number of bytes used for mspan structures obtained from system. go_memstats_next_gc_bytes gauge job, ins, ip, instance, cls Number of heap bytes when next garbage collection will take place. go_memstats_other_sys_bytes gauge job, ins, ip, instance, cls Number of bytes used for other system allocations. go_memstats_stack_inuse_bytes gauge job, ins, ip, instance, cls Number of bytes in use by the stack allocator. go_memstats_stack_sys_bytes gauge job, ins, ip, instance, cls Number of bytes obtained from system for stack allocator. go_memstats_sys_bytes gauge job, ins, ip, instance, cls Number of bytes obtained from system. go_threads gauge job, ins, ip, instance, cls Number of OS threads created. ins:pressure1 Unknown job, ins, ip, cls N/A ins:pressure15 Unknown job, ins, ip, cls N/A ins:pressure5 Unknown job, ins, ip, cls N/A patroni_cluster_unlocked gauge job, ins, ip, instance, cls, scope Value is 1 if the cluster is unlocked, 0 if locked. patroni_dcs_last_seen gauge job, ins, ip, instance, cls, scope Epoch timestamp when DCS was last contacted successfully by Patroni. patroni_failsafe_mode_is_active gauge job, ins, ip, instance, cls, scope Value is 1 if failsafe mode is active, 0 if inactive. patroni_is_paused gauge job, ins, ip, instance, cls, scope Value is 1 if auto failover is disabled, 0 otherwise. patroni_master gauge job, ins, ip, instance, cls, scope Value is 1 if this node is the leader, 0 otherwise. patroni_pending_restart gauge job, ins, ip, instance, cls, scope Value is 1 if the node needs a restart, 0 otherwise. patroni_postgres_in_archive_recovery gauge job, ins, ip, instance, cls, scope Value is 1 if Postgres is replicating from archive, 0 otherwise. patroni_postgres_running gauge job, ins, ip, instance, cls, scope Value is 1 if Postgres is running, 0 otherwise. patroni_postgres_server_version gauge job, ins, ip, instance, cls, scope Version of Postgres (if running), 0 otherwise. patroni_postgres_streaming gauge job, ins, ip, instance, cls, scope Value is 1 if Postgres is streaming, 0 otherwise. patroni_postgres_timeline counter job, ins, ip, instance, cls, scope Postgres timeline of this node (if running), 0 otherwise. patroni_postmaster_start_time gauge job, ins, ip, instance, cls, scope Epoch seconds since Postgres started. patroni_primary gauge job, ins, ip, instance, cls, scope Value is 1 if this node is the leader, 0 otherwise. patroni_replica gauge job, ins, ip, instance, cls, scope Value is 1 if this node is a replica, 0 otherwise. patroni_standby_leader gauge job, ins, ip, instance, cls, scope Value is 1 if this node is the standby_leader, 0 otherwise. patroni_sync_standby gauge job, ins, ip, instance, cls, scope Value is 1 if this node is a sync standby replica, 0 otherwise. patroni_up Unknown job, ins, ip, instance, cls N/A patroni_version gauge job, ins, ip, instance, cls, scope Patroni semver without periods. patroni_xlog_location counter job, ins, ip, instance, cls, scope Current location of the Postgres transaction log, 0 if this node is not the leader. patroni_xlog_paused gauge job, ins, ip, instance, cls, scope Value is 1 if the Postgres xlog is paused, 0 otherwise. patroni_xlog_received_location counter job, ins, ip, instance, cls, scope Current location of the received Postgres transaction log, 0 if this node is not a replica. patroni_xlog_replayed_location counter job, ins, ip, instance, cls, scope Current location of the replayed Postgres transaction log, 0 if this node is not a replica. patroni_xlog_replayed_timestamp gauge job, ins, ip, instance, cls, scope Current timestamp of the replayed Postgres transaction log, 0 if null. pg:cls:active_backends Unknown job, cls N/A pg:cls:active_time_rate15m Unknown job, cls N/A pg:cls:active_time_rate1m Unknown job, cls N/A pg:cls:active_time_rate5m Unknown job, cls N/A pg:cls:age Unknown job, cls N/A pg:cls:buf_alloc_rate1m Unknown job, cls N/A pg:cls:buf_clean_rate1m Unknown job, cls N/A pg:cls:buf_flush_backend_rate1m Unknown job, cls N/A pg:cls:buf_flush_checkpoint_rate1m Unknown job, cls N/A pg:cls:cpu_count Unknown job, cls N/A pg:cls:cpu_usage Unknown job, cls N/A pg:cls:cpu_usage_15m Unknown job, cls N/A pg:cls:cpu_usage_1m Unknown job, cls N/A pg:cls:cpu_usage_5m Unknown job, cls N/A pg:cls:db_size Unknown job, cls N/A pg:cls:file_size Unknown job, cls N/A pg:cls:ixact_backends Unknown job, cls N/A pg:cls:ixact_time_rate1m Unknown job, cls N/A pg:cls:lag_bytes Unknown job, cls N/A pg:cls:lag_seconds Unknown job, cls N/A pg:cls:leader Unknown job, ins, ip, instance, cls N/A pg:cls:load1 Unknown job, cls N/A pg:cls:load15 Unknown job, cls N/A pg:cls:load5 Unknown job, cls N/A pg:cls:lock_count Unknown job, cls N/A pg:cls:locks Unknown job, cls, mode N/A pg:cls:log_size Unknown job, cls N/A pg:cls:lsn_rate1m Unknown job, cls N/A pg:cls:members Unknown job, ins, ip, cls N/A pg:cls:num_backends Unknown job, cls N/A pg:cls:partition Unknown job, cls N/A pg:cls:receiver Unknown state, slot_name, job, appname, ip, cls, sender_host, sender_port N/A pg:cls:rlock_count Unknown job, cls N/A pg:cls:saturation1 Unknown job, cls N/A pg:cls:saturation15 Unknown job, cls N/A pg:cls:saturation5 Unknown job, cls N/A pg:cls:sender Unknown pid, usename, address, job, ins, appname, ip, cls N/A pg:cls:session_time_rate1m Unknown job, cls N/A pg:cls:size Unknown job, cls N/A pg:cls:slot_count Unknown job, cls N/A pg:cls:slot_retained_bytes Unknown job, cls N/A pg:cls:standby_count Unknown job, cls N/A pg:cls:sync_state Unknown job, cls N/A pg:cls:timeline Unknown job, cls N/A pg:cls:tup_deleted_rate1m Unknown job, cls N/A pg:cls:tup_fetched_rate1m Unknown job, cls N/A pg:cls:tup_inserted_rate1m Unknown job, cls N/A pg:cls:tup_modified_rate1m Unknown job, cls N/A pg:cls:tup_returned_rate1m Unknown job, cls N/A pg:cls:wal_size Unknown job, cls N/A pg:cls:xact_commit_rate15m Unknown job, cls N/A pg:cls:xact_commit_rate1m Unknown job, cls N/A pg:cls:xact_commit_rate5m Unknown job, cls N/A pg:cls:xact_rollback_rate15m Unknown job, cls N/A pg:cls:xact_rollback_rate1m Unknown job, cls N/A pg:cls:xact_rollback_rate5m Unknown job, cls N/A pg:cls:xact_total_rate15m Unknown job, cls N/A pg:cls:xact_total_rate1m Unknown job, cls N/A pg:cls:xact_total_sigma15m Unknown job, cls N/A pg:cls:xlock_count Unknown job, cls N/A pg:db:active_backends Unknown datname, job, ins, ip, instance, cls N/A pg:db:active_time_rate15m Unknown datname, job, ins, ip, instance, cls N/A pg:db:active_time_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:active_time_rate5m Unknown datname, job, ins, ip, instance, cls N/A pg:db:age Unknown datname, job, ins, ip, instance, cls N/A pg:db:age_deriv1h Unknown datname, job, ins, ip, instance, cls N/A pg:db:age_exhaust Unknown datname, job, ins, ip, instance, cls N/A pg:db:blk_io_time_seconds_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:blk_read_time_seconds_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:blk_write_time_seconds_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:blks_access_1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:blks_hit_1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:blks_hit_ratio1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:blks_read_1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:conn_limit Unknown datname, job, ins, ip, instance, cls N/A pg:db:conn_usage Unknown datname, job, ins, ip, instance, cls N/A pg:db:db_size Unknown datname, job, ins, ip, instance, cls N/A pg:db:ixact_backends Unknown datname, job, ins, ip, instance, cls N/A pg:db:ixact_time_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:lock_count Unknown datname, job, ins, ip, instance, cls N/A pg:db:num_backends Unknown datname, job, ins, ip, instance, cls N/A pg:db:rlock_count Unknown datname, job, ins, ip, instance, cls N/A pg:db:session_time_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:temp_bytes_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:temp_files_1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:tup_deleted_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:tup_fetched_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:tup_inserted_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:tup_modified_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:tup_returned_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:wlock_count Unknown datname, job, ins, ip, instance, cls N/A pg:db:xact_commit_rate15m Unknown datname, job, ins, ip, instance, cls N/A pg:db:xact_commit_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:xact_commit_rate5m Unknown datname, job, ins, ip, instance, cls N/A pg:db:xact_rollback_rate15m Unknown datname, job, ins, ip, instance, cls N/A pg:db:xact_rollback_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:xact_rollback_rate5m Unknown datname, job, ins, ip, instance, cls N/A pg:db:xact_total_rate15m Unknown datname, job, ins, ip, instance, cls N/A pg:db:xact_total_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:xact_total_rate5m Unknown datname, job, ins, ip, instance, cls N/A pg:db:xact_total_sigma15m Unknown datname, job, ins, ip, instance, cls N/A pg:db:xlock_count Unknown datname, job, ins, ip, instance, cls N/A pg:env:active_backends Unknown job N/A pg:env:active_time_rate15m Unknown job N/A pg:env:active_time_rate1m Unknown job N/A pg:env:active_time_rate5m Unknown job N/A pg:env:age Unknown job N/A pg:env:cpu_count Unknown job N/A pg:env:cpu_usage Unknown job N/A pg:env:cpu_usage_15m Unknown job N/A pg:env:cpu_usage_1m Unknown job N/A pg:env:cpu_usage_5m Unknown job N/A pg:env:ixact_backends Unknown job N/A pg:env:ixact_time_rate1m Unknown job N/A pg:env:lag_bytes Unknown job N/A pg:env:lag_seconds Unknown job N/A pg:env:lsn_rate1m Unknown job N/A pg:env:session_time_rate1m Unknown job N/A pg:env:tup_deleted_rate1m Unknown job N/A pg:env:tup_fetched_rate1m Unknown job N/A pg:env:tup_inserted_rate1m Unknown job N/A pg:env:tup_modified_rate1m Unknown job N/A pg:env:tup_returned_rate1m Unknown job N/A pg:env:xact_commit_rate15m Unknown job N/A pg:env:xact_commit_rate1m Unknown job N/A pg:env:xact_commit_rate5m Unknown job N/A pg:env:xact_rollback_rate15m Unknown job N/A pg:env:xact_rollback_rate1m Unknown job N/A pg:env:xact_rollback_rate5m Unknown job N/A pg:env:xact_total_rate15m Unknown job N/A pg:env:xact_total_rate1m Unknown job N/A pg:env:xact_total_sigma15m Unknown job N/A pg:ins:active_backends Unknown job, ins, ip, instance, cls N/A pg:ins:active_time_rate15m Unknown job, ins, ip, instance, cls N/A pg:ins:active_time_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:active_time_rate5m Unknown job, ins, ip, instance, cls N/A pg:ins:age Unknown job, ins, ip, instance, cls N/A pg:ins:blks_hit_ratio1m Unknown job, ins, ip, instance, cls N/A pg:ins:buf_alloc_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:buf_clean_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:buf_flush_backend_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:buf_flush_checkpoint_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:ckpt_1h Unknown job, ins, ip, instance, cls N/A pg:ins:ckpt_req_1m Unknown job, ins, ip, instance, cls N/A pg:ins:ckpt_timed_1m Unknown job, ins, ip, instance, cls N/A pg:ins:conn_limit Unknown job, ins, ip, instance, cls N/A pg:ins:conn_usage Unknown job, ins, ip, instance, cls N/A pg:ins:cpu_count Unknown job, ins, ip, instance, cls N/A pg:ins:cpu_usage Unknown job, ins, ip, instance, cls N/A pg:ins:cpu_usage_15m Unknown job, ins, ip, instance, cls N/A pg:ins:cpu_usage_1m Unknown job, ins, ip, instance, cls N/A pg:ins:cpu_usage_5m Unknown job, ins, ip, instance, cls N/A pg:ins:db_size Unknown job, ins, ip, instance, cls N/A pg:ins:file_size Unknown job, ins, ip, instance, cls N/A pg:ins:fs_size Unknown job, ins, ip, instance, cls N/A pg:ins:is_leader Unknown job, ins, ip, instance, cls N/A pg:ins:ixact_backends Unknown job, ins, ip, instance, cls N/A pg:ins:ixact_time_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:lag_bytes Unknown job, ins, ip, instance, cls N/A pg:ins:lag_seconds Unknown job, ins, ip, instance, cls N/A pg:ins:load1 Unknown job, ins, ip, instance, cls N/A pg:ins:load15 Unknown job, ins, ip, instance, cls N/A pg:ins:load5 Unknown job, ins, ip, instance, cls N/A pg:ins:lock_count Unknown job, ins, ip, instance, cls N/A pg:ins:locks Unknown job, ins, ip, mode, instance, cls N/A pg:ins:log_size Unknown job, ins, ip, instance, cls N/A pg:ins:lsn_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:mem_size Unknown job, ins, ip, instance, cls N/A pg:ins:num_backends Unknown job, ins, ip, instance, cls N/A pg:ins:rlock_count Unknown job, ins, ip, instance, cls N/A pg:ins:saturation1 Unknown job, ins, ip, cls N/A pg:ins:saturation15 Unknown job, ins, ip, cls N/A pg:ins:saturation5 Unknown job, ins, ip, cls N/A pg:ins:session_time_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:slot_retained_bytes Unknown job, ins, ip, instance, cls N/A pg:ins:space_usage Unknown job, ins, ip, instance, cls N/A pg:ins:status Unknown job, ins, ip, instance, cls N/A pg:ins:sync_state Unknown job, ins, instance, cls N/A pg:ins:target_count Unknown job, cls, ins N/A pg:ins:timeline Unknown job, ins, ip, instance, cls N/A pg:ins:tup_deleted_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:tup_fetched_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:tup_inserted_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:tup_modified_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:tup_returned_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:wal_size Unknown job, ins, ip, instance, cls N/A pg:ins:wlock_count Unknown job, ins, ip, instance, cls N/A pg:ins:xact_commit_rate15m Unknown job, ins, ip, instance, cls N/A pg:ins:xact_commit_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:xact_commit_rate5m Unknown job, ins, ip, instance, cls N/A pg:ins:xact_rollback_rate15m Unknown job, ins, ip, instance, cls N/A pg:ins:xact_rollback_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:xact_rollback_rate5m Unknown job, ins, ip, instance, cls N/A pg:ins:xact_total_rate15m Unknown job, ins, ip, instance, cls N/A pg:ins:xact_total_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:xact_total_rate5m Unknown job, ins, ip, instance, cls N/A pg:ins:xact_total_sigma15m Unknown job, ins, ip, instance, cls N/A pg:ins:xlock_count Unknown job, ins, ip, instance, cls N/A pg:query:call_rate1m Unknown datname, query, job, ins, ip, instance, cls N/A pg:query:rt_1m Unknown datname, query, job, ins, ip, instance, cls N/A pg:table:scan_rate1m Unknown datname, relname, job, ins, ip, instance, cls N/A pg_activity_count gauge datname, state, job, ins, ip, instance, cls Count of connection among (datname,state) pg_activity_max_conn_duration gauge datname, state, job, ins, ip, instance, cls Max backend session duration since state change among (datname, state) pg_activity_max_duration gauge datname, state, job, ins, ip, instance, cls Max duration since last state change among (datname, state) pg_activity_max_tx_duration gauge datname, state, job, ins, ip, instance, cls Max transaction duration since state change among (datname, state) pg_archiver_failed_count counter job, ins, ip, instance, cls Number of failed attempts for archiving WAL files pg_archiver_finish_count counter job, ins, ip, instance, cls Number of WAL files that have been successfully archived pg_archiver_last_failed_time counter job, ins, ip, instance, cls Time of the last failed archival operation pg_archiver_last_finish_time counter job, ins, ip, instance, cls Time of the last successful archive operation pg_archiver_reset_time gauge job, ins, ip, instance, cls Time at which archive statistics were last reset pg_backend_count gauge type, job, ins, ip, instance, cls Database backend process count by backend_type pg_bgwriter_buffers_alloc counter job, ins, ip, instance, cls Number of buffers allocated pg_bgwriter_buffers_backend counter job, ins, ip, instance, cls Number of buffers written directly by a backend pg_bgwriter_buffers_backend_fsync counter job, ins, ip, instance, cls Number of times a backend had to execute its own fsync call pg_bgwriter_buffers_checkpoint counter job, ins, ip, instance, cls Number of buffers written during checkpoints pg_bgwriter_buffers_clean counter job, ins, ip, instance, cls Number of buffers written by the background writer pg_bgwriter_checkpoint_sync_time counter job, ins, ip, instance, cls Total amount of time that has been spent in the portion of checkpoint processing where files are synchronized to disk, in seconds pg_bgwriter_checkpoint_write_time counter job, ins, ip, instance, cls Total amount of time that has been spent in the portion of checkpoint processing where files are written to disk, in seconds pg_bgwriter_checkpoints_req counter job, ins, ip, instance, cls Number of requested checkpoints that have been performed pg_bgwriter_checkpoints_timed counter job, ins, ip, instance, cls Number of scheduled checkpoints that have been performed pg_bgwriter_maxwritten_clean counter job, ins, ip, instance, cls Number of times the background writer stopped a cleaning scan because it had written too many buffers pg_bgwriter_reset_time counter job, ins, ip, instance, cls Time at which bgwriter statistics were last reset pg_boot_time gauge job, ins, ip, instance, cls unix timestamp when postmaster boot pg_checkpoint_checkpoint_lsn counter job, ins, ip, instance, cls Latest checkpoint location pg_checkpoint_elapse gauge job, ins, ip, instance, cls Seconds elapsed since latest checkpoint in seconds pg_checkpoint_full_page_writes gauge job, ins, ip, instance, cls Latest checkpoint’s full_page_writes enabled pg_checkpoint_newest_commit_ts_xid counter job, ins, ip, instance, cls Latest checkpoint’s newestCommitTsXid pg_checkpoint_next_multi_offset counter job, ins, ip, instance, cls Latest checkpoint’s NextMultiOffset pg_checkpoint_next_multixact_id counter job, ins, ip, instance, cls Latest checkpoint’s NextMultiXactId pg_checkpoint_next_oid counter job, ins, ip, instance, cls Latest checkpoint’s NextOID pg_checkpoint_next_xid counter job, ins, ip, instance, cls Latest checkpoint’s NextXID xid pg_checkpoint_next_xid_epoch counter job, ins, ip, instance, cls Latest checkpoint’s NextXID epoch pg_checkpoint_oldest_active_xid counter job, ins, ip, instance, cls Latest checkpoint’s oldestActiveXID pg_checkpoint_oldest_commit_ts_xid counter job, ins, ip, instance, cls Latest checkpoint’s oldestCommitTsXid pg_checkpoint_oldest_multi_dbid gauge job, ins, ip, instance, cls Latest checkpoint’s oldestMulti’s DB OID pg_checkpoint_oldest_multi_xid counter job, ins, ip, instance, cls Latest checkpoint’s oldestMultiXid pg_checkpoint_oldest_xid counter job, ins, ip, instance, cls Latest checkpoint’s oldestXID pg_checkpoint_oldest_xid_dbid gauge job, ins, ip, instance, cls Latest checkpoint’s oldestXID’s DB OID pg_checkpoint_prev_tli counter job, ins, ip, instance, cls Latest checkpoint’s PrevTimeLineID pg_checkpoint_redo_lsn counter job, ins, ip, instance, cls Latest checkpoint’s REDO location pg_checkpoint_time counter job, ins, ip, instance, cls Time of latest checkpoint pg_checkpoint_tli counter job, ins, ip, instance, cls Latest checkpoint’s TimeLineID pg_conf_reload_time gauge job, ins, ip, instance, cls seconds since last configuration reload pg_db_active_time counter datname, job, ins, ip, instance, cls Time spent executing SQL statements in this database, in seconds pg_db_age gauge datname, job, ins, ip, instance, cls Age of database calculated from datfrozenxid pg_db_allow_conn gauge datname, job, ins, ip, instance, cls If false(0) then no one can connect to this database. pg_db_blk_read_time counter datname, job, ins, ip, instance, cls Time spent reading data file blocks by backends in this database, in seconds pg_db_blk_write_time counter datname, job, ins, ip, instance, cls Time spent writing data file blocks by backends in this database, in seconds pg_db_blks_access counter datname, job, ins, ip, instance, cls Number of times disk blocks that accessed read+hit pg_db_blks_hit counter datname, job, ins, ip, instance, cls Number of times disk blocks were found already in the buffer cache pg_db_blks_read counter datname, job, ins, ip, instance, cls Number of disk blocks read in this database pg_db_cks_fail_time gauge datname, job, ins, ip, instance, cls Time at which the last data page checksum failure was detected in this database pg_db_cks_fails counter datname, job, ins, ip, instance, cls Number of data page checksum failures detected in this database, -1 for not enabled pg_db_confl_confl_bufferpin counter datname, job, ins, ip, instance, cls Number of queries in this database that have been canceled due to pinned buffers pg_db_confl_confl_deadlock counter datname, job, ins, ip, instance, cls Number of queries in this database that have been canceled due to deadlocks pg_db_confl_confl_lock counter datname, job, ins, ip, instance, cls Number of queries in this database that have been canceled due to lock timeouts pg_db_confl_confl_snapshot counter datname, job, ins, ip, instance, cls Number of queries in this database that have been canceled due to old snapshots pg_db_confl_confl_tablespace counter datname, job, ins, ip, instance, cls Number of queries in this database that have been canceled due to dropped tablespaces pg_db_conflicts counter datname, job, ins, ip, instance, cls Number of queries canceled due to conflicts with recovery in this database pg_db_conn_limit gauge datname, job, ins, ip, instance, cls Sets maximum number of concurrent connections that can be made to this database. -1 means no limit. pg_db_datid gauge datname, job, ins, ip, instance, cls OID of the database pg_db_deadlocks counter datname, job, ins, ip, instance, cls Number of deadlocks detected in this database pg_db_frozen_xid gauge datname, job, ins, ip, instance, cls All transaction IDs before this one have been frozened pg_db_is_template gauge datname, job, ins, ip, instance, cls If true(1), then this database can be cloned by any user with CREATEDB privileges pg_db_ixact_time counter datname, job, ins, ip, instance, cls Time spent idling while in a transaction in this database, in seconds pg_db_numbackends gauge datname, job, ins, ip, instance, cls Number of backends currently connected to this database pg_db_reset_time counter datname, job, ins, ip, instance, cls Time at which database statistics were last reset pg_db_session_time counter datname, job, ins, ip, instance, cls Time spent by database sessions in this database, in seconds pg_db_sessions counter datname, job, ins, ip, instance, cls Total number of sessions established to this database pg_db_sessions_abandoned counter datname, job, ins, ip, instance, cls Number of database sessions to this database that were terminated because connection to the client was lost pg_db_sessions_fatal counter datname, job, ins, ip, instance, cls Number of database sessions to this database that were terminated by fatal errors pg_db_sessions_killed counter datname, job, ins, ip, instance, cls Number of database sessions to this database that were terminated by operator intervention pg_db_temp_bytes counter datname, job, ins, ip, instance, cls Total amount of data written to temporary files by queries in this database. pg_db_temp_files counter datname, job, ins, ip, instance, cls Number of temporary files created by queries in this database pg_db_tup_deleted counter datname, job, ins, ip, instance, cls Number of rows deleted by queries in this database pg_db_tup_fetched counter datname, job, ins, ip, instance, cls Number of rows fetched by queries in this database pg_db_tup_inserted counter datname, job, ins, ip, instance, cls Number of rows inserted by queries in this database pg_db_tup_modified counter datname, job, ins, ip, instance, cls Number of rows modified by queries in this database pg_db_tup_returned counter datname, job, ins, ip, instance, cls Number of rows returned by queries in this database pg_db_tup_updated counter datname, job, ins, ip, instance, cls Number of rows updated by queries in this database pg_db_xact_commit counter datname, job, ins, ip, instance, cls Number of transactions in this database that have been committed pg_db_xact_rollback counter datname, job, ins, ip, instance, cls Number of transactions in this database that have been rolled back pg_db_xact_total counter datname, job, ins, ip, instance, cls Number of transactions in this database pg_downstream_count gauge state, job, ins, ip, instance, cls Count of corresponding state pg_exporter_agent_up Unknown job, ins, ip, instance, cls N/A pg_exporter_last_scrape_time gauge job, ins, ip, instance, cls seconds exporter spending on scrapping pg_exporter_query_cache_ttl gauge datname, query, job, ins, ip, instance, cls times to live of query cache pg_exporter_query_scrape_duration gauge datname, query, job, ins, ip, instance, cls seconds query spending on scrapping pg_exporter_query_scrape_error_count gauge datname, query, job, ins, ip, instance, cls times the query failed pg_exporter_query_scrape_hit_count gauge datname, query, job, ins, ip, instance, cls numbers been scrapped from this query pg_exporter_query_scrape_metric_count gauge datname, query, job, ins, ip, instance, cls numbers of metrics been scrapped from this query pg_exporter_query_scrape_total_count gauge datname, query, job, ins, ip, instance, cls times exporter server was scraped for metrics pg_exporter_scrape_duration gauge job, ins, ip, instance, cls seconds exporter spending on scrapping pg_exporter_scrape_error_count counter job, ins, ip, instance, cls times exporter was scraped for metrics and failed pg_exporter_scrape_total_count counter job, ins, ip, instance, cls times exporter was scraped for metrics pg_exporter_server_scrape_duration gauge datname, job, ins, ip, instance, cls seconds exporter server spending on scrapping pg_exporter_server_scrape_error_count Unknown datname, job, ins, ip, instance, cls N/A pg_exporter_server_scrape_total_count gauge datname, job, ins, ip, instance, cls times exporter server was scraped for metrics pg_exporter_server_scrape_total_seconds gauge datname, job, ins, ip, instance, cls seconds exporter server spending on scrapping pg_exporter_up gauge job, ins, ip, instance, cls always be 1 if your could retrieve metrics pg_exporter_uptime gauge job, ins, ip, instance, cls seconds since exporter primary server inited pg_flush_lsn counter job, ins, ip, instance, cls primary only, location of current wal syncing pg_func_calls counter datname, funcname, job, ins, ip, instance, cls Number of times this function has been called pg_func_self_time counter datname, funcname, job, ins, ip, instance, cls Total time spent in this function itself, not including other functions called by it, in ms pg_func_total_time counter datname, funcname, job, ins, ip, instance, cls Total time spent in this function and all other functions called by it, in ms pg_in_recovery gauge job, ins, ip, instance, cls server is in recovery mode? 1 for yes 0 for no pg_index_idx_blks_hit counter datname, relname, job, ins, relid, ip, instance, cls, idxname Number of buffer hits in this index pg_index_idx_blks_read counter datname, relname, job, ins, relid, ip, instance, cls, idxname Number of disk blocks read from this index pg_index_idx_scan counter datname, relname, job, ins, relid, ip, instance, cls, idxname Number of index scans initiated on this index pg_index_idx_tup_fetch counter datname, relname, job, ins, relid, ip, instance, cls, idxname Number of live table rows fetched by simple index scans using this index pg_index_idx_tup_read counter datname, relname, job, ins, relid, ip, instance, cls, idxname Number of index entries returned by scans on this index pg_index_relpages gauge datname, relname, job, ins, relid, ip, instance, cls, idxname Size of the on-disk representation of this index in pages pg_index_reltuples gauge datname, relname, job, ins, relid, ip, instance, cls, idxname Estimate relation tuples pg_insert_lsn counter job, ins, ip, instance, cls primary only, location of current wal inserting pg_io_evictions counter type, job, ins, object, ip, context, instance, cls Number of times a block has been written out from a shared or local buffer pg_io_extend_time counter type, job, ins, object, ip, context, instance, cls Time spent in extend operations in seconds pg_io_extends counter type, job, ins, object, ip, context, instance, cls Number of relation extend operations, each of the size specified in op_bytes. pg_io_fsync_time counter type, job, ins, object, ip, context, instance, cls Time spent in fsync operations in seconds pg_io_fsyncs counter type, job, ins, object, ip, context, instance, cls Number of fsync calls. These are only tracked in context normal pg_io_hits counter type, job, ins, object, ip, context, instance, cls The number of times a desired block was found in a shared buffer. pg_io_op_bytes gauge type, job, ins, object, ip, context, instance, cls The number of bytes per unit of I/O read, written, or extended. 8192 by default pg_io_read_time counter type, job, ins, object, ip, context, instance, cls Time spent in read operations in seconds pg_io_reads counter type, job, ins, object, ip, context, instance, cls Number of read operations, each of the size specified in op_bytes. pg_io_reset_time gauge type, job, ins, object, ip, context, instance, cls Timestamp at which these statistics were last reset pg_io_reuses counter type, job, ins, object, ip, context, instance, cls The number of times an existing buffer in reused pg_io_write_time counter type, job, ins, object, ip, context, instance, cls Time spent in write operations in seconds pg_io_writeback_time counter type, job, ins, object, ip, context, instance, cls Time spent in writeback operations in seconds pg_io_writebacks counter type, job, ins, object, ip, context, instance, cls Number of units of size op_bytes which the process requested the kernel write out to permanent storage. pg_io_writes counter type, job, ins, object, ip, context, instance, cls Number of write operations, each of the size specified in op_bytes. pg_is_in_recovery gauge job, ins, ip, instance, cls 1 if in recovery mode pg_is_wal_replay_paused gauge job, ins, ip, instance, cls 1 if wal play paused pg_lag gauge job, ins, ip, instance, cls replica only, replication lag in seconds pg_last_replay_time gauge job, ins, ip, instance, cls time when last transaction been replayed pg_lock_count gauge datname, job, ins, ip, mode, instance, cls Number of locks of corresponding mode and database pg_lsn counter job, ins, ip, instance, cls log sequence number, current write location pg_meta_info gauge cls, extensions, version, job, ins, primary_conninfo, conf_path, hba_path, ip, cluster_id, instance, listen_port, wal_level, ver_num, cluster_name, data_dir constant 1 pg_query_calls counter datname, query, job, ins, ip, instance, cls Number of times the statement was executed pg_query_exec_time counter datname, query, job, ins, ip, instance, cls Total time spent executing the statement, in seconds pg_query_io_time counter datname, query, job, ins, ip, instance, cls Total time the statement spent reading and writing blocks, in seconds pg_query_rows counter datname, query, job, ins, ip, instance, cls Total number of rows retrieved or affected by the statement pg_query_sblk_dirtied counter datname, query, job, ins, ip, instance, cls Total number of shared blocks dirtied by the statement pg_query_sblk_hit counter datname, query, job, ins, ip, instance, cls Total number of shared block cache hits by the statement pg_query_sblk_read counter datname, query, job, ins, ip, instance, cls Total number of shared blocks read by the statement pg_query_sblk_written counter datname, query, job, ins, ip, instance, cls Total number of shared blocks written by the statement pg_query_wal_bytes counter datname, query, job, ins, ip, instance, cls Total amount of WAL bytes generated by the statement pg_receive_lsn counter job, ins, ip, instance, cls replica only, location of wal synced to disk pg_recovery_backup_end_lsn counter job, ins, ip, instance, cls Backup end location pg_recovery_backup_start_lsn counter job, ins, ip, instance, cls Backup start location pg_recovery_min_lsn counter job, ins, ip, instance, cls Minimum recovery ending location pg_recovery_min_timeline counter job, ins, ip, instance, cls Min recovery ending loc’s timeline pg_recovery_prefetch_block_distance gauge job, ins, ip, instance, cls How many blocks ahead the prefetcher is looking pg_recovery_prefetch_hit counter job, ins, ip, instance, cls Number of blocks not prefetched because they were already in the buffer pool pg_recovery_prefetch_io_depth gauge job, ins, ip, instance, cls How many prefetches have been initiated but are not yet known to have completed pg_recovery_prefetch_prefetch counter job, ins, ip, instance, cls Number of blocks prefetched because they were not in the buffer pool pg_recovery_prefetch_reset_time counter job, ins, ip, instance, cls Time at which these recovery prefetch statistics were last reset pg_recovery_prefetch_skip_fpw gauge job, ins, ip, instance, cls Number of blocks not prefetched because a full page image was included in the WAL pg_recovery_prefetch_skip_init counter job, ins, ip, instance, cls Number of blocks not prefetched because they would be zero-initialized pg_recovery_prefetch_skip_new counter job, ins, ip, instance, cls Number of blocks not prefetched because they didn’t exist yet pg_recovery_prefetch_skip_rep counter job, ins, ip, instance, cls Number of blocks not prefetched because they were already recently prefetched pg_recovery_prefetch_wal_distance gauge job, ins, ip, instance, cls How many bytes ahead the prefetcher is looking pg_recovery_require_record gauge job, ins, ip, instance, cls End-of-backup record required pg_recv_flush_lsn counter state, slot_name, job, ins, ip, instance, cls, sender_host, sender_port Last write-ahead log location already received and flushed to disk pg_recv_flush_tli counter state, slot_name, job, ins, ip, instance, cls, sender_host, sender_port Timeline number of last write-ahead log location received and flushed to disk pg_recv_init_lsn counter state, slot_name, job, ins, ip, instance, cls, sender_host, sender_port First write-ahead log location used when WAL receiver is started pg_recv_init_tli counter state, slot_name, job, ins, ip, instance, cls, sender_host, sender_port First timeline number used when WAL receiver is started pg_recv_msg_recv_time gauge state, slot_name, job, ins, ip, instance, cls, sender_host, sender_port Receipt time of last message received from origin WAL sender pg_recv_msg_send_time gauge state, slot_name, job, ins, ip, instance, cls, sender_host, sender_port Send time of last message received from origin WAL sender pg_recv_pid gauge state, slot_name, job, ins, ip, instance, cls, sender_host, sender_port Process ID of the WAL receiver process pg_recv_reported_lsn counter state, slot_name, job, ins, ip, instance, cls, sender_host, sender_port Last write-ahead log location reported to origin WAL sender pg_recv_reported_time gauge state, slot_name, job, ins, ip, instance, cls, sender_host, sender_port Time of last write-ahead log location reported to origin WAL sender pg_recv_time gauge state, slot_name, job, ins, ip, instance, cls, sender_host, sender_port Time of current snapshot pg_recv_write_lsn counter state, slot_name, job, ins, ip, instance, cls, sender_host, sender_port Last write-ahead log location already received and written to disk, but not flushed. pg_relkind_count gauge datname, job, ins, ip, instance, cls, relkind Number of relations of corresponding relkind pg_repl_backend_xmin counter pid, usename, address, job, ins, appname, ip, instance, cls This standby’s xmin horizon reported by hot_standby_feedback. pg_repl_client_port gauge pid, usename, address, job, ins, appname, ip, instance, cls TCP port number that the client is using for communication with this WAL sender, or -1 if a Unix socket is used pg_repl_flush_diff gauge pid, usename, address, job, ins, appname, ip, instance, cls Last log position flushed to disk by this standby server diff with current lsn pg_repl_flush_lag gauge pid, usename, address, job, ins, appname, ip, instance, cls Time elapsed between flushing recent WAL locally and receiving notification that this standby server has written and flushed it pg_repl_flush_lsn counter pid, usename, address, job, ins, appname, ip, instance, cls Last write-ahead log location flushed to disk by this standby server pg_repl_launch_time counter pid, usename, address, job, ins, appname, ip, instance, cls Time when this process was started, i.e., when the client connected to this WAL sender pg_repl_lsn counter pid, usename, address, job, ins, appname, ip, instance, cls Current log position on this server pg_repl_replay_diff gauge pid, usename, address, job, ins, appname, ip, instance, cls Last log position replayed into the database on this standby server diff with current lsn pg_repl_replay_lag gauge pid, usename, address, job, ins, appname, ip, instance, cls Time elapsed between flushing recent WAL locally and receiving notification that this standby server has written, flushed and applied it pg_repl_replay_lsn counter pid, usename, address, job, ins, appname, ip, instance, cls Last write-ahead log location replayed into the database on this standby server pg_repl_reply_time gauge pid, usename, address, job, ins, appname, ip, instance, cls Send time of last reply message received from standby server pg_repl_sent_diff gauge pid, usename, address, job, ins, appname, ip, instance, cls Last log position sent to this standby server diff with current lsn pg_repl_sent_lsn counter pid, usename, address, job, ins, appname, ip, instance, cls Last write-ahead log location sent on this connection pg_repl_state gauge pid, usename, address, job, ins, appname, ip, instance, cls Current WAL sender encoded state 0-4 for streaming startup catchup backup stopping pg_repl_sync_priority gauge pid, usename, address, job, ins, appname, ip, instance, cls Priority of this standby server for being chosen as the synchronous standby pg_repl_sync_state gauge pid, usename, address, job, ins, appname, ip, instance, cls Encoded synchronous state of this standby server, 0-3 for async potential sync quorum pg_repl_time counter pid, usename, address, job, ins, appname, ip, instance, cls Current timestamp in unix epoch pg_repl_write_diff gauge pid, usename, address, job, ins, appname, ip, instance, cls Last log position written to disk by this standby server diff with current lsn pg_repl_write_lag gauge pid, usename, address, job, ins, appname, ip, instance, cls Time elapsed between flushing recent WAL locally and receiving notification that this standby server has written it pg_repl_write_lsn counter pid, usename, address, job, ins, appname, ip, instance, cls Last write-ahead log location written to disk by this standby server pg_replay_lsn counter job, ins, ip, instance, cls replica only, location of wal applied pg_seq_blks_hit counter datname, job, ins, ip, instance, cls, seqname Number of buffer hits in this sequence pg_seq_blks_read counter datname, job, ins, ip, instance, cls, seqname Number of disk blocks read from this sequence pg_seq_last_value counter datname, job, ins, ip, instance, cls, seqname The last sequence value written to disk pg_setting_block_size gauge job, ins, ip, instance, cls pg page block size, 8192 by default pg_setting_data_checksums gauge job, ins, ip, instance, cls whether data checksum is enabled, 1 enabled 0 disabled pg_setting_max_connections gauge job, ins, ip, instance, cls number of concurrent connections to the database server pg_setting_max_locks_per_transaction gauge job, ins, ip, instance, cls no more than this many distinct objects can be locked at any one time pg_setting_max_prepared_transactions gauge job, ins, ip, instance, cls maximum number of transactions that can be in the prepared state simultaneously pg_setting_max_replication_slots gauge job, ins, ip, instance, cls maximum number of replication slots pg_setting_max_wal_senders gauge job, ins, ip, instance, cls maximum number of concurrent connections from standby servers pg_setting_max_worker_processes gauge job, ins, ip, instance, cls maximum number of background processes that the system can support pg_setting_wal_log_hints gauge job, ins, ip, instance, cls whether wal_log_hints is enabled, 1 enabled 0 disabled pg_size_bytes gauge datname, job, ins, ip, instance, cls File size in bytes pg_slot_active gauge slot_name, job, ins, ip, instance, cls True(1) if this slot is currently actively being used pg_slot_catalog_xmin counter slot_name, job, ins, ip, instance, cls The oldest transaction affecting the system catalogs that this slot needs the database to retain. pg_slot_confirm_lsn counter slot_name, job, ins, ip, instance, cls The address (LSN) up to which the logical slot’s consumer has confirmed receiving data. pg_slot_reset_time counter slot_name, job, ins, ip, instance, cls When statistics were last reset pg_slot_restart_lsn counter slot_name, job, ins, ip, instance, cls The address (LSN) of oldest WAL which still might be required by the consumer of this slot pg_slot_retained_bytes gauge slot_name, job, ins, ip, instance, cls Size of bytes that retained for this slot pg_slot_safe_wal_size gauge slot_name, job, ins, ip, instance, cls bytes that can be written to WAL which will not make slot into lost pg_slot_spill_bytes counter slot_name, job, ins, ip, instance, cls Bytes that spilled to disk due to logical decode mem exceeding pg_slot_spill_count counter slot_name, job, ins, ip, instance, cls Xacts that spilled to disk due to logical decode mem exceeding (a xact can be spilled multiple times) pg_slot_spill_txns counter slot_name, job, ins, ip, instance, cls Xacts that spilled to disk due to logical decode mem exceeding (subtrans included) pg_slot_stream_bytes counter slot_name, job, ins, ip, instance, cls Bytes that streamed to decoding output plugin after mem exceed pg_slot_stream_count counter slot_name, job, ins, ip, instance, cls Xacts that streamed to decoding output plugin after mem exceed (a xact can be streamed multiple times) pg_slot_stream_txns counter slot_name, job, ins, ip, instance, cls Xacts that streamed to decoding output plugin after mem exceed pg_slot_temporary gauge slot_name, job, ins, ip, instance, cls True(1) if this is a temporary replication slot. pg_slot_total_bytes counter slot_name, job, ins, ip, instance, cls Number of decoded bytes sent to the decoding output plugin for this slot pg_slot_total_txns counter slot_name, job, ins, ip, instance, cls Number of decoded xacts sent to the decoding output plugin for this slot pg_slot_wal_status gauge slot_name, job, ins, ip, instance, cls WAL reserve status 0-3 means reserved,extended,unreserved,lost, -1 means other pg_slot_xmin counter slot_name, job, ins, ip, instance, cls The oldest transaction that this slot needs the database to retain. pg_slru_blks_exists counter job, ins, ip, instance, cls Number of blocks checked for existence for this SLRU pg_slru_blks_hit counter job, ins, ip, instance, cls Number of times disk blocks were found already in the SLRU, so that a read was not necessary pg_slru_blks_read counter job, ins, ip, instance, cls Number of disk blocks read for this SLRU pg_slru_blks_written counter job, ins, ip, instance, cls Number of disk blocks written for this SLRU pg_slru_blks_zeroed counter job, ins, ip, instance, cls Number of blocks zeroed during initializations pg_slru_flushes counter job, ins, ip, instance, cls Number of flushes of dirty data for this SLRU pg_slru_reset_time counter job, ins, ip, instance, cls Time at which these statistics were last reset pg_slru_truncates counter job, ins, ip, instance, cls Number of truncates for this SLRU pg_ssl_disabled gauge job, ins, ip, instance, cls Number of client connection that does not use ssl pg_ssl_enabled gauge job, ins, ip, instance, cls Number of client connection that use ssl pg_sync_standby_enabled gauge job, ins, ip, names, instance, cls Synchronous commit enabled, 1 if enabled, 0 if disabled pg_table_age gauge datname, relname, job, ins, ip, instance, cls Age of this table in vacuum cycles pg_table_analyze_count counter datname, relname, job, ins, ip, instance, cls Number of times this table has been manually analyzed pg_table_autoanalyze_count counter datname, relname, job, ins, ip, instance, cls Number of times this table has been analyzed by the autovacuum daemon pg_table_autovacuum_count counter datname, relname, job, ins, ip, instance, cls Number of times this table has been vacuumed by the autovacuum daemon pg_table_frozenxid counter datname, relname, job, ins, ip, instance, cls All txid before this have been frozen on this table pg_table_heap_blks_hit counter datname, relname, job, ins, ip, instance, cls Number of buffer hits in this table pg_table_heap_blks_read counter datname, relname, job, ins, ip, instance, cls Number of disk blocks read from this table pg_table_idx_blks_hit counter datname, relname, job, ins, ip, instance, cls Number of buffer hits in all indexes on this table pg_table_idx_blks_read counter datname, relname, job, ins, ip, instance, cls Number of disk blocks read from all indexes on this table pg_table_idx_scan counter datname, relname, job, ins, ip, instance, cls Number of index scans initiated on this table pg_table_idx_tup_fetch counter datname, relname, job, ins, ip, instance, cls Number of live rows fetched by index scans pg_table_kind gauge datname, relname, job, ins, ip, instance, cls Relation kind r/table/114 pg_table_n_dead_tup gauge datname, relname, job, ins, ip, instance, cls Estimated number of dead rows pg_table_n_ins_since_vacuum gauge datname, relname, job, ins, ip, instance, cls Estimated number of rows inserted since this table was last vacuumed pg_table_n_live_tup gauge datname, relname, job, ins, ip, instance, cls Estimated number of live rows pg_table_n_mod_since_analyze gauge datname, relname, job, ins, ip, instance, cls Estimated number of rows modified since this table was last analyzed pg_table_n_tup_del counter datname, relname, job, ins, ip, instance, cls Number of rows deleted pg_table_n_tup_hot_upd counter datname, relname, job, ins, ip, instance, cls Number of rows HOT updated (i.e with no separate index update required) pg_table_n_tup_ins counter datname, relname, job, ins, ip, instance, cls Number of rows inserted pg_table_n_tup_mod counter datname, relname, job, ins, ip, instance, cls Number of rows modified (insert + update + delete) pg_table_n_tup_newpage_upd counter datname, relname, job, ins, ip, instance, cls Number of rows updated where the successor version goes onto a new heap page pg_table_n_tup_upd counter datname, relname, job, ins, ip, instance, cls Number of rows updated (includes HOT updated rows) pg_table_ncols gauge datname, relname, job, ins, ip, instance, cls Number of columns in the table pg_table_pages gauge datname, relname, job, ins, ip, instance, cls Size of the on-disk representation of this table in pages pg_table_relid gauge datname, relname, job, ins, ip, instance, cls Relation oid of this table pg_table_seq_scan counter datname, relname, job, ins, ip, instance, cls Number of sequential scans initiated on this table pg_table_seq_tup_read counter datname, relname, job, ins, ip, instance, cls Number of live rows fetched by sequential scans pg_table_size_bytes gauge datname, relname, job, ins, ip, instance, cls Total bytes of this table (including toast, index, toast index) pg_table_size_indexsize gauge datname, relname, job, ins, ip, instance, cls Bytes of all related indexes of this table pg_table_size_relsize gauge datname, relname, job, ins, ip, instance, cls Bytes of this table itself (main, vm, fsm) pg_table_size_toastsize gauge datname, relname, job, ins, ip, instance, cls Bytes of toast tables of this table pg_table_tbl_scan counter datname, relname, job, ins, ip, instance, cls Number of scans initiated on this table pg_table_tup_read counter datname, relname, job, ins, ip, instance, cls Number of live rows fetched by scans pg_table_tuples counter datname, relname, job, ins, ip, instance, cls All txid before this have been frozen on this table pg_table_vacuum_count counter datname, relname, job, ins, ip, instance, cls Number of times this table has been manually vacuumed (not counting VACUUM FULL) pg_timestamp gauge job, ins, ip, instance, cls database current timestamp pg_up gauge job, ins, ip, instance, cls last scrape was able to connect to the server: 1 for yes, 0 for no pg_uptime gauge job, ins, ip, instance, cls seconds since postmaster start pg_version gauge job, ins, ip, instance, cls server version number pg_wait_count gauge datname, job, ins, event, ip, instance, cls Count of WaitEvent on target database pg_wal_buffers_full counter job, ins, ip, instance, cls Number of times WAL data was written to disk because WAL buffers became full pg_wal_bytes counter job, ins, ip, instance, cls Total amount of WAL generated in bytes pg_wal_fpi counter job, ins, ip, instance, cls Total number of WAL full page images generated pg_wal_records counter job, ins, ip, instance, cls Total number of WAL records generated pg_wal_reset_time counter job, ins, ip, instance, cls When statistics were last reset pg_wal_sync counter job, ins, ip, instance, cls Number of times WAL files were synced to disk via issue_xlog_fsync request pg_wal_sync_time counter job, ins, ip, instance, cls Total amount of time spent syncing WAL files to disk via issue_xlog_fsync request, in seconds pg_wal_write counter job, ins, ip, instance, cls Number of times WAL buffers were written out to disk via XLogWrite request. pg_wal_write_time counter job, ins, ip, instance, cls Total amount of time spent writing WAL buffers to disk via XLogWrite request in seconds pg_write_lsn counter job, ins, ip, instance, cls primary only, location of current wal writing pg_xact_xmax counter job, ins, ip, instance, cls First as-yet-unassigned txid. txid \u003e= this are invisible. pg_xact_xmin counter job, ins, ip, instance, cls Earliest txid that is still active pg_xact_xnum gauge job, ins, ip, instance, cls Current active transaction count pgbouncer:cls:load1 Unknown job, cls N/A pgbouncer:cls:load15 Unknown job, cls N/A pgbouncer:cls:load5 Unknown job, cls N/A pgbouncer:db:conn_usage Unknown datname, job, ins, ip, instance, host, cls, real_datname, port N/A pgbouncer:db:conn_usage_reserve Unknown datname, job, ins, ip, instance, host, cls, real_datname, port N/A pgbouncer:db:pool_current_conn Unknown datname, job, ins, ip, instance, host, cls, real_datname, port N/A pgbouncer:db:pool_disabled Unknown datname, job, ins, ip, instance, host, cls, real_datname, port N/A pgbouncer:db:pool_max_conn Unknown datname, job, ins, ip, instance, host, cls, real_datname, port N/A pgbouncer:db:pool_paused Unknown datname, job, ins, ip, instance, host, cls, real_datname, port N/A pgbouncer:db:pool_reserve_size Unknown datname, job, ins, ip, instance, host, cls, real_datname, port N/A pgbouncer:db:pool_size Unknown datname, job, ins, ip, instance, host, cls, real_datname, port N/A pgbouncer:ins:free_clients Unknown job, ins, ip, instance, cls N/A pgbouncer:ins:free_servers Unknown job, ins, ip, instance, cls N/A pgbouncer:ins:load1 Unknown job, ins, ip, instance, cls N/A pgbouncer:ins:load15 Unknown job, ins, ip, instance, cls N/A pgbouncer:ins:load5 Unknown job, ins, ip, instance, cls N/A pgbouncer:ins:login_clients Unknown job, ins, ip, instance, cls N/A pgbouncer:ins:pool_databases Unknown job, ins, ip, instance, cls N/A pgbouncer:ins:pool_users Unknown job, ins, ip, instance, cls N/A pgbouncer:ins:pools Unknown job, ins, ip, instance, cls N/A pgbouncer:ins:used_clients Unknown job, ins, ip, instance, cls N/A pgbouncer_database_current_connections gauge datname, job, ins, ip, instance, host, cls, real_datname, port Current number of connections for this database pgbouncer_database_disabled gauge datname, job, ins, ip, instance, host, cls, real_datname, port True(1) if this database is currently disabled, else 0 pgbouncer_database_max_connections gauge datname, job, ins, ip, instance, host, cls, real_datname, port Maximum number of allowed connections for this database pgbouncer_database_min_pool_size gauge datname, job, ins, ip, instance, host, cls, real_datname, port Minimum number of server connections pgbouncer_database_paused gauge datname, job, ins, ip, instance, host, cls, real_datname, port True(1) if this database is currently paused, else 0 pgbouncer_database_pool_size gauge datname, job, ins, ip, instance, host, cls, real_datname, port Maximum number of server connections pgbouncer_database_reserve_pool gauge datname, job, ins, ip, instance, host, cls, real_datname, port Maximum number of additional connections for this database pgbouncer_exporter_agent_up Unknown job, ins, ip, instance, cls N/A pgbouncer_exporter_last_scrape_time gauge job, ins, ip, instance, cls seconds exporter spending on scrapping pgbouncer_exporter_query_cache_ttl gauge datname, query, job, ins, ip, instance, cls times to live of query cache pgbouncer_exporter_query_scrape_duration gauge datname, query, job, ins, ip, instance, cls seconds query spending on scrapping pgbouncer_exporter_query_scrape_error_count gauge datname, query, job, ins, ip, instance, cls times the query failed pgbouncer_exporter_query_scrape_hit_count gauge datname, query, job, ins, ip, instance, cls numbers been scrapped from this query pgbouncer_exporter_query_scrape_metric_count gauge datname, query, job, ins, ip, instance, cls numbers of metrics been scrapped from this query pgbouncer_exporter_query_scrape_total_count gauge datname, query, job, ins, ip, instance, cls times exporter server was scraped for metrics pgbouncer_exporter_scrape_duration gauge job, ins, ip, instance, cls seconds exporter spending on scrapping pgbouncer_exporter_scrape_error_count counter job, ins, ip, instance, cls times exporter was scraped for metrics and failed pgbouncer_exporter_scrape_total_count counter job, ins, ip, instance, cls times exporter was scraped for metrics pgbouncer_exporter_server_scrape_duration gauge datname, job, ins, ip, instance, cls seconds exporter server spending on scrapping pgbouncer_exporter_server_scrape_total_count gauge datname, job, ins, ip, instance, cls times exporter server was scraped for metrics pgbouncer_exporter_server_scrape_total_seconds gauge datname, job, ins, ip, instance, cls seconds exporter server spending on scrapping pgbouncer_exporter_up gauge job, ins, ip, instance, cls always be 1 if your could retrieve metrics pgbouncer_exporter_uptime gauge job, ins, ip, instance, cls seconds since exporter primary server inited pgbouncer_in_recovery gauge job, ins, ip, instance, cls server is in recovery mode? 1 for yes 0 for no pgbouncer_list_items gauge job, ins, ip, instance, list, cls Number of corresponding pgbouncer object pgbouncer_pool_active_cancel_clients gauge datname, job, ins, ip, instance, user, cls, pool_mode Client connections that have forwarded query cancellations to the server and are waiting for the server response. pgbouncer_pool_active_cancel_servers gauge datname, job, ins, ip, instance, user, cls, pool_mode Server connections that are currently forwarding a cancel request pgbouncer_pool_active_clients gauge datname, job, ins, ip, instance, user, cls, pool_mode Client connections that are linked to server connection and can process queries pgbouncer_pool_active_servers gauge datname, job, ins, ip, instance, user, cls, pool_mode Server connections that are linked to a client pgbouncer_pool_cancel_clients gauge datname, job, ins, ip, instance, user, cls, pool_mode Client connections that have not forwarded query cancellations to the server yet. pgbouncer_pool_cancel_servers gauge datname, job, ins, ip, instance, user, cls, pool_mode cancel requests have completed that were sent to cancel a query on this server pgbouncer_pool_idle_servers gauge datname, job, ins, ip, instance, user, cls, pool_mode Server connections that are unused and immediately usable for client queries pgbouncer_pool_login_servers gauge datname, job, ins, ip, instance, user, cls, pool_mode Server connections currently in the process of logging in pgbouncer_pool_maxwait gauge datname, job, ins, ip, instance, user, cls, pool_mode How long the first(oldest) client in the queue has waited, in seconds, key metric pgbouncer_pool_maxwait_us gauge datname, job, ins, ip, instance, user, cls, pool_mode Microsecond part of the maximum waiting time. pgbouncer_pool_tested_servers gauge datname, job, ins, ip, instance, user, cls, pool_mode Server connections that are currently running reset or check query pgbouncer_pool_used_servers gauge datname, job, ins, ip, instance, user, cls, pool_mode Server connections that have been idle for more than server_check_delay (means have to run check query) pgbouncer_pool_waiting_clients gauge datname, job, ins, ip, instance, user, cls, pool_mode Client connections that have sent queries but have not yet got a server connection pgbouncer_stat_avg_query_count gauge datname, job, ins, ip, instance, cls Average queries per second in last stat period pgbouncer_stat_avg_query_time gauge datname, job, ins, ip, instance, cls Average query duration, in seconds pgbouncer_stat_avg_recv gauge datname, job, ins, ip, instance, cls Average received (from clients) bytes per second pgbouncer_stat_avg_sent gauge datname, job, ins, ip, instance, cls Average sent (to clients) bytes per second pgbouncer_stat_avg_wait_time gauge datname, job, ins, ip, instance, cls Time spent by clients waiting for a server, in seconds (average per second). pgbouncer_stat_avg_xact_count gauge datname, job, ins, ip, instance, cls Average transactions per second in last stat period pgbouncer_stat_avg_xact_time gauge datname, job, ins, ip, instance, cls Average transaction duration, in seconds pgbouncer_stat_total_query_count gauge datname, job, ins, ip, instance, cls Total number of SQL queries pooled by pgbouncer pgbouncer_stat_total_query_time counter datname, job, ins, ip, instance, cls Total number of seconds spent when executing queries pgbouncer_stat_total_received counter datname, job, ins, ip, instance, cls Total volume in bytes of network traffic received by pgbouncer pgbouncer_stat_total_sent counter datname, job, ins, ip, instance, cls Total volume in bytes of network traffic sent by pgbouncer pgbouncer_stat_total_wait_time counter datname, job, ins, ip, instance, cls Time spent by clients waiting for a server, in seconds pgbouncer_stat_total_xact_count gauge datname, job, ins, ip, instance, cls Total number of SQL transactions pooled by pgbouncer pgbouncer_stat_total_xact_time counter datname, job, ins, ip, instance, cls Total number of seconds spent when in a transaction pgbouncer_up gauge job, ins, ip, instance, cls last scrape was able to connect to the server: 1 for yes, 0 for no pgbouncer_version gauge job, ins, ip, instance, cls server version number process_cpu_seconds_total counter job, ins, ip, instance, cls Total user and system CPU time spent in seconds. process_max_fds gauge job, ins, ip, instance, cls Maximum number of open file descriptors. process_open_fds gauge job, ins, ip, instance, cls Number of open file descriptors. process_resident_memory_bytes gauge job, ins, ip, instance, cls Resident memory size in bytes. process_start_time_seconds gauge job, ins, ip, instance, cls Start time of the process since unix epoch in seconds. process_virtual_memory_bytes gauge job, ins, ip, instance, cls Virtual memory size in bytes. process_virtual_memory_max_bytes gauge job, ins, ip, instance, cls Maximum amount of virtual memory available in bytes. promhttp_metric_handler_requests_in_flight gauge job, ins, ip, instance, cls Current number of scrapes being served. promhttp_metric_handler_requests_total counter code, job, ins, ip, instance, cls Total number of scrapes by HTTP status code. scrape_duration_seconds Unknown job, ins, ip, instance, cls N/A scrape_samples_post_metric_relabeling Unknown job, ins, ip, instance, cls N/A scrape_samples_scraped Unknown job, ins, ip, instance, cls N/A scrape_series_added Unknown job, ins, ip, instance, cls N/A up Unknown job, ins, ip, instance, cls N/A ","categories":["Reference"],"description":"Complete monitoring metrics reference for the Pigsty PGSQL module","excerpt":"Complete monitoring metrics reference for the Pigsty PGSQL module","ref":"/docs/pgsql/metric/","tags":"","title":"Metrics"},{"body":"The PGSQL module contains 638 types of available monitoring metrics.\nMetric Name Type Labels Description ALERTS Unknown category, job, level, ins, severity, ip, alertname, alertstate, instance, cls N/A ALERTS_FOR_STATE Unknown category, job, level, ins, severity, ip, alertname, instance, cls N/A cls:pressure1 Unknown job, cls N/A cls:pressure15 Unknown job, cls N/A cls:pressure5 Unknown job, cls N/A go_gc_duration_seconds summary job, ins, ip, instance, quantile, cls A summary of the pause duration of garbage collection cycles. go_gc_duration_seconds_count Unknown job, ins, ip, instance, cls N/A go_gc_duration_seconds_sum Unknown job, ins, ip, instance, cls N/A go_goroutines gauge job, ins, ip, instance, cls Number of goroutines that currently exist. go_info gauge version, job, ins, ip, instance, cls Information about the Go environment. go_memstats_alloc_bytes gauge job, ins, ip, instance, cls Number of bytes allocated and still in use. go_memstats_alloc_bytes_total counter job, ins, ip, instance, cls Total number of bytes allocated, even if freed. go_memstats_buck_hash_sys_bytes gauge job, ins, ip, instance, cls Number of bytes used by the profiling bucket hash table. go_memstats_frees_total counter job, ins, ip, instance, cls Total number of frees. go_memstats_gc_sys_bytes gauge job, ins, ip, instance, cls Number of bytes used for garbage collection system metadata. go_memstats_heap_alloc_bytes gauge job, ins, ip, instance, cls Number of heap bytes allocated and still in use. go_memstats_heap_idle_bytes gauge job, ins, ip, instance, cls Number of heap bytes waiting to be used. go_memstats_heap_inuse_bytes gauge job, ins, ip, instance, cls Number of heap bytes that are in use. go_memstats_heap_objects gauge job, ins, ip, instance, cls Number of allocated objects. go_memstats_heap_released_bytes gauge job, ins, ip, instance, cls Number of heap bytes released to OS. go_memstats_heap_sys_bytes gauge job, ins, ip, instance, cls Number of heap bytes obtained from system. go_memstats_last_gc_time_seconds gauge job, ins, ip, instance, cls Number of seconds since 1970 of last garbage collection. go_memstats_lookups_total counter job, ins, ip, instance, cls Total number of pointer lookups. go_memstats_mallocs_total counter job, ins, ip, instance, cls Total number of mallocs. go_memstats_mcache_inuse_bytes gauge job, ins, ip, instance, cls Number of bytes in use by mcache structures. go_memstats_mcache_sys_bytes gauge job, ins, ip, instance, cls Number of bytes used for mcache structures obtained from system. go_memstats_mspan_inuse_bytes gauge job, ins, ip, instance, cls Number of bytes in use by mspan structures. go_memstats_mspan_sys_bytes gauge job, ins, ip, instance, cls Number of bytes used for mspan structures obtained from system. go_memstats_next_gc_bytes gauge job, ins, ip, instance, cls Number of heap bytes when next garbage collection will take place. go_memstats_other_sys_bytes gauge job, ins, ip, instance, cls Number of bytes used for other system allocations. go_memstats_stack_inuse_bytes gauge job, ins, ip, instance, cls Number of bytes in use by the stack allocator. go_memstats_stack_sys_bytes gauge job, ins, ip, instance, cls Number of bytes obtained from system for stack allocator. go_memstats_sys_bytes gauge job, ins, ip, instance, cls Number of bytes obtained from system. go_threads gauge job, ins, ip, instance, cls Number of OS threads created. ins:pressure1 Unknown job, ins, ip, cls N/A ins:pressure15 Unknown job, ins, ip, cls N/A ins:pressure5 Unknown job, ins, ip, cls N/A patroni_cluster_unlocked gauge job, ins, ip, instance, cls, scope Value is 1 if the cluster is unlocked, 0 if locked. patroni_dcs_last_seen gauge job, ins, ip, instance, cls, scope Epoch timestamp when DCS was last contacted successfully by Patroni. patroni_failsafe_mode_is_active gauge job, ins, ip, instance, cls, scope Value is 1 if failsafe mode is active, 0 if inactive. patroni_is_paused gauge job, ins, ip, instance, cls, scope Value is 1 if auto failover is disabled, 0 otherwise. patroni_master gauge job, ins, ip, instance, cls, scope Value is 1 if this node is the leader, 0 otherwise. patroni_pending_restart gauge job, ins, ip, instance, cls, scope Value is 1 if the node needs a restart, 0 otherwise. patroni_postgres_in_archive_recovery gauge job, ins, ip, instance, cls, scope Value is 1 if Postgres is replicating from archive, 0 otherwise. patroni_postgres_running gauge job, ins, ip, instance, cls, scope Value is 1 if Postgres is running, 0 otherwise. patroni_postgres_server_version gauge job, ins, ip, instance, cls, scope Version of Postgres (if running), 0 otherwise. patroni_postgres_streaming gauge job, ins, ip, instance, cls, scope Value is 1 if Postgres is streaming, 0 otherwise. patroni_postgres_timeline counter job, ins, ip, instance, cls, scope Postgres timeline of this node (if running), 0 otherwise. patroni_postmaster_start_time gauge job, ins, ip, instance, cls, scope Epoch seconds since Postgres started. patroni_primary gauge job, ins, ip, instance, cls, scope Value is 1 if this node is the leader, 0 otherwise. patroni_replica gauge job, ins, ip, instance, cls, scope Value is 1 if this node is a replica, 0 otherwise. patroni_standby_leader gauge job, ins, ip, instance, cls, scope Value is 1 if this node is the standby_leader, 0 otherwise. patroni_sync_standby gauge job, ins, ip, instance, cls, scope Value is 1 if this node is a sync standby replica, 0 otherwise. patroni_up Unknown job, ins, ip, instance, cls N/A patroni_version gauge job, ins, ip, instance, cls, scope Patroni semver without periods. patroni_xlog_location counter job, ins, ip, instance, cls, scope Current location of the Postgres transaction log, 0 if this node is not the leader. patroni_xlog_paused gauge job, ins, ip, instance, cls, scope Value is 1 if the Postgres xlog is paused, 0 otherwise. patroni_xlog_received_location counter job, ins, ip, instance, cls, scope Current location of the received Postgres transaction log, 0 if this node is not a replica. patroni_xlog_replayed_location counter job, ins, ip, instance, cls, scope Current location of the replayed Postgres transaction log, 0 if this node is not a replica. patroni_xlog_replayed_timestamp gauge job, ins, ip, instance, cls, scope Current timestamp of the replayed Postgres transaction log, 0 if null. pg:cls:active_backends Unknown job, cls N/A pg:cls:active_time_rate15m Unknown job, cls N/A pg:cls:active_time_rate1m Unknown job, cls N/A pg:cls:active_time_rate5m Unknown job, cls N/A pg:cls:age Unknown job, cls N/A pg:cls:buf_alloc_rate1m Unknown job, cls N/A pg:cls:buf_clean_rate1m Unknown job, cls N/A pg:cls:buf_flush_backend_rate1m Unknown job, cls N/A pg:cls:buf_flush_checkpoint_rate1m Unknown job, cls N/A pg:cls:cpu_count Unknown job, cls N/A pg:cls:cpu_usage Unknown job, cls N/A pg:cls:cpu_usage_15m Unknown job, cls N/A pg:cls:cpu_usage_1m Unknown job, cls N/A pg:cls:cpu_usage_5m Unknown job, cls N/A pg:cls:db_size Unknown job, cls N/A pg:cls:file_size Unknown job, cls N/A pg:cls:ixact_backends Unknown job, cls N/A pg:cls:ixact_time_rate1m Unknown job, cls N/A pg:cls:lag_bytes Unknown job, cls N/A pg:cls:lag_seconds Unknown job, cls N/A pg:cls:leader Unknown job, ins, ip, instance, cls N/A pg:cls:load1 Unknown job, cls N/A pg:cls:load15 Unknown job, cls N/A pg:cls:load5 Unknown job, cls N/A pg:cls:lock_count Unknown job, cls N/A pg:cls:locks Unknown job, cls, mode N/A pg:cls:log_size Unknown job, cls N/A pg:cls:lsn_rate1m Unknown job, cls N/A pg:cls:members Unknown job, ins, ip, cls N/A pg:cls:num_backends Unknown job, cls N/A pg:cls:partition Unknown job, cls N/A pg:cls:receiver Unknown state, slot_name, job, appname, ip, cls, sender_host, sender_port N/A pg:cls:rlock_count Unknown job, cls N/A pg:cls:saturation1 Unknown job, cls N/A pg:cls:saturation15 Unknown job, cls N/A pg:cls:saturation5 Unknown job, cls N/A pg:cls:sender Unknown pid, usename, address, job, ins, appname, ip, cls N/A pg:cls:session_time_rate1m Unknown job, cls N/A pg:cls:size Unknown job, cls N/A pg:cls:slot_count Unknown job, cls N/A pg:cls:slot_retained_bytes Unknown job, cls N/A pg:cls:standby_count Unknown job, cls N/A pg:cls:sync_state Unknown job, cls N/A pg:cls:timeline Unknown job, cls N/A pg:cls:tup_deleted_rate1m Unknown job, cls N/A pg:cls:tup_fetched_rate1m Unknown job, cls N/A pg:cls:tup_inserted_rate1m Unknown job, cls N/A pg:cls:tup_modified_rate1m Unknown job, cls N/A pg:cls:tup_returned_rate1m Unknown job, cls N/A pg:cls:wal_size Unknown job, cls N/A pg:cls:xact_commit_rate15m Unknown job, cls N/A pg:cls:xact_commit_rate1m Unknown job, cls N/A pg:cls:xact_commit_rate5m Unknown job, cls N/A pg:cls:xact_rollback_rate15m Unknown job, cls N/A pg:cls:xact_rollback_rate1m Unknown job, cls N/A pg:cls:xact_rollback_rate5m Unknown job, cls N/A pg:cls:xact_total_rate15m Unknown job, cls N/A pg:cls:xact_total_rate1m Unknown job, cls N/A pg:cls:xact_total_sigma15m Unknown job, cls N/A pg:cls:xlock_count Unknown job, cls N/A pg:db:active_backends Unknown datname, job, ins, ip, instance, cls N/A pg:db:active_time_rate15m Unknown datname, job, ins, ip, instance, cls N/A pg:db:active_time_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:active_time_rate5m Unknown datname, job, ins, ip, instance, cls N/A pg:db:age Unknown datname, job, ins, ip, instance, cls N/A pg:db:age_deriv1h Unknown datname, job, ins, ip, instance, cls N/A pg:db:age_exhaust Unknown datname, job, ins, ip, instance, cls N/A pg:db:blk_io_time_seconds_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:blk_read_time_seconds_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:blk_write_time_seconds_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:blks_access_1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:blks_hit_1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:blks_hit_ratio1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:blks_read_1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:conn_limit Unknown datname, job, ins, ip, instance, cls N/A pg:db:conn_usage Unknown datname, job, ins, ip, instance, cls N/A pg:db:db_size Unknown datname, job, ins, ip, instance, cls N/A pg:db:ixact_backends Unknown datname, job, ins, ip, instance, cls N/A pg:db:ixact_time_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:lock_count Unknown datname, job, ins, ip, instance, cls N/A pg:db:num_backends Unknown datname, job, ins, ip, instance, cls N/A pg:db:rlock_count Unknown datname, job, ins, ip, instance, cls N/A pg:db:session_time_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:temp_bytes_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:temp_files_1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:tup_deleted_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:tup_fetched_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:tup_inserted_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:tup_modified_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:tup_returned_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:wlock_count Unknown datname, job, ins, ip, instance, cls N/A pg:db:xact_commit_rate15m Unknown datname, job, ins, ip, instance, cls N/A pg:db:xact_commit_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:xact_commit_rate5m Unknown datname, job, ins, ip, instance, cls N/A pg:db:xact_rollback_rate15m Unknown datname, job, ins, ip, instance, cls N/A pg:db:xact_rollback_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:xact_rollback_rate5m Unknown datname, job, ins, ip, instance, cls N/A pg:db:xact_total_rate15m Unknown datname, job, ins, ip, instance, cls N/A pg:db:xact_total_rate1m Unknown datname, job, ins, ip, instance, cls N/A pg:db:xact_total_rate5m Unknown datname, job, ins, ip, instance, cls N/A pg:db:xact_total_sigma15m Unknown datname, job, ins, ip, instance, cls N/A pg:db:xlock_count Unknown datname, job, ins, ip, instance, cls N/A pg:env:active_backends Unknown job N/A pg:env:active_time_rate15m Unknown job N/A pg:env:active_time_rate1m Unknown job N/A pg:env:active_time_rate5m Unknown job N/A pg:env:age Unknown job N/A pg:env:cpu_count Unknown job N/A pg:env:cpu_usage Unknown job N/A pg:env:cpu_usage_15m Unknown job N/A pg:env:cpu_usage_1m Unknown job N/A pg:env:cpu_usage_5m Unknown job N/A pg:env:ixact_backends Unknown job N/A pg:env:ixact_time_rate1m Unknown job N/A pg:env:lag_bytes Unknown job N/A pg:env:lag_seconds Unknown job N/A pg:env:lsn_rate1m Unknown job N/A pg:env:session_time_rate1m Unknown job N/A pg:env:tup_deleted_rate1m Unknown job N/A pg:env:tup_fetched_rate1m Unknown job N/A pg:env:tup_inserted_rate1m Unknown job N/A pg:env:tup_modified_rate1m Unknown job N/A pg:env:tup_returned_rate1m Unknown job N/A pg:env:xact_commit_rate15m Unknown job N/A pg:env:xact_commit_rate1m Unknown job N/A pg:env:xact_commit_rate5m Unknown job N/A pg:env:xact_rollback_rate15m Unknown job N/A pg:env:xact_rollback_rate1m Unknown job N/A pg:env:xact_rollback_rate5m Unknown job N/A pg:env:xact_total_rate15m Unknown job N/A pg:env:xact_total_rate1m Unknown job N/A pg:env:xact_total_sigma15m Unknown job N/A pg:ins:active_backends Unknown job, ins, ip, instance, cls N/A pg:ins:active_time_rate15m Unknown job, ins, ip, instance, cls N/A pg:ins:active_time_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:active_time_rate5m Unknown job, ins, ip, instance, cls N/A pg:ins:age Unknown job, ins, ip, instance, cls N/A pg:ins:blks_hit_ratio1m Unknown job, ins, ip, instance, cls N/A pg:ins:buf_alloc_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:buf_clean_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:buf_flush_backend_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:buf_flush_checkpoint_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:ckpt_1h Unknown job, ins, ip, instance, cls N/A pg:ins:ckpt_req_1m Unknown job, ins, ip, instance, cls N/A pg:ins:ckpt_timed_1m Unknown job, ins, ip, instance, cls N/A pg:ins:conn_limit Unknown job, ins, ip, instance, cls N/A pg:ins:conn_usage Unknown job, ins, ip, instance, cls N/A pg:ins:cpu_count Unknown job, ins, ip, instance, cls N/A pg:ins:cpu_usage Unknown job, ins, ip, instance, cls N/A pg:ins:cpu_usage_15m Unknown job, ins, ip, instance, cls N/A pg:ins:cpu_usage_1m Unknown job, ins, ip, instance, cls N/A pg:ins:cpu_usage_5m Unknown job, ins, ip, instance, cls N/A pg:ins:db_size Unknown job, ins, ip, instance, cls N/A pg:ins:file_size Unknown job, ins, ip, instance, cls N/A pg:ins:fs_size Unknown job, ins, ip, instance, cls N/A pg:ins:is_leader Unknown job, ins, ip, instance, cls N/A pg:ins:ixact_backends Unknown job, ins, ip, instance, cls N/A pg:ins:ixact_time_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:lag_bytes Unknown job, ins, ip, instance, cls N/A pg:ins:lag_seconds Unknown job, ins, ip, instance, cls N/A pg:ins:load1 Unknown job, ins, ip, instance, cls N/A pg:ins:load15 Unknown job, ins, ip, instance, cls N/A pg:ins:load5 Unknown job, ins, ip, instance, cls N/A pg:ins:lock_count Unknown job, ins, ip, instance, cls N/A pg:ins:locks Unknown job, ins, ip, mode, instance, cls N/A pg:ins:log_size Unknown job, ins, ip, instance, cls N/A pg:ins:lsn_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:mem_size Unknown job, ins, ip, instance, cls N/A pg:ins:num_backends Unknown job, ins, ip, instance, cls N/A pg:ins:rlock_count Unknown job, ins, ip, instance, cls N/A pg:ins:saturation1 Unknown job, ins, ip, cls N/A pg:ins:saturation15 Unknown job, ins, ip, cls N/A pg:ins:saturation5 Unknown job, ins, ip, cls N/A pg:ins:session_time_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:slot_retained_bytes Unknown job, ins, ip, instance, cls N/A pg:ins:space_usage Unknown job, ins, ip, instance, cls N/A pg:ins:status Unknown job, ins, ip, instance, cls N/A pg:ins:sync_state Unknown job, ins, instance, cls N/A pg:ins:target_count Unknown job, cls, ins N/A pg:ins:timeline Unknown job, ins, ip, instance, cls N/A pg:ins:tup_deleted_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:tup_fetched_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:tup_inserted_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:tup_modified_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:tup_returned_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:wal_size Unknown job, ins, ip, instance, cls N/A pg:ins:wlock_count Unknown job, ins, ip, instance, cls N/A pg:ins:xact_commit_rate15m Unknown job, ins, ip, instance, cls N/A pg:ins:xact_commit_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:xact_commit_rate5m Unknown job, ins, ip, instance, cls N/A pg:ins:xact_rollback_rate15m Unknown job, ins, ip, instance, cls N/A pg:ins:xact_rollback_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:xact_rollback_rate5m Unknown job, ins, ip, instance, cls N/A pg:ins:xact_total_rate15m Unknown job, ins, ip, instance, cls N/A pg:ins:xact_total_rate1m Unknown job, ins, ip, instance, cls N/A pg:ins:xact_total_rate5m Unknown job, ins, ip, instance, cls N/A pg:ins:xact_total_sigma15m Unknown job, ins, ip, instance, cls N/A pg:ins:xlock_count Unknown job, ins, ip, instance, cls N/A pg:query:call_rate1m Unknown datname, query, job, ins, ip, instance, cls N/A pg:query:rt_1m Unknown datname, query, job, ins, ip, instance, cls N/A pg:table:scan_rate1m Unknown datname, relname, job, ins, ip, instance, cls N/A pg_activity_count gauge datname, state, job, ins, ip, instance, cls Count of connection among (datname,state) pg_activity_max_conn_duration gauge datname, state, job, ins, ip, instance, cls Max backend session duration since state change among (datname, state) pg_activity_max_duration gauge datname, state, job, ins, ip, instance, cls Max duration since last state change among (datname, state) pg_activity_max_tx_duration gauge datname, state, job, ins, ip, instance, cls Max transaction duration since state change among (datname, state) pg_archiver_failed_count counter job, ins, ip, instance, cls Number of failed attempts for archiving WAL files pg_archiver_finish_count counter job, ins, ip, instance, cls Number of WAL files that have been successfully archived pg_archiver_last_failed_time counter job, ins, ip, instance, cls Time of the last failed archival operation pg_archiver_last_finish_time counter job, ins, ip, instance, cls Time of the last successful archive operation pg_archiver_reset_time gauge job, ins, ip, instance, cls Time at which archive statistics were last reset pg_backend_count gauge type, job, ins, ip, instance, cls Database backend process count by backend_type pg_bgwriter_buffers_alloc counter job, ins, ip, instance, cls Number of buffers allocated pg_bgwriter_buffers_backend counter job, ins, ip, instance, cls Number of buffers written directly by a backend pg_bgwriter_buffers_backend_fsync counter job, ins, ip, instance, cls Number of times a backend had to execute its own fsync call pg_bgwriter_buffers_checkpoint counter job, ins, ip, instance, cls Number of buffers written during checkpoints pg_bgwriter_buffers_clean counter job, ins, ip, instance, cls Number of buffers written by the background writer pg_bgwriter_checkpoint_sync_time counter job, ins, ip, instance, cls Total amount of time that has been spent in the portion of checkpoint processing where files are synchronized to disk, in seconds pg_bgwriter_checkpoint_write_time counter job, ins, ip, instance, cls Total amount of time that has been spent in the portion of checkpoint processing where files are written to disk, in seconds pg_bgwriter_checkpoints_req counter job, ins, ip, instance, cls Number of requested checkpoints that have been performed pg_bgwriter_checkpoints_timed counter job, ins, ip, instance, cls Number of scheduled checkpoints that have been performed pg_bgwriter_maxwritten_clean counter job, ins, ip, instance, cls Number of times the background writer stopped a cleaning scan because it had written too many buffers pg_bgwriter_reset_time counter job, ins, ip, instance, cls Time at which bgwriter statistics were last reset pg_boot_time gauge job, ins, ip, instance, cls unix timestamp when postmaster boot pg_checkpoint_checkpoint_lsn counter job, ins, ip, instance, cls Latest checkpoint location pg_checkpoint_elapse gauge job, ins, ip, instance, cls Seconds elapsed since latest checkpoint in seconds pg_checkpoint_full_page_writes gauge job, ins, ip, instance, cls Latest checkpoint’s full_page_writes enabled pg_checkpoint_newest_commit_ts_xid counter job, ins, ip, instance, cls Latest checkpoint’s newestCommitTsXid pg_checkpoint_next_multi_offset counter job, ins, ip, instance, cls Latest checkpoint’s NextMultiOffset pg_checkpoint_next_multixact_id counter job, ins, ip, instance, cls Latest checkpoint’s NextMultiXactId pg_checkpoint_next_oid counter job, ins, ip, instance, cls Latest checkpoint’s NextOID pg_checkpoint_next_xid counter job, ins, ip, instance, cls Latest checkpoint’s NextXID xid pg_checkpoint_next_xid_epoch counter job, ins, ip, instance, cls Latest checkpoint’s NextXID epoch pg_checkpoint_oldest_active_xid counter job, ins, ip, instance, cls Latest checkpoint’s oldestActiveXID pg_checkpoint_oldest_commit_ts_xid counter job, ins, ip, instance, cls Latest checkpoint’s oldestCommitTsXid pg_checkpoint_oldest_multi_dbid gauge job, ins, ip, instance, cls Latest checkpoint’s oldestMulti’s DB OID pg_checkpoint_oldest_multi_xid counter job, ins, ip, instance, cls Latest checkpoint’s oldestMultiXid pg_checkpoint_oldest_xid counter job, ins, ip, instance, cls Latest checkpoint’s oldestXID pg_checkpoint_oldest_xid_dbid gauge job, ins, ip, instance, cls Latest checkpoint’s oldestXID’s DB OID pg_checkpoint_prev_tli counter job, ins, ip, instance, cls Latest checkpoint’s PrevTimeLineID pg_checkpoint_redo_lsn counter job, ins, ip, instance, cls Latest checkpoint’s REDO location pg_checkpoint_time counter job, ins, ip, instance, cls Time of latest checkpoint pg_checkpoint_tli counter job, ins, ip, instance, cls Latest checkpoint’s TimeLineID pg_conf_reload_time gauge job, ins, ip, instance, cls seconds since last configuration reload pg_db_active_time counter datname, job, ins, ip, instance, cls Time spent executing SQL statements in this database, in seconds pg_db_age gauge datname, job, ins, ip, instance, cls Age of database calculated from datfrozenxid pg_db_allow_conn gauge datname, job, ins, ip, instance, cls If false(0) then no one can connect to this database. pg_db_blk_read_time counter datname, job, ins, ip, instance, cls Time spent reading data file blocks by backends in this database, in seconds pg_db_blk_write_time counter datname, job, ins, ip, instance, cls Time spent writing data file blocks by backends in this database, in seconds pg_db_blks_access counter datname, job, ins, ip, instance, cls Number of times disk blocks that accessed read+hit pg_db_blks_hit counter datname, job, ins, ip, instance, cls Number of times disk blocks were found already in the buffer cache pg_db_blks_read counter datname, job, ins, ip, instance, cls Number of disk blocks read in this database pg_db_cks_fail_time gauge datname, job, ins, ip, instance, cls Time at which the last data page checksum failure was detected in this database pg_db_cks_fails counter datname, job, ins, ip, instance, cls Number of data page checksum failures detected in this database, -1 for not enabled pg_db_confl_confl_bufferpin counter datname, job, ins, ip, instance, cls Number of queries in this database that have been canceled due to pinned buffers pg_db_confl_confl_deadlock counter datname, job, ins, ip, instance, cls Number of queries in this database that have been canceled due to deadlocks pg_db_confl_confl_lock counter datname, job, ins, ip, instance, cls Number of queries in this database that have been canceled due to lock timeouts pg_db_confl_confl_snapshot counter datname, job, ins, ip, instance, cls Number of queries in this database that have been canceled due to old snapshots pg_db_confl_confl_tablespace counter datname, job, ins, ip, instance, cls Number of queries in this database that have been canceled due to dropped tablespaces pg_db_conflicts counter datname, job, ins, ip, instance, cls Number of queries canceled due to conflicts with recovery in this database pg_db_conn_limit gauge datname, job, ins, ip, instance, cls Sets maximum number of concurrent connections that can be made to this database. -1 means no limit. pg_db_datid gauge datname, job, ins, ip, instance, cls OID of the database pg_db_deadlocks counter datname, job, ins, ip, instance, cls Number of deadlocks detected in this database pg_db_frozen_xid gauge datname, job, ins, ip, instance, cls All transaction IDs before this one have been frozened pg_db_is_template gauge datname, job, ins, ip, instance, cls If true(1), then this database can be cloned by any user with CREATEDB privileges pg_db_ixact_time counter datname, job, ins, ip, instance, cls Time spent idling while in a transaction in this database, in seconds pg_db_numbackends gauge datname, job, ins, ip, instance, cls Number of backends currently connected to this database pg_db_reset_time counter datname, job, ins, ip, instance, cls Time at which database statistics were last reset pg_db_session_time counter datname, job, ins, ip, instance, cls Time spent by database sessions in this database, in seconds pg_db_sessions counter datname, job, ins, ip, instance, cls Total number of sessions established to this database pg_db_sessions_abandoned counter datname, job, ins, ip, instance, cls Number of database sessions to this database that were terminated because connection to the client was lost pg_db_sessions_fatal counter datname, job, ins, ip, instance, cls Number of database sessions to this database that were terminated by fatal errors pg_db_sessions_killed counter datname, job, ins, ip, instance, cls Number of database sessions to this database that were terminated by operator intervention pg_db_temp_bytes counter datname, job, ins, ip, instance, cls Total amount of data written to temporary files by queries in this database. pg_db_temp_files counter datname, job, ins, ip, instance, cls Number of temporary files created by queries in this database pg_db_tup_deleted counter datname, job, ins, ip, instance, cls Number of rows deleted by queries in this database pg_db_tup_fetched counter datname, job, ins, ip, instance, cls Number of rows fetched by queries in this database pg_db_tup_inserted counter datname, job, ins, ip, instance, cls Number of rows inserted by queries in this database pg_db_tup_modified counter datname, job, ins, ip, instance, cls Number of rows modified by queries in this database pg_db_tup_returned counter datname, job, ins, ip, instance, cls Number of rows returned by queries in this database pg_db_tup_updated counter datname, job, ins, ip, instance, cls Number of rows updated by queries in this database pg_db_xact_commit counter datname, job, ins, ip, instance, cls Number of transactions in this database that have been committed pg_db_xact_rollback counter datname, job, ins, ip, instance, cls Number of transactions in this database that have been rolled back pg_db_xact_total counter datname, job, ins, ip, instance, cls Number of transactions in this database pg_downstream_count gauge state, job, ins, ip, instance, cls Count of corresponding state pg_exporter_agent_up Unknown job, ins, ip, instance, cls N/A pg_exporter_last_scrape_time gauge job, ins, ip, instance, cls seconds exporter spending on scrapping pg_exporter_query_cache_ttl gauge datname, query, job, ins, ip, instance, cls times to live of query cache pg_exporter_query_scrape_duration gauge datname, query, job, ins, ip, instance, cls seconds query spending on scrapping pg_exporter_query_scrape_error_count gauge datname, query, job, ins, ip, instance, cls times the query failed pg_exporter_query_scrape_hit_count gauge datname, query, job, ins, ip, instance, cls numbers been scrapped from this query pg_exporter_query_scrape_metric_count gauge datname, query, job, ins, ip, instance, cls numbers of metrics been scrapped from this query pg_exporter_query_scrape_total_count gauge datname, query, job, ins, ip, instance, cls times exporter server was scraped for metrics pg_exporter_scrape_duration gauge job, ins, ip, instance, cls seconds exporter spending on scrapping pg_exporter_scrape_error_count counter job, ins, ip, instance, cls times exporter was scraped for metrics and failed pg_exporter_scrape_total_count counter job, ins, ip, instance, cls times exporter was scraped for metrics pg_exporter_server_scrape_duration gauge datname, job, ins, ip, instance, cls seconds exporter server spending on scrapping pg_exporter_server_scrape_error_count Unknown datname, job, ins, ip, instance, cls N/A pg_exporter_server_scrape_total_count gauge datname, job, ins, ip, instance, cls times exporter server was scraped for metrics pg_exporter_server_scrape_total_seconds gauge datname, job, ins, ip, instance, cls seconds exporter server spending on scrapping pg_exporter_up gauge job, ins, ip, instance, cls always be 1 if your could retrieve metrics pg_exporter_uptime gauge job, ins, ip, instance, cls seconds since exporter primary server inited pg_flush_lsn counter job, ins, ip, instance, cls primary only, location of current wal syncing pg_func_calls counter datname, funcname, job, ins, ip, instance, cls Number of times this function has been called pg_func_self_time counter datname, funcname, job, ins, ip, instance, cls Total time spent in this function itself, not including other functions called by it, in ms pg_func_total_time counter datname, funcname, job, ins, ip, instance, cls Total time spent in this function and all other functions called by it, in ms pg_in_recovery gauge job, ins, ip, instance, cls server is in recovery mode? 1 for yes 0 for no pg_index_idx_blks_hit counter datname, relname, job, ins, relid, ip, instance, cls, idxname Number of buffer hits in this index pg_index_idx_blks_read counter datname, relname, job, ins, relid, ip, instance, cls, idxname Number of disk blocks read from this index pg_index_idx_scan counter datname, relname, job, ins, relid, ip, instance, cls, idxname Number of index scans initiated on this index pg_index_idx_tup_fetch counter datname, relname, job, ins, relid, ip, instance, cls, idxname Number of live table rows fetched by simple index scans using this index pg_index_idx_tup_read counter datname, relname, job, ins, relid, ip, instance, cls, idxname Number of index entries returned by scans on this index pg_index_relpages gauge datname, relname, job, ins, relid, ip, instance, cls, idxname Size of the on-disk representation of this index in pages pg_index_reltuples gauge datname, relname, job, ins, relid, ip, instance, cls, idxname Estimate relation tuples pg_insert_lsn counter job, ins, ip, instance, cls primary only, location of current wal inserting pg_io_evictions counter type, job, ins, object, ip, context, instance, cls Number of times a block has been written out from a shared or local buffer pg_io_extend_time counter type, job, ins, object, ip, context, instance, cls Time spent in extend operations in seconds pg_io_extends counter type, job, ins, object, ip, context, instance, cls Number of relation extend operations, each of the size specified in op_bytes. pg_io_fsync_time counter type, job, ins, object, ip, context, instance, cls Time spent in fsync operations in seconds pg_io_fsyncs counter type, job, ins, object, ip, context, instance, cls Number of fsync calls. These are only tracked in context normal pg_io_hits counter type, job, ins, object, ip, context, instance, cls The number of times a desired block was found in a shared buffer. pg_io_op_bytes gauge type, job, ins, object, ip, context, instance, cls The number of bytes per unit of I/O read, written, or extended. 8192 by default pg_io_read_time counter type, job, ins, object, ip, context, instance, cls Time spent in read operations in seconds pg_io_reads counter type, job, ins, object, ip, context, instance, cls Number of read operations, each of the size specified in op_bytes. pg_io_reset_time gauge type, job, ins, object, ip, context, instance, cls Timestamp at which these statistics were last reset pg_io_reuses counter type, job, ins, object, ip, context, instance, cls The number of times an existing buffer in reused pg_io_write_time counter type, job, ins, object, ip, context, instance, cls Time spent in write operations in seconds pg_io_writeback_time counter type, job, ins, object, ip, context, instance, cls Time spent in writeback operations in seconds pg_io_writebacks counter type, job, ins, object, ip, context, instance, cls Number of units of size op_bytes which the process requested the kernel write out to permanent storage. pg_io_writes counter type, job, ins, object, ip, context, instance, cls Number of write operations, each of the size specified in op_bytes. pg_is_in_recovery gauge job, ins, ip, instance, cls 1 if in recovery mode pg_is_wal_replay_paused gauge job, ins, ip, instance, cls 1 if wal play paused pg_lag gauge job, ins, ip, instance, cls replica only, replication lag in seconds pg_last_replay_time gauge job, ins, ip, instance, cls time when last transaction been replayed pg_lock_count gauge datname, job, ins, ip, mode, instance, cls Number of locks of corresponding mode and database pg_lsn counter job, ins, ip, instance, cls log sequence number, current write location pg_meta_info gauge cls, extensions, version, job, ins, primary_conninfo, conf_path, hba_path, ip, cluster_id, instance, listen_port, wal_level, ver_num, cluster_name, data_dir constant 1 pg_query_calls counter datname, query, job, ins, ip, instance, cls Number of times the statement was executed pg_query_exec_time counter datname, query, job, ins, ip, instance, cls Total time spent executing the statement, in seconds pg_query_io_time counter datname, query, job, ins, ip, instance, cls Total time the statement spent reading and writing blocks, in seconds pg_query_rows counter datname, query, job, ins, ip, instance, cls Total number of rows retrieved or affected by the statement pg_query_sblk_dirtied counter datname, query, job, ins, ip, instance, cls Total number of shared blocks dirtied by the statement pg_query_sblk_hit counter datname, query, job, ins, ip, instance, cls Total number of shared block cache hits by the statement pg_query_sblk_read counter datname, query, job, ins, ip, instance, cls Total number of shared blocks read by the statement pg_query_sblk_written counter datname, query, job, ins, ip, instance, cls Total number of shared blocks written by the statement pg_query_wal_bytes counter datname, query, job, ins, ip, instance, cls Total amount of WAL bytes generated by the statement pg_receive_lsn counter job, ins, ip, instance, cls replica only, location of wal synced to disk pg_recovery_backup_end_lsn counter job, ins, ip, instance, cls Backup end location pg_recovery_backup_start_lsn counter job, ins, ip, instance, cls Backup start location pg_recovery_min_lsn counter job, ins, ip, instance, cls Minimum recovery ending location pg_recovery_min_timeline counter job, ins, ip, instance, cls Min recovery ending loc’s timeline pg_recovery_prefetch_block_distance gauge job, ins, ip, instance, cls How many blocks ahead the prefetcher is looking pg_recovery_prefetch_hit counter job, ins, ip, instance, cls Number of blocks not prefetched because they were already in the buffer pool pg_recovery_prefetch_io_depth gauge job, ins, ip, instance, cls How many prefetches have been initiated but are not yet known to have completed pg_recovery_prefetch_prefetch counter job, ins, ip, instance, cls Number of blocks prefetched because they were not in the buffer pool pg_recovery_prefetch_reset_time counter job, ins, ip, instance, cls Time at which these recovery prefetch statistics were last reset pg_recovery_prefetch_skip_fpw gauge job, ins, ip, instance, cls Number of blocks not prefetched because a full page image was included in the WAL pg_recovery_prefetch_skip_init counter job, ins, ip, instance, cls Number of blocks not prefetched because they would be zero-initialized pg_recovery_prefetch_skip_new counter job, ins, ip, instance, cls Number of blocks not prefetched because they didn’t exist yet pg_recovery_prefetch_skip_rep counter job, ins, ip, instance, cls Number of blocks not prefetched because they were already recently prefetched pg_recovery_prefetch_wal_distance gauge job, ins, ip, instance, cls How many bytes ahead the prefetcher is looking pg_recovery_require_record gauge job, ins, ip, instance, cls End-of-backup record required pg_recv_flush_lsn counter state, slot_name, job, ins, ip, instance, cls, sender_host, sender_port Last write-ahead log location already received and flushed to disk pg_recv_flush_tli counter state, slot_name, job, ins, ip, instance, cls, sender_host, sender_port Timeline number of last write-ahead log location received and flushed to disk pg_recv_init_lsn counter state, slot_name, job, ins, ip, instance, cls, sender_host, sender_port First write-ahead log location used when WAL receiver is started pg_recv_init_tli counter state, slot_name, job, ins, ip, instance, cls, sender_host, sender_port First timeline number used when WAL receiver is started pg_recv_msg_recv_time gauge state, slot_name, job, ins, ip, instance, cls, sender_host, sender_port Receipt time of last message received from origin WAL sender pg_recv_msg_send_time gauge state, slot_name, job, ins, ip, instance, cls, sender_host, sender_port Send time of last message received from origin WAL sender pg_recv_pid gauge state, slot_name, job, ins, ip, instance, cls, sender_host, sender_port Process ID of the WAL receiver process pg_recv_reported_lsn counter state, slot_name, job, ins, ip, instance, cls, sender_host, sender_port Last write-ahead log location reported to origin WAL sender pg_recv_reported_time gauge state, slot_name, job, ins, ip, instance, cls, sender_host, sender_port Time of last write-ahead log location reported to origin WAL sender pg_recv_time gauge state, slot_name, job, ins, ip, instance, cls, sender_host, sender_port Time of current snapshot pg_recv_write_lsn counter state, slot_name, job, ins, ip, instance, cls, sender_host, sender_port Last write-ahead log location already received and written to disk, but not flushed. pg_relkind_count gauge datname, job, ins, ip, instance, cls, relkind Number of relations of corresponding relkind pg_repl_backend_xmin counter pid, usename, address, job, ins, appname, ip, instance, cls This standby’s xmin horizon reported by hot_standby_feedback. pg_repl_client_port gauge pid, usename, address, job, ins, appname, ip, instance, cls TCP port number that the client is using for communication with this WAL sender, or -1 if a Unix socket is used pg_repl_flush_diff gauge pid, usename, address, job, ins, appname, ip, instance, cls Last log position flushed to disk by this standby server diff with current lsn pg_repl_flush_lag gauge pid, usename, address, job, ins, appname, ip, instance, cls Time elapsed between flushing recent WAL locally and receiving notification that this standby server has written and flushed it pg_repl_flush_lsn counter pid, usename, address, job, ins, appname, ip, instance, cls Last write-ahead log location flushed to disk by this standby server pg_repl_launch_time counter pid, usename, address, job, ins, appname, ip, instance, cls Time when this process was started, i.e., when the client connected to this WAL sender pg_repl_lsn counter pid, usename, address, job, ins, appname, ip, instance, cls Current log position on this server pg_repl_replay_diff gauge pid, usename, address, job, ins, appname, ip, instance, cls Last log position replayed into the database on this standby server diff with current lsn pg_repl_replay_lag gauge pid, usename, address, job, ins, appname, ip, instance, cls Time elapsed between flushing recent WAL locally and receiving notification that this standby server has written, flushed and applied it pg_repl_replay_lsn counter pid, usename, address, job, ins, appname, ip, instance, cls Last write-ahead log location replayed into the database on this standby server pg_repl_reply_time gauge pid, usename, address, job, ins, appname, ip, instance, cls Send time of last reply message received from standby server pg_repl_sent_diff gauge pid, usename, address, job, ins, appname, ip, instance, cls Last log position sent to this standby server diff with current lsn pg_repl_sent_lsn counter pid, usename, address, job, ins, appname, ip, instance, cls Last write-ahead log location sent on this connection pg_repl_state gauge pid, usename, address, job, ins, appname, ip, instance, cls Current WAL sender encoded state 0-4 for streaming startup catchup backup stopping pg_repl_sync_priority gauge pid, usename, address, job, ins, appname, ip, instance, cls Priority of this standby server for being chosen as the synchronous standby pg_repl_sync_state gauge pid, usename, address, job, ins, appname, ip, instance, cls Encoded synchronous state of this standby server, 0-3 for async potential sync quorum pg_repl_time counter pid, usename, address, job, ins, appname, ip, instance, cls Current timestamp in unix epoch pg_repl_write_diff gauge pid, usename, address, job, ins, appname, ip, instance, cls Last log position written to disk by this standby server diff with current lsn pg_repl_write_lag gauge pid, usename, address, job, ins, appname, ip, instance, cls Time elapsed between flushing recent WAL locally and receiving notification that this standby server has written it pg_repl_write_lsn counter pid, usename, address, job, ins, appname, ip, instance, cls Last write-ahead log location written to disk by this standby server pg_replay_lsn counter job, ins, ip, instance, cls replica only, location of wal applied pg_seq_blks_hit counter datname, job, ins, ip, instance, cls, seqname Number of buffer hits in this sequence pg_seq_blks_read counter datname, job, ins, ip, instance, cls, seqname Number of disk blocks read from this sequence pg_seq_last_value counter datname, job, ins, ip, instance, cls, seqname The last sequence value written to disk pg_setting_block_size gauge job, ins, ip, instance, cls pg page block size, 8192 by default pg_setting_data_checksums gauge job, ins, ip, instance, cls whether data checksum is enabled, 1 enabled 0 disabled pg_setting_max_connections gauge job, ins, ip, instance, cls number of concurrent connections to the database server pg_setting_max_locks_per_transaction gauge job, ins, ip, instance, cls no more than this many distinct objects can be locked at any one time pg_setting_max_prepared_transactions gauge job, ins, ip, instance, cls maximum number of transactions that can be in the prepared state simultaneously pg_setting_max_replication_slots gauge job, ins, ip, instance, cls maximum number of replication slots pg_setting_max_wal_senders gauge job, ins, ip, instance, cls maximum number of concurrent connections from standby servers pg_setting_max_worker_processes gauge job, ins, ip, instance, cls maximum number of background processes that the system can support pg_setting_wal_log_hints gauge job, ins, ip, instance, cls whether wal_log_hints is enabled, 1 enabled 0 disabled pg_size_bytes gauge datname, job, ins, ip, instance, cls File size in bytes pg_slot_active gauge slot_name, job, ins, ip, instance, cls True(1) if this slot is currently actively being used pg_slot_catalog_xmin counter slot_name, job, ins, ip, instance, cls The oldest transaction affecting the system catalogs that this slot needs the database to retain. pg_slot_confirm_lsn counter slot_name, job, ins, ip, instance, cls The address (LSN) up to which the logical slot’s consumer has confirmed receiving data. pg_slot_reset_time counter slot_name, job, ins, ip, instance, cls When statistics were last reset pg_slot_restart_lsn counter slot_name, job, ins, ip, instance, cls The address (LSN) of oldest WAL which still might be required by the consumer of this slot pg_slot_retained_bytes gauge slot_name, job, ins, ip, instance, cls Size of bytes that retained for this slot pg_slot_safe_wal_size gauge slot_name, job, ins, ip, instance, cls bytes that can be written to WAL which will not make slot into lost pg_slot_spill_bytes counter slot_name, job, ins, ip, instance, cls Bytes that spilled to disk due to logical decode mem exceeding pg_slot_spill_count counter slot_name, job, ins, ip, instance, cls Xacts that spilled to disk due to logical decode mem exceeding (a xact can be spilled multiple times) pg_slot_spill_txns counter slot_name, job, ins, ip, instance, cls Xacts that spilled to disk due to logical decode mem exceeding (subtrans included) pg_slot_stream_bytes counter slot_name, job, ins, ip, instance, cls Bytes that streamed to decoding output plugin after mem exceed pg_slot_stream_count counter slot_name, job, ins, ip, instance, cls Xacts that streamed to decoding output plugin after mem exceed (a xact can be streamed multiple times) pg_slot_stream_txns counter slot_name, job, ins, ip, instance, cls Xacts that streamed to decoding output plugin after mem exceed pg_slot_temporary gauge slot_name, job, ins, ip, instance, cls True(1) if this is a temporary replication slot. pg_slot_total_bytes counter slot_name, job, ins, ip, instance, cls Number of decoded bytes sent to the decoding output plugin for this slot pg_slot_total_txns counter slot_name, job, ins, ip, instance, cls Number of decoded xacts sent to the decoding output plugin for this slot pg_slot_wal_status gauge slot_name, job, ins, ip, instance, cls WAL reserve status 0-3 means reserved,extended,unreserved,lost, -1 means other pg_slot_xmin counter slot_name, job, ins, ip, instance, cls The oldest transaction that this slot needs the database to retain. pg_slru_blks_exists counter job, ins, ip, instance, cls Number of blocks checked for existence for this SLRU pg_slru_blks_hit counter job, ins, ip, instance, cls Number of times disk blocks were found already in the SLRU, so that a read was not necessary pg_slru_blks_read counter job, ins, ip, instance, cls Number of disk blocks read for this SLRU pg_slru_blks_written counter job, ins, ip, instance, cls Number of disk blocks written for this SLRU pg_slru_blks_zeroed counter job, ins, ip, instance, cls Number of blocks zeroed during initializations pg_slru_flushes counter job, ins, ip, instance, cls Number of flushes of dirty data for this SLRU pg_slru_reset_time counter job, ins, ip, instance, cls Time at which these statistics were last reset pg_slru_truncates counter job, ins, ip, instance, cls Number of truncates for this SLRU pg_ssl_disabled gauge job, ins, ip, instance, cls Number of client connection that does not use ssl pg_ssl_enabled gauge job, ins, ip, instance, cls Number of client connection that use ssl pg_sync_standby_enabled gauge job, ins, ip, names, instance, cls Synchronous commit enabled, 1 if enabled, 0 if disabled pg_table_age gauge datname, relname, job, ins, ip, instance, cls Age of this table in vacuum cycles pg_table_analyze_count counter datname, relname, job, ins, ip, instance, cls Number of times this table has been manually analyzed pg_table_autoanalyze_count counter datname, relname, job, ins, ip, instance, cls Number of times this table has been analyzed by the autovacuum daemon pg_table_autovacuum_count counter datname, relname, job, ins, ip, instance, cls Number of times this table has been vacuumed by the autovacuum daemon pg_table_frozenxid counter datname, relname, job, ins, ip, instance, cls All txid before this have been frozen on this table pg_table_heap_blks_hit counter datname, relname, job, ins, ip, instance, cls Number of buffer hits in this table pg_table_heap_blks_read counter datname, relname, job, ins, ip, instance, cls Number of disk blocks read from this table pg_table_idx_blks_hit counter datname, relname, job, ins, ip, instance, cls Number of buffer hits in all indexes on this table pg_table_idx_blks_read counter datname, relname, job, ins, ip, instance, cls Number of disk blocks read from all indexes on this table pg_table_idx_scan counter datname, relname, job, ins, ip, instance, cls Number of index scans initiated on this table pg_table_idx_tup_fetch counter datname, relname, job, ins, ip, instance, cls Number of live rows fetched by index scans pg_table_kind gauge datname, relname, job, ins, ip, instance, cls Relation kind r/table/114 pg_table_n_dead_tup gauge datname, relname, job, ins, ip, instance, cls Estimated number of dead rows pg_table_n_ins_since_vacuum gauge datname, relname, job, ins, ip, instance, cls Estimated number of rows inserted since this table was last vacuumed pg_table_n_live_tup gauge datname, relname, job, ins, ip, instance, cls Estimated number of live rows pg_table_n_mod_since_analyze gauge datname, relname, job, ins, ip, instance, cls Estimated number of rows modified since this table was last analyzed pg_table_n_tup_del counter datname, relname, job, ins, ip, instance, cls Number of rows deleted pg_table_n_tup_hot_upd counter datname, relname, job, ins, ip, instance, cls Number of rows HOT updated (i.e with no separate index update required) pg_table_n_tup_ins counter datname, relname, job, ins, ip, instance, cls Number of rows inserted pg_table_n_tup_mod counter datname, relname, job, ins, ip, instance, cls Number of rows modified (insert + update + delete) pg_table_n_tup_newpage_upd counter datname, relname, job, ins, ip, instance, cls Number of rows updated where the successor version goes onto a new heap page pg_table_n_tup_upd counter datname, relname, job, ins, ip, instance, cls Number of rows updated (includes HOT updated rows) pg_table_ncols gauge datname, relname, job, ins, ip, instance, cls Number of columns in the table pg_table_pages gauge datname, relname, job, ins, ip, instance, cls Size of the on-disk representation of this table in pages pg_table_relid gauge datname, relname, job, ins, ip, instance, cls Relation oid of this table pg_table_seq_scan counter datname, relname, job, ins, ip, instance, cls Number of sequential scans initiated on this table pg_table_seq_tup_read counter datname, relname, job, ins, ip, instance, cls Number of live rows fetched by sequential scans pg_table_size_bytes gauge datname, relname, job, ins, ip, instance, cls Total bytes of this table (including toast, index, toast index) pg_table_size_indexsize gauge datname, relname, job, ins, ip, instance, cls Bytes of all related indexes of this table pg_table_size_relsize gauge datname, relname, job, ins, ip, instance, cls Bytes of this table itself (main, vm, fsm) pg_table_size_toastsize gauge datname, relname, job, ins, ip, instance, cls Bytes of toast tables of this table pg_table_tbl_scan counter datname, relname, job, ins, ip, instance, cls Number of scans initiated on this table pg_table_tup_read counter datname, relname, job, ins, ip, instance, cls Number of live rows fetched by scans pg_table_tuples counter datname, relname, job, ins, ip, instance, cls All txid before this have been frozen on this table pg_table_vacuum_count counter datname, relname, job, ins, ip, instance, cls Number of times this table has been manually vacuumed (not counting VACUUM FULL) pg_timestamp gauge job, ins, ip, instance, cls database current timestamp pg_up gauge job, ins, ip, instance, cls last scrape was able to connect to the server: 1 for yes, 0 for no pg_uptime gauge job, ins, ip, instance, cls seconds since postmaster start pg_version gauge job, ins, ip, instance, cls server version number pg_wait_count gauge datname, job, ins, event, ip, instance, cls Count of WaitEvent on target database pg_wal_buffers_full counter job, ins, ip, instance, cls Number of times WAL data was written to disk because WAL buffers became full pg_wal_bytes counter job, ins, ip, instance, cls Total amount of WAL generated in bytes pg_wal_fpi counter job, ins, ip, instance, cls Total number of WAL full page images generated pg_wal_records counter job, ins, ip, instance, cls Total number of WAL records generated pg_wal_reset_time counter job, ins, ip, instance, cls When statistics were last reset pg_wal_sync counter job, ins, ip, instance, cls Number of times WAL files were synced to disk via issue_xlog_fsync request pg_wal_sync_time counter job, ins, ip, instance, cls Total amount of time spent syncing WAL files to disk via issue_xlog_fsync request, in seconds pg_wal_write counter job, ins, ip, instance, cls Number of times WAL buffers were written out to disk via XLogWrite request. pg_wal_write_time counter job, ins, ip, instance, cls Total amount of time spent writing WAL buffers to disk via XLogWrite request in seconds pg_write_lsn counter job, ins, ip, instance, cls primary only, location of current wal writing pg_xact_xmax counter job, ins, ip, instance, cls First as-yet-unassigned txid. txid \u003e= this are invisible. pg_xact_xmin counter job, ins, ip, instance, cls Earliest txid that is still active pg_xact_xnum gauge job, ins, ip, instance, cls Current active transaction count pgbouncer:cls:load1 Unknown job, cls N/A pgbouncer:cls:load15 Unknown job, cls N/A pgbouncer:cls:load5 Unknown job, cls N/A pgbouncer:db:conn_usage Unknown datname, job, ins, ip, instance, host, cls, real_datname, port N/A pgbouncer:db:conn_usage_reserve Unknown datname, job, ins, ip, instance, host, cls, real_datname, port N/A pgbouncer:db:pool_current_conn Unknown datname, job, ins, ip, instance, host, cls, real_datname, port N/A pgbouncer:db:pool_disabled Unknown datname, job, ins, ip, instance, host, cls, real_datname, port N/A pgbouncer:db:pool_max_conn Unknown datname, job, ins, ip, instance, host, cls, real_datname, port N/A pgbouncer:db:pool_paused Unknown datname, job, ins, ip, instance, host, cls, real_datname, port N/A pgbouncer:db:pool_reserve_size Unknown datname, job, ins, ip, instance, host, cls, real_datname, port N/A pgbouncer:db:pool_size Unknown datname, job, ins, ip, instance, host, cls, real_datname, port N/A pgbouncer:ins:free_clients Unknown job, ins, ip, instance, cls N/A pgbouncer:ins:free_servers Unknown job, ins, ip, instance, cls N/A pgbouncer:ins:load1 Unknown job, ins, ip, instance, cls N/A pgbouncer:ins:load15 Unknown job, ins, ip, instance, cls N/A pgbouncer:ins:load5 Unknown job, ins, ip, instance, cls N/A pgbouncer:ins:login_clients Unknown job, ins, ip, instance, cls N/A pgbouncer:ins:pool_databases Unknown job, ins, ip, instance, cls N/A pgbouncer:ins:pool_users Unknown job, ins, ip, instance, cls N/A pgbouncer:ins:pools Unknown job, ins, ip, instance, cls N/A pgbouncer:ins:used_clients Unknown job, ins, ip, instance, cls N/A pgbouncer_database_current_connections gauge datname, job, ins, ip, instance, host, cls, real_datname, port Current number of connections for this database pgbouncer_database_disabled gauge datname, job, ins, ip, instance, host, cls, real_datname, port True(1) if this database is currently disabled, else 0 pgbouncer_database_max_connections gauge datname, job, ins, ip, instance, host, cls, real_datname, port Maximum number of allowed connections for this database pgbouncer_database_min_pool_size gauge datname, job, ins, ip, instance, host, cls, real_datname, port Minimum number of server connections pgbouncer_database_paused gauge datname, job, ins, ip, instance, host, cls, real_datname, port True(1) if this database is currently paused, else 0 pgbouncer_database_pool_size gauge datname, job, ins, ip, instance, host, cls, real_datname, port Maximum number of server connections pgbouncer_database_reserve_pool gauge datname, job, ins, ip, instance, host, cls, real_datname, port Maximum number of additional connections for this database pgbouncer_exporter_agent_up Unknown job, ins, ip, instance, cls N/A pgbouncer_exporter_last_scrape_time gauge job, ins, ip, instance, cls seconds exporter spending on scrapping pgbouncer_exporter_query_cache_ttl gauge datname, query, job, ins, ip, instance, cls times to live of query cache pgbouncer_exporter_query_scrape_duration gauge datname, query, job, ins, ip, instance, cls seconds query spending on scrapping pgbouncer_exporter_query_scrape_error_count gauge datname, query, job, ins, ip, instance, cls times the query failed pgbouncer_exporter_query_scrape_hit_count gauge datname, query, job, ins, ip, instance, cls numbers been scrapped from this query pgbouncer_exporter_query_scrape_metric_count gauge datname, query, job, ins, ip, instance, cls numbers of metrics been scrapped from this query pgbouncer_exporter_query_scrape_total_count gauge datname, query, job, ins, ip, instance, cls times exporter server was scraped for metrics pgbouncer_exporter_scrape_duration gauge job, ins, ip, instance, cls seconds exporter spending on scrapping pgbouncer_exporter_scrape_error_count counter job, ins, ip, instance, cls times exporter was scraped for metrics and failed pgbouncer_exporter_scrape_total_count counter job, ins, ip, instance, cls times exporter was scraped for metrics pgbouncer_exporter_server_scrape_duration gauge datname, job, ins, ip, instance, cls seconds exporter server spending on scrapping pgbouncer_exporter_server_scrape_total_count gauge datname, job, ins, ip, instance, cls times exporter server was scraped for metrics pgbouncer_exporter_server_scrape_total_seconds gauge datname, job, ins, ip, instance, cls seconds exporter server spending on scrapping pgbouncer_exporter_up gauge job, ins, ip, instance, cls always be 1 if your could retrieve metrics pgbouncer_exporter_uptime gauge job, ins, ip, instance, cls seconds since exporter primary server inited pgbouncer_in_recovery gauge job, ins, ip, instance, cls server is in recovery mode? 1 for yes 0 for no pgbouncer_list_items gauge job, ins, ip, instance, list, cls Number of corresponding pgbouncer object pgbouncer_pool_active_cancel_clients gauge datname, job, ins, ip, instance, user, cls, pool_mode Client connections that have forwarded query cancellations to the server and are waiting for the server response. pgbouncer_pool_active_cancel_servers gauge datname, job, ins, ip, instance, user, cls, pool_mode Server connections that are currently forwarding a cancel request pgbouncer_pool_active_clients gauge datname, job, ins, ip, instance, user, cls, pool_mode Client connections that are linked to server connection and can process queries pgbouncer_pool_active_servers gauge datname, job, ins, ip, instance, user, cls, pool_mode Server connections that are linked to a client pgbouncer_pool_cancel_clients gauge datname, job, ins, ip, instance, user, cls, pool_mode Client connections that have not forwarded query cancellations to the server yet. pgbouncer_pool_cancel_servers gauge datname, job, ins, ip, instance, user, cls, pool_mode cancel requests have completed that were sent to cancel a query on this server pgbouncer_pool_idle_servers gauge datname, job, ins, ip, instance, user, cls, pool_mode Server connections that are unused and immediately usable for client queries pgbouncer_pool_login_servers gauge datname, job, ins, ip, instance, user, cls, pool_mode Server connections currently in the process of logging in pgbouncer_pool_maxwait gauge datname, job, ins, ip, instance, user, cls, pool_mode How long the first(oldest) client in the queue has waited, in seconds, key metric pgbouncer_pool_maxwait_us gauge datname, job, ins, ip, instance, user, cls, pool_mode Microsecond part of the maximum waiting time. pgbouncer_pool_tested_servers gauge datname, job, ins, ip, instance, user, cls, pool_mode Server connections that are currently running reset or check query pgbouncer_pool_used_servers gauge datname, job, ins, ip, instance, user, cls, pool_mode Server connections that have been idle for more than server_check_delay (means have to run check query) pgbouncer_pool_waiting_clients gauge datname, job, ins, ip, instance, user, cls, pool_mode Client connections that have sent queries but have not yet got a server connection pgbouncer_stat_avg_query_count gauge datname, job, ins, ip, instance, cls Average queries per second in last stat period pgbouncer_stat_avg_query_time gauge datname, job, ins, ip, instance, cls Average query duration, in seconds pgbouncer_stat_avg_recv gauge datname, job, ins, ip, instance, cls Average received (from clients) bytes per second pgbouncer_stat_avg_sent gauge datname, job, ins, ip, instance, cls Average sent (to clients) bytes per second pgbouncer_stat_avg_wait_time gauge datname, job, ins, ip, instance, cls Time spent by clients waiting for a server, in seconds (average per second). pgbouncer_stat_avg_xact_count gauge datname, job, ins, ip, instance, cls Average transactions per second in last stat period pgbouncer_stat_avg_xact_time gauge datname, job, ins, ip, instance, cls Average transaction duration, in seconds pgbouncer_stat_total_query_count gauge datname, job, ins, ip, instance, cls Total number of SQL queries pooled by pgbouncer pgbouncer_stat_total_query_time counter datname, job, ins, ip, instance, cls Total number of seconds spent when executing queries pgbouncer_stat_total_received counter datname, job, ins, ip, instance, cls Total volume in bytes of network traffic received by pgbouncer pgbouncer_stat_total_sent counter datname, job, ins, ip, instance, cls Total volume in bytes of network traffic sent by pgbouncer pgbouncer_stat_total_wait_time counter datname, job, ins, ip, instance, cls Time spent by clients waiting for a server, in seconds pgbouncer_stat_total_xact_count gauge datname, job, ins, ip, instance, cls Total number of SQL transactions pooled by pgbouncer pgbouncer_stat_total_xact_time counter datname, job, ins, ip, instance, cls Total number of seconds spent when in a transaction pgbouncer_up gauge job, ins, ip, instance, cls last scrape was able to connect to the server: 1 for yes, 0 for no pgbouncer_version gauge job, ins, ip, instance, cls server version number process_cpu_seconds_total counter job, ins, ip, instance, cls Total user and system CPU time spent in seconds. process_max_fds gauge job, ins, ip, instance, cls Maximum number of open file descriptors. process_open_fds gauge job, ins, ip, instance, cls Number of open file descriptors. process_resident_memory_bytes gauge job, ins, ip, instance, cls Resident memory size in bytes. process_start_time_seconds gauge job, ins, ip, instance, cls Start time of the process since unix epoch in seconds. process_virtual_memory_bytes gauge job, ins, ip, instance, cls Virtual memory size in bytes. process_virtual_memory_max_bytes gauge job, ins, ip, instance, cls Maximum amount of virtual memory available in bytes. promhttp_metric_handler_requests_in_flight gauge job, ins, ip, instance, cls Current number of scrapes being served. promhttp_metric_handler_requests_total counter code, job, ins, ip, instance, cls Total number of scrapes by HTTP status code. scrape_duration_seconds Unknown job, ins, ip, instance, cls N/A scrape_samples_post_metric_relabeling Unknown job, ins, ip, instance, cls N/A scrape_samples_scraped Unknown job, ins, ip, instance, cls N/A scrape_series_added Unknown job, ins, ip, instance, cls N/A up Unknown job, ins, ip, instance, cls N/A ","categories":["Reference"],"description":"Complete list and explanation of monitoring metrics provided by the Pigsty PGSQL module","excerpt":"Complete list and explanation of monitoring metrics provided by the …","ref":"/docs/pgsql/monitor/metric/","tags":"","title":"Metrics List"},{"body":"The PGSQL module needs to be installed on nodes managed by Pigsty (i.e., nodes that have the NODE module configured), and also requires an available ETCD cluster in your deployment to store cluster metadata.\nInstalling the PGSQL module on a single node will create a standalone PGSQL server/instance, i.e., a primary instance. Installing on additional nodes will create read replicas, which can serve as standby instances and handle read-only requests. You can also create offline instances for ETL/OLAP/interactive queries, use sync standby and quorum commit to improve data consistency, or even set up standby clusters and delayed clusters to quickly respond to data loss caused by human errors and software defects.\nYou can define multiple PGSQL clusters and further organize them into a horizontal sharding cluster: Pigsty natively supports Citus cluster groups, allowing you to upgrade your standard PGSQL cluster in-place to a distributed database cluster.\nPigsty v4.0 uses PostgreSQL 18 by default and introduces new parameters such as pg_io_method and pgbackrest_exporter.\nSection Description PG_ID PostgreSQL cluster and instance identity parameters PG_BUSINESS Business users, databases, services and access control rule definition PG_INSTALL PostgreSQL installation: version, paths, packages PG_BOOTSTRAP PostgreSQL cluster initialization: Patroni high availability PG_PROVISION PostgreSQL cluster template provisioning: roles, privileges, extensions PG_BACKUP pgBackRest backup and recovery configuration PG_ACCESS Service exposure, connection pooling, VIP, DNS client access config PG_MONITOR PostgreSQL monitoring exporter configuration PG_REMOVE PostgreSQL instance cleanup and uninstall configuration Parameter Overview PG_ID parameters are used to define PostgreSQL cluster and instance identity, including cluster name, instance sequence number, role, shard, and other core identity parameters.\nParameter Type Level Description pg_mode enum C pgsql cluster mode: pgsql,citus,mssql,mysql,polar,ivory,oracle,gpsql pg_cluster string C pgsql cluster name, REQUIRED identity parameter pg_seq int I pgsql instance seq number, REQUIRED identity parameter pg_role enum I pgsql instance role, REQUIRED, could be primary, replica, offline pg_instances dict I define multiple pg instances on node in {port:ins_vars} format pg_upstream ip I repl upstream ip addr for standby cluster or cascade replica pg_shard string C pgsql shard name, REQUIRED identity for sharding clusters like citus pg_group int C pgsql shard index, REQUIRED identity for sharding clusters like citus gp_role enum C greenplum role of this cluster, could be master or segment pg_exporters dict C additional pg_exporters to monitor remote postgres instances pg_offline_query bool I set to true to mark this replica as offline instance for offline queries PG_BUSINESS parameters are used to define business users, databases, services and access control rules, as well as default system user credentials.\nParameter Type Level Description pg_users user[] C postgres business users pg_databases database[] C postgres business databases pg_services service[] C postgres business services pg_hba_rules hba[] C business hba rules for postgres pgb_hba_rules hba[] C business hba rules for pgbouncer pg_crontab string[] C crontab entries for postgres dbsu pg_replication_username username G postgres replication username, replicator by default pg_replication_password password G postgres replication password, DBUser.Replicator by default pg_admin_username username G postgres admin username, dbuser_dba by default pg_admin_password password G postgres admin password in plain text, DBUser.DBA by default pg_monitor_username username G postgres monitor username, dbuser_monitor by default pg_monitor_password password G postgres monitor password, DBUser.Monitor by default pg_dbsu_password password G/C dbsu password, empty string disables it by default, best not set PG_INSTALL parameters are used to configure PostgreSQL installation options, including version, paths, packages, and extensions.\nParameter Type Level Description pg_dbsu username C os dbsu name, postgres by default, better not change it pg_dbsu_uid int C os dbsu uid and gid, 26 for default postgres user and group pg_dbsu_sudo enum C dbsu sudo privilege, none,limit,all,nopass. limit by default pg_dbsu_home path C postgresql home directory, /var/lib/pgsql by default pg_dbsu_ssh_exchange bool C exchange postgres dbsu ssh key among same pgsql cluster pg_version enum C postgres major version to be installed, 18 by default pg_bin_dir path C postgres binary dir, /usr/pgsql/bin by default pg_log_dir path C postgres log dir, /pg/log/postgres by default pg_packages string[] C pg packages to be installed, ${pg_version} will be replaced pg_extensions string[] C pg extensions to be installed, ${pg_version} will be replaced PG_BOOTSTRAP parameters are used to configure PostgreSQL cluster initialization, including Patroni high availability, data directory, storage, networking, encoding, and other core settings.\nParameter Type Level Description pg_data path C postgres data directory, /pg/data by default pg_fs_main path C mountpoint/path for pg main data, /data/postgres by default pg_fs_backup path C mountpoint/path for pg backup data, /data/backups by default pg_storage_type enum C storage type for pg main data, SSD,HDD. SSD by default pg_dummy_filesize size C size of /pg/dummy, hold 64MB disk space for emergency use pg_listen ip(s) C/I postgres/pgbouncer listen addr, comma separated list, 0.0.0.0 pg_port port C postgres listen port, 5432 by default pg_localhost path C postgres unix socket dir for localhost connection pg_namespace path C top level key namespace in etcd, used by patroni \u0026 vip patroni_enabled bool C if disabled, no postgres cluster will be created during init patroni_mode enum C patroni working mode: default,pause,remove patroni_port port C patroni listen port, 8008 by default patroni_log_dir path C patroni log dir, /pg/log/patroni by default patroni_ssl_enabled bool G secure patroni RestAPI communications with SSL? patroni_watchdog_mode enum C patroni watchdog mode: automatic,required,off. off by default patroni_username username C patroni restapi username, postgres by default patroni_password password C patroni restapi password, Patroni.API by default pg_primary_db string C primary database name, used by citus,etc. postgres by default pg_parameters dict C extra parameters in postgresql.auto.conf pg_files path[] C extra files to be copied to PGDATA (e.g. license files) pg_conf enum C config template: oltp,olap,crit,tiny. oltp.yml by default pg_max_conn int C postgres max connections, auto will use recommended value pg_shared_buffer_ratio float C postgres shared buffer memory ratio, 0.25 by default, 0.1~0.4 pg_rto int C recovery time objective in seconds, 30s by default pg_rpo int C recovery point objective in bytes, 1MiB by default pg_libs string C preloaded libraries, pg_stat_statements,auto_explain by default pg_delay interval I WAL replay apply delay for standby cluster, for delayed replica pg_checksum bool C enable data checksum for postgres cluster? pg_pwd_enc enum C password encryption algorithm: fixed to scram-sha-256 pg_encoding enum C database cluster encoding, UTF8 by default pg_locale enum C database cluster locale, C by default pg_lc_collate enum C database cluster collate, C by default pg_lc_ctype enum C database character type, C by default pg_io_method enum C PostgreSQL IO method: auto, sync, worker, io_uring pg_etcd_password password C etcd password for this PostgreSQL cluster, cluster name by default pgsodium_key string C pgsodium encryption master key, 64 hex digits, sha256(pg_cluster) pgsodium_getkey_script path C pgsodium getkey script path, uses template pgsodium_getkey PG_PROVISION parameters are used to configure PostgreSQL cluster template provisioning, including default roles, privileges, schemas, extensions, and HBA rules.\nParameter Type Level Description pg_provision bool C provision postgres cluster content after bootstrap? pg_init string G/C init script for cluster template, pg-init by default pg_default_roles role[] G/C default predefined roles and system users in postgres pg_default_privileges string[] G/C default privileges when created by admin user pg_default_schemas string[] G/C default schemas to be created pg_default_extensions extension[] G/C default extensions to be created pg_reload bool A reload postgres config after hba changes? pg_default_hba_rules hba[] G/C postgres default host-based auth rules, global default HBA pgb_default_hba_rules hba[] G/C pgbouncer default host-based auth rules, global default HBA PG_BACKUP parameters are used to configure pgBackRest backup and recovery, including repository type, paths, and retention policies.\nParameter Type Level Description pgbackrest_enabled bool C enable pgbackrest on pgsql host? pgbackrest_clean bool C remove previous pg backup data during init? pgbackrest_log_dir path C pgbackrest log dir, /pg/log/pgbackrest by default pgbackrest_method enum C pgbackrest repo method: local,minio,etc… pgbackrest_init_backup bool C perform full backup after init? true by default pgbackrest_repo dict G/C pgbackrest repo definition PG_ACCESS parameters are used to configure service exposure, connection pooling, VIP, DNS, and other client access options.\nParameter Type Level Description pgbouncer_enabled bool C if disabled, pgbouncer will not be configured pgbouncer_port port C pgbouncer listen port, 6432 by default pgbouncer_log_dir path C pgbouncer log dir, /pg/log/pgbouncer by default pgbouncer_auth_query bool C use AuthQuery to get unlisted business users from postgres? pgbouncer_poolmode enum C pool mode: transaction,session,statement. transaction by default pgbouncer_sslmode enum C pgbouncer client ssl mode, disabled by default pgbouncer_ignore_param string[] C pgbouncer ignore startup parameters list pg_weight int I relative load balancing weight in service, 0-255, 100 by default pg_service_provider string G/C dedicated haproxy node group name, or use local haproxy pg_default_service_dest enum G/C default service dest if svc.dest=‘default’: postgres or pgbouncer pg_default_services service[] G/C postgres default service definition list, shared globally pg_vip_enabled bool C enable L2 VIP for pgsql primary? disabled by default pg_vip_address cidr4 C vip address in \u003cipv4\u003e/\u003cmask\u003e format, required if vip enabled pg_vip_interface string C/I vip network interface to bindg, eth0 by default pg_dns_suffix string C pgsql dns suffix, empty by default pg_dns_target enum C PG DNS resolves to: auto, primary, vip, none, or specific IP PG_MONITOR parameters are used to configure PostgreSQL monitoring exporters, including pg_exporter, pgbouncer_exporter, and pgbackrest_exporter.\nParameter Type Level Description pg_exporter_enabled bool C enable pg_exporter on pgsql host? pg_exporter_config string C pg_exporter config file/template name pg_exporter_cache_ttls string C pg_exporter collector ttl stages, ‘1,10,60,300’ by default pg_exporter_port port C pg_exporter listen port, 9630 by default pg_exporter_params string C extra URL parameters for pg_exporter dsn pg_exporter_url pgurl C overwrite auto-generated postgres DSN connection string pg_exporter_auto_discovery bool C enable auto database discovery for monitoring? enabled pg_exporter_exclude_database string C excluded database list when auto-discovery, comma separated pg_exporter_include_database string C only monitor these databases when auto-discovery enabled pg_exporter_connect_timeout int C pg_exporter connect timeout in ms, 200 by default pg_exporter_options arg C extra command line options for pg_exporter pgbouncer_exporter_enabled bool C enable pgbouncer_exporter on pgsql host? pgbouncer_exporter_port port C pgbouncer_exporter listen port, 9631 by default pgbouncer_exporter_url pgurl C overwrite auto-generated pgbouncer dsn connection string pgbouncer_exporter_options arg C extra command line options for pgbouncer_exporter pgbackrest_exporter_enabled bool C enable pgbackrest_exporter on pgsql host? pgbackrest_exporter_port port C pgbackrest_exporter listen port, 9854 by default pgbackrest_exporter_options arg C extra command line options for pgbackrest_exporter PG_REMOVE parameters are used to configure PostgreSQL instance cleanup and uninstall behavior, including data directory, backup, and package removal control.\nParameter Type Level Description pg_rm_data bool G/C/A remove postgres data directory when removing instance? pg_rm_backup bool G/C/A remove pgbackrest backup when removing primary? pg_rm_pkg bool G/C/A uninstall related packages when removing pgsql instance? pg_safeguard bool G/C/A prevent accidental pgsql cleanup operations? false PG_ID Here are commonly used parameters for identifying entities in the PGSQL module: clusters, instances, services, etc…\n# pg_cluster: #CLUSTER # pgsql cluster name, required identity parameter # pg_seq: 0 #INSTANCE # pgsql instance seq number, required identity parameter # pg_role: replica #INSTANCE # pgsql role, required, could be primary,replica,offline # pg_instances: {} #INSTANCE # define multiple pg instances on node in `{port:ins_vars}` format # pg_upstream: #INSTANCE # repl upstream ip addr for standby cluster or cascade replica # pg_shard: #CLUSTER # pgsql shard name, optional identity for sharding clusters # pg_group: 0 #CLUSTER # pgsql shard index number, optional identity for sharding clusters # gp_role: master #CLUSTER # greenplum role of this cluster, could be master or segment pg_offline_query: false #INSTANCE # set to true to enable offline query on this instance You must explicitly specify these identity parameters, they have no default values:\nName Type Level Description pg_cluster string C PG cluster name pg_seq number I PG instance ID pg_role enum I PG instance role pg_shard string C Shard name pg_group number C Shard index pg_cluster: Identifies the cluster name, configured at cluster level. pg_role: Configured at instance level, identifies the role of the instance. Only primary role is treated specially. If not specified, defaults to replica role, with special delayed and offline roles. pg_seq: Used to identify instances within a cluster, typically an integer starting from 0 or 1, once assigned it doesn’t change. {{ pg_cluster }}-{{ pg_seq }} uniquely identifies an instance, i.e., pg_instance. {{ pg_cluster }}-{{ pg_role }} identifies services within the cluster, i.e., pg_service. pg_shard and pg_group are used for horizontal sharding clusters, only for citus, greenplum, and matrixdb. pg_cluster, pg_role, pg_seq are core identity parameters, required for any Postgres cluster and must be explicitly specified. Here is an example:\npg-test: hosts: 10.10.10.11: {pg_seq: 1, pg_role: replica} 10.10.10.12: {pg_seq: 2, pg_role: primary} 10.10.10.13: {pg_seq: 3, pg_role: replica} vars: pg_cluster: pg-test All other parameters can be inherited from global or default configuration, but identity parameters must be explicitly specified and manually assigned.\npg_mode Parameter Name: pg_mode, Type: enum, Level: C\nPostgreSQL cluster mode, default value is pgsql, i.e., standard PostgreSQL cluster.\nAvailable mode options include:\npgsql: Standard PostgreSQL cluster citus: Citus distributed database cluster mssql: Babelfish MSSQL wire protocol compatible kernel mysql: OpenHalo/HaloDB MySQL wire protocol compatible kernel ivory: IvorySQL Oracle compatible kernel polar: PolarDB for PostgreSQL kernel oracle: PolarDB for Oracle kernel gpsql: Greenplum parallel database cluster (monitoring) If pg_mode is set to citus or gpsql, two additional required identity parameters pg_shard and pg_group are needed to define the horizontal sharding cluster identity.\nIn both cases, each PostgreSQL cluster is part of a larger business unit.\npg_cluster Parameter Name: pg_cluster, Type: string, Level: C\nPostgreSQL cluster name, required identity parameter, no default value.\nThe cluster name is used as the namespace for resources.\nCluster naming must follow a specific pattern: [a-z][a-z0-9-]*, i.e., only numbers and lowercase letters, not starting with a number, to meet different identifier constraints.\npg_seq Parameter Name: pg_seq, Type: int, Level: I\nPostgreSQL instance sequence number, required identity parameter, no default value.\nThe sequence number of this instance, uniquely assigned within its cluster, typically using natural numbers starting from 0 or 1, usually not recycled or reused.\npg_role Parameter Name: pg_role, Type: enum, Level: I\nPostgreSQL instance role, required identity parameter, no default value. Values can be: primary, replica, offline\nThe role of a PGSQL instance can be: primary, replica, standby, or offline.\nprimary: Primary instance, there is one and only one in a cluster. replica: Replica for serving online read-only traffic, may have slight replication delay under high load (10ms~100ms, 100KB). offline: Offline replica for handling offline read-only traffic, such as analytics/ETL/personal queries. pg_instances Parameter Name: pg_instances, Type: dict, Level: I\nDefine multiple PostgreSQL instances on a single host using {port:ins_vars} format.\nThis parameter is reserved for multi-instance deployment on a single node. Pigsty has not yet implemented this feature and strongly recommends dedicated node deployment.\npg_upstream Parameter Name: pg_upstream, Type: ip, Level: I\nUpstream instance IP address for standby cluster or cascade replica.\nSetting pg_upstream on the primary instance of a cluster indicates this cluster is a standby cluster, and this instance will act as a standby leader, receiving and applying changes from the upstream cluster.\nSetting pg_upstream on a non-primary instance specifies a specific instance as the upstream for physical replication. If different from the primary instance IP address, this instance becomes a cascade replica. It is the user’s responsibility to ensure the upstream IP address is another instance in the same cluster.\npg_shard Parameter Name: pg_shard, Type: string, Level: C\nPostgreSQL horizontal shard name, required identity parameter for sharding clusters (e.g., citus clusters).\nWhen multiple standard PostgreSQL clusters serve the same business together in a horizontal sharding manner, Pigsty marks this group of clusters as a horizontal sharding cluster.\npg_shard is the shard group name. It is typically a prefix of pg_cluster.\nFor example, if we have a shard group pg-citus with 4 clusters, their identity parameters would be:\ncls pg_shard: pg-citus cls pg_group = 0: pg-citus0 cls pg_group = 1: pg-citus1 cls pg_group = 2: pg-citus2 cls pg_group = 3: pg-citus3 pg_group Parameter Name: pg_group, Type: int, Level: C\nPostgreSQL horizontal sharding cluster shard index number, required identity parameter for sharding clusters (e.g., citus clusters).\nThis parameter is used in conjunction with pg_shard, typically using non-negative integers as index numbers.\ngp_role Parameter Name: gp_role, Type: enum, Level: C\nGreenplum/Matrixdb role of the PostgreSQL cluster, can be master or segment.\nmaster: Marks the postgres cluster as a greenplum master instance (coordinator node), this is the default value. segment: Marks the postgres cluster as a greenplum segment cluster (data node). This parameter is only used for Greenplum/MatrixDB databases (pg_mode is gpsql) and has no meaning for regular PostgreSQL clusters.\npg_exporters Parameter Name: pg_exporters, Type: dict, Level: C\nAdditional exporter definitions for monitoring remote PostgreSQL instances, default value: {}\nIf you want to monitor remote PostgreSQL instances, define them in the pg_exporters parameter on the cluster where the monitoring system resides (Infra node), and use the pgsql-monitor.yml playbook to complete the deployment.\npg_exporters: # list all remote instances here, alloc a unique unused local port as k 20001: { pg_cluster: pg-foo, pg_seq: 1, pg_host: 10.10.10.10 } 20004: { pg_cluster: pg-foo, pg_seq: 2, pg_host: 10.10.10.11 } 20002: { pg_cluster: pg-bar, pg_seq: 1, pg_host: 10.10.10.12 } 20003: { pg_cluster: pg-bar, pg_seq: 1, pg_host: 10.10.10.13 } pg_offline_query Parameter Name: pg_offline_query, Type: bool, Level: I\nSet to true to enable offline queries on this instance, default is false.\nWhen this parameter is enabled on a PostgreSQL instance, users belonging to the dbrole_offline group can directly connect to this PostgreSQL instance to execute offline queries (slow queries, interactive queries, ETL/analytics queries).\nInstances with this flag have an effect similar to setting pg_role = offline for the instance, with the only difference being that offline instances by default do not serve replica service requests and exist as dedicated offline/analytics replica instances.\nIf you don’t have spare instances available for this purpose, you can select a regular replica and enable this parameter at the instance level to handle offline queries when needed.\nPG_BUSINESS Customize cluster templates: users, databases, services, and permission rules.\nUsers should pay close attention to this section of parameters, as this is where business declares its required database objects.\nBusiness user definition: pg_users Business database definition: pg_databases Cluster-specific service definition: pg_services (global definition: pg_default_services) PostgreSQL cluster/instance-specific HBA rules: pg_hba_rules Pgbouncer connection pool-specific HBA rules: pgb_hba_rules Cron job (crontab) definition: pg_crontab Default database users and their credentials. It is strongly recommended to change these user passwords in production environments.\nPG admin user: pg_admin_username / pg_admin_password PG replication user: pg_replication_username / pg_replication_password PG monitor user: pg_monitor_username / pg_monitor_password # postgres business object definition, overwrite in group vars pg_users: [] # postgres business users pg_databases: [] # postgres business databases pg_services: [] # postgres business services pg_hba_rules: [] # business hba rules for postgres pgb_hba_rules: [] # business hba rules for pgbouncer pg_crontab: [] # crontab entries for postgres dbsu # global credentials, overwrite in global vars pg_dbsu_password: '' # dbsu password, empty string means no dbsu password by default pg_replication_username: replicator pg_replication_password: DBUser.Replicator pg_admin_username: dbuser_dba pg_admin_password: DBUser.DBA pg_monitor_username: dbuser_monitor pg_monitor_password: DBUser.Monitor pg_users Parameter Name: pg_users, Type: user[], Level: C\nPostgreSQL business user list, needs to be defined at the PG cluster level. Default value: [] empty list.\nEach array element is a user/role definition, for example:\n- name: dbuser_meta # required, `name` is the only required field for user definition password: DBUser.Meta # optional, password, can be scram-sha-256 hash string or plaintext login: true # optional, can login by default superuser: false # optional, default false, is superuser? createdb: false # optional, default false, can create database? createrole: false # optional, default false, can create role? inherit: true # optional, by default, can this role use inherited privileges? replication: false # optional, default false, can this role do replication? bypassrls: false # optional, default false, can this role bypass row-level security? pgbouncer: true # optional, default false, add this user to pgbouncer user list? (production users using connection pool should explicitly set to true) connlimit: -1 # optional, user connection limit, default -1 disables limit expire_in: 3650 # optional, this role expires: calculated from creation + n days (higher priority than expire_at) expire_at: '2030-12-31' # optional, when this role expires, use YYYY-MM-DD format string to specify a specific date (lower priority than expire_in) comment: pigsty admin user # optional, description and comment string for this user/role roles: [dbrole_admin] # optional, default roles are: dbrole_{admin,readonly,readwrite,offline} parameters: {} # optional, use `ALTER ROLE SET` for this role, configure role-level database parameters pool_mode: transaction # optional, pgbouncer pool mode at user level, default transaction pool_connlimit: -1 # optional, user-level max database connections, default -1 disables limit search_path: public # optional, key-value config parameter per postgresql docs (e.g., use pigsty as default search_path) pg_databases Parameter Name: pg_databases, Type: database[], Level: C\nPostgreSQL business database list, needs to be defined at the PG cluster level. Default value: [] empty list.\nEach array element is a business database definition, for example:\n- name: meta # required, `name` is the only required field for database definition baseline: cmdb.sql # optional, database sql baseline file path (relative path in ansible search path, e.g., files/) pgbouncer: true # optional, add this database to pgbouncer database list? default true schemas: [pigsty] # optional, additional schemas to create, array of schema name strings extensions: # optional, additional extensions to install: array of extension objects - { name: postgis , schema: public } # can specify which schema to install extension into, or not (if not specified, installs to first schema in search_path) - { name: timescaledb } # some extensions create and use fixed schemas, so no need to specify schema comment: pigsty meta database # optional, description and comment for the database owner: postgres # optional, database owner, default is postgres template: template1 # optional, template to use, default is template1, target must be a template database encoding: UTF8 # optional, database encoding, default UTF8 (must match template database) locale: C # optional, database locale setting, default C (must match template database) lc_collate: C # optional, database collate rule, default C (must match template database), no reason to change lc_ctype: C # optional, database ctype character set, default C (must match template database) tablespace: pg_default # optional, default tablespace, default is 'pg_default' allowconn: true # optional, allow connections, default true. Explicitly set false to completely forbid connections revokeconn: false # optional, revoke public connect privileges. default false, when true, CONNECT privilege revoked from users other than owner and admin register_datasource: true # optional, register this database to grafana datasource? default true, explicitly false skips registration connlimit: -1 # optional, database connection limit, default -1 means no limit, positive integer limits connections pool_auth_user: dbuser_meta # optional, all connections to this pgbouncer database will authenticate using this user (useful when pgbouncer_auth_query enabled) pool_mode: transaction # optional, database-level pgbouncer pooling mode, default transaction pool_size: 64 # optional, database-level pgbouncer default pool size, default 64 pool_size_reserve: 32 # optional, database-level pgbouncer pool reserve, default 32, max additional burst connections when default pool insufficient pool_size_min: 0 # optional, database-level pgbouncer pool minimum size, default 0 pool_max_db_conn: 100 # optional, database-level max database connections, default 100 In each database definition object, only name is a required field, all other fields are optional.\npg_services Parameter Name: pg_services, Type: service[], Level: C\nPostgreSQL service list, needs to be defined at the PG cluster level. Default value: [], empty list.\nUsed to define additional services at the database cluster level. Each object in the array defines a service. A complete service definition example:\n- name: standby # required, service name, final svc name will use `pg_cluster` as prefix, e.g., pg-meta-standby port: 5435 # required, exposed service port (as kubernetes service node port mode) ip: \"*\" # optional, IP address to bind service, default is all IP addresses selector: \"[]\" # required, service member selector, use JMESPath to filter inventory backup: \"[? pg_role == `primary`]\" # optional, service member selector (backup), service is handled by these instances when default selector instances are all down dest: default # optional, target port, default|postgres|pgbouncer|\u003cport_number\u003e, default is 'default', Default means use pg_default_service_dest value to decide check: /sync # optional, health check URL path, default is /, here uses Patroni API: /sync, only sync standby and primary return 200 health status maxconn: 5000 # optional, max frontend connections allowed, default 5000 balance: roundrobin # optional, haproxy load balancing algorithm (default roundrobin, other option: leastconn) options: 'inter 3s fastinter 1s downinter 5s rise 3 fall 3 on-marked-down shutdown-sessions slowstart 30s maxconn 3000 maxqueue 128 weight 100' Note that this parameter is used to add additional services at the cluster level. If you want to globally define services that all PostgreSQL databases should provide, use the pg_default_services parameter.\npg_hba_rules Parameter Name: pg_hba_rules, Type: hba[], Level: C\nClient IP whitelist/blacklist rules for database cluster/instance. Default: [] empty list.\nArray of objects, each object represents a rule. HBA rule object definition:\n- title: allow intranet password access role: common rules: - host all all 10.0.0.0/8 md5 - host all all 172.16.0.0/12 md5 - host all all 192.168.0.0/16 md5 title: Rule title name, rendered as comment in HBA file. rules: Rule array, each element is a standard HBA rule string. role: Rule application scope, which instance roles will enable this rule? common: Applies to all instances primary, replica, offline: Only applies to instances with specific pg_role. Special case: role: 'offline' rules apply to instances with pg_role : offline, and also to instances with pg_offline_query flag. In addition to the native HBA rule definition above, Pigsty also provides a more convenient alias form:\n- addr: 'intra' # world|intra|infra|admin|local|localhost|cluster|\u003ccidr\u003e auth: 'pwd' # trust|pwd|ssl|cert|deny|\u003cofficial auth method\u003e user: 'all' # all|${dbsu}|${repl}|${admin}|${monitor}|\u003cuser\u003e|\u003cgroup\u003e db: 'all' # all|replication|.... rules: [] # raw hba string precedence over above all title: allow intranet password access pg_default_hba_rules is similar to this parameter, but it’s used to define global HBA rules, while this parameter is typically used to customize HBA rules for specific clusters/instances.\npgb_hba_rules Parameter Name: pgb_hba_rules, Type: hba[], Level: C\nPgbouncer business HBA rules, default value: [], empty array.\nThis parameter is similar to pg_hba_rules, both are arrays of hba rule objects, the difference is that this parameter is for Pgbouncer.\npgb_default_hba_rules is similar to this parameter, but it’s used to define global connection pool HBA rules, while this parameter is typically used to customize HBA rules for specific connection pool clusters/instances.\npg_crontab Parameter Name: pg_crontab, Type: string[], Level: C\nCron job list for the PostgreSQL database superuser (dbsu, default postgres), default value: [] empty array.\nEach array element is a crontab entry line, using standard user crontab format: minute hour day month weekday command (no need to specify username).\npg_crontab: - '00 01 * * * /pg/bin/pg-backup full' # Full backup at 1 AM daily - '00 13 * * * /pg/bin/pg-backup' # Incremental backup at 1 PM daily This parameter writes cron jobs to the postgres user’s personal crontab file:\nEL systems: /var/spool/cron/postgres Debian systems: /var/spool/cron/crontabs/postgres Note: This parameter replaces the old practice of configuring postgres user tasks in node_crontab. Because node_crontab is written to /etc/crontab during NODE initialization, the postgres user may not exist yet, causing cron errors.\npg_replication_username Parameter Name: pg_replication_username, Type: username, Level: G\nPostgreSQL physical replication username, default is replicator, not recommended to change this parameter.\npg_replication_password Parameter Name: pg_replication_password, Type: password, Level: G\nPostgreSQL physical replication user password, default value: DBUser.Replicator.\nWarning: Please change this password in production environments!\npg_admin_username Parameter Name: pg_admin_username, Type: username, Level: G\nPostgreSQL / Pgbouncer admin name, default: dbuser_dba.\nThis is the globally used database administrator with database Superuser privileges and connection pool traffic management permissions. Please control its usage scope.\npg_admin_password Parameter Name: pg_admin_password, Type: password, Level: G\nPostgreSQL / Pgbouncer admin password, default: DBUser.DBA.\nWarning: Please change this password in production environments!\npg_monitor_username Parameter Name: pg_monitor_username, Type: username, Level: G\nPostgreSQL/Pgbouncer monitor username, default: dbuser_monitor.\nThis is a database/connection pool user for monitoring, not recommended to change this username.\nHowever, if your existing database uses a different monitor user, you can use this parameter to specify the monitor username when defining monitoring targets.\npg_monitor_password Parameter Name: pg_monitor_password, Type: password, Level: G\nPassword used by PostgreSQL/Pgbouncer monitor user, default: DBUser.Monitor.\nTry to avoid using characters like @:/ that can be confused with URL delimiters in passwords to reduce unnecessary trouble.\nWarning: Please change this password in production environments!\npg_dbsu_password Parameter Name: pg_dbsu_password, Type: password, Level: G/C\nPostgreSQL pg_dbsu superuser password, default is empty string, meaning no password is set.\nWe don’t recommend configuring password login for dbsu as it increases the attack surface. The exception is: pg_mode = citus, in which case you need to configure a password for each shard cluster’s dbsu to allow connections within the shard cluster.\nPG_INSTALL This section is responsible for installing PostgreSQL and its extensions. If you want to install different major versions and extension plugins, just modify pg_version and pg_extensions. Note that not all extensions are available for all major versions.\npg_dbsu: postgres # os dbsu name, default is postgres, better not change it pg_dbsu_uid: 26 # os dbsu uid and gid, default is 26, for default postgres user and group pg_dbsu_sudo: limit # dbsu sudo privilege, none,limit,all,nopass. default is limit pg_dbsu_home: /var/lib/pgsql # postgresql home directory, default is `/var/lib/pgsql` pg_dbsu_ssh_exchange: true # exchange postgres dbsu ssh key among same pgsql cluster pg_version: 18 # postgres major version to be installed, default is 18 pg_bin_dir: /usr/pgsql/bin # postgres binary dir, default is `/usr/pgsql/bin` pg_log_dir: /pg/log/postgres # postgres log dir, default is `/pg/log/postgres` pg_packages: # pg packages to be installed, alias can be used - pgsql-main pgsql-common pg_extensions: [] # pg extensions to be installed, alias can be used pg_dbsu Parameter Name: pg_dbsu, Type: username, Level: C\nOS dbsu username used by PostgreSQL, default is postgres, changing this username is not recommended.\nHowever, in certain situations, you may need a username different from postgres, for example, when installing and configuring Greenplum / MatrixDB, you need to use gpadmin / mxadmin as the corresponding OS superuser.\npg_dbsu_uid Parameter Name: pg_dbsu_uid, Type: int, Level: C\nOS database superuser uid and gid, 26 is the default postgres user UID/GID from PGDG RPM.\nFor Debian/Ubuntu systems, there is no default value, and user 26 is often taken. Therefore, when Pigsty detects the installation environment is Debian-based and uid is 26, it will automatically use the replacement pg_dbsu_uid = 543.\npg_dbsu_sudo Parameter Name: pg_dbsu_sudo, Type: enum, Level: C\nDatabase superuser sudo privilege, can be none, limit, all, or nopass. Default is limit\nnone: No sudo privilege\nlimit: Limited sudo privilege for executing systemctl commands for database-related components (default option).\nall: Full sudo privilege, requires password.\nnopass: Full sudo privilege without password (not recommended).\nDefault value is limit, only allows executing sudo systemctl \u003cstart|stop|reload\u003e \u003cpostgres|patroni|pgbouncer|...\u003e.\npg_dbsu_home Parameter Name: pg_dbsu_home, Type: path, Level: C\nPostgreSQL home directory, default is /var/lib/pgsql, consistent with official pgdg RPM.\npg_dbsu_ssh_exchange Parameter Name: pg_dbsu_ssh_exchange, Type: bool, Level: C\nWhether to exchange OS dbsu ssh keys within the same PostgreSQL cluster?\nDefault is true, meaning database superusers in the same cluster can ssh to each other.\npg_version Parameter Name: pg_version, Type: enum, Level: C\nPostgreSQL major version to install, default is 18.\nNote that PostgreSQL physical streaming replication cannot cross major versions, so it’s best not to configure this at the instance level.\nYou can use parameters in pg_packages and pg_extensions to install different packages and extensions for specific PG major versions.\npg_bin_dir Parameter Name: pg_bin_dir, Type: path, Level: C\nPostgreSQL binary directory, default is /usr/pgsql/bin.\nThe default value is a symlink manually created during installation, pointing to the specific installed Postgres version directory.\nFor example /usr/pgsql -\u003e /usr/pgsql-15. On Ubuntu/Debian it points to /usr/lib/postgresql/15/bin.\nFor more details, see PGSQL File Structure.\npg_log_dir Parameter Name: pg_log_dir, Type: path, Level: C\nPostgreSQL log directory, default: /pg/log/postgres. The Vector log agent uses this variable to collect PostgreSQL logs.\nNote that if the log directory pg_log_dir is prefixed with the data directory pg_data, it won’t be explicitly created (created automatically during data directory initialization).\npg_packages Parameter Name: pg_packages, Type: string[], Level: C\nPostgreSQL packages to install (RPM/DEB), this is an array of package names where elements can be space or comma-separated package aliases.\nPigsty v4 converges the default value to two aliases:\npg_packages: - pgsql-main pgsql-common pgsql-main: Maps to PostgreSQL kernel, client, PL languages, and core extensions like pg_repack, wal2json, pgvector on the current platform. pgsql-common: Maps to companion components required for running the database, such as Patroni, Pgbouncer, pgBackRest, pg_exporter, vip-manager, and other daemons. Alias definitions can be found in pg_package_map under roles/node_id/vars/. Pigsty first resolves aliases based on OS and architecture, then replaces $v/${pg_version} with the actual major version pg_version, and finally installs the real packages. This shields package name differences between distributions.\nIf additional packages are needed (e.g., specific FDW or extensions), you can append aliases or real package names directly to pg_packages. But remember to keep pgsql-main pgsql-common, otherwise core components will be missing.\npg_extensions Parameter Name: pg_extensions, Type: string[], Level: G/C\nPostgreSQL extension packages to install (RPM/DEB), this is an array of extension package names or aliases.\nStarting from v4, the default value is an empty list []. Pigsty no longer forces installation of large extensions, users can choose as needed to avoid extra disk and dependency usage.\nTo install extensions, fill in like this:\npg_extensions: - postgis timescaledb pgvector - pgsql-fdw # use alias to install common FDWs at once pg_package_map provides many aliases to shield package name differences between distributions. Here are available extension combinations for EL9 platform for reference (pick as needed):\npg_extensions: # extensions to be installed on this cluster - timescaledb periods temporal_tables emaj table_version pg_cron pg_later pg_background pg_timetable - postgis pgrouting pointcloud pg_h3 q3c ogr_fdw geoip #pg_geohash #mobilitydb - pgvector pgvectorscale pg_vectorize pg_similarity pg_tiktoken pgml #smlar - pg_search pg_bigm zhparser hunspell - hydra pg_analytics pg_lakehouse pg_duckdb duckdb_fdw pg_fkpart pg_partman plproxy #pg_strom citus - pg_hint_plan age hll rum pg_graphql pg_jsonschema jsquery index_advisor hypopg imgsmlr pg_ivm pgmq pgq #rdkit - pg_tle plv8 pllua plprql pldebugger plpgsql_check plprofiler plsh #pljava plr pgtap faker dbt2 - prefix semver pgunit md5hash asn1oid roaringbitmap pgfaceting pgsphere pg_country pg_currency pgmp numeral pg_rational pguint ip4r timestamp9 chkpass #pg_uri #pgemailaddr #acl #debversion #pg_rrule - topn pg_gzip pg_http pg_net pg_html5_email_address pgsql_tweaks pg_extra_time pg_timeit count_distinct extra_window_functions first_last_agg tdigest aggs_for_arrays pg_arraymath pg_idkit pg_uuidv7 permuteseq pg_hashids - sequential_uuids pg_math pg_random pg_base36 pg_base62 floatvec pg_financial pgjwt pg_hashlib shacrypt cryptint pg_ecdsa pgpcre icu_ext envvar url_encode #pg_zstd #aggs_for_vecs #quantile #lower_quantile #pgqr #pg_protobuf - pg_repack pg_squeeze pg_dirtyread pgfincore pgdd ddlx pg_prioritize pg_checksums pg_readonly safeupdate pg_permissions pgautofailover pg_catcheck preprepare pgcozy pg_orphaned pg_crash pg_cheat_funcs pg_savior table_log pg_fio #pgpool pgagent - pg_profile pg_show_plans pg_stat_kcache pg_stat_monitor pg_qualstats pg_store_plans pg_track_settings pg_wait_sampling system_stats pg_meta pgnodemx pg_sqlog bgw_replstatus pgmeminfo toastinfo pagevis powa pg_top #pg_statviz #pgexporter_ext #pg_mon - passwordcheck supautils pgsodium pg_vault anonymizer pg_tde pgsmcrypto pgaudit pgauditlogtofile pg_auth_mon credcheck pgcryptokey pg_jobmon logerrors login_hook set_user pg_snakeoil pgextwlist pg_auditor noset #sslutils - wrappers multicorn odbc_fdw mysql_fdw tds_fdw sqlite_fdw pgbouncer_fdw mongo_fdw redis_fdw pg_redis_pubsub kafka_fdw hdfs_fdw firebird_fdw aws_s3 log_fdw #oracle_fdw #db2_fdw #jdbc_fdw - orafce pgtt session_variable pg_statement_rollback pg_dbms_metadata pg_dbms_lock pgmemcache #pg_dbms_job #wiltondb - pglogical pgl_ddl_deploy pg_failover_slots wal2json wal2mongo decoderbufs decoder_raw mimeo pgcopydb pgloader pg_fact_loader pg_bulkload pg_comparator pgimportdoc pgexportdoc #repmgr #slony - gis-stack rag-stack fdw-stack fts-stack etl-stack feat-stack olap-stack supa-stack stat-stack json-stack For complete list, see: roles/node_id/vars\nPG_BOOTSTRAP Bootstrap PostgreSQL cluster with Patroni and set up 1:1 corresponding Pgbouncer connection pool.\nIt also initializes the database cluster with default roles, users, privileges, schemas, and extensions defined in PG_PROVISION.\npg_data: /pg/data # postgres data directory, `/pg/data` by default pg_fs_main: /data/postgres # postgres main data directory, `/data/postgres` by default pg_fs_backup: /data/backups # postgres backup data directory, `/data/backups` by default pg_storage_type: SSD # storage type for pg main data, SSD,HDD, SSD by default pg_dummy_filesize: 64MiB # size of `/pg/dummy`, hold 64MB disk space for emergency use pg_listen: '0.0.0.0' # postgres/pgbouncer listen addresses, comma separated list pg_port: 5432 # postgres listen port, 5432 by default pg_localhost: /var/run/postgresql # postgres unix socket dir for localhost connection patroni_enabled: true # if disabled, no postgres cluster will be created during init patroni_mode: default # patroni working mode: default,pause,remove pg_namespace: /pg # top level key namespace in etcd, used by patroni \u0026 vip patroni_port: 8008 # patroni listen port, 8008 by default patroni_log_dir: /pg/log/patroni # patroni log dir, `/pg/log/patroni` by default patroni_ssl_enabled: false # secure patroni RestAPI communications with SSL? patroni_watchdog_mode: off # patroni watchdog mode: automatic,required,off. off by default patroni_username: postgres # patroni restapi username, `postgres` by default patroni_password: Patroni.API # patroni restapi password, `Patroni.API` by default pg_etcd_password: '' # etcd password for this pg cluster, '' to use pg_cluster pg_primary_db: postgres # primary database name, used by citus,etc... ,postgres by default pg_parameters: {} # extra parameters in postgresql.auto.conf pg_files: [] # extra files to be copied to postgres data directory (e.g. license) pg_conf: oltp.yml # config template: oltp,olap,crit,tiny. `oltp.yml` by default pg_max_conn: auto # postgres max connections, `auto` will use recommended value pg_shared_buffer_ratio: 0.25 # postgres shared buffers ratio, 0.25 by default, 0.1~0.4 pg_io_method: worker # io method for postgres, auto,fsync,worker,io_uring, worker by default pg_rto: 30 # recovery time objective in seconds, `30s` by default pg_rpo: 1048576 # recovery point objective in bytes, `1MiB` at most by default pg_libs: 'pg_stat_statements, auto_explain' # preloaded libraries, `pg_stat_statements,auto_explain` by default pg_delay: 0 # replication apply delay for standby cluster leader pg_checksum: true # enable data checksum for postgres cluster? pg_pwd_enc: scram-sha-256 # passwords encryption algorithm: fixed to scram-sha-256 pg_encoding: UTF8 # database cluster encoding, `UTF8` by default pg_locale: C # database cluster local, `C` by default pg_lc_collate: C # database cluster collate, `C` by default pg_lc_ctype: C # database character type, `C` by default #pgsodium_key: \"\" # pgsodium key, 64 hex digit, default to sha256(pg_cluster) #pgsodium_getkey_script: \"\" # pgsodium getkey script path, pgsodium_getkey by default pg_data Parameter Name: pg_data, Type: path, Level: C\nPostgres data directory, default is /pg/data.\nThis is a symlink to the underlying actual data directory, used in multiple places, please don’t modify it. See PGSQL File Structure for details.\npg_fs_main Parameter Name: pg_fs_main, Type: path, Level: C\nMount point/file system path for PostgreSQL main data disk, default is /data/postgres.\nDefault value: /data/postgres, which will be used directly as the parent directory of PostgreSQL main data directory.\nNVME SSD is recommended for PostgreSQL main data storage. Pigsty is optimized for SSD storage by default, but also supports HDD.\nYou can change pg_storage_type to HDD for HDD storage optimization.\npg_fs_backup Parameter Name: pg_fs_backup, Type: path, Level: C\nMount point/file system path for PostgreSQL backup data disk, default is /data/backups.\nIf you’re using the default pgbackrest_method = local, it’s recommended to use a separate disk for backup storage.\nThe backup disk should be large enough to hold all backups, at least sufficient for 3 base backups + 2 days of WAL archives. Usually capacity isn’t a big issue since you can use cheap large HDDs as backup disks.\nIt’s recommended to use a separate disk for backup storage, otherwise Pigsty will fall back to the main data disk and consume main data disk capacity and IO.\npg_storage_type Parameter Name: pg_storage_type, Type: enum, Level: C\nType of PostgreSQL data storage media: SSD or HDD, default is SSD.\nDefault value: SSD, which affects some tuning parameters like random_page_cost and effective_io_concurrency.\npg_dummy_filesize Parameter Name: pg_dummy_filesize, Type: size, Level: C\nSize of /pg/dummy, default is 64MiB, 64MB disk space for emergency use.\nWhen disk is full, deleting the placeholder file can free some space for emergency use. Recommend at least 8GiB for production.\npg_listen Parameter Name: pg_listen, Type: ip, Level: C\nPostgreSQL / Pgbouncer listen address, default is 0.0.0.0 (all ipv4 addresses).\nYou can use placeholders in this variable, for example: '${ip},${lo}' or '${ip},${vip},${lo}':\n${ip}: Translates to inventory_hostname, which is the primary internal IP address defined in the inventory. ${vip}: If pg_vip_enabled is enabled, will use the host part of pg_vip_address. ${lo}: Will be replaced with 127.0.0.1 For production environments with high security requirements, it’s recommended to restrict listen IP addresses.\npg_port Parameter Name: pg_port, Type: port, Level: C\nPort that PostgreSQL server listens on, default is 5432.\npg_localhost Parameter Name: pg_localhost, Type: path, Level: C\nUnix socket directory for localhost PostgreSQL connection, default is /var/run/postgresql.\nUnix socket directory for PostgreSQL and Pgbouncer local connections. pg_exporter and patroni will preferentially use Unix sockets to access PostgreSQL.\npg_namespace Parameter Name: pg_namespace, Type: path, Level: C\nTop-level namespace used in etcd, used by patroni and vip-manager, default is: /pg, not recommended to change.\npatroni_enabled Parameter Name: patroni_enabled, Type: bool, Level: C\nEnable Patroni? Default is: true.\nIf disabled, no Postgres cluster will be created during initialization. Pigsty will skip the task of starting patroni, which can be used when trying to add some components to existing postgres instances.\npatroni_mode Parameter Name: patroni_mode, Type: enum, Level: C\nPatroni working mode: default, pause, remove. Default: default.\ndefault: Normal use of Patroni to bootstrap PostgreSQL cluster pause: Similar to default, but enters maintenance mode after bootstrap remove: Use Patroni to initialize cluster, then remove Patroni and use raw PostgreSQL. patroni_port Parameter Name: patroni_port, Type: port, Level: C\nPatroni listen port, default is 8008, not recommended to change.\nPatroni API server listens on this port for health checks and API requests.\npatroni_log_dir Parameter Name: patroni_log_dir, Type: path, Level: C\nPatroni log directory, default is /pg/log/patroni, collected by Vector log agent.\npatroni_ssl_enabled Parameter Name: patroni_ssl_enabled, Type: bool, Level: G\nSecure patroni RestAPI communications with SSL? Default is false.\nThis parameter is a global flag that can only be set before deployment. Because if SSL is enabled for patroni, you will have to use HTTPS instead of HTTP for health checks, fetching metrics, and calling APIs.\npatroni_watchdog_mode Parameter Name: patroni_watchdog_mode, Type: string, Level: C\nPatroni watchdog mode: automatic, required, off, default is off.\nIn case of primary failure, Patroni can use watchdog to force shutdown old primary node to avoid split-brain.\noff: Don’t use watchdog. No fencing at all (default behavior) automatic: Enable watchdog if kernel has softdog module enabled and watchdog belongs to dbsu. required: Force enable watchdog, refuse to start Patroni/PostgreSQL if softdog unavailable. Default is off. You should not enable watchdog on Infra nodes. Critical systems where data consistency takes priority over availability, especially business clusters involving money, can consider enabling this option.\nNote that if all your access traffic uses HAproxy health check service access, there is normally no split-brain risk.\npatroni_username Parameter Name: patroni_username, Type: username, Level: C\nPatroni REST API username, default is postgres, used with patroni_password.\nPatroni’s dangerous REST APIs (like restarting cluster) are protected by additional username/password. See Configure Cluster and Patroni RESTAPI for details.\npatroni_password Parameter Name: patroni_password, Type: password, Level: C\nPatroni REST API password, default is Patroni.API.\nWarning: Must change this parameter in production environments!\npg_primary_db Parameter Name: pg_primary_db, Type: string, Level: C\nSpecify the primary database name in the cluster, used for citus and other business databases, default is postgres.\nFor example, when using Patroni to manage HA Citus clusters, you must choose a “primary database”.\nAdditionally, the database name specified here will be displayed in the printed connection string after PGSQL module installation is complete.\npg_parameters Parameter Name: pg_parameters, Type: dict, Level: G/C/I\nUsed to specify and manage configuration parameters in postgresql.auto.conf.\nAfter all cluster instances are initialized, the pg_param task will write the key/value pairs from this dictionary sequentially to /pg/data/postgresql.auto.conf.\nNote: Do not manually modify this configuration file, or modify cluster configuration parameters via ALTER SYSTEM, changes will be overwritten on the next configuration sync.\nThis variable has higher priority than cluster configuration in Patroni / DCS (i.e., higher priority than cluster configuration edited by Patroni edit-config), so it can typically be used to override cluster default parameters at instance level.\nWhen your cluster members have different specifications (not recommended!), you can use this parameter for fine-grained configuration management of each instance.\npg-test: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary , pg_parameters: { shared_buffers: '5GB' } } 10.10.10.12: { pg_seq: 2, pg_role: replica , pg_parameters: { shared_buffers: '4GB' } } 10.10.10.13: { pg_seq: 3, pg_role: replica , pg_parameters: { shared_buffers: '3GB' } } Note that some important cluster parameters (with requirements on primary/replica parameter values) are managed directly by Patroni via command line arguments, have highest priority, and cannot be overridden this way. For these parameters, you must use Patroni edit-config for management and configuration.\nPostgreSQL parameters that must be consistent on primary and replicas (inconsistency will cause replica to fail to start!):\nwal_level max_connections max_locks_per_transaction max_worker_processes max_prepared_transactions track_commit_timestamp Parameters that should preferably be consistent on primary and replicas (considering possibility of failover):\nlisten_addresses port cluster_name hot_standby wal_log_hints max_wal_senders max_replication_slots wal_keep_segments wal_keep_size You can set non-existent parameters (e.g., GUCs from extensions, thus configuring “not yet existing” parameters that ALTER SYSTEM cannot modify), but modifying existing configuration to illegal values may cause PostgreSQL to fail to start, configure with caution!\npg_files Parameter Name: pg_files, Type: path[], Level: C\nUsed to specify a list of files to be copied to the PGDATA directory, default is empty array: []\nFiles specified in this parameter will be copied to the {{ pg_data }} directory, mainly used to distribute license files required by special commercial PostgreSQL kernels.\nCurrently only PolarDB (Oracle compatible) kernel requires license files. For example, you can place the license.lic file in the files/ directory and specify in pg_files:\npg_files: [ license.lic ] pg_conf Parameter Name: pg_conf, Type: enum, Level: C\nConfiguration template: {oltp,olap,crit,tiny}.yml, default is oltp.yml.\ntiny.yml: Optimized for small nodes, VMs, small demos (1-8 cores, 1-16GB) oltp.yml: Optimized for OLTP workloads and latency-sensitive applications (4C8GB+) (default template) olap.yml: Optimized for OLAP workloads and throughput (4C8G+) crit.yml: Optimized for data consistency and critical applications (4C8G+) Default is oltp.yml, but the configure script will set this to tiny.yml when current node is a small node.\nYou can have your own templates, just place them under templates/\u003cmode\u003e.yml and set this value to the template name to use.\npg_max_conn Parameter Name: pg_max_conn, Type: int, Level: C\nPostgreSQL server max connections. You can choose a value between 50 and 5000, or use auto for recommended value.\nDefault is auto, which sets max connections based on pg_conf and pg_default_service_dest.\ntiny: 100 olap: 200 oltp: 200 (pgbouncer) / 1000 (postgres) pg_default_service_dest = pgbouncer : 200 pg_default_service_dest = postgres : 1000 crit: 200 (pgbouncer) / 1000 (postgres) pg_default_service_dest = pgbouncer : 200 pg_default_service_dest = postgres : 1000 Not recommended to set this value above 5000, otherwise you’ll need to manually increase haproxy service connection limits.\nPgbouncer’s transaction pool can mitigate excessive OLTP connection issues, so setting a large connection count is not recommended by default.\nFor OLAP scenarios, change pg_default_service_dest to postgres to bypass connection pooling.\npg_shared_buffer_ratio Parameter Name: pg_shared_buffer_ratio, Type: float, Level: C\nPostgres shared buffer memory ratio, default is 0.25, normal range is 0.1~0.4.\nDefault: 0.25, meaning 25% of node memory will be used as PostgreSQL’s shared buffer. If you want to enable huge pages for PostgreSQL, this value should be appropriately smaller than node_hugepage_ratio.\nSetting this value above 0.4 (40%) is usually not a good idea, but may be useful in extreme cases.\nNote that shared buffers are only part of PostgreSQL’s shared memory. To calculate total shared memory, use show shared_memory_size_in_huge_pages;.\npg_rto Parameter Name: pg_rto, Type: int, Level: C\nRecovery Time Objective (RTO) in seconds. This is used to calculate Patroni’s TTL value, default is 30 seconds.\nIf the primary instance is missing for this long, a new leader election will be triggered. This value is not the lower the better, it involves trade-offs:\nReducing this value can reduce unavailable time (unable to write) during cluster failover, but makes the cluster more sensitive to short-term network jitter, thus increasing the chance of false positives triggering failover.\nYou need to configure this value based on network conditions and business constraints, making a trade-off between failure probability and failure impact. Default is 30s, which affects the following Patroni parameters:\n# TTL for acquiring leader lease (in seconds). Think of it as the time before starting automatic failover. Default: 30 ttl: {{ pg_rto }} # Seconds the loop will sleep. Default: 10, this is patroni check loop interval loop_wait: {{ (pg_rto / 3)|round(0, 'ceil')|int }} # Timeout for DCS and PostgreSQL operation retries (in seconds). DCS or network issues shorter than this won't cause Patroni to demote leader. Default: 10 retry_timeout: {{ (pg_rto / 3)|round(0, 'ceil')|int }} # Time (in seconds) allowed for primary to recover from failure before triggering failover, max RTO: 2x loop_wait + primary_start_timeout primary_start_timeout: {{ (pg_rto / 3)|round(0, 'ceil')|int }} pg_rpo Parameter Name: pg_rpo, Type: int, Level: C\nRecovery Point Objective (RPO) in bytes, default: 1048576.\nDefault is 1MiB, meaning up to 1MiB of data loss can be tolerated during failover.\nWhen the primary goes down and all replicas are lagging, you must make a difficult choice, trade-off between availability and consistency:\nPromote a replica to become new primary and restore service ASAP, but at the cost of acceptable data loss (e.g., less than 1MB). Wait for primary to come back online (may never happen), or manual intervention to avoid any data loss. You can use the crit.yml conf template to ensure no data loss during failover, but this sacrifices some performance.\npg_libs Parameter Name: pg_libs, Type: string, Level: C\nPreloaded dynamic shared libraries, default is pg_stat_statements,auto_explain, two PostgreSQL built-in extensions that are strongly recommended to enable.\nFor existing clusters, you can directly configure cluster shared_preload_libraries parameter and apply.\nIf you want to use TimescaleDB or Citus extensions, you need to add timescaledb or citus to this list. timescaledb and citus should be placed at the front of this list, for example:\ncitus,timescaledb,pg_stat_statements,auto_explain Other extensions requiring dynamic loading can also be added to this list, such as pg_cron, pgml, etc. Typically citus and timescaledb have highest priority and should be added to the front of the list.\npg_delay Parameter Name: pg_delay, Type: interval, Level: I\nDelayed standby replication delay, default: 0.\nIf this value is set to a positive value, the standby cluster leader will be delayed by this time before applying WAL changes. Setting to 1h means data in this cluster will always lag the original cluster by one hour.\nSee Delayed Standby Cluster for details.\npg_checksum Parameter Name: pg_checksum, Type: bool, Level: C\nEnable data checksum for PostgreSQL cluster? Default is true, enabled.\nThis parameter can only be set before PGSQL deployment (but you can enable it manually later).\nData checksums help detect disk corruption and hardware failures. This feature is enabled by default since Pigsty v3.5 to ensure data integrity.\npg_pwd_enc Parameter Name: pg_pwd_enc, Type: enum, Level: C\nPassword encryption algorithm, fixed to scram-sha-256 since Pigsty v4.\nAll new users will use SCRAM credentials. md5 has been deprecated. For compatibility with old clients, upgrade to SCRAM in business connection pools or client drivers.\npg_encoding Parameter Name: pg_encoding, Type: enum, Level: C\nDatabase cluster encoding, default is UTF8.\nUsing other non-UTF8 encodings is not recommended.\npg_locale Parameter Name: pg_locale, Type: enum, Level: C\nDatabase cluster locale, default is C.\nThis parameter controls the database’s default Locale setting, affecting collation, character classification, and other behaviors. Using C or POSIX provides best performance and predictable sorting behavior.\nIf you need specific language localization support, you can set it to the corresponding Locale, such as en_US.UTF-8 or zh_CN.UTF-8. Note that Locale settings affect index sort order, so they cannot be changed after cluster initialization.\npg_lc_collate Parameter Name: pg_lc_collate, Type: enum, Level: C\nDatabase cluster collation, default is C.\nUnless you know what you’re doing, modifying cluster-level collation settings is not recommended.\npg_lc_ctype Parameter Name: pg_lc_ctype, Type: enum, Level: C\nDatabase character set CTYPE, default is C.\nStarting from Pigsty v3.5, to be consistent with pg_lc_collate, the default value changed to C.\npg_io_method Parameter Name: pg_io_method, Type: enum, Level: C\nPostgreSQL IO method, default is worker. Available options include:\nauto: Automatically select based on operating system, uses io_uring on Debian-based systems or EL 10+, otherwise uses worker sync: Use traditional synchronous IO method worker: Use background worker processes to handle IO (default option) io_uring: Use Linux’s io_uring asynchronous IO interface This parameter only applies to PostgreSQL 17 and above, controlling PostgreSQL’s data block layer IO strategy.\nIn PostgreSQL 17, io_uring can provide higher IO performance, but requires operating system kernel support (Linux 5.1+) and the liburing library installed. In PostgreSQL 18, the default IO method changed from sync to worker, using background worker processes for asynchronous IO without additional dependencies. If you’re using Debian 12/Ubuntu 22+ or EL 10+ systems and want optimal IO performance, consider setting this to io_uring. Note that setting this value on systems that don’t support io_uring may cause PostgreSQL startup to fail, so auto or worker are safer choices.\npg_etcd_password Parameter Name: pg_etcd_password, Type: password, Level: C\nThe password used by this PostgreSQL cluster in etcd, default is empty string ''.\nIf set to empty string, the pg_cluster parameter value will be used as the password (for Citus clusters, the pg_shard parameter value is used).\nThis password is used for authentication when Patroni connects to etcd and when vip-manager accesses etcd.\npgsodium_key Parameter Name: pgsodium_key, Type: string, Level: C\nThe encryption master key for the pgsodium extension, consisting of 64 hexadecimal digits.\nThis parameter is not set by default. If not specified, Pigsty will automatically generate a deterministic key using the value of sha256(pg_cluster).\npgsodium is a PostgreSQL extension based on libsodium that provides encryption functions and transparent column encryption capabilities. If you need to use pgsodium’s encryption features, it’s recommended to explicitly specify a secure random key and keep it safe.\nExample command to generate a random key:\nopenssl rand -hex 32 # Generate 64-digit hexadecimal key pgsodium_getkey_script Parameter Name: pgsodium_getkey_script, Type: path, Level: C\nPath to the pgsodium key retrieval script, default uses the pgsodium_getkey script from Pigsty templates.\nThis script is used to retrieve pgsodium’s master key when PostgreSQL starts. The default script reads the key from environment variables or configuration files.\nIf you have custom key management requirements (such as using HashiCorp Vault, AWS KMS, etc.), you can provide a custom script path.\nPG_PROVISION If PG_BOOTSTRAP is about creating a new cluster, then PG_PROVISION is about creating default objects in the cluster, including:\nDefault Roles Default Users Default Privileges Default HBA Rules Default Schemas Default Extensions pg_provision: true # provision postgres cluster after bootstrap pg_init: pg-init # init script for cluster template, default is `pg-init` pg_default_roles: # default roles and users in postgres cluster - { name: dbrole_readonly ,login: false ,comment: role for global read-only access } - { name: dbrole_offline ,login: false ,comment: role for restricted read-only access } - { name: dbrole_readwrite ,login: false ,roles: [dbrole_readonly] ,comment: role for global read-write access } - { name: dbrole_admin ,login: false ,roles: [pg_monitor, dbrole_readwrite] ,comment: role for object creation } - { name: postgres ,superuser: true ,comment: system superuser } - { name: replicator ,replication: true ,roles: [pg_monitor, dbrole_readonly] ,comment: system replicator } - { name: dbuser_dba ,superuser: true ,roles: [dbrole_admin] ,pgbouncer: true ,pool_mode: session, pool_connlimit: 16 ,comment: pgsql admin user } - { name: dbuser_monitor ,roles: [pg_monitor, dbrole_readonly] ,pgbouncer: true ,parameters: {log_min_duration_statement: 1000 } ,pool_mode: session ,pool_connlimit: 8 ,comment: pgsql monitor user } pg_default_privileges: # default privileges when admin user creates objects - GRANT USAGE ON SCHEMAS TO dbrole_readonly - GRANT SELECT ON TABLES TO dbrole_readonly - GRANT SELECT ON SEQUENCES TO dbrole_readonly - GRANT EXECUTE ON FUNCTIONS TO dbrole_readonly - GRANT USAGE ON SCHEMAS TO dbrole_offline - GRANT SELECT ON TABLES TO dbrole_offline - GRANT SELECT ON SEQUENCES TO dbrole_offline - GRANT EXECUTE ON FUNCTIONS TO dbrole_offline - GRANT INSERT ON TABLES TO dbrole_readwrite - GRANT UPDATE ON TABLES TO dbrole_readwrite - GRANT DELETE ON TABLES TO dbrole_readwrite - GRANT USAGE ON SEQUENCES TO dbrole_readwrite - GRANT UPDATE ON SEQUENCES TO dbrole_readwrite - GRANT TRUNCATE ON TABLES TO dbrole_admin - GRANT REFERENCES ON TABLES TO dbrole_admin - GRANT TRIGGER ON TABLES TO dbrole_admin - GRANT CREATE ON SCHEMAS TO dbrole_admin pg_default_schemas: [ monitor ] # default schemas pg_default_extensions: # default extensions - { name: pg_stat_statements ,schema: monitor } - { name: pgstattuple ,schema: monitor } - { name: pg_buffercache ,schema: monitor } - { name: pageinspect ,schema: monitor } - { name: pg_prewarm ,schema: monitor } - { name: pg_visibility ,schema: monitor } - { name: pg_freespacemap ,schema: monitor } - { name: postgres_fdw ,schema: public } - { name: file_fdw ,schema: public } - { name: btree_gist ,schema: public } - { name: btree_gin ,schema: public } - { name: pg_trgm ,schema: public } - { name: intagg ,schema: public } - { name: intarray ,schema: public } - { name: pg_repack } pg_reload: true # reload config after HBA changes? pg_default_hba_rules: # postgres default HBA rules, ordered by `order` - {user: '${dbsu}' ,db: all ,addr: local ,auth: ident ,title: 'dbsu access via local os user ident' ,order: 100} - {user: '${dbsu}' ,db: replication ,addr: local ,auth: ident ,title: 'dbsu replication from local os ident' ,order: 150} - {user: '${repl}' ,db: replication ,addr: localhost ,auth: pwd ,title: 'replicator replication from localhost',order: 200} - {user: '${repl}' ,db: replication ,addr: intra ,auth: pwd ,title: 'replicator replication from intranet' ,order: 250} - {user: '${repl}' ,db: postgres ,addr: intra ,auth: pwd ,title: 'replicator postgres db from intranet' ,order: 300} - {user: '${monitor}' ,db: all ,addr: localhost ,auth: pwd ,title: 'monitor from localhost with password' ,order: 350} - {user: '${monitor}' ,db: all ,addr: infra ,auth: pwd ,title: 'monitor from infra host with password',order: 400} - {user: '${admin}' ,db: all ,addr: infra ,auth: ssl ,title: 'admin @ infra nodes with pwd \u0026 ssl' ,order: 450} - {user: '${admin}' ,db: all ,addr: world ,auth: ssl ,title: 'admin @ everywhere with ssl \u0026 pwd' ,order: 500} - {user: '+dbrole_readonly',db: all ,addr: localhost ,auth: pwd ,title: 'pgbouncer read/write via local socket',order: 550} - {user: '+dbrole_readonly',db: all ,addr: intra ,auth: pwd ,title: 'read/write biz user via password' ,order: 600} - {user: '+dbrole_offline' ,db: all ,addr: intra ,auth: pwd ,title: 'allow etl offline tasks from intranet',order: 650} pgb_default_hba_rules: # pgbouncer default HBA rules, ordered by `order` - {user: '${dbsu}' ,db: pgbouncer ,addr: local ,auth: peer ,title: 'dbsu local admin access with os ident',order: 100} - {user: 'all' ,db: all ,addr: localhost ,auth: pwd ,title: 'allow all user local access with pwd' ,order: 150} - {user: '${monitor}' ,db: pgbouncer ,addr: intra ,auth: pwd ,title: 'monitor access via intranet with pwd' ,order: 200} - {user: '${monitor}' ,db: all ,addr: world ,auth: deny ,title: 'reject all other monitor access addr' ,order: 250} - {user: '${admin}' ,db: all ,addr: intra ,auth: pwd ,title: 'admin access via intranet with pwd' ,order: 300} - {user: '${admin}' ,db: all ,addr: world ,auth: deny ,title: 'reject all other admin access addr' ,order: 350} - {user: 'all' ,db: all ,addr: intra ,auth: pwd ,title: 'allow all user intra access with pwd' ,order: 400} pg_provision Parameter Name: pg_provision, Type: bool, Level: C\nComplete the PostgreSQL cluster provisioning work defined in this section after the cluster is bootstrapped. Default value is true.\nIf disabled, the PostgreSQL cluster will not be provisioned. For some special “PostgreSQL” clusters, such as Greenplum, you can disable this option to skip the provisioning phase.\npg_init Parameter Name: pg_init, Type: string, Level: G/C\nLocation of the shell script for initializing database templates, default is pg-init. This script is copied to /pg/bin/pg-init and then executed.\nThis script is located at roles/pgsql/templates/pg-init\nYou can add your own logic to this script, or provide a new script in the templates/ directory and set pg_init to the new script name. When using a custom script, please preserve the existing initialization logic.\npg_default_roles Parameter Name: pg_default_roles, Type: role[], Level: G/C\nDefault roles and users in Postgres cluster.\nPigsty has a built-in role system. Please check PGSQL Access Control: Role System for details.\npg_default_roles: # default roles and users in postgres cluster - { name: dbrole_readonly ,login: false ,comment: role for global read-only access } - { name: dbrole_offline ,login: false ,comment: role for restricted read-only access } - { name: dbrole_readwrite ,login: false ,roles: [dbrole_readonly] ,comment: role for global read-write access } - { name: dbrole_admin ,login: false ,roles: [pg_monitor, dbrole_readwrite] ,comment: role for object creation } - { name: postgres ,superuser: true ,comment: system superuser } - { name: replicator ,replication: true ,roles: [pg_monitor, dbrole_readonly] ,comment: system replicator } - { name: dbuser_dba ,superuser: true ,roles: [dbrole_admin] ,pgbouncer: true ,pool_mode: session, pool_connlimit: 16 , comment: pgsql admin user } - { name: dbuser_monitor ,roles: [pg_monitor, dbrole_readonly] ,pgbouncer: true ,parameters: {log_min_duration_statement: 1000 } ,pool_mode: session ,pool_connlimit: 8 ,comment: pgsql monitor user } pg_default_privileges Parameter Name: pg_default_privileges, Type: string[], Level: G/C\nDefault privileges (DEFAULT PRIVILEGE) settings in each database:\npg_default_privileges: # default privileges when admin user creates objects - GRANT USAGE ON SCHEMAS TO dbrole_readonly - GRANT SELECT ON TABLES TO dbrole_readonly - GRANT SELECT ON SEQUENCES TO dbrole_readonly - GRANT EXECUTE ON FUNCTIONS TO dbrole_readonly - GRANT USAGE ON SCHEMAS TO dbrole_offline - GRANT SELECT ON TABLES TO dbrole_offline - GRANT SELECT ON SEQUENCES TO dbrole_offline - GRANT EXECUTE ON FUNCTIONS TO dbrole_offline - GRANT INSERT ON TABLES TO dbrole_readwrite - GRANT UPDATE ON TABLES TO dbrole_readwrite - GRANT DELETE ON TABLES TO dbrole_readwrite - GRANT USAGE ON SEQUENCES TO dbrole_readwrite - GRANT UPDATE ON SEQUENCES TO dbrole_readwrite - GRANT TRUNCATE ON TABLES TO dbrole_admin - GRANT REFERENCES ON TABLES TO dbrole_admin - GRANT TRIGGER ON TABLES TO dbrole_admin - GRANT CREATE ON SCHEMAS TO dbrole_admin Pigsty provides corresponding default privilege settings based on the default role system. Please check PGSQL Access Control: Privileges for details.\npg_default_schemas Parameter Name: pg_default_schemas, Type: string[], Level: G/C\nDefault schemas to create, default value is: [ monitor ]. This will create a monitor schema on all databases for placing various monitoring extensions, tables, views, and functions.\npg_default_extensions Parameter Name: pg_default_extensions, Type: extension[], Level: G/C\nList of extensions to be created and enabled by default in all databases, default value:\npg_default_extensions: # default extensions to be created - { name: pg_stat_statements ,schema: monitor } - { name: pgstattuple ,schema: monitor } - { name: pg_buffercache ,schema: monitor } - { name: pageinspect ,schema: monitor } - { name: pg_prewarm ,schema: monitor } - { name: pg_visibility ,schema: monitor } - { name: pg_freespacemap ,schema: monitor } - { name: postgres_fdw ,schema: public } - { name: file_fdw ,schema: public } - { name: btree_gist ,schema: public } - { name: btree_gin ,schema: public } - { name: pg_trgm ,schema: public } - { name: intagg ,schema: public } - { name: intarray ,schema: public } - { name: pg_repack } The only third-party extension is pg_repack, which is important for database maintenance. All other extensions are built-in PostgreSQL Contrib extensions.\nMonitoring-related extensions are installed in the monitor schema by default, which is created by pg_default_schemas.\npg_reload Parameter Name: pg_reload, Type: bool, Level: A\nReload PostgreSQL after HBA changes, default value is true.\nSet it to false to disable automatic configuration reload when you want to check before applying HBA changes.\npg_default_hba_rules Parameter Name: pg_default_hba_rules, Type: hba[], Level: G/C\nPostgreSQL host-based authentication rules, global default rules definition. Default value is:\npg_default_hba_rules: # postgres default host-based authentication rules, ordered by `order` - {user: '${dbsu}' ,db: all ,addr: local ,auth: ident ,title: 'dbsu access via local os user ident' ,order: 100} - {user: '${dbsu}' ,db: replication ,addr: local ,auth: ident ,title: 'dbsu replication from local os ident' ,order: 150} - {user: '${repl}' ,db: replication ,addr: localhost ,auth: pwd ,title: 'replicator replication from localhost',order: 200} - {user: '${repl}' ,db: replication ,addr: intra ,auth: pwd ,title: 'replicator replication from intranet' ,order: 250} - {user: '${repl}' ,db: postgres ,addr: intra ,auth: pwd ,title: 'replicator postgres db from intranet' ,order: 300} - {user: '${monitor}' ,db: all ,addr: localhost ,auth: pwd ,title: 'monitor from localhost with password' ,order: 350} - {user: '${monitor}' ,db: all ,addr: infra ,auth: pwd ,title: 'monitor from infra host with password',order: 400} - {user: '${admin}' ,db: all ,addr: infra ,auth: ssl ,title: 'admin @ infra nodes with pwd \u0026 ssl' ,order: 450} - {user: '${admin}' ,db: all ,addr: world ,auth: ssl ,title: 'admin @ everywhere with ssl \u0026 pwd' ,order: 500} - {user: '+dbrole_readonly',db: all ,addr: localhost ,auth: pwd ,title: 'pgbouncer read/write via local socket',order: 550} - {user: '+dbrole_readonly',db: all ,addr: intra ,auth: pwd ,title: 'read/write biz user via password' ,order: 600} - {user: '+dbrole_offline' ,db: all ,addr: intra ,auth: pwd ,title: 'allow etl offline tasks from intranet',order: 650} The default value provides a fair security level for common scenarios. Please check PGSQL Authentication for details.\nThis parameter is an array of HBA rule objects, identical in format to pg_hba_rules. It’s recommended to configure unified pg_default_hba_rules globally, and use pg_hba_rules for additional customization on specific clusters. Rules from both parameters are applied sequentially, with the latter having higher priority.\npgb_default_hba_rules Parameter Name: pgb_default_hba_rules, Type: hba[], Level: G/C\nPgbouncer default host-based authentication rules, array of HBA rule objects.\nDefault value provides a fair security level for common scenarios. Check PGSQL Authentication for details.\npgb_default_hba_rules: # pgbouncer default host-based authentication rules, ordered by `order` - {user: '${dbsu}' ,db: pgbouncer ,addr: local ,auth: peer ,title: 'dbsu local admin access with os ident',order: 100} - {user: 'all' ,db: all ,addr: localhost ,auth: pwd ,title: 'allow all user local access with pwd' ,order: 150} - {user: '${monitor}' ,db: pgbouncer ,addr: intra ,auth: pwd ,title: 'monitor access via intranet with pwd' ,order: 200} - {user: '${monitor}' ,db: all ,addr: world ,auth: deny ,title: 'reject all other monitor access addr' ,order: 250} - {user: '${admin}' ,db: all ,addr: intra ,auth: pwd ,title: 'admin access via intranet with pwd' ,order: 300} - {user: '${admin}' ,db: all ,addr: world ,auth: deny ,title: 'reject all other admin access addr' ,order: 350} - {user: 'all' ,db: all ,addr: intra ,auth: pwd ,title: 'allow all user intra access with pwd' ,order: 400} The default Pgbouncer HBA rules are simple:\nAllow login from localhost with password Allow login from intranet with password Users can customize according to their own needs.\nThis parameter is identical in format to pgb_hba_rules. It’s recommended to configure unified pgb_default_hba_rules globally, and use pgb_hba_rules for additional customization on specific clusters. Rules from both parameters are applied sequentially, with the latter having higher priority.\nPG_BACKUP This section defines variables for pgBackRest, which is used for PGSQL Point-in-Time Recovery (PITR).\nCheck PGSQL Backup \u0026 PITR for detailed information.\npgbackrest_enabled: true # enable pgBackRest on pgsql host? pgbackrest_clean: true # remove pg backup data during init? pgbackrest_log_dir: /pg/log/pgbackrest # pgbackrest log dir, default is `/pg/log/pgbackrest` pgbackrest_method: local # pgbackrest repo method: local, minio, [user defined...] pgbackrest_init_backup: true # perform a full backup immediately after pgbackrest init? pgbackrest_repo: # pgbackrest repo: https://pgbackrest.org/configuration.html#section-repository local: # default pgbackrest repo with local posix filesystem path: /pg/backup # local backup directory, default is `/pg/backup` retention_full_type: count # retain full backup by count retention_full: 2 # keep at most 3 full backups when using local filesystem repo, at least 2 minio: # optional minio repo for pgbackrest type: s3 # minio is s3-compatible, so use s3 s3_endpoint: sss.pigsty # minio endpoint domain, default is `sss.pigsty` s3_region: us-east-1 # minio region, default is us-east-1, not effective for minio s3_bucket: pgsql # minio bucket name, default is `pgsql` s3_key: pgbackrest # minio user access key for pgbackrest s3_key_secret: S3User.Backup # minio user secret key for pgbackrest s3_uri_style: path # use path style uri for minio, instead of host style path: /pgbackrest # minio backup path, default is `/pgbackrest` storage_port: 9000 # minio port, default is 9000 storage_ca_file: /etc/pki/ca.crt # minio ca file path, default is `/etc/pki/ca.crt` block: y # enable block-level incremental backup (pgBackRest 2.46+) bundle: y # bundle small files into one file bundle_limit: 20MiB # object storage file bundling threshold, default 20MiB bundle_size: 128MiB # object storage file bundling target size, default 128MiB cipher_type: aes-256-cbc # enable AES encryption for remote backup repo cipher_pass: pgBackRest # AES encryption password, default is 'pgBackRest' retention_full_type: time # retain full backup by time on minio repo retention_full: 14 # keep full backups from the past 14 days pgbackrest_enabled Parameter Name: pgbackrest_enabled, Type: bool, Level: C\nEnable pgBackRest on PGSQL nodes? Default value is: true\nWhen using local filesystem backup repository (local), only the cluster primary will actually enable pgbackrest. Other instances will only initialize an empty repository.\npgbackrest_clean Parameter Name: pgbackrest_clean, Type: bool, Level: C\nRemove PostgreSQL backup data during initialization? Default value is true.\npgbackrest_log_dir Parameter Name: pgbackrest_log_dir, Type: path, Level: C\npgBackRest log directory, default is /pg/log/pgbackrest. The Vector log agent references this parameter for log collection.\npgbackrest_method Parameter Name: pgbackrest_method, Type: enum, Level: C\npgBackRest repository method: default options are local, minio, or other user-defined methods, default is local.\nThis parameter determines which repository to use for pgBackRest. All available repository methods are defined in pgbackrest_repo.\nPigsty uses the local backup repository by default, which creates a backup repository in the /pg/backup directory on the primary instance. The underlying storage path is specified by pg_fs_backup.\npgbackrest_init_backup Parameter Name: pgbackrest_init_backup, Type: bool, Level: C\nPerform a full backup immediately after pgBackRest initialization completes? Default is true.\nThis operation is only executed on cluster primary and non-cascading replicas (no pg_upstream defined). Enabling this parameter ensures you have a base backup immediately after cluster initialization for recovery when needed.\npgbackrest_repo Parameter Name: pgbackrest_repo, Type: dict, Level: G/C\npgBackRest repository documentation: https://pgbackrest.org/configuration.html#section-repository\nDefault value includes two repository methods: local and minio, defined as follows:\npgbackrest_repo: # pgbackrest repo: https://pgbackrest.org/configuration.html#section-repository local: # default pgbackrest repo with local posix filesystem path: /pg/backup # local backup directory, default is `/pg/backup` retention_full_type: count # retain full backup by count retention_full: 2 # keep at most 3 full backups when using local filesystem repo, at least 2 minio: # optional minio repo for pgbackrest type: s3 # minio is s3-compatible, so use s3 s3_endpoint: sss.pigsty # minio endpoint domain, default is `sss.pigsty` s3_region: us-east-1 # minio region, default is us-east-1, not effective for minio s3_bucket: pgsql # minio bucket name, default is `pgsql` s3_key: pgbackrest # minio user access key for pgbackrest s3_key_secret: S3User.Backup # minio user secret key for pgbackrest s3_uri_style: path # use path style uri for minio, instead of host style path: /pgbackrest # minio backup path, default is `/pgbackrest` storage_port: 9000 # minio port, default is 9000 storage_ca_file: /etc/pki/ca.crt # minio ca file path, default is `/etc/pki/ca.crt` block: y # enable block-level incremental backup (pgBackRest 2.46+) bundle: y # bundle small files into one file bundle_limit: 20MiB # object storage file bundling threshold, default 20MiB bundle_size: 128MiB # object storage file bundling target size, default 128MiB cipher_type: aes-256-cbc # enable AES encryption for remote backup repo cipher_pass: pgBackRest # AES encryption password, default is 'pgBackRest' retention_full_type: time # retain full backup by time on minio repo retention_full: 14 # keep full backups from the past 14 days You can define new backup repositories, such as using AWS S3, GCP, or other cloud providers’ S3-compatible storage services.\nBlock Incremental Backup: Starting from pgBackRest 2.46, the block: y option enables block-level incremental backup. This means during incremental backups, pgBackRest only backs up changed data blocks instead of entire changed files, significantly reducing backup data volume and backup time. This feature is particularly useful for large databases, and it’s recommended to enable this option on object storage repositories.\nPG_ACCESS This section handles database access paths, including:\nDeploy Pgbouncer connection pooler on each PGSQL node and set default behavior Publish service ports through local or dedicated haproxy nodes Bind optional L2 VIP and register DNS records pgbouncer_enabled: true # if disabled, pgbouncer will not be launched on pgsql host pgbouncer_port: 6432 # pgbouncer listen port, 6432 by default pgbouncer_log_dir: /pg/log/pgbouncer # pgbouncer log dir, `/pg/log/pgbouncer` by default pgbouncer_auth_query: false # query postgres to retrieve unlisted business users? pgbouncer_poolmode: transaction # pooling mode: transaction,session,statement, transaction by default pgbouncer_sslmode: disable # pgbouncer client ssl mode, disable by default pgbouncer_ignore_param: [ extra_float_digits, application_name, TimeZone, DateStyle, IntervalStyle, search_path ] pg_weight: 100 #INSTANCE # relative load balance weight in service, 100 by default, 0-255 pg_service_provider: '' # dedicate haproxy node group name, or empty string for local nodes by default pg_default_service_dest: pgbouncer # default service destination if svc.dest='default' pg_default_services: # postgres default service definitions - { name: primary ,port: 5433 ,dest: default ,check: /primary ,selector: \"[]\" } - { name: replica ,port: 5434 ,dest: default ,check: /read-only ,selector: \"[]\" , backup: \"[? pg_role == `primary` || pg_role == `offline` ]\" } - { name: default ,port: 5436 ,dest: postgres ,check: /primary ,selector: \"[]\" } - { name: offline ,port: 5438 ,dest: postgres ,check: /replica ,selector: \"[? pg_role == `offline` || pg_offline_query ]\" , backup: \"[? pg_role == `replica` \u0026\u0026 !pg_offline_query]\"} pg_vip_enabled: false # enable a l2 vip for pgsql primary? false by default pg_vip_address: 127.0.0.1/24 # vip address in `\u003cipv4\u003e/\u003cmask\u003e` format, require if vip is enabled pg_vip_interface: eth0 # vip network interface to listen, eth0 by default pg_dns_suffix: '' # pgsql dns suffix, '' by default pg_dns_target: auto # auto, primary, vip, none, or ad hoc ip pgbouncer_enabled Parameter Name: pgbouncer_enabled, Type: bool, Level: C\nDefault value is true. If disabled, the Pgbouncer connection pooler will not be configured on PGSQL nodes.\npgbouncer_port Parameter Name: pgbouncer_port, Type: port, Level: C\nPgbouncer listen port, default is 6432.\npgbouncer_log_dir Parameter Name: pgbouncer_log_dir, Type: path, Level: C\nPgbouncer log directory, default is /pg/log/pgbouncer. The Vector log agent collects Pgbouncer logs based on this parameter.\npgbouncer_auth_query Parameter Name: pgbouncer_auth_query, Type: bool, Level: C\nAllow Pgbouncer to query PostgreSQL to allow users not explicitly listed to access PostgreSQL through the connection pool? Default value is false.\nIf enabled, pgbouncer users will authenticate against the postgres database using SELECT username, password FROM monitor.pgbouncer_auth($1). Otherwise, only business users with pgbouncer: true are allowed to connect to the Pgbouncer connection pool.\npgbouncer_poolmode Parameter Name: pgbouncer_poolmode, Type: enum, Level: C\nPgbouncer connection pool pooling mode: transaction, session, statement, default is transaction.\nsession: Session-level pooling with best feature compatibility. transaction: Transaction-level pooling with better performance (many small connections), may break some session-level features like NOTIFY/LISTEN, etc. statements: Statement-level pooling for simple read-only queries. If your application has feature compatibility issues, consider changing this parameter to session.\npgbouncer_sslmode Parameter Name: pgbouncer_sslmode, Type: enum, Level: C\nPgbouncer client SSL mode, default is disable.\nNote that enabling SSL may have a significant performance impact on your pgbouncer.\ndisable: Ignore if client requests TLS (default) allow: Use TLS if client requests it. Use plain TCP if not. Does not verify client certificate. prefer: Same as allow. require: Client must use TLS. Reject client connection if not. Does not verify client certificate. verify-ca: Client must use TLS with a valid client certificate. verify-full: Same as verify-ca. pgbouncer_ignore_param Parameter Name: pgbouncer_ignore_param, Type: string[], Level: C\nList of startup parameters ignored by PgBouncer, default value is:\n[ extra_float_digits, application_name, TimeZone, DateStyle, IntervalStyle, search_path ] These parameters are configured in the ignore_startup_parameters option in the PgBouncer configuration file. When clients set these parameters during connection, PgBouncer will not create new connections due to parameter mismatch in the connection pool.\nThis allows different clients to use the same connection pool even if they set different values for these parameters. This parameter was added in Pigsty v3.5.\npg_weight Parameter Name: pg_weight, Type: int, Level: I\nRelative load balancing weight in service, default is 100, range 0-255.\nDefault value: 100. You must define it in instance variables and reload service for it to take effect.\npg_service_provider Parameter Name: pg_service_provider, Type: string, Level: G/C\nDedicated haproxy node group name, or empty string for local nodes by default.\nIf specified, PostgreSQL services will be registered to the dedicated haproxy node group instead of the current PGSQL cluster nodes.\nRemember to allocate unique ports for each service on the dedicated haproxy nodes!\nFor example, if we define the following parameters on a 3-node pg-test cluster:\npg_service_provider: infra # use load balancer on group `infra` pg_default_services: # alloc port 10001 and 10002 for pg-test primary/replica service - { name: primary ,port: 10001 ,dest: postgres ,check: /primary ,selector: \"[]\" } - { name: replica ,port: 10002 ,dest: postgres ,check: /read-only ,selector: \"[]\" , backup: \"[? pg_role == `primary` || pg_role == `offline` ]\" } pg_default_service_dest Parameter Name: pg_default_service_dest, Type: enum, Level: G/C\nWhen defining a service, if svc.dest='default', this parameter will be used as the default value.\nDefault value: pgbouncer, meaning the 5433 primary service and 5434 replica service will route traffic to pgbouncer by default.\nIf you don’t want to use pgbouncer, set it to postgres. Traffic will be routed directly to postgres.\npg_default_services Parameter Name: pg_default_services, Type: service[], Level: G/C\nPostgres default service definitions.\nDefault value is four default service definitions, as described in PGSQL Service.\npg_default_services: # postgres default service definitions - { name: primary ,port: 5433 ,dest: default ,check: /primary ,selector: \"[]\" } - { name: replica ,port: 5434 ,dest: default ,check: /read-only ,selector: \"[]\" , backup: \"[? pg_role == `primary` || pg_role == `offline` ]\" } - { name: default ,port: 5436 ,dest: postgres ,check: /primary ,selector: \"[]\" } - { name: offline ,port: 5438 ,dest: postgres ,check: /replica ,selector: \"[? pg_role == `offline` || pg_offline_query ]\" , backup: \"[? pg_role == `replica` \u0026\u0026 !pg_offline_query]\"} pg_vip_enabled Parameter Name: pg_vip_enabled, Type: bool, Level: C\nEnable L2 VIP for PGSQL cluster? Default value is false, meaning no L2 VIP will be created.\nWhen L2 VIP is enabled, a VIP will be bound to the cluster primary instance node, managed by vip-manager based on data in etcd.\nL2 VIP can only be used within the same L2 network, which may impose additional constraints on your network topology.\npg_vip_address Parameter Name: pg_vip_address, Type: cidr4, Level: C\nVIP address in \u003cipv4\u003e/\u003cmask\u003e format is required if VIP is enabled.\nDefault value: 127.0.0.1/24. This value consists of two parts: ipv4 and mask, separated by /.\npg_vip_interface Parameter Name: pg_vip_interface, Type: string, Level: C/I\nVIP network interface to listen, eth0 by default.\nIt should be your node’s primary network interface name, i.e., the IP address used in your inventory.\nIf your nodes have multiple network interfaces with different names, you can override it in instance variables:\npg-test: hosts: 10.10.10.11: {pg_seq: 1, pg_role: replica ,pg_vip_interface: eth0 } 10.10.10.12: {pg_seq: 2, pg_role: primary ,pg_vip_interface: eth1 } 10.10.10.13: {pg_seq: 3, pg_role: replica ,pg_vip_interface: eth2 } vars: pg_vip_enabled: true # enable L2 VIP for this cluster, binds to primary by default pg_vip_address: 10.10.10.3/24 # L2 network CIDR: 10.10.10.0/24, vip address: 10.10.10.3 # pg_vip_interface: eth1 # if your nodes have a unified interface, you can define it here pg_dns_suffix Parameter Name: pg_dns_suffix, Type: string, Level: C\nPostgreSQL DNS name suffix, default is empty string.\nBy default, the PostgreSQL cluster name is registered as a DNS domain in dnsmasq on Infra nodes for external resolution.\nYou can specify a domain suffix with this parameter, which will use {{ pg_cluster }}{{ pg_dns_suffix }} as the cluster DNS name.\nFor example, if you set pg_dns_suffix to .db.vip.company.tld, the pg-test cluster DNS name will be pg-test.db.vip.company.tld.\npg_dns_target Parameter Name: pg_dns_target, Type: enum, Level: C\nCould be: auto, primary, vip, none, or an ad hoc IP address, which will be the target IP address of cluster DNS record.\nDefault value: auto, which will bind to pg_vip_address if pg_vip_enabled, or fallback to cluster primary instance IP address.\nvip: bind to pg_vip_address primary: resolve to cluster primary instance IP address auto: resolve to pg_vip_address if pg_vip_enabled, or fallback to cluster primary instance IP address none: do not bind to any IP address \u003cipv4\u003e: bind to the given IP address PG_MONITOR The PG_MONITOR group parameters are used to monitor the status of PostgreSQL databases, Pgbouncer connection pools, and pgBackRest backup systems.\nThis parameter group defines three Exporter configurations: pg_exporter for monitoring PostgreSQL, pgbouncer_exporter for monitoring connection pools, and pgbackrest_exporter for monitoring backup status.\npg_exporter_enabled: true # enable pg_exporter on pgsql host? pg_exporter_config: pg_exporter.yml # pg_exporter config file name pg_exporter_cache_ttls: '1,10,60,300' # pg_exporter collector ttl stages (seconds), default is '1,10,60,300' pg_exporter_port: 9630 # pg_exporter listen port, default is 9630 pg_exporter_params: 'sslmode=disable' # extra url parameters for pg_exporter dsn pg_exporter_url: '' # if specified, will override auto-generated pg dsn pg_exporter_auto_discovery: true # enable auto database discovery? enabled by default pg_exporter_exclude_database: 'template0,template1,postgres' # csv list of databases not monitored during auto-discovery pg_exporter_include_database: '' # csv list of databases monitored during auto-discovery pg_exporter_connect_timeout: 200 # pg_exporter connection timeout (ms), default is 200 pg_exporter_options: '' # extra options to override pg_exporter pgbouncer_exporter_enabled: true # enable pgbouncer_exporter on pgsql host? pgbouncer_exporter_port: 9631 # pgbouncer_exporter listen port, default is 9631 pgbouncer_exporter_url: '' # if specified, will override auto-generated pgbouncer dsn pgbouncer_exporter_options: '' # extra options to override pgbouncer_exporter pgbackrest_exporter_enabled: true # enable pgbackrest_exporter on pgsql host? pgbackrest_exporter_port: 9854 # pgbackrest_exporter listen port, default is 9854 pgbackrest_exporter_options: '' # extra options to override pgbackrest_exporter pg_exporter_enabled Parameter Name: pg_exporter_enabled, Type: bool, Level: C\nEnable pg_exporter on PGSQL nodes? Default value is: true.\nPG Exporter is used to monitor PostgreSQL database instances. Set to false if you don’t want to install pg_exporter.\npg_exporter_config Parameter Name: pg_exporter_config, Type: string, Level: C\npg_exporter configuration file name, both PG Exporter and PGBouncer Exporter will use this configuration file. Default value: pg_exporter.yml.\nIf you want to use a custom configuration file, you can define it here. Your custom configuration file should be placed in files/\u003cname\u003e.yml.\nFor example, when you want to monitor a remote PolarDB database instance, you can use the sample configuration: files/polar_exporter.yml.\npg_exporter_cache_ttls Parameter Name: pg_exporter_cache_ttls, Type: string, Level: C\npg_exporter collector TTL stages (seconds), default is ‘1,10,60,300’.\nDefault value: 1,10,60,300, which will use different TTL values for different metric collectors: 1s, 10s, 60s, 300s.\nPG Exporter has a built-in caching mechanism to avoid the improper impact of multiple Prometheus scrapes on the database. All metric collectors are divided into four categories by TTL:\nttl_fast: \"{{ pg_exporter_cache_ttls.split(',')[0]|int }}\" # critical queries ttl_norm: \"{{ pg_exporter_cache_ttls.split(',')[1]|int }}\" # common queries ttl_slow: \"{{ pg_exporter_cache_ttls.split(',')[2]|int }}\" # slow queries (e.g table size) ttl_slowest: \"{{ pg_exporter_cache_ttls.split(',')[3]|int }}\" # ver slow queries (e.g bloat) For example, with default configuration, liveness metrics are cached for at most 1s, most common metrics are cached for 10s (should match the monitoring scrape interval victoria_scrape_interval). A few slow-changing queries have 60s TTL, and very few high-overhead monitoring queries have 300s TTL.\npg_exporter_port Parameter Name: pg_exporter_port, Type: port, Level: C\npg_exporter listen port, default value is: 9630\npg_exporter_params Parameter Name: pg_exporter_params, Type: string, Level: C\nExtra URL path parameters in the DSN used by pg_exporter.\nDefault value: sslmode=disable, which disables SSL for monitoring connections (since local unix sockets are used by default).\npg_exporter_url Parameter Name: pg_exporter_url, Type: pgurl, Level: C\nIf specified, will override the auto-generated PostgreSQL DSN and use the specified DSN to connect to PostgreSQL. Default value is empty string.\nIf not specified, PG Exporter will use the following connection string to access PostgreSQL by default:\npostgres://{{ pg_monitor_username }}:{{ pg_monitor_password }}@{{ pg_host }}:{{ pg_port }}/postgres{% if pg_exporter_params != '' %}?{{ pg_exporter_params }}{% endif %} Use this parameter when you want to monitor a remote PostgreSQL instance, or need to use different monitoring user/password or configuration options.\npg_exporter_auto_discovery Parameter Name: pg_exporter_auto_discovery, Type: bool, Level: C\nEnable auto database discovery? Enabled by default: true.\nBy default, PG Exporter connects to the database specified in the DSN (default is the admin database postgres) to collect global metrics. If you want to collect metrics from all business databases, enable this option. PG Exporter will automatically discover all databases in the target PostgreSQL instance and collect database-level monitoring metrics from these databases.\npg_exporter_exclude_database Parameter Name: pg_exporter_exclude_database, Type: string, Level: C\nIf database auto-discovery is enabled (enabled by default), databases in this parameter’s list will not be monitored. Default value is: template0,template1,postgres, meaning the admin database postgres and template databases are excluded from auto-monitoring.\nAs an exception, the database specified in the DSN is not affected by this parameter. For example, if PG Exporter connects to the postgres database, it will be monitored even if postgres is in this list.\npg_exporter_include_database Parameter Name: pg_exporter_include_database, Type: string, Level: C\nIf database auto-discovery is enabled (enabled by default), only databases in this parameter’s list will be monitored. Default value is empty string, meaning this feature is not enabled.\nThe parameter format is a comma-separated list of database names, e.g., db1,db2,db3.\nThis parameter has higher priority than pg_exporter_exclude_database, acting as a whitelist mode. Use this parameter if you only want to monitor specific databases.\npg_exporter_connect_timeout Parameter Name: pg_exporter_connect_timeout, Type: int, Level: C\npg_exporter connection timeout (milliseconds), default is 200 (in milliseconds).\nHow long will PG Exporter wait when trying to connect to a PostgreSQL database? Beyond this time, PG Exporter will give up the connection and report an error.\nThe default value of 200ms is sufficient for most scenarios (e.g., same availability zone monitoring), but if your monitored remote PostgreSQL is on another continent, you may need to increase this value to avoid connection timeouts.\npg_exporter_options Parameter Name: pg_exporter_options, Type: arg, Level: C\nCommand line arguments passed to PG Exporter, default value is: \"\" empty string.\nWhen using empty string, the default command arguments will be used:\n{% if pg_exporter_port != '' %} PG_EXPORTER_OPTS='--web.listen-address=:{{ pg_exporter_port }} {{ pg_exporter_options }}' {% else %} PG_EXPORTER_OPTS='--web.listen-address=:{{ pg_exporter_port }} --log.level=info' {% endif %} Note: Do not override the pg_exporter_port port configuration in this parameter.\npgbouncer_exporter_enabled Parameter Name: pgbouncer_exporter_enabled, Type: bool, Level: C\nEnable pgbouncer_exporter on PGSQL nodes? Default value is: true.\npgbouncer_exporter_port Parameter Name: pgbouncer_exporter_port, Type: port, Level: C\npgbouncer_exporter listen port, default value is: 9631\npgbouncer_exporter_url Parameter Name: pgbouncer_exporter_url, Type: pgurl, Level: C\nIf specified, will override the auto-generated pgbouncer DSN and use the specified DSN to connect to pgbouncer. Default value is empty string.\nIf not specified, Pgbouncer Exporter will use the following connection string to access Pgbouncer by default:\npostgres://{{ pg_monitor_username }}:{{ pg_monitor_password }}@:{{ pgbouncer_port }}/pgbouncer?host={{ pg_localhost }}\u0026sslmode=disable Use this parameter when you want to monitor a remote Pgbouncer instance, or need to use different monitoring user/password or configuration options.\npgbouncer_exporter_options Parameter Name: pgbouncer_exporter_options, Type: arg, Level: C\nCommand line arguments passed to Pgbouncer Exporter, default value is: \"\" empty string.\nWhen using empty string, the default command arguments will be used:\n{% if pgbouncer_exporter_options != '' %} PG_EXPORTER_OPTS='--web.listen-address=:{{ pgbouncer_exporter_port }} {{ pgbouncer_exporter_options }}' {% else %} PG_EXPORTER_OPTS='--web.listen-address=:{{ pgbouncer_exporter_port }} --log.level=info' {% endif %} Note: Do not override the pgbouncer_exporter_port port configuration in this parameter.\npgbackrest_exporter_enabled Parameter Name: pgbackrest_exporter_enabled, Type: bool, Level: C\nEnable pgbackrest_exporter on PGSQL nodes? Default value is: true.\npgbackrest_exporter is used to monitor the status of the pgBackRest backup system, including key metrics such as backup size, time, type, and duration.\npgbackrest_exporter_port Parameter Name: pgbackrest_exporter_port, Type: port, Level: C\npgbackrest_exporter listen port, default value is: 9854.\nThis port needs to be referenced in the Prometheus service discovery configuration to scrape backup-related monitoring metrics.\npgbackrest_exporter_options Parameter Name: pgbackrest_exporter_options, Type: arg, Level: C\nCommand line arguments passed to pgbackrest_exporter, default value is: \"\" empty string.\nWhen using empty string, the default command argument configuration will be used. You can specify additional parameter options here to adjust the exporter’s behavior.\nPG_REMOVE pgsql-rm.yml invokes the pg_remove role to safely remove PostgreSQL instances. This section’s parameters control cleanup behavior to avoid accidental deletion.\npg_rm_data: true # remove postgres data during remove? true by default pg_rm_backup: true # remove pgbackrest backup during primary remove? true by default pg_rm_pkg: true # uninstall postgres packages during remove? true by default pg_safeguard: false # stop pg_remove running if pg_safeguard is enabled, false by default pg_rm_data Parameter Name: pg_rm_data, Type: bool, Level: G/C/A\nWhether to clean up pg_data and symlinks when removing PGSQL instances, default is true.\nThis switch affects both pgsql-rm.yml and other scenarios that trigger pg_remove. Set to false to preserve the data directory for manual inspection or remounting.\npg_rm_backup Parameter Name: pg_rm_backup, Type: bool, Level: G/C/A\nWhether to also clean up the pgBackRest repository and configuration when removing the primary, default is true.\nThis parameter only applies to primary instances with pg_role=primary: pg_remove will first stop pgBackRest, delete the current cluster’s stanza, and remove data in pg_fs_backup when pgbackrest_method == 'local'. Standby clusters or upstream backups are not affected.\npg_rm_pkg Parameter Name: pg_rm_pkg, Type: bool, Level: G/C/A\nWhether to uninstall all packages installed by pg_packages when cleaning up PGSQL instances, default is true.\nIf you only want to temporarily stop and preserve binaries, set it to false. Otherwise, pg_remove will call the system package manager to completely uninstall PostgreSQL-related components.\npg_safeguard Parameter Name: pg_safeguard, Type: bool, Level: G/C/A\nAccidental deletion protection, default is false. When explicitly set to true, pg_remove will immediately terminate with a prompt, and will only continue after using -e pg_safeguard=false or disabling it in variables.\nIt’s recommended to enable this switch before batch cleanup in production environments, verify the commands and target nodes are correct, then disable it to avoid accidental deletion of instances.\n","categories":["Reference"],"description":"Customize PostgreSQL clusters with 120 parameters in the PGSQL module","excerpt":"Customize PostgreSQL clusters with 120 parameters in the PGSQL module","ref":"/docs/pgsql/param/","tags":"","title":"Parameters"},{"body":"Accidental Data Deletion If it’s a small-scale DELETE misoperation, you can consider using the pg_surgery or pg_dirtyread extension for in-place surgical recovery.\n-- Immediately disable Auto Vacuum on this table and abort Auto Vacuum worker processes for this table ALTER TABLE public.some_table SET (autovacuum_enabled = off, toast.autovacuum_enabled = off); CREATE EXTENSION pg_dirtyread; SELECT * FROM pg_dirtyread('tablename') AS t(col1 type1, col2 type2, ...); If the deleted data has already been reclaimed by VACUUM, then use the general accidental deletion recovery process.\nAccidental Object Deletion When DROP/DELETE type misoperations occur, typically decide on a recovery plan according to the following process:\nConfirm whether this data can be recovered from the business system or other data systems. If yes, recover directly from the business side. Confirm whether there is a delayed replica. If yes, advance the delayed replica to the time point before deletion and query the data for recovery. If the data has been confirmed deleted, confirm backup information and whether the backup range covers the deletion time point. If it does, start PITR. Confirm whether to perform in-place cluster PITR rollback, or start a new server for replay, or use a replica for replay, and execute the recovery strategy. Accidental Cluster Deletion If an entire database cluster is accidentally deleted through Pigsty management commands, for example, incorrectly executing the pgsql-rm.yml playbook or the bin/pgsql-rm command. Unless you have set the pg_rm_backup parameter to false, the backup will be deleted along with the database cluster.\nWarning: In this situation, your data will be unrecoverable! Please think three times before proceeding!\nRecommendation: For production environments, you can globally configure this parameter to false in the configuration manifest to preserve backups when removing clusters.\n","categories":["Task"],"description":"Handling accidental data deletion, table deletion, and database deletion","excerpt":"Handling accidental data deletion, table deletion, and database …","ref":"/docs/pgsql/tutorial/drop/","tags":["SOP","Recovery"],"title":"Accidental Deletion"},{"body":" Pigsty provides a series of playbooks for cluster provisioning, scaling, user/database management, monitoring, backup \u0026 recovery, and migration.\nPlaybook Function pgsql.yml Initialize PostgreSQL cluster or add new replicas pgsql-rm.yml Remove PostgreSQL cluster or specific instances pgsql-user.yml Add new business user to existing PostgreSQL cluster pgsql-db.yml Add new business database to existing PostgreSQL cluster pgsql-monitor.yml Monitor remote PostgreSQL instances pgsql-migration.yml Generate migration manual and scripts for existing PostgreSQL pgsql-pitr.yml Perform Point-In-Time Recovery (PITR) Safeguard Be extra cautious when using PGSQL playbooks. Misuse of pgsql.yml and pgsql-rm.yml can lead to accidental database deletion!\nAlways add the -l parameter to limit the execution scope, and ensure you’re executing the right tasks on the right targets. Limiting scope to a single cluster is recommended. Running pgsql.yml without parameters in production is a high-risk operation—think twice before proceeding. To prevent accidental deletion, Pigsty’s PGSQL module provides a safeguard mechanism controlled by the pg_safeguard parameter. When pg_safeguard is set to true, the pgsql-rm.yml playbook will abort immediately, protecting your database cluster.\n# Will abort execution, protecting data ./pgsql-rm.yml -l pg-test # Force override the safeguard via command line parameter ./pgsql-rm.yml -l pg-test -e pg_safeguard=false In addition to pg_safeguard, pgsql-rm.yml provides finer-grained control parameters:\nParameter Default Description pg_safeguard false Safeguard switch; when true, playbook aborts pg_rm_data true Whether to remove PostgreSQL data directory pg_rm_backup true Whether to remove pgBackRest backup data (only when removing primary) pg_rm_pkg false Whether to uninstall PostgreSQL packages These parameters allow precise control over removal behavior:\n# Remove cluster but keep data directory (only stop services) ./pgsql-rm.yml -l pg-test -e pg_rm_data=false # Remove cluster but keep backup data ./pgsql-rm.yml -l pg-test -e pg_rm_backup=false # Remove cluster and uninstall packages ./pgsql-rm.yml -l pg-test -e pg_rm_pkg=true pgsql.yml The pgsql.yml playbook is used to initialize PostgreSQL clusters or add new replicas.\nHere’s a demo of initializing a PostgreSQL cluster in the sandbox environment:\nBasic Usage\n./pgsql.yml -l pg-meta # Initialize cluster pg-meta ./pgsql.yml -l 10.10.10.13 # Initialize/add instance 10.10.10.13 ./pgsql.yml -l pg-test -t pg_service # Refresh services for cluster pg-test ./pgsql.yml -l pg-test -t pg_hba,pgbouncer_hba,pgbouncer_reload -e pg_reload=true # Reload HBA rules Wrapper Scripts\nPigsty provides convenient wrapper scripts to simplify common operations:\nbin/pgsql-add pg-meta # Initialize pgsql cluster pg-meta bin/pgsql-add 10.10.10.10 # Initialize pgsql instance 10.10.10.10 bin/pgsql-add pg-test 10.10.10.13 # Add 10.10.10.13 to cluster pg-test (auto refresh services) bin/pgsql-svc pg-test # Refresh haproxy services for pg-test (use after membership changes) bin/pgsql-hba pg-test # Reload pg/pgb HBA rules for pg-test Subtasks\nThis playbook contains the following subtasks:\n# pg_install : install postgres packages \u0026 extensions # - pg_dbsu : setup postgres superuser # - pg_dbsu_create : create dbsu user # - pg_dbsu_sudo : configure dbsu sudo privileges # - pg_ssh : exchange dbsu SSH keys # - pg_pkg : install postgres packages # - pg_pre : pre-installation tasks # - pg_ext : install postgres extension packages # - pg_post : post-installation tasks # - pg_link : link pgsql version bin to /usr/pgsql # - pg_path : add pgsql bin to system path # - pg_dir : create postgres directories and setup FHS # - pg_bin : sync /pg/bin scripts # - pg_alias : configure pgsql/psql aliases # - pg_dummy : create dummy placeholder file # # pg_bootstrap : bootstrap postgres cluster # - pg_config : generate postgres config # - pg_conf : generate patroni config # - pg_key : generate pgsodium key # - pg_cert : issue certificates for postgres # - pg_cert_private : check pg private key existence # - pg_cert_issue : sign pg server certificate # - pg_cert_copy : copy key \u0026 certs to pg node # - pg_launch : launch patroni primary \u0026 replicas # - pg_watchdog : grant watchdog permission to postgres # - pg_primary : launch patroni/postgres primary # - pg_init : init pg cluster with roles/templates # - pg_pass : write .pgpass file to pg home # - pg_replica : launch patroni/postgres replicas # - pg_hba : generate pg HBA rules # - patroni_reload : reload patroni config # - pg_patroni : pause or remove patroni if necessary # # pg_provision : provision postgres business users \u0026 databases # - pg_user : provision postgres business users # - pg_user_config : render create user SQL # - pg_user_create : create user on postgres # - pg_db : provision postgres business databases # - pg_db_drop : drop database on postgres (state=absent/recreate) # - pg_db_config : render create database SQL # - pg_db_create : create database on postgres # # pg_backup : init postgres PITR backup # - pgbackrest : setup pgbackrest for backup # - pgbackrest_config : generate pgbackrest config # - pgbackrest_init : init pgbackrest repo # - pgbackrest_backup : make initial backup after bootstrap # # pg_access : init postgres service access layer # - pgbouncer : deploy pgbouncer connection pooler # - pgbouncer_dir : create pgbouncer directories # - pgbouncer_config : generate pgbouncer config # - pgbouncer_hba : generate pgbouncer HBA config # - pgbouncer_user : generate pgbouncer userlist # - pgbouncer_launch : launch pgbouncer service # - pgbouncer_reload : reload pgbouncer config # - pg_vip : bind VIP to primary with vip-manager # - pg_vip_config : generate vip-manager config # - pg_vip_launch : launch vip-manager to bind VIP # - pg_dns : register DNS name to infra dnsmasq # - pg_dns_ins : register pg instance name # - pg_dns_cls : register pg cluster name # - pg_service : expose pgsql service with haproxy # - pg_service_config : generate local haproxy config for pg services # - pg_service_reload : expose postgres services with haproxy # # pg_monitor : setup pgsql monitoring and register to infra # - pg_exporter : configure and launch pg_exporter # - pgbouncer_exporter : configure and launch pgbouncer_exporter # - pgbackrest_exporter : configure and launch pgbackrest_exporter # - pg_register : register pgsql to monitoring/logging/datasource # - add_metrics : register pg as VictoriaMetrics monitoring target # - add_logs : register pg as Vector log source # - add_ds : register pg database as Grafana datasource Related Administration Tasks\nCreate Cluster Add Instance Reload Service Reload HBA Notes\nWhen running this playbook on a single replica, ensure the cluster primary is already initialized! After scaling out, you need to Reload Service and Reload HBA. The wrapper script bin/pgsql-add handles these tasks automatically. When scaling a cluster, if Patroni takes too long to bring up a replica, the Ansible playbook may abort due to timeout:\nTypical error message: wait for postgres/patroni replica task runs for a long time before aborting However, the replica creation process continues. For scenarios where replica creation takes more than a day, see FAQ: Replica creation failed. pgsql-rm.yml The pgsql-rm.yml playbook is used to remove PostgreSQL clusters or specific instances.\nHere’s a demo of removing a PostgreSQL cluster in the sandbox environment:\nBasic Usage\n./pgsql-rm.yml -l pg-test # Remove cluster pg-test ./pgsql-rm.yml -l 10.10.10.13 # Remove instance 10.10.10.13 Command Line Arguments\nThis playbook supports the following command line arguments:\n./pgsql-rm.yml -l pg-test # Remove cluster pg-test -e pg_safeguard=false # Safeguard switch, disabled by default; override when enabled -e pg_rm_data=true # Whether to remove PostgreSQL data directory, default: remove -e pg_rm_backup=true # Whether to remove pgBackRest backup (primary only), default: remove -e pg_rm_pkg=false # Whether to uninstall PostgreSQL packages, default: keep Wrapper Scripts\nbin/pgsql-rm pg-meta # Remove pgsql cluster pg-meta bin/pgsql-rm pg-test 10.10.10.13 # Remove instance 10.10.10.13 from cluster pg-test Subtasks\nThis playbook contains the following subtasks:\n# pg_safeguard : abort if pg_safeguard is enabled # # pg_monitor : remove registration from monitoring system # - pg_deregister : remove pg monitoring targets from infra # - rm_metrics : remove monitoring targets from prometheus # - rm_ds : remove datasource from grafana # - rm_logs : remove log targets from vector # - pg_exporter : remove pg_exporter # - pgbouncer_exporter : remove pgbouncer_exporter # - pgbackrest_exporter: remove pgbackrest_exporter # # pg_access : remove pg service access layer # - dns : remove pg DNS records # - vip : remove vip-manager # - pg_service : remove pg service from haproxy # - pgbouncer : remove pgbouncer connection middleware # # postgres : remove postgres instances # - pg_replica : remove all replicas # - pg_primary : remove primary # - pg_meta : remove metadata from etcd # # pg_backup : remove backup repo (disable with pg_rm_backup=false) # pg_data : remove postgres data (disable with pg_rm_data=false) # pg_pkg : uninstall pg packages (enable with pg_rm_pkg=true) # - pg_ext : uninstall postgres extensions alone Related Administration Tasks\nRemove Instance Remove Cluster Notes\nDo not run this playbook on a primary that still has replicas—otherwise, remaining replicas will trigger automatic failover. Always remove all replicas first, then remove the primary. This is not a concern when removing the entire cluster at once. Refresh cluster services after removing instances. When you remove a replica from a cluster, it remains in the load balancer configuration file. Since health checks will fail, the removed instance won’t affect cluster services. However, you should Reload Service at an appropriate time to ensure consistency between the production environment and configuration inventory. pgsql-user.yml The pgsql-user.yml playbook is used to add new business users to existing PostgreSQL clusters.\nBasic Usage\n./pgsql-user.yml -l pg-meta -e username=dbuser_meta Wrapper Scripts\nbin/pgsql-user pg-meta dbuser_meta # Create user dbuser_meta on cluster pg-meta Workflow\nDefine user in the config inventory: all.children.\u003cpg_cluster\u003e.vars.pg_users[i] Execute playbook specifying cluster and username: pgsql-user.yml -l \u003cpg_cluster\u003e -e username=\u003cname\u003e The playbook will:\nGenerate user creation SQL at /pg/tmp/pg-user-{{ user.name }}.sql Execute user creation/update SQL on the cluster primary Update /etc/pgbouncer/userlist.txt and useropts.txt Reload pgbouncer to apply configuration User Definition Example\npg_users: - name: dbuser_meta # Required, username is the only mandatory field password: DBUser.Meta # Optional, can be scram-sha-256 hash or plaintext login: true # Optional, can login, default: true superuser: false # Optional, is superuser, default: false createdb: false # Optional, can create database, default: false createrole: false # Optional, can create role, default: false inherit: true # Optional, inherit privileges, default: true replication: false # Optional, can replicate, default: false bypassrls: false # Optional, bypass RLS, default: false pgbouncer: true # Optional, add to pgbouncer userlist, default: false connlimit: -1 # Optional, connection limit, -1 means unlimited expire_in: 3650 # Optional, expire in N days (overrides expire_at) expire_at: '2030-12-31' # Optional, specify expiration date comment: pigsty admin user # Optional, user comment roles: [dbrole_admin] # Optional, roles to grant parameters: {} # Optional, role-level parameters pool_mode: transaction # Optional, pgbouncer user-level pool mode pool_connlimit: -1 # Optional, user-level max connections For details, see: Admin SOP: Create User\npgsql-db.yml The pgsql-db.yml playbook is used to add new business databases to existing PostgreSQL clusters.\nBasic Usage\n./pgsql-db.yml -l pg-meta -e dbname=meta Wrapper Scripts\nbin/pgsql-db pg-meta meta # Create database meta on cluster pg-meta Workflow\nDefine database in the config inventory: all.children.\u003cpg_cluster\u003e.vars.pg_databases[i] Execute playbook specifying cluster and database name: pgsql-db.yml -l \u003cpg_cluster\u003e -e dbname=\u003cname\u003e The playbook will:\nGenerate database creation SQL at /pg/tmp/pg-db-{{ database.name }}.sql Execute database creation/update SQL on the cluster primary If db.register_datasource is true, register database as Grafana datasource Update /etc/pgbouncer/database.txt and reload pgbouncer Database Definition Example\npg_databases: - name: meta # Required, database name is the only mandatory field baseline: cmdb.sql # Optional, database initialization SQL file path pgbouncer: true # Optional, add to pgbouncer, default: true schemas: [pigsty] # Optional, additional schemas to create extensions: # Optional, extensions to install - { name: postgis, schema: public } - { name: timescaledb } comment: pigsty meta database # Optional, database comment owner: postgres # Optional, database owner template: template1 # Optional, template database encoding: UTF8 # Optional, character encoding locale: C # Optional, locale setting tablespace: pg_default # Optional, default tablespace allowconn: true # Optional, allow connections revokeconn: false # Optional, revoke public connect privilege register_datasource: true # Optional, register as Grafana datasource connlimit: -1 # Optional, connection limit pool_mode: transaction # Optional, pgbouncer pool mode pool_size: 64 # Optional, pgbouncer pool size pool_size_reserve: 32 # Optional, pgbouncer reserve pool size For details, see: Admin SOP: Create Database\npgsql-monitor.yml The pgsql-monitor.yml playbook is used to bring remote PostgreSQL instances into Pigsty’s monitoring system.\nBasic Usage\n./pgsql-monitor.yml -e clsname=pg-foo # Monitor remote cluster pg-foo Wrapper Scripts\nbin/pgmon-add pg-foo # Monitor a remote pgsql cluster pg-foo bin/pgmon-add pg-foo pg-bar # Monitor multiple clusters simultaneously Configuration\nFirst, define pg_exporters in the infra group variables:\ninfra: hosts: 10.10.10.10: pg_exporters: # List all remote instances, assign unique unused local ports 20001: { pg_cluster: pg-foo, pg_seq: 1, pg_host: 10.10.10.10 } 20002: { pg_cluster: pg-foo, pg_seq: 2, pg_host: 10.10.10.11 } Architecture Diagram\n------ infra ------ | | | prometheus | v---- pg-foo-1 ----v | ^ | metrics | ^ | | pg_exporter \u003c-|------------|---- postgres | | (port: 20001) | | 10.10.10.10:5432 | | ^ | ^------------------^ | ^ | ^ | ^ | v---- pg-foo-2 ----v | ^ | metrics | ^ | | pg_exporter \u003c-|------------|---- postgres | | (port: 20002) | | 10.10.10.11:5433 | ------------------- ^------------------^ Configurable Parameters\npg_exporter_config: pg_exporter.yml # pg_exporter config file name pg_exporter_cache_ttls: '1,10,60,300' # pg_exporter collector TTL stages pg_exporter_port: 9630 # pg_exporter listen port pg_exporter_params: 'sslmode=disable' # DSN extra URL parameters pg_exporter_url: '' # Directly override auto-generated DSN pg_exporter_auto_discovery: true # Enable auto database discovery pg_exporter_exclude_database: 'template0,template1,postgres' # Databases to exclude pg_exporter_include_database: '' # Databases to include only pg_exporter_connect_timeout: 200 # Connection timeout (milliseconds) pg_monitor_username: dbuser_monitor # Monitor username pg_monitor_password: DBUser.Monitor # Monitor password Remote Database Setup\nRemote PostgreSQL instances need a monitoring user:\nCREATE USER dbuser_monitor; COMMENT ON ROLE dbuser_monitor IS 'system monitor user'; ALTER USER dbuser_monitor PASSWORD 'DBUser.Monitor'; GRANT pg_monitor TO dbuser_monitor; CREATE EXTENSION IF NOT EXISTS \"pg_stat_statements\" WITH SCHEMA \"monitor\"; Limitations\nOnly postgres metrics available node, pgbouncer, patroni, haproxy metrics not available For details, see: Admin SOP: Monitor RDS\npgsql-migration.yml The pgsql-migration.yml playbook generates migration manuals and scripts for zero-downtime logical replication-based migration of existing PostgreSQL clusters.\nBasic Usage\n./pgsql-migration.yml -e@files/migration/pg-meta.yml Workflow\nDefine migration task configuration file (e.g., files/migration/pg-meta.yml) Execute playbook to generate migration manual and scripts Follow the manual to execute scripts step by step for migration Migration Task Definition Example\n# files/migration/pg-meta.yml context_dir: ~/migration # Migration manual and scripts output directory src_cls: pg-meta # Source cluster name (required) src_db: meta # Source database name (required) src_ip: 10.10.10.10 # Source cluster primary IP (required) dst_cls: pg-test # Target cluster name (required) dst_db: test # Target database name (required) dst_ip: 10.10.10.11 # Target cluster primary IP (required) # Optional parameters pg_dbsu: postgres pg_replication_username: replicator pg_replication_password: DBUser.Replicator pg_admin_username: dbuser_dba pg_admin_password: DBUser.DBA pg_monitor_username: dbuser_monitor pg_monitor_password: DBUser.Monitor For details, see: Admin SOP: Migrate Cluster\npgsql-pitr.yml The pgsql-pitr.yml playbook performs PostgreSQL Point-In-Time Recovery (PITR).\nBasic Usage\n# Recover to latest state (end of WAL archive stream) ./pgsql-pitr.yml -l pg-meta -e '{\"pg_pitr\": {}}' # Recover to specific point in time ./pgsql-pitr.yml -l pg-meta -e '{\"pg_pitr\": {\"time\": \"2025-07-13 10:00:00+00\"}}' # Recover to specific LSN ./pgsql-pitr.yml -l pg-meta -e '{\"pg_pitr\": {\"lsn\": \"0/4001C80\"}}' # Recover to specific transaction ID ./pgsql-pitr.yml -l pg-meta -e '{\"pg_pitr\": {\"xid\": \"250000\"}}' # Recover to named restore point ./pgsql-pitr.yml -l pg-meta -e '{\"pg_pitr\": {\"name\": \"some_restore_point\"}}' # Recover from another cluster's backup ./pgsql-pitr.yml -l pg-test -e '{\"pg_pitr\": {\"cluster\": \"pg-meta\"}}' PITR Task Parameters\npg_pitr: # Define PITR task cluster: \"pg-meta\" # Source cluster name (for restoring from another cluster's backup) type: latest # Recovery target type: time, xid, name, lsn, immediate, latest time: \"2025-01-01 10:00:00+00\" # Recovery target: point in time name: \"some_restore_point\" # Recovery target: named restore point xid: \"100000\" # Recovery target: transaction ID lsn: \"0/3000000\" # Recovery target: log sequence number set: latest # Backup set to restore from, default: latest timeline: latest # Target timeline, can be integer, default: latest exclusive: false # Exclude target point, default: false action: pause # Post-recovery action: pause, promote, shutdown archive: false # Keep archive settings, default: false backup: false # Backup existing data to /pg/data-backup before restore? default: false db_include: [] # Include only these databases db_exclude: [] # Exclude these databases link_map: {} # Tablespace link mapping process: 4 # Parallel recovery processes repo: {} # Recovery source repo configuration data: /pg/data # Recovery data directory port: 5432 # Recovery instance listen port Subtasks\nThis playbook contains the following subtasks:\n# down : stop HA and shutdown patroni and postgres # - pause : pause patroni auto failover # - stop : stop patroni and postgres services # - stop_patroni : stop patroni service # - stop_postgres : stop postgres service # # pitr : execute PITR recovery process # - config : generate pgbackrest config and recovery script # - backup : perform optional backup to original data # - restore : run pgbackrest restore command # - recovery : start postgres and complete recovery # - verify : verify recovered cluster control data # # up : start postgres/patroni and restore HA # - etcd : clean etcd metadata before startup # - start : start patroni and postgres services # - start_postgres : start postgres service # - start_patroni : start patroni service # - resume : resume patroni auto failover Recovery Target Types\nType Description Example latest Recover to end of WAL archive stream (latest state) {\"pg_pitr\": {}} time Recover to specific point in time {\"pg_pitr\": {\"time\": \"2025-07-13 10:00:00\"}} xid Recover to specific transaction ID {\"pg_pitr\": {\"xid\": \"250000\"}} name Recover to named restore point {\"pg_pitr\": {\"name\": \"before_ddl\"}} lsn Recover to specific LSN {\"pg_pitr\": {\"lsn\": \"0/4001C80\"}} immediate Stop immediately after reaching consistent state {\"pg_pitr\": {\"type\": \"immediate\"}} For details, see: Backup \u0026 Recovery Tutorial\n","categories":["Task"],"description":"How to manage PostgreSQL clusters with Ansible playbooks","excerpt":"How to manage PostgreSQL clusters with Ansible playbooks","ref":"/docs/pgsql/playbook/","tags":"","title":"Playbook"},{"body":"Pigsty provides 440+ extensions, covering 16 major categories including time-series, geospatial, vector, full-text search, analytics, and feature enhancements, ready to use out-of-the-box.\nUsing extensions in Pigsty involves four core steps: Download, Install, Config/Load, and Create.\npg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta pg_databases: - name: meta extensions: [ postgis, timescaledb, vector ] # Create: Create extensions in database pg_libs: 'timescaledb, pg_stat_statements, auto_explain' # Config: Preload extension libraries pg_extensions: [ postgis, timescaledb, pgvector ] # Install: Install extension packages ","categories":["Reference"],"description":"Harness the synergistic power of PostgreSQL extensions","excerpt":"Harness the synergistic power of PostgreSQL extensions","ref":"/docs/pgsql/ext/","tags":["Extension"],"title":"Extensions"},{"body":"If a classic 3-node HA deployment experiences simultaneous failure of two nodes (majority), the system typically cannot complete automatic failover and requires manual intervention.\nFirst, assess the status of the other two servers. If they can be brought up quickly, prioritize recovering those two servers. Otherwise, enter the Emergency Recovery Procedure.\nThe Emergency Recovery Procedure assumes your admin node has failed and only a single regular database node survives. In this case, the fastest recovery process is:\nAdjust HAProxy configuration to direct traffic to the primary. Stop Patroni and manually promote the PostgreSQL replica to primary. Adjust HAProxy Configuration If you access the cluster bypassing HAProxy, you can skip this step. If you access the database cluster through HAProxy, you need to adjust the load balancer configuration to manually direct read/write traffic to the primary.\nEdit the /etc/haproxy/\u003cpg_cluster\u003e-primary.cfg configuration file, where \u003cpg_cluster\u003e is your PostgreSQL cluster name, e.g., pg-meta. Comment out the health check configuration options to stop health checks. Comment out the other two failed machines in the server list, keeping only the current primary server. listen pg-meta-primary bind *:5433 mode tcp maxconn 5000 balance roundrobin # Comment out the following four health check lines #option httpchk # \u003c---- remove this #option http-keep-alive # \u003c---- remove this #http-check send meth OPTIONS uri /primary # \u003c---- remove this #http-check expect status 200 # \u003c---- remove this default-server inter 3s fastinter 1s downinter 5s rise 3 fall 3 on-marked-down shutdown-sessions slowstart 30s maxconn 3000 maxqueue 128 weight 100 server pg-meta-1 10.10.10.10:6432 check port 8008 weight 100 # Comment out the other two failed machines #server pg-meta-2 10.10.10.11:6432 check port 8008 weight 100 \u003c---- comment this #server pg-meta-3 10.10.10.12:6432 check port 8008 weight 100 \u003c---- comment this After adjusting the configuration, don’t rush to execute systemctl reload haproxy to reload. Wait until after promoting the primary, then execute together. The effect of this configuration is that HAProxy will no longer perform primary health checks (which by default use Patroni), but will directly direct write traffic to the current primary.\nManually Promote Replica Log in to the target server, switch to the dbsu user, execute CHECKPOINT to flush to disk, stop Patroni, restart PostgreSQL, and execute Promote.\nsudo su - postgres # Switch to database dbsu user psql -c 'checkpoint; checkpoint;' # Two Checkpoints to flush dirty pages, avoid long PG restart sudo systemctl stop patroni # Stop Patroni pg-restart # Restart PostgreSQL pg-promote # Promote PostgreSQL replica to primary psql -c 'SELECT pg_is_in_recovery();' # If result is f, it has been promoted to primary If you adjusted the HAProxy configuration above, you can now execute systemctl reload haproxy to reload the HAProxy configuration and direct traffic to the new primary.\nsystemctl reload haproxy # Reload HAProxy configuration to direct write traffic to current instance Avoid Split Brain After emergency recovery, the second priority is: Avoid Split Brain. Users should prevent the other two servers from coming back online and forming a split brain with the current primary, causing data inconsistency.\nSimple approaches:\nPower off/disconnect network the other two servers to ensure they don’t come online uncontrollably. Adjust the database connection string used by applications to point directly to the surviving server’s primary. Then decide the next steps based on the specific situation:\nA: The two servers have temporary failures (e.g., network/power outage) and can be repaired in place to continue service. B: The two failed servers have permanent failures (e.g., hardware damage) and will be removed and decommissioned. Recovery After Temporary Failure If the other two servers have temporary failures and can be repaired to continue service, follow these steps for repair and rebuild:\nHandle one failed server at a time, prioritize the admin node / INFRA node. Start the failed server and stop Patroni after startup. After the ETCD cluster quorum is restored, it will resume work. Then start Patroni on the surviving server (current primary) to take over the existing PostgreSQL and regain cluster leadership. After Patroni starts, enter maintenance mode.\nsystemctl restart patroni pg pause \u003cpg_cluster\u003e On the other two instances, create the touch /pg/data/standby.signal marker file as the postgres user to mark them as replicas, then start Patroni:\nsystemctl restart patroni After confirming Patroni cluster identity/roles are correct, exit maintenance mode:\npg resume \u003cpg_cluster\u003e Recovery After Permanent Failure After permanent failure, first recover the ~/pigsty directory on the admin node. The key files needed are pigsty.yml and files/pki/ca/ca.key.\nIf you cannot retrieve or don’t have backups of these two files, you can deploy a new Pigsty and migrate the existing cluster to the new deployment via Backup Cluster.\nPlease regularly backup the pigsty directory (e.g., using Git for version control). Learn from this and avoid such mistakes in the future.\nConfiguration Repair You can use the surviving node as the new admin node, copy the ~/pigsty directory to the new admin node, then start adjusting the configuration. For example, replace the original default admin node 10.10.10.10 with the surviving node 10.10.10.12:\nall: vars: admin_ip: 10.10.10.12 # Use new admin node address node_etc_hosts: [10.10.10.12 h.pigsty a.pigsty p.pigsty g.pigsty sss.pigsty] infra_portal: {} # Also modify other configs referencing old admin IP (10.10.10.10) children: infra: # Adjust Infra cluster hosts: # 10.10.10.10: { infra_seq: 1 } # Old Infra node 10.10.10.12: { infra_seq: 3 } # New Infra node etcd: # Adjust ETCD cluster hosts: #10.10.10.10: { etcd_seq: 1 } # Comment out this failed node #10.10.10.11: { etcd_seq: 2 } # Comment out this failed node 10.10.10.12: { etcd_seq: 3 } # Keep surviving node vars: etcd_cluster: etcd pg-meta: # Adjust PGSQL cluster configuration hosts: #10.10.10.10: { pg_seq: 1, pg_role: primary } #10.10.10.11: { pg_seq: 2, pg_role: replica } #10.10.10.12: { pg_seq: 3, pg_role: replica , pg_offline_query: true } 10.10.10.12: { pg_seq: 3, pg_role: primary , pg_offline_query: true } vars: pg_cluster: pg-meta ETCD Repair Then execute the following command to reset ETCD to a single-node cluster:\n./etcd.yml -e etcd_safeguard=false -e etcd_clean=true Follow the instructions in ETCD Reload Configuration to adjust ETCD Endpoint references.\nINFRA Repair If the surviving node doesn’t have the INFRA module, configure and install a new INFRA module on the current node. Execute the following command to deploy the INFRA module to the surviving node:\n./infra.yml -l 10.10.10.12 Repair monitoring on the current node:\n./node.yml -t node_monitor PGSQL Repair ./pgsql.yml -t pg_conf # Regenerate PG configuration files systemctl reload patroni # Reload Patroni configuration on surviving node After repairing each module, you can follow the standard expansion process to add new nodes to the cluster and restore cluster high availability.\n","categories":["Task","Concept"],"description":"HA scenario response plan: When two of three nodes fail and auto-failover doesn't work, how to recover from the emergency state?","excerpt":"HA scenario response plan: When two of three nodes fail and …","ref":"/docs/pgsql/tutorial/drill/","tags":"","title":"HA Drill: Handling 2-of-3 Node Failure"},{"body":"PostgreSQL is the world’s most advanced and popular open-source database.\nPigsty supports PostgreSQL 13 ~ 18 and provides 440 PG extensions.\nQuick Start Install Pigsty using the pgsql configuration template.\n./configure -c pgsql # Use postgres kernel ./deploy.yml # Set up everything with pigsty Most configuration templates use PostgreSQL kernel by default, for example:\nmeta : Default, postgres with core extensions (vector, postgis, timescale) rich : postgres with all extensions installed slim : postgres only, no monitoring infrastructure full : 4-node sandbox for HA demonstration pgsql : minimal postgres kernel configuration example Configuration Vanilla PostgreSQL kernel requires no special adjustments:\npg-meta: hosts: 10.10.10.10: { pg_seq: 1, pg_role: primary } vars: pg_cluster: pg-meta pg_users: - { name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [dbrole_admin ] ,comment: pigsty admin user } - { name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [dbrole_readonly] ,comment: read-only viewer } pg_databases: - { name: meta, baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [pigsty] ,extensions: [ vector ]} pg_hba_rules: - { user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes' } node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] # Full backup at 1 AM daily pg_packages: [ pgsql-main, pgsql-common ] # pg kernel and common utilities #pg_extensions: [ pg18-time ,pg18-gis ,pg18-rag ,pg18-fts ,pg18-olap ,pg18-feat ,pg18-lang ,pg18-type ,pg18-util ,pg18-func ,pg18-admin ,pg18-stat ,pg18-sec ,pg18-fdw ,pg18-sim ,pg18-etl] Version Selection To use a different PostgreSQL major version, you can configure it using the -v parameter:\n./configure -c pgsql # Default is postgresql 18, no need to specify explicitly ./configure -c pgsql -v 17 # Use postgresql 17 ./configure -c pgsql -v 16 # Use postgresql 16 ./configure -c pgsql -v 15 # Use postgresql 15 ./configure -c pgsql -v 14 # Use postgresql 14 ./configure -c pgsql -v 13 # Use postgresql 13 If a PostgreSQL cluster is already installed, you need to uninstall it before installing a new version:\n./pgsql-rm.yml # -l pg-meta Extension Ecosystem Pigsty provides a rich extension ecosystem for PostgreSQL, including:\nTime-series: timescaledb, pg_cron, periods Geospatial: postgis, h3, pgrouting Vector: pgvector, pgml, vchord Search: pg_trgm, zhparser, pgroonga Analytics: citus, pg_duckdb, pg_analytics Features: age, pg_graphql, rum Languages: plpython3u, pljava, plv8 Types: hstore, ltree, citext Utilities: http, pg_net, pgjwt Functions: pgcrypto, uuid-ossp, pg_uuidv7 Administration: pg_repack, pgagent, pg_squeeze Statistics: pg_stat_statements, pg_qualstats, auto_explain Security: pgaudit, pgcrypto, pgsodium Foreign: postgres_fdw, mysql_fdw, oracle_fdw Compatibility: orafce, babelfishpg_tds Data: pglogical, wal2json, decoderbufs For details, please refer to Extension Catalog.\n","categories":["Concept"],"description":"Vanilla PostgreSQL kernel with 440 extensions","excerpt":"Vanilla PostgreSQL kernel with 440 extensions","ref":"/docs/pgsql/kernel/postgres/","tags":"","title":"PostgreSQL"},{"body":" Supabase — Build in a weekend, Scale to millions\nSupabase is an open-source Firebase alternative that wraps PostgreSQL and provides authentication, out-of-the-box APIs, edge functions, real-time subscriptions, object storage, and vector embedding capabilities. This is a low-code all-in-one backend platform that lets you skip most backend development work, requiring only database design and frontend knowledge to quickly ship products!\nSupabase’s motto is: “Build in a weekend, Scale to millions”. Indeed, Supabase is extremely cost-effective at small to micro scales (4c8g), like a cyber bodhisattva. — But when you really scale to millions of users — you should seriously consider self-hosting Supabase — whether for functionality, performance, or cost considerations.\nPigsty provides you with a complete one-click self-hosting solution for Supabase. Self-hosted Supabase enjoys full PostgreSQL monitoring, IaC, PITR, and high availability, and compared to Supabase cloud services, it provides up to 440 out-of-the-box PostgreSQL extensions and can more fully utilize the performance and cost advantages of modern hardware.\nFor the complete self-hosting tutorial, please refer to: Supabase Self-Hosting Guide\nQuick Start Pigsty’s default supa.yml configuration template defines a single-node Supabase.\nFirst, use Pigsty’s standard installation process to install the MinIO and PostgreSQL instances required for Supabase:\ncurl -fsSL https://repo.pigsty.io/get | bash ./bootstrap # Environment check, install dependencies ./configure -c supa # Important: modify passwords and other key info in config! ./deploy.yml # Install Pigsty, deploy PGSQL and MINIO! Before deploying Supabase, please modify the Supabase parameters in the pigsty.yml config file according to your actual situation (mainly passwords!)\nThen, run docker.yml and app.yml to complete the remaining work and deploy Supabase containers:\n./docker.yml # Install Docker module ./app.yml # Start Supabase stateless components! For users in China, please configure appropriate Docker mirror sites or proxy servers to bypass GFW to pull DockerHub images. For professional subscriptions, we provide the ability to offline install Pigsty and Supabase without internet access.\nPigsty exposes web services through Nginx on the admin node/INFRA node by default. You can add DNS resolution for supa.pigsty pointing to this node locally, then access https://supa.pigsty through a browser to enter the Supabase Studio management interface.\nDefault username and password: supabase / pigsty\n","categories":["Concept"],"description":"How to self-host Supabase with Pigsty, deploy an open-source Firebase alternative with a complete backend stack in one click.","excerpt":"How to self-host Supabase with Pigsty, deploy an open-source Firebase …","ref":"/docs/pgsql/kernel/supabase/","tags":"","title":"Supabase"},{"body":"You can define an OPTIONAL L2 VIP on a PostgreSQL cluster, provided that all nodes in the cluster are in the same L2 network.\nThis VIP works on Master-Backup mode and always points to the node where the primary instance of the database cluster is located.\nThis VIP is managed by the VIP-Manager, which reads the Leader Key written by Patroni from DCS (etcd) to determine whether it is the master.\nEnable VIP Define pg_vip_enabled parameter as true in the cluster level to enable the VIP component on the cluster. You can also enable this configuration in the global configuration.\n# pgsql 3 node ha cluster: pg-test pg-test: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } # primary instance, leader of cluster 10.10.10.12: { pg_seq: 2, pg_role: replica } # replica instance, follower of leader 10.10.10.13: { pg_seq: 3, pg_role: replica, pg_offline_query: true } # replica with offline access vars: pg_cluster: pg-test # define pgsql cluster name pg_users: [{ name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] }] pg_databases: [{ name: test }] # Enable L2 VIP pg_vip_enabled: true pg_vip_address: 10.10.10.3/24 pg_vip_interface: eth1 Beware that pg_vip_address must be a valid IP address with subnet and available in the current L2 network.\nBeware that pg_vip_interface must be a valid network interface name and should be the same as the one using IPv4 address in the inventory.\nIf the network interface name is different among cluster members, users should explicitly specify the pg_vip_interface parameter for each instance, for example:\npg-test: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary , pg_vip_interface: eth0 } 10.10.10.12: { pg_seq: 2, pg_role: replica , pg_vip_interface: eth1 } 10.10.10.13: { pg_seq: 3, pg_role: replica , pg_vip_interface: ens33 } vars: pg_cluster: pg-test # define pgsql cluster name pg_users: [{ name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] }] pg_databases: [{ name: test }] # Enable L2 VIP pg_vip_enabled: true pg_vip_address: 10.10.10.3/24 #pg_vip_interface: eth1 To refresh the VIP configuration and restart the VIP-Manager, use the following command:\n./pgsql.yml -t pg_vip ","categories":["Task","Reference"],"description":"","excerpt":"You can define an OPTIONAL L2 VIP on a PostgreSQL cluster, provided …","ref":"/docs/pgsql/tutorial/pg-vip/","tags":"","title":"Bind a L2 VIP to PostgreSQL Primary with VIP-Manager"},{"body":"Citus is a PostgreSQL extension that transforms PostgreSQL into a distributed database, enabling horizontal scaling across multiple nodes to handle large amounts of data and queries.\nPatroni v3.0+ provides native high-availability support for Citus, simplifying the setup of Citus clusters. Pigsty also provides native support for this.\nWhat is Citus Patroni Citus Support Note: The current Citus version (13.0) supports PostgreSQL 17, 16, 15, and 14. Pigsty extension repo provides Citus ARM64 packages.\nCitus Cluster Pigsty natively supports Citus. See conf/citus.yml for reference.\nHere we use the Pigsty 4-node sandbox to define a Citus cluster pg-citus, which includes a 2-node coordinator cluster pg-citus0 and two Worker clusters pg-citus1 and pg-citus2.\npg-citus: hosts: 10.10.10.10: { pg_group: 0, pg_cluster: pg-citus0 ,pg_vip_address: 10.10.10.2/24 ,pg_seq: 1, pg_role: primary } 10.10.10.11: { pg_group: 0, pg_cluster: pg-citus0 ,pg_vip_address: 10.10.10.2/24 ,pg_seq: 2, pg_role: replica } 10.10.10.12: { pg_group: 1, pg_cluster: pg-citus1 ,pg_vip_address: 10.10.10.3/24 ,pg_seq: 1, pg_role: primary } 10.10.10.13: { pg_group: 2, pg_cluster: pg-citus2 ,pg_vip_address: 10.10.10.4/24 ,pg_seq: 1, pg_role: primary } vars: pg_mode: citus # pgsql cluster mode: citus pg_version: 17 # citus 13.0 supports PG 14-17 pg_shard: pg-citus # citus shard name: pg-citus pg_primary_db: citus # primary database used by citus pg_vip_enabled: true # enable vip for citus cluster pg_vip_interface: eth1 # vip interface for all members pg_dbsu_password: DBUser.Postgres # all dbsu password access for citus cluster pg_extensions: [ citus, postgis, pgvector, topn, pg_cron, hll ] # install these extensions pg_libs: 'citus, pg_cron, pg_stat_statements' # citus will be added by patroni automatically pg_users: [{ name: dbuser_citus ,password: DBUser.Citus ,pgbouncer: true ,roles: [ dbrole_admin ] }] pg_databases: [{ name: citus ,owner: dbuser_citus ,extensions: [ citus, vector, topn, pg_cron, hll ] }] pg_parameters: cron.database_name: citus citus.node_conninfo: 'sslmode=require sslrootcert=/pg/cert/ca.crt sslmode=verify-full' pg_hba_rules: - { user: 'all' ,db: all ,addr: 127.0.0.1/32 ,auth: ssl ,title: 'all user ssl access from localhost' } - { user: 'all' ,db: all ,addr: intra ,auth: ssl ,title: 'all user ssl access from intranet' } Compared to standard PostgreSQL clusters, Citus cluster configuration has some special requirements. First, you need to ensure the Citus extension is downloaded, installed, loaded, and enabled, which involves the following four parameters:\nrepo_packages: Must include the citus extension, or you need to use a PostgreSQL offline package that includes Citus. pg_extensions: Must include the citus extension, i.e., you must install the citus extension on each node. pg_libs: Must include the citus extension at the first position, though Patroni now handles this automatically. pg_databases: Define a primary database that must have the citus extension installed. Second, you need to ensure the Citus cluster is configured correctly:\npg_mode: Must be set to citus to tell Patroni to use Citus mode. pg_primary_db: Must specify the name of the primary database with citus extension, named citus here. pg_shard: Must specify a unified name as the cluster name prefix for all horizontal shard PG clusters, pg-citus here. pg_group: Must specify a shard number, integers starting from zero. 0 represents the coordinator cluster, others are Worker clusters. pg_cluster: Must correspond to the combination of pg_shard and pg_group. pg_dbsu_password: Must be set to a non-empty plaintext password, otherwise Citus will not work properly. pg_parameters: Recommended to set citus.node_conninfo to enforce SSL access and require node-to-node client certificate verification. After configuration, you can deploy the Citus cluster using pgsql.yml just like a regular PostgreSQL cluster.\nManage Citus Cluster After defining the Citus cluster, deploy it using the pgsql.yml playbook:\n./pgsql.yml -l pg-citus # Deploy Citus cluster pg-citus Using any member’s DBSU (postgres) user, you can list the Citus cluster status with patronictl (alias: pg):\n$ pg list + Citus cluster: pg-citus ----------+---------+-----------+----+-----------+--------------------+ | Group | Member | Host | Role | State | TL | Lag in MB | Tags | +-------+-------------+-------------+---------+-----------+----+-----------+--------------------+ | 0 | pg-citus0-1 | 10.10.10.10 | Leader | running | 1 | | clonefrom: true | | | | | | | | | conf: tiny.yml | | | | | | | | | spec: 20C.40G.125G | | | | | | | | | version: '16' | +-------+-------------+-------------+---------+-----------+----+-----------+--------------------+ | 1 | pg-citus1-1 | 10.10.10.11 | Leader | running | 1 | | clonefrom: true | | | | | | | | | conf: tiny.yml | | | | | | | | | spec: 10C.20G.125G | | | | | | | | | version: '16' | +-------+-------------+-------------+---------+-----------+----+-----------+--------------------+ | 2 | pg-citus2-1 | 10.10.10.12 | Leader | running | 1 | | clonefrom: true | | | | | | | | | conf: tiny.yml | | | | | | | | | spec: 10C.20G.125G | | | | | | | | | version: '16' | +-------+-------------+-------------+---------+-----------+----+-----------+--------------------+ | 2 | pg-citus2-2 | 10.10.10.13 | Replica | streaming | 1 | 0 | clonefrom: true | | | | | | | | | conf: tiny.yml | | | | | | | | | spec: 10C.20G.125G | | | | | | | | | version: '16' | +-------+-------------+-------------+---------+-----------+----+-----------+--------------------+ You can treat each horizontal shard cluster as an independent PGSQL cluster and manage them with the pg (patronictl) command. Note that when using the pg command to manage Citus clusters, you need to use the --group parameter to specify the cluster shard number:\npg list pg-citus --group 0 # Use --group 0 to specify cluster shard number Citus has a system table called pg_dist_node that records Citus cluster node information. Patroni automatically maintains this table.\nPGURL=postgres://postgres:DBUser.Postgres@10.10.10.10/citus psql $PGURL -c 'SELECT * FROM pg_dist_node;' # View node information nodeid | groupid | nodename | nodeport | noderack | hasmetadata | isactive | noderole | nodecluster | metadatasynced | shouldhaveshards --------+---------+-------------+----------+----------+-------------+----------+-----------+-------------+----------------+------------------ 1 | 0 | 10.10.10.10 | 5432 | default | t | t | primary | default | t | f 4 | 1 | 10.10.10.12 | 5432 | default | t | t | primary | default | t | t 5 | 2 | 10.10.10.13 | 5432 | default | t | t | primary | default | t | t 6 | 0 | 10.10.10.11 | 5432 | default | t | t | secondary | default | t | f You can also view user authentication information (superuser access only):\n$ psql $PGURL -c 'SELECT * FROM pg_dist_authinfo;' # View node auth info (superuser only) Then you can use a regular business user (e.g., dbuser_citus with DDL privileges) to access the Citus cluster:\npsql postgres://dbuser_citus:DBUser.Citus@10.10.10.10/citus -c 'SELECT * FROM pg_dist_node;' Using Citus Cluster When using Citus clusters, we strongly recommend reading the Citus official documentation to understand its architecture and core concepts.\nThe key is understanding the five types of tables in Citus and their characteristics and use cases:\nDistributed Table Reference Table Local Table Local Management Table Schema Table On the coordinator node, you can create distributed tables and reference tables and query them from any data node. Since 11.2, any Citus database node can act as a coordinator.\nWe can use pgbench to create some tables and distribute the main table (pgbench_accounts) across nodes, then use other small tables as reference tables:\nPGURL=postgres://dbuser_citus:DBUser.Citus@10.10.10.10/citus pgbench -i $PGURL psql $PGURL \u003c\u003c-EOF SELECT create_distributed_table('pgbench_accounts', 'aid'); SELECT truncate_local_data_after_distributing_table('public.pgbench_accounts'); SELECT create_reference_table('pgbench_branches') ; SELECT truncate_local_data_after_distributing_table('public.pgbench_branches'); SELECT create_reference_table('pgbench_history') ; SELECT truncate_local_data_after_distributing_table('public.pgbench_history'); SELECT create_reference_table('pgbench_tellers') ; SELECT truncate_local_data_after_distributing_table('public.pgbench_tellers'); EOF Run read/write tests:\npgbench -nv -P1 -c10 -T500 postgres://dbuser_citus:DBUser.Citus@10.10.10.10/citus # Direct connect to coordinator port 5432 pgbench -nv -P1 -c10 -T500 postgres://dbuser_citus:DBUser.Citus@10.10.10.10:6432/citus # Through connection pool, reduce client connection pressure pgbench -nv -P1 -c10 -T500 postgres://dbuser_citus:DBUser.Citus@10.10.10.13/citus # Any primary node can act as coordinator pgbench --select-only -nv -P1 -c10 -T500 postgres://dbuser_citus:DBUser.Citus@10.10.10.11/citus # Read-only queries Production Deployment For production use of Citus, you typically need to set up streaming replication physical replicas for the Coordinator and each Worker cluster.\nFor example, simu.yml defines a 10-node Citus cluster:\npg-citus: # citus group hosts: 10.10.10.50: { pg_group: 0, pg_cluster: pg-citus0 ,pg_vip_address: 10.10.10.60/24 ,pg_seq: 0, pg_role: primary } 10.10.10.51: { pg_group: 0, pg_cluster: pg-citus0 ,pg_vip_address: 10.10.10.60/24 ,pg_seq: 1, pg_role: replica } 10.10.10.52: { pg_group: 1, pg_cluster: pg-citus1 ,pg_vip_address: 10.10.10.61/24 ,pg_seq: 0, pg_role: primary } 10.10.10.53: { pg_group: 1, pg_cluster: pg-citus1 ,pg_vip_address: 10.10.10.61/24 ,pg_seq: 1, pg_role: replica } 10.10.10.54: { pg_group: 2, pg_cluster: pg-citus2 ,pg_vip_address: 10.10.10.62/24 ,pg_seq: 0, pg_role: primary } 10.10.10.55: { pg_group: 2, pg_cluster: pg-citus2 ,pg_vip_address: 10.10.10.62/24 ,pg_seq: 1, pg_role: replica } 10.10.10.56: { pg_group: 3, pg_cluster: pg-citus3 ,pg_vip_address: 10.10.10.63/24 ,pg_seq: 0, pg_role: primary } 10.10.10.57: { pg_group: 3, pg_cluster: pg-citus3 ,pg_vip_address: 10.10.10.63/24 ,pg_seq: 1, pg_role: replica } 10.10.10.58: { pg_group: 4, pg_cluster: pg-citus4 ,pg_vip_address: 10.10.10.64/24 ,pg_seq: 0, pg_role: primary } 10.10.10.59: { pg_group: 4, pg_cluster: pg-citus4 ,pg_vip_address: 10.10.10.64/24 ,pg_seq: 1, pg_role: replica } vars: pg_mode: citus # pgsql cluster mode: citus pg_version: 17 # citus 13.0 supports PG 14-17 pg_shard: pg-citus # citus shard name: pg-citus pg_primary_db: citus # primary database used by citus pg_vip_enabled: true # enable vip for citus cluster pg_vip_interface: eth1 # vip interface for all members pg_dbsu_password: DBUser.Postgres # enable dbsu password access for citus pg_extensions: [ citus, postgis, pgvector, topn, pg_cron, hll ] # install these extensions pg_libs: 'citus, pg_cron, pg_stat_statements' # citus will be added by patroni automatically pg_users: [{ name: dbuser_citus ,password: DBUser.Citus ,pgbouncer: true ,roles: [ dbrole_admin ] }] pg_databases: [{ name: citus ,owner: dbuser_citus ,extensions: [ citus, vector, topn, pg_cron, hll ] }] pg_parameters: cron.database_name: citus citus.node_conninfo: 'sslrootcert=/pg/cert/ca.crt sslmode=verify-full' pg_hba_rules: - { user: 'all' ,db: all ,addr: 127.0.0.1/32 ,auth: ssl ,title: 'all user ssl access from localhost' } - { user: 'all' ,db: all ,addr: intra ,auth: ssl ,title: 'all user ssl access from intranet' } We will cover a series of advanced Citus topics in subsequent tutorials:\nRead/write separation Failure handling Consistent backup and recovery Advanced monitoring and diagnostics Connection pooling ","categories":["Task","Tutorial","Concept"],"description":"How to deploy a Citus high-availability distributed cluster?","excerpt":"How to deploy a Citus high-availability distributed cluster?","ref":"/docs/pgsql/tutorial/citus/","tags":"","title":"Deploy HA Citus Cluster"},{"body":"Percona Postgres is a patched Postgres kernel with pg_tde (Transparent Data Encryption) extension.\nIt’s compatible with PostgreSQL 18.1 and available on all Pigsty-supported platforms.\nPerformance Test for Percona Transparent Data Encryption (TDE) Quick Start Use Pigsty’s standard installation process with the pgtde configuration template.\ncurl -fsSL https://repo.pigsty.io/get | bash; cd ~/pigsty; ./configure -c pgtde # Use percona postgres kernel ./deploy.yml # Set up everything with pigsty Configuration The following parameters need to be adjusted to deploy a Percona cluster:\npg-meta: hosts: 10.10.10.10: { pg_seq: 1, pg_role: primary } vars: pg_cluster: pg-meta pg_users: - { name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [dbrole_admin ] ,comment: pgsql admin user } - { name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [dbrole_readonly] ,comment: read-only viewer } pg_databases: - name: meta baseline: cmdb.sql comment: pigsty tde database schemas: [pigsty] extensions: [ vector, postgis, pg_tde ,pgaudit, { name: pg_stat_monitor, schema: monitor } ] pg_hba_rules: - { user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes' } node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] # Full backup at 1 AM daily # Percona PostgreSQL TDE specific settings pg_packages: [ percona-main, pgsql-common ] # Install percona postgres packages pg_libs: 'pg_tde, pgaudit, pg_stat_statements, pg_stat_monitor, auto_explain' Extensions Percona provides 80 available extensions, including pg_tde, pgvector, postgis, pgaudit, set_user, pg_stat_monitor, and other useful third-party extensions.\nExtension Version Description pg_tde 2.1 Percona transparent data encryption access method vector 0.8.1 Vector data type and ivfflat and hnsw access methods postgis 3.5.4 PostGIS geometry and geography types and functions pgaudit 18.0 Provides auditing functionality pg_stat_monitor 2.3 PostgreSQL query performance monitoring tool set_user 4.2.0 Similar to SET ROLE but with additional logging pg_repack 1.5.3 Reorganize tables in PostgreSQL databases with minimal locks hstore 1.8 Data type for storing sets of (key, value) pairs ltree 1.3 Data type for hierarchical tree-like structures pg_trgm 1.6 Text similarity measurement and index searching based on trigrams For the complete list of 80 extensions, please refer to the Percona Postgres official documentation.\nKey Features Transparent Data Encryption: Provides data-at-rest encryption using the pg_tde extension PostgreSQL 18 Compatible: Based on the latest PostgreSQL 18 version Enterprise Extensions: Includes enterprise-grade features like pgaudit, pg_stat_monitor Complete Ecosystem: Supports popular extensions like pgvector, PostGIS Note: Currently in stable stage - thoroughly evaluate before production use.\n","categories":["Concept"],"description":"Percona Postgres distribution with TDE transparent encryption support","excerpt":"Percona Postgres distribution with TDE transparent encryption support","ref":"/docs/pgsql/kernel/percona/","tags":"","title":"Percona"},{"body":"OpenHalo is an open-source PostgreSQL kernel that provides MySQL wire protocol compatibility.\nOpenHalo is based on PostgreSQL 14.10 kernel version and provides wire protocol compatibility with MySQL 5.7.32-log / 8.0 versions.\nPigsty provides deployment support for OpenHalo on all supported Linux platforms.\nQuick Start Use Pigsty’s standard installation process with the mysql configuration template.\ncurl -fsSL https://repo.pigsty.io/get | bash; cd ~/pigsty; ./configure -c mysql # Use MySQL (openHalo) configuration template ./deploy.yml # Install, for production deployment please modify passwords in pigsty.yml first For production deployment, ensure you modify the password parameters in the pigsty.yml configuration file before running the install playbook.\nConfiguration pg-meta: hosts: 10.10.10.10: { pg_seq: 1, pg_role: primary } vars: pg_cluster: pg-meta pg_users: - {name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [dbrole_admin] ,comment: pigsty admin user } - {name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [dbrole_readonly] ,comment: read-only viewer for meta database } pg_databases: - {name: postgres, extensions: [aux_mysql]} # mysql compatible database - {name: meta ,baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [pigsty]} pg_hba_rules: - {user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes'} node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] # Full backup at 1 AM daily # OpenHalo specific settings pg_mode: mysql # HaloDB's MySQL compatibility mode pg_version: 14 # Current HaloDB compatible PG major version 14 pg_packages: [ openhalodb, pgsql-common ] # Install openhalodb instead of postgresql kernel Usage When accessing MySQL, the actual connection uses the postgres database. Please note that the concept of “database” in MySQL actually corresponds to “Schema” in PostgreSQL. Therefore, use mysql actually uses the mysql Schema within the postgres database.\nThe username and password for MySQL are the same as in PostgreSQL. You can manage users and permissions using standard PostgreSQL methods.\nClient Access OpenHalo provides MySQL wire protocol compatibility, listening on port 3306 by default, allowing MySQL clients and drivers to connect directly.\nPigsty’s conf/mysql configuration installs the mysql client tool by default.\nYou can access MySQL using the following command:\nmysql -h 127.0.0.1 -u dbuser_dba Currently, OpenHalo officially ensures Navicat can properly access this MySQL port, but Intellij IDEA’s DataGrip access will cause errors.\nModification Notes The OpenHalo kernel installed by Pigsty is based on the HaloTech-Co-Ltd/openHalo kernel with minor modifications:\nChanged the default database name from halo0root back to postgres Removed the 1.0. prefix from the default version number, restoring it to 14.10 Modified the default configuration file to enable MySQL compatibility and listen on port 3306 by default Please note that Pigsty does not provide any warranty for using the OpenHalo kernel. Any issues or requirements encountered when using this kernel should be addressed with the original vendor.\nWarning: Currently experimental - thoroughly evaluate before production use.\n","categories":["Concept"],"description":"MySQL compatible Postgres 14 fork","excerpt":"MySQL compatible Postgres 14 fork","ref":"/docs/pgsql/kernel/openhalo/","tags":"","title":"OpenHalo"},{"body":"OrioleDB is a PostgreSQL storage engine extension that claims to provide 4x OLTP performance, no xid wraparound and table bloat issues, and “cloud-native” (data stored in S3) capabilities.\nOrioleDB’s latest version is based on a patched PostgreSQL 17.0 and an additional extension\nYou can run OrioleDB as an RDS using Pigsty. It’s compatible with PG 17 and available on all supported Linux platforms. The latest version is beta12, based on PG 17_11 patch.\nQuick Start Follow Pigsty’s standard installation process using the oriole configuration template.\ncurl -fsSL https://repo.pigsty.io/get | bash; cd ~/pigsty; ./configure -c oriole # Use OrioleDB configuration template ./deploy.yml # Install Pigsty with OrioleDB For production deployment, ensure you modify the password parameters in the pigsty.yml configuration before running the install playbook.\nConfiguration pg-meta: hosts: 10.10.10.10: { pg_seq: 1, pg_role: primary } vars: pg_cluster: pg-meta pg_users: - {name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [dbrole_admin] ,comment: pigsty admin user } - {name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [dbrole_readonly] ,comment: read-only viewer for meta database } pg_databases: - {name: meta ,baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [pigsty], extensions: [orioledb]} pg_hba_rules: - {user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes'} node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] # Full backup at 1 AM daily # OrioleDB specific settings pg_mode: oriole # oriole compatibility mode pg_packages: [ orioledb, pgsql-common ] # Install OrioleDB kernel pg_libs: 'orioledb, pg_stat_statements, auto_explain' # Load OrioleDB extension Usage To use OrioleDB, you need to install the orioledb_17 and oriolepg_17 packages (currently only RPM versions are available).\nInitialize TPC-B-like tables with pgbench using 100 warehouses:\npgbench -is 100 meta pgbench -nv -P1 -c10 -S -T1000 meta pgbench -nv -P1 -c50 -S -T1000 meta pgbench -nv -P1 -c10 -T1000 meta pgbench -nv -P1 -c50 -T1000 meta Next, you can rebuild these tables using the orioledb storage engine and observe the performance difference:\n-- Create OrioleDB tables CREATE TABLE pgbench_accounts_o (LIKE pgbench_accounts INCLUDING ALL) USING orioledb; CREATE TABLE pgbench_branches_o (LIKE pgbench_branches INCLUDING ALL) USING orioledb; CREATE TABLE pgbench_history_o (LIKE pgbench_history INCLUDING ALL) USING orioledb; CREATE TABLE pgbench_tellers_o (LIKE pgbench_tellers INCLUDING ALL) USING orioledb; -- Copy data from regular tables to OrioleDB tables INSERT INTO pgbench_accounts_o SELECT * FROM pgbench_accounts; INSERT INTO pgbench_branches_o SELECT * FROM pgbench_branches; INSERT INTO pgbench_history_o SELECT * FROM pgbench_history; INSERT INTO pgbench_tellers_o SELECT * FROM pgbench_tellers; -- Drop original tables and rename OrioleDB tables DROP TABLE pgbench_accounts, pgbench_branches, pgbench_history, pgbench_tellers; ALTER TABLE pgbench_accounts_o RENAME TO pgbench_accounts; ALTER TABLE pgbench_branches_o RENAME TO pgbench_branches; ALTER TABLE pgbench_history_o RENAME TO pgbench_history; ALTER TABLE pgbench_tellers_o RENAME TO pgbench_tellers; Key Features No XID Wraparound: Eliminates transaction ID wraparound maintenance No Table Bloat: Advanced storage management prevents table bloat Cloud Storage: Native support for S3-compatible object storage OLTP Optimized: Designed for transactional workloads Improved Performance: Better space utilization and query performance Note: Currently in Beta stage - thoroughly evaluate before production use.\n","categories":["Concept"],"description":"Next-generation OLTP engine for PostgreSQL","excerpt":"Next-generation OLTP engine for PostgreSQL","ref":"/docs/pgsql/kernel/orioledb/","tags":"","title":"OrioleDB"},{"body":"Pigsty provides four preset Patroni/PostgreSQL config templates optimized for different workloads:\nTemplate CPU Cores Use Case Characteristics oltp.yml 4-128C OLTP transactions High concurrency, low latency olap.yml 4-128C OLAP analytics Large queries, high parallelism crit.yml 4-128C Critical/Finance Data safety, audit, zero-loss tiny.yml 1-3C Tiny instances Resource-constrained envs Use pg_conf to select a template; default is oltp.yml.\nThe database tuning template pg_conf should be paired with the OS tuning template node_tune.\nUsage Set pg_conf in your cluster definition. It’s recommended to set node_tune accordingly for OS-level tuning:\npg-test: hosts: 10.10.10.11: { pg_seq: 1, pg_role: primary } 10.10.10.12: { pg_seq: 2, pg_role: replica } vars: pg_cluster: pg-test pg_conf: oltp.yml # PostgreSQL config template (default) node_tune: oltp # OS tuning template (default) For critical financial workloads, use crit.yml:\npg-finance: hosts: 10.10.10.21: { pg_seq: 1, pg_role: primary } 10.10.10.22: { pg_seq: 2, pg_role: replica } 10.10.10.23: { pg_seq: 3, pg_role: replica } vars: pg_cluster: pg-finance pg_conf: crit.yml # PostgreSQL critical template node_tune: crit # OS critical tuning For low-spec VMs or dev environments, use tiny.yml:\npg-dev: hosts: 10.10.10.31: { pg_seq: 1, pg_role: primary } vars: pg_cluster: pg-dev pg_conf: tiny.yml # PostgreSQL tiny template node_tune: tiny # OS tiny tuning Comparison The four templates differ significantly in key parameters:\nConnections \u0026 Memory Parameter OLTP OLAP CRIT TINY max_connections 500/1000 500 500/1000 250 work_mem range 64MB-1GB 64MB-8GB 64MB-1GB 16MB-256MB maintenance_work_mem 25% shmem 50% shmem 25% shmem 25% shmem max_locks_per_transaction 1-2x maxconn 2-4x maxconn 1-2x maxconn 1-2x maxconn Parallel Query Parameter OLTP OLAP CRIT TINY max_worker_processes cpu+8 cpu+12 cpu+8 cpu+4 max_parallel_workers 50% cpu 80% cpu 50% cpu 50% cpu max_parallel_workers_per_gather 20% cpu (max 8) 50% cpu 0 (off) 0 (off) parallel_setup_cost 2000 1000 2000 1000 parallel_tuple_cost 0.2 0.1 0.2 0.1 Sync Replication Parameter OLTP OLAP CRIT TINY synchronous_mode depends pg_rpo depends pg_rpo forced on depends pg_rpo data_checksums optional optional forced on optional Vacuum Config Parameter OLTP OLAP CRIT TINY vacuum_cost_delay 20ms 10ms 20ms 20ms vacuum_cost_limit 2000 10000 2000 2000 autovacuum_max_workers 3 3 3 2 Timeout \u0026 Security Parameter OLTP OLAP CRIT TINY idle_in_transaction_session_timeout 10min off 1min 10min log_min_duration_statement 100ms 1000ms 100ms 100ms default_statistics_target 400 1000 400 200 track_activity_query_size 8KB 8KB 32KB 8KB log_connections auth auth full default IO Config (PG17+) Parameter OLTP OLAP CRIT TINY io_workers 25% cpu (4-16) 50% cpu (4-32) 25% cpu (4-8) 3 temp_file_limit 1/20 disk 1/5 disk 1/20 disk 1/20 disk Selection Guide OLTP Template: Default choice for most transaction processing. Ideal for e-commerce, social, gaming apps.\nOLAP Template: For data warehouses, BI reports, ETL. Allows large queries, high parallelism, relaxed timeouts.\nCRIT Template: For financial transactions, core accounting with strict consistency/security requirements. Forced sync replication, checksums, full audit.\nTINY Template: For dev/test environments, resource-constrained VMs, Raspberry Pi. Minimizes resource usage, disables parallel queries.\nCustom Templates Create custom templates based on existing ones. Templates are in roles/pgsql/templates/:\nroles/pgsql/templates/ ├── oltp.yml # OLTP template (default) ├── olap.yml # OLAP template ├── crit.yml # CRIT critical template └── tiny.yml # TINY micro template Steps to create a custom template:\nCopy an existing template as base Modify parameters as needed Place in roles/pgsql/templates/ Reference via pg_conf Example:\ncp roles/pgsql/templates/oltp.yml roles/pgsql/templates/myapp.yml # Edit myapp.yml as needed Then use in your cluster:\npg-myapp: vars: pg_conf: myapp.yml Templates use Jinja2 syntax; parameters are dynamically computed based on node resources (CPU, memory, disk).\nTuning Strategy For technical details on template parameter optimization, see Tuning Strategy:\nMemory tuning (shared buffers, work mem, max connections) CPU tuning (parallel query worker config) Storage tuning (WAL size, temp file limits) Manual parameter adjustment Related Parameters pg_conf: PostgreSQL config template node_tune: OS tuning template, should match pg_conf pg_rto: Recovery time objective, affects failover timeout pg_rpo: Recovery point objective, affects sync replication pg_max_conn: Override template max connections pg_shared_buffer_ratio: Shared buffer memory ratio pg_storage_type: Storage type, affects IO params ","categories":["Reference"],"description":"Use Pigsty's built-in Patroni config templates or customize your own","excerpt":"Use Pigsty's built-in Patroni config templates or customize your own","ref":"/docs/pgsql/template/","tags":"","title":"Param Templates"},{"body":"In Pigsty, you can replace the “native PG kernel” with different “flavors” of PostgreSQL forks to achieve special features and effects.\nPigsty supports various PostgreSQL kernels and compatible forks, enabling you to simulate different database systems while leveraging PostgreSQL’s ecosystem. Each kernel provides unique capabilities and compatibility layers.\nKernel Key Feature Description PostgreSQL Original Flavor Vanilla PostgreSQL with 440 extensions Citus Horizontal Scaling Distributed PostgreSQL via native extension WiltonDB SQL Server Compatible SQL Server wire-protocol compatibility IvorySQL Oracle Compatible Oracle syntax and PL/SQL compatibility OpenHalo MySQL Compatible MySQL wire-protocol compatibility Percona Transparent Encryption Percona Distribution with pg_tde FerretDB MongoDB Migration MongoDB wire-protocol compatibility OrioleDB OLTP Optimization Zheap, No bloat, S3 Storage PolarDB Aurora-style RAC RAC, China domestic compliance Supabase Backend as a Service BaaS based on PostgreSQL, Firebase alternative Cloudberry MPP DW \u0026 Analytics Massively parallel processing data warehouse ","categories":["Reference","Concept"],"description":"How to use other PostgreSQL kernel forks in Pigsty? Such as Citus, Babelfish, IvorySQL, PolarDB, etc.","excerpt":"How to use other PostgreSQL kernel forks in Pigsty? Such as Citus, …","ref":"/docs/pgsql/kernel/","tags":["Kernel"],"title":"PG Kernels"},{"body":"Using extensions in Pigsty requires four steps: Download, Install, Config, and Create.\nDownload: Download extension packages to the local repository (Pigsty has already downloaded mainstream extensions by default) Install: Install extension packages on cluster nodes Config: Some extensions need to be preloaded or configured with parameters Create: Execute CREATE EXTENSION in the database to create the extension Declarative Configuration Declare extensions in the Pigsty configuration manifest, and they will be automatically installed and created during cluster initialization:\npg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta pg_databases: - name: meta extensions: [ postgis, timescaledb, vector ] # Create extensions in database pg_libs: 'timescaledb, pg_stat_statements, auto_explain' # Preload extension libraries pg_extensions: [ postgis, timescaledb, pgvector ] # Install extension packages After executing ./pgsql.yml to initialize the cluster, the three extensions postgis, timescaledb, and vector will be available in the meta database.\nImperative Operations For existing clusters, you can add extensions using command-line methods:\n# 1. Install extension packages ./pgsql.yml -l pg-meta -t pg_extension -e '{\"pg_extensions\":[\"pgvector\"]}' # 2. Preload extension (if needed, requires restart after modification) pg edit-config pg-meta --force -p shared_preload_libraries='timescaledb, pg_stat_statements, auto_explain' # 3. Create extension in database psql -d meta -c 'CREATE EXTENSION vector;' You can also use the pig package manager to install directly:\npig install pgvector # Install extension package pig extension create vector # Create extension in database Process Quick Reference Step Parameter/Command Description Download repo_extra_packages Specify extension packages to download to local repository Install pg_extensions Specify extension packages to install on cluster Config pg_libs Preload extensions to shared_preload_libraries Create pg_databases.extensions Automatically execute CREATE EXTENSION in database For detailed instructions, please refer to each subsection: Download, Install, Config, Create\n","categories":["Tutorial"],"description":"Four-step process overview for using extensions","excerpt":"Four-step process overview for using extensions","ref":"/docs/pgsql/ext/start/","tags":["Extension"],"title":"Quick Start"},{"body":"Extensions are the soul of PostgreSQL. Pigsty includes 440+ pre-compiled, out-of-the-box extension plugins, fully unleashing PostgreSQL’s potential.\nWhat are Extensions PostgreSQL extensions are a modular mechanism that allows enhancing database functionality without modifying the core code. An extension typically consists of three parts:\nControl file (.control): Required, contains extension metadata SQL scripts (.sql): Optional, defines functions, types, operators, and other database objects Dynamic library (.so): Optional, provides high-performance functionality implemented in C Extensions can add to PostgreSQL: new data types, index methods, functions and operators, foreign data access, procedural languages, performance monitoring, security auditing, and more.\nCore Extensions Among the extensions included in Pigsty, the following are most representative:\nExtension Description PostGIS Geospatial data types and indexes, de facto GIS standard TimescaleDB Time-series database with continuous aggregates, columnar storage, auto-compression PGVector Vector data type with HNSW/IVFFlat indexes, essential for AI applications Citus Distributed database with horizontal sharding capabilities pg_duckdb Embedded DuckDB analytical engine for OLAP acceleration ParadeDB ElasticSearch-level full-text search capabilities Apache AGE Graph database supporting OpenCypher query language pg_graphql Native GraphQL query support Most extensions can coexist and even be combined, creating synergistic effects far greater than the sum of their parts.\nExtension Categories Pigsty organizes extensions into 16 categories:\nCategory Alias Description Typical Extensions Time-series time Time-series data processing timescaledb, pg_cron, periods Geospatial gis Geospatial data postgis, h3, pgrouting Vector rag Vector retrieval and AI pgvector, vchord, pg_vectorize Search fts Full-text search pgroonga, zhparser, pg_bigm Analytics olap OLAP and analytics pg_duckdb, pg_mooncake, citus Feature feat Feature enhancements age, pg_graphql, hll, rum Language lang Procedural languages plpython3u, pljava, plv8 Type type Data types hstore, ltree, ip4r Utility util Utility tools http, pg_net, pgjwt Function func Function libraries pg_uuidv7, topn, tdigest Admin admin Operations management pg_repack, pg_squeeze, pgagent Stat stat Monitoring statistics pg_stat_statements, pg_qualstats, auto_explain Security sec Security auditing pgaudit, pgsodium, pg_tde FDW fdw Foreign data access postgres_fdw, mysql_fdw, oracle_fdw Compatibility sim Database compatibility orafce, babelfish ETL etl Data synchronization pglogical, wal2json, decoderbufs You can batch install an entire category of extensions using category aliases, for example: pg_extensions: [ pgsql-gis, pgsql-rag ].\nPredefined Extension Stacks Pigsty provides several predefined extension stacks for convenient scenario-based selection:\nStack Included Extensions gis-stack postgis, pgrouting, pointcloud, h3, q3c, ogr_fdw rag-stack pgvector, vchord, pgvectorscale, pg_similarity, pg_tiktoken fts-stack pgroonga, pg_bigm, zhparser, hunspell olap-stack pg_duckdb, pg_mooncake, timescaledb, pg_partman, plproxy feat-stack age, hll, rum, pg_graphql, pg_jsonschema, jsquery stat-stack pg_show_plans, pg_stat_kcache, pg_qualstats, pg_wait_sampling supa-stack pg_graphql, pg_jsonschema, wrappers, pgvector, pgsodium, vault Simply use these names in pg_extensions to install the entire stack.\nExtension Resources Extension Catalog: Browse detailed information about all available extensions Extension Repository: Pigsty extension software repository pig Package Manager: Command-line extension management tool GitHub Pigsty: Pigsty source code repository ","categories":["Reference"],"description":"Core concepts of PostgreSQL extensions and the Pigsty extension ecosystem","excerpt":"Core concepts of PostgreSQL extensions and the Pigsty extension …","ref":"/docs/pgsql/ext/intro/","tags":["Extension"],"title":"Introduction"},{"body":"Pigsty uses a package alias mechanism to simplify extension installation and management.\nPackage Alias Mechanism Managing extensions involves multiple layers of name mapping:\nLayer Example pgvector Example postgis Extension Name vector postgis, postgis_topology, … Package Alias pgvector postgis RPM Package Name pgvector_18 postgis36_18* DEB Package Name postgresql-18-pgvector postgresql-18-postgis-3* Pigsty provides a package alias abstraction layer, so users don’t need to worry about specific RPM/DEB package names:\npg_extensions: [ pgvector, postgis, timescaledb ] # Use package aliases Pigsty automatically translates to the correct package names based on the operating system and PostgreSQL version.\nNote: When using CREATE EXTENSION, you use the extension name (e.g., vector), not the package alias (pgvector).\nCategory Aliases All extensions are organized into 16 categories, which can be batch installed using category aliases:\n# Use generic category aliases (auto-adapt to current PG version) pg_extensions: [ pgsql-gis, pgsql-rag, pgsql-fts ] # Or use version-specific category aliases pg_extensions: [ pg18-gis, pg18-rag, pg18-fts ] Except for the olap category, all category extensions can be installed simultaneously. Within the olap category, there are conflicts: pg_duckdb and pg_mooncake are mutually exclusive.\nCategory List Category Description Typical Extensions time Time-series timescaledb, pg_cron, periods gis Geospatial postgis, h3, pgrouting rag Vector/RAG pgvector, pgml, vchord fts Full-text Search pg_trgm, zhparser, pgroonga olap Analytics citus, pg_duckdb, pg_analytics feat Feature age, pg_graphql, rum lang Language plpython3u, pljava, plv8 type Data Type hstore, ltree, citext util Utility http, pg_net, pgjwt func Function pgcrypto, uuid-ossp, pg_uuidv7 admin Admin pg_repack, pgagent, pg_squeeze stat Statistics pg_stat_statements, pg_qualstats, auto_explain sec Security pgaudit, pgcrypto, pgsodium fdw Foreign Data Wrapper postgres_fdw, mysql_fdw, oracle_fdw sim Compatibility orafce, babelfishpg_tds etl Data/ETL pglogical, wal2json, decoderbufs Browse Extension Catalog You can browse detailed information about all available extensions on the Pigsty Extension Catalog website, including:\nExtension name, description, version Supported PostgreSQL versions Supported OS distributions Installation methods, preloading requirements License, source repository ","categories":["Reference"],"description":"Extension package aliases and category naming conventions","excerpt":"Extension package aliases and category naming conventions","ref":"/docs/pgsql/ext/pkg/","tags":["Extension"],"title":"Packages"},{"body":"Before installing extensions, ensure that extension packages are downloaded to the local repository or available from upstream.\nDefault Behavior Pigsty automatically downloads mainstream extensions available for the default PostgreSQL version to the local software repository during installation.\nBenefits of using a local repository:\nAccelerated installation, avoiding repeated downloads Reduced network traffic consumption Improved delivery reliability Ensured version consistency Download New Extensions To download additional extensions, add them to repo_extra_packages and rebuild the repository:\nall: vars: repo_extra_packages: [ pgvector, postgis, timescaledb, pg_duckdb ] # Re-download packages to local repository ./infra.yml -t repo_build # Refresh package source cache on all nodes ./node.yml -t node_repo Using Upstream Repositories You can also install directly from internet upstream repositories without pre-downloading:\n# Add upstream software sources on nodes ./node.yml -t node_repo -e node_repo_modules=node,pgsql This approach is suitable for:\nQuick testing of latest versions Installing rare extensions Environments with good network conditions But may face:\nNetwork instability affecting installation Version inconsistency risks Extension Sources Extension packages come from two main sources:\nRepository Description PGDG PostgreSQL official repository, providing core extensions Pigsty Pigsty supplementary repository, providing additional extensions The Pigsty repository only includes extensions not present in the PGDG repository. Once an extension enters the PGDG repository, the Pigsty repository will remove it or keep it consistent.\nRepository URLs:\nPGDG YUM: https://download.postgresql.org/pub/repos/yum/ PGDG APT: https://apt.postgresql.org/pub/repos/apt/ Pigsty YUM: https://repo.pigsty.io/yum/ Pigsty APT: https://repo.pigsty.io/apt/ For detailed repository configuration, see Extension Repository.\n","categories":["Reference"],"description":"Download extension packages from software repositories to local","excerpt":"Download extension packages from software repositories to local","ref":"/docs/pgsql/ext/download/","tags":["Extension"],"title":"Download"},{"body":"Pigsty uses the operating system’s package manager (yum/apt) to install extension packages.\nRelated Parameters Two parameters are used to specify extensions to install:\nParameter Purpose Default Behavior pg_packages Global common packages Ensure present (no upgrade) pg_extensions Cluster-specific extensions Install latest version pg_packages is typically used to specify base components needed by all clusters (PostgreSQL kernel, Patroni, pgBouncer, etc.) and essential extensions.\npg_extensions is used to specify extensions needed by specific clusters.\npg_packages: # Global base packages - pgsql-main pgsql-common pg_extensions: # Cluster extensions - postgis timescaledb pgvector Install During Cluster Initialization Declare extensions in cluster configuration, and they will be automatically installed during initialization:\npg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta pg_extensions: [ postgis, timescaledb, pgvector, pg_duckdb ] When executing ./pgsql.yml to initialize the cluster, extensions will be automatically installed.\nInstall Extensions on Existing Cluster For initialized clusters, there are multiple ways to install extensions:\nUsing Pigsty Playbook # Install using playbook after modifying configuration ./pgsql.yml -l pg-meta -t pg_extension # Or specify extensions directly on command line ./pgsql.yml -l pg-meta -t pg_extension -e '{\"pg_extensions\":[\"pg_duckdb\"]}' Using pig Package Manager # Install extension using pig pig install pg_duckdb # Batch install ansible pg-meta -b -a 'pig install pg_duckdb pgvector' Using Package Manager Directly # EL systems sudo yum install -y pg_duckdb_18* # Debian/Ubuntu systems sudo apt install -y postgresql-18-pg-duckdb Using Package Aliases Pigsty supports using standardized package aliases, automatically translating to package names for the corresponding PG version:\npg_extensions: - pgvector # Auto-translates to pgvector_18* (EL) or postgresql-18-pgvector (Debian) - postgis # Auto-translates to postgis36_18* (EL) or postgresql-18-postgis-3* (Debian) - pgsql-gis # Category alias, installs entire GIS category of extensions You can also use raw package names directly:\npg_extensions: - pgvector_18* # EL system raw package name - postgresql-18-pgvector # Debian system raw package name For package alias definitions, see:\nEL8 Extension List EL9 Extension List D12 Extension List U22 Extension List U24 Extension List Verify Installation After installation, verify in the database:\n-- Check installed extensions SELECT * FROM pg_available_extensions WHERE name = 'vector'; -- Check if extension files exist \\dx ","categories":["Tutorial"],"description":"Install extension packages on cluster nodes","excerpt":"Install extension packages on cluster nodes","ref":"/docs/pgsql/ext/install/","tags":["Extension"],"title":"Install"},{"body":"Some extensions require preloading dynamic libraries or configuring parameters before use. This section describes how to configure extensions.\nPreload Extensions Most extensions can be enabled directly with CREATE EXTENSION after installation, but some extensions using PostgreSQL’s Hook mechanism require preloading.\nPreloading is specified via the shared_preload_libraries parameter and requires a database restart to take effect.\nExtensions Requiring Preload Common extensions that require preloading:\nExtension Description timescaledb Time-series database extension, must be placed first citus Distributed database extension, must be placed first pg_stat_statements SQL statement statistics, enabled by default in Pigsty auto_explain Automatically log slow query execution plans, enabled by default in Pigsty pg_cron Scheduled task scheduling pg_net Asynchronous HTTP requests pg_tle Trusted language extensions pgaudit Audit logging pg_stat_kcache Kernel statistics pg_squeeze Online table space reclamation pgml PostgresML machine learning For the complete list, see the Extension Catalog (marked with LOAD).\nPreload Order The loading order of extensions in shared_preload_libraries is important:\ntimescaledb and citus must be placed first If using both, citus should come before timescaledb Statistics extensions should come after pg_stat_statements to use the same query_id pg_libs: 'citus, timescaledb, pg_stat_statements, auto_explain' Configure During Cluster Initialization When creating a new cluster, use the pg_libs parameter to specify preloaded extensions:\npg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta pg_libs: 'timescaledb, pg_stat_statements, auto_explain' pg_extensions: [ timescaledb, postgis, pgvector ] The value of pg_libs will be written to shared_preload_libraries during cluster initialization.\nDefault Value The default value of pg_libs is pg_stat_statements, auto_explain. These two Contrib extensions provide basic observability:\npg_stat_statements: Track execution statistics of all SQL statements auto_explain: Automatically log execution plans for slow queries Modify Configuration on Existing Cluster For initialized clusters, use patronictl to modify shared_preload_libraries:\n# Add timescaledb to preload libraries pg edit-config pg-meta --force -p shared_preload_libraries='timescaledb, pg_stat_statements, auto_explain' # Restart cluster to apply configuration pg restart pg-meta You can also directly modify postgresql.conf or use ALTER SYSTEM:\nALTER SYSTEM SET shared_preload_libraries = 'timescaledb, pg_stat_statements, auto_explain'; A PostgreSQL service restart is required after modification.\nExtension Parameter Configuration Many extensions have configurable parameters that can be set in the following locations:\nDuring Cluster Initialization Use the pg_parameters parameter to specify:\npg-meta: vars: pg_cluster: pg-meta pg_libs: 'pg_cron, pg_stat_statements, auto_explain' pg_parameters: cron.database_name: postgres # Database used by pg_cron pg_stat_statements.track: all # Track all statements auto_explain.log_min_duration: 1000 # Log queries exceeding 1 second Runtime Modification Use ALTER SYSTEM or patronictl:\n-- Modify parameter ALTER SYSTEM SET pg_stat_statements.track = 'all'; -- Reload configuration SELECT pg_reload_conf(); # Modify using patronictl pg edit-config pg-meta --force -p 'pg_stat_statements.track=all' Important Notes Preload errors prevent startup: If an extension in shared_preload_libraries doesn’t exist or fails to load, PostgreSQL will not start. Ensure extensions are properly installed before adding to preload.\nModification requires restart: Changes to shared_preload_libraries require restarting the PostgreSQL service to take effect.\nPartial functionality available: Some extensions can be partially used without preloading, but full functionality requires preloading.\nView current configuration: Use the following command to view current preload libraries:\nSHOW shared_preload_libraries; ","categories":["Reference"],"description":"Preload extension libraries and configure extension parameters","excerpt":"Preload extension libraries and configure extension parameters","ref":"/docs/pgsql/ext/config/","tags":["Extension"],"title":"Config"},{"body":"After installing extension packages, you need to execute CREATE EXTENSION in the database to use extension features.\nView Available Extensions After installing extension packages, you can view available extensions:\n-- View all available extensions SELECT * FROM pg_available_extensions; -- View specific extension SELECT * FROM pg_available_extensions WHERE name = 'vector'; -- View enabled extensions SELECT * FROM pg_extension; Create Extensions Use CREATE EXTENSION to enable extensions in the database:\n-- Create extension CREATE EXTENSION vector; -- Create extension in specific schema CREATE EXTENSION postgis SCHEMA public; -- Automatically install dependent extensions CREATE EXTENSION postgis_topology CASCADE; -- Create if not exists CREATE EXTENSION IF NOT EXISTS vector; Note: CREATE EXTENSION uses the extension name (e.g., vector), not the package alias (pgvector).\nCreate During Cluster Initialization Declare extensions in pg_databases, and they will be automatically created during cluster initialization:\npg-meta: vars: pg_cluster: pg-meta pg_databases: - name: meta extensions: - { name: vector } # Use default schema - { name: postgis, schema: public } # Specify schema - { name: pg_stat_statements, schema: monitor } Pigsty will automatically execute CREATE EXTENSION after database creation.\nExtensions Requiring Preload Some extensions must be added to shared_preload_libraries and restarted before creation:\npg-meta: vars: pg_cluster: pg-meta pg_libs: 'timescaledb, pg_stat_statements, auto_explain' pg_databases: - name: meta extensions: - { name: timescaledb } # Requires preload If you try to create without preloading, you will receive an error message.\nCommon extensions requiring preload: timescaledb, citus, pg_cron, pg_net, pgaudit, etc. See Configure Extensions.\nExtension Dependencies Some extensions depend on other extensions and need to be created in order:\n-- postgis_topology depends on postgis CREATE EXTENSION postgis; CREATE EXTENSION postgis_topology; -- Or use CASCADE to automatically install dependencies CREATE EXTENSION postgis_topology CASCADE; Extensions Not Requiring Creation A few extensions don’t provide SQL interfaces and don’t need CREATE EXTENSION:\nExtension Description wal2json Logical decoding plugin, used directly in replication slots decoderbufs Logical decoding plugin decoder_raw Logical decoding plugin These extensions can be used immediately after installation, for example:\n-- Create logical replication slot using wal2json SELECT * FROM pg_create_logical_replication_slot('test_slot', 'wal2json'); View Extension Information -- View extension details \\dx+ vector -- View objects contained in extension SELECT * FROM pg_extension_config_dump('vector'); -- View extension version SELECT extversion FROM pg_extension WHERE extname = 'vector'; ","categories":["Reference"],"description":"Create and enable extensions in databases","excerpt":"Create and enable extensions in databases","ref":"/docs/pgsql/ext/create/","tags":["Extension"],"title":"Create"},{"body":"Extension updates involve two levels: package updates (operating system level) and extension object updates (database level).\nUpdate Packages Use package managers to update extension packages:\n# EL systems sudo yum update pgvector_18* # Debian/Ubuntu systems sudo apt update \u0026\u0026 sudo apt upgrade postgresql-18-pgvector Batch update using Pigsty:\n# Update extension packages for specified cluster ./pgsql.yml -l pg-meta -t pg_extension -e '{\"pg_extensions\":[\"pgvector\"]}' # Using pig package manager pig update pgvector Update Extension Objects After package updates, extension objects in the database may need to be synchronized.\nView Updatable Extensions -- View installed extensions and their versions SELECT name, default_version, installed_version FROM pg_available_extensions WHERE installed_version IS NOT NULL; -- View upgradable extensions SELECT name, installed_version, default_version FROM pg_available_extensions WHERE installed_version IS NOT NULL AND installed_version \u003c\u003e default_version; Execute Extension Update -- Update to latest version ALTER EXTENSION pgvector UPDATE; -- Update to specific version ALTER EXTENSION pgvector UPDATE TO '0.8.0'; View Update Paths -- View available upgrade paths for extension SELECT * FROM pg_extension_update_paths('pgvector'); Important Notes Backup first: Backup the database before updating extensions, especially for extensions involving data type changes.\nCheck compatibility: Some extension major version upgrades may be incompatible. Consult the extension’s upgrade documentation.\nPreloaded extensions: If updating a preloaded extension (like timescaledb), a database restart may be required after the update.\nDependencies: If other extensions depend on the updated extension, update them in dependency order.\nReplication environments: In master-slave replication environments, test updates on slaves first, then update the master after confirmation.\nCommon Issues Update Failure If ALTER EXTENSION UPDATE fails, it may be because:\nNo available upgrade path Extension is in use Insufficient permissions -- View extension dependencies SELECT * FROM pg_depend WHERE refobjid = (SELECT oid FROM pg_extension WHERE extname = 'pgvector'); Rollback Update PostgreSQL extensions typically don’t support direct rollback. To rollback:\nRestore from backup Or: Uninstall new version extension, install old version package, recreate extension ","categories":["Reference"],"description":"Upgrade PostgreSQL extension versions","excerpt":"Upgrade PostgreSQL extension versions","ref":"/docs/pgsql/ext/update/","tags":["Extension"],"title":"Update"},{"body":"Removing extensions involves two levels: dropping extension objects (database level) and uninstalling packages (operating system level).\nDrop Extension Objects Use DROP EXTENSION to remove extensions from the database:\n-- Drop extension DROP EXTENSION pgvector; -- If there are dependent objects, cascade delete is required DROP EXTENSION pgvector CASCADE; Warning: CASCADE will drop all objects that depend on this extension (tables, functions, views, etc.). Use with caution.\nCheck Extension Dependencies It’s recommended to check dependencies before dropping:\n-- View objects that depend on an extension SELECT classid::regclass, objid, deptype FROM pg_depend WHERE refobjid = (SELECT oid FROM pg_extension WHERE extname = 'pgvector'); -- View tables using extension types SELECT c.relname AS table_name, a.attname AS column_name, t.typname AS type_name FROM pg_attribute a JOIN pg_class c ON a.attrelid = c.oid JOIN pg_type t ON a.atttypid = t.oid WHERE t.typname = 'vector'; Remove Preload If the extension is in shared_preload_libraries, it must be removed from the preload list after dropping:\n# Modify shared_preload_libraries, remove extension pg edit-config pg-meta --force -p shared_preload_libraries='pg_stat_statements, auto_explain' # Restart to apply configuration pg restart pg-meta Uninstall Packages After dropping the extension from the database, you can optionally uninstall the package:\n# EL systems sudo yum remove pgvector_18* # Debian/Ubuntu systems sudo apt remove postgresql-18-pgvector # Using pig package manager pig remove pgvector Typically keeping the package doesn’t cause issues. Only uninstall when you need to free disk space or resolve conflicts.\nImportant Notes Data loss risk: Using CASCADE will drop dependent objects, potentially causing data loss.\nApplication compatibility: Ensure applications no longer use the extension’s functionality before dropping.\nPreload order: If dropping a preloaded extension, be sure to also remove it from shared_preload_libraries, otherwise the database may fail to start.\nMaster-slave environments: In replication environments, DROP EXTENSION automatically replicates to slaves.\nOperation Sequence Complete extension removal workflow:\n# 1. Check dependencies psql -d mydb -c \"SELECT * FROM pg_depend WHERE refobjid = (SELECT oid FROM pg_extension WHERE extname = 'pgvector');\" # 2. Drop extension from database psql -d mydb -c \"DROP EXTENSION pgvector;\" # 3. If it's a preloaded extension, remove from shared_preload_libraries pg edit-config pg-meta --force -p shared_preload_libraries='pg_stat_statements, auto_explain' # 4. Restart database (if preload configuration was modified) pg restart pg-meta # 5. Optional: Uninstall package sudo yum remove pgvector_18* ","categories":["Reference"],"description":"Uninstall PostgreSQL extensions","excerpt":"Uninstall PostgreSQL extensions","ref":"/docs/pgsql/ext/remove/","tags":["Extension"],"title":"Remove"},{"body":"Pigsty installs and enables some core extensions by default when initializing PostgreSQL clusters.\nDefault Installed Extensions Extensions installed by default via pg_packages:\nExtension Description pg_repack Handle table bloat online, important maintenance tool wal2json Logical decoding outputs JSON format changes, commonly used in CDC scenarios Extensions optionally installed via pg_extensions (commented by default):\nExtension Description postgis Geospatial database extension timescaledb Time-series database extension pgvector Vector data type and indexes Default Enabled Extensions Extensions enabled by default in all databases via pg_default_extensions:\nExtension Schema Description pg_stat_statements monitor SQL statement execution statistics pgstattuple monitor Tuple-level statistics pg_buffercache monitor Buffer cache inspection pageinspect monitor Page-level inspection pg_prewarm monitor Relation prewarming pg_visibility monitor Visibility map inspection pg_freespacemap monitor Free space map inspection postgres_fdw public PostgreSQL foreign data wrapper file_fdw public File foreign data wrapper btree_gist public B-tree GiST operator classes btree_gin public B-tree GIN operator classes pg_trgm public Trigram matching intagg public Integer aggregator intarray public Integer array functions pg_repack - Online table reorganization These extensions provide basic monitoring, operations, and feature enhancement capabilities.\nDefault Preloaded Extensions Extensions preloaded by default into shared_preload_libraries via pg_libs:\nExtension Description pg_stat_statements Track execution statistics of all SQL statements auto_explain Automatically log execution plans for slow queries These two extensions provide basic observability and are strongly recommended to keep.\nCustomize Default Extensions You can customize default installed and enabled extensions by modifying configuration parameters:\nall: vars: # Modify default extension packages pg_packages: - pgsql-main pgsql-common - pg_repack_$v* wal2json_$v* # Modify default installed extensions pg_extensions: [ postgis, timescaledb, pgvector ] # Modify default preloaded extensions pg_libs: 'timescaledb, pg_stat_statements, auto_explain' # Modify default enabled extensions pg_default_extensions: - { name: pg_stat_statements, schema: monitor } - { name: pg_repack } # ... add more For detailed extension usage, please refer to:\nQuick Start: Overview of the extension usage process Extension Introduction: Core concepts of extensions Install Extensions: How to install extensions Configure Extensions: Preloading and parameter configuration Create Extensions: Creating extensions in databases ","categories":["Reference"],"description":"PostgreSQL extensions installed by default in Pigsty","excerpt":"PostgreSQL extensions installed by default in Pigsty","ref":"/docs/pgsql/ext/extension/","tags":["Extension"],"title":"Default Extensions"},{"body":" Why can’t my current user use the pg admin alias? Starting from Pigsty v4.0, permissions to manage global Patroni / PostgreSQL clusters using the pg admin alias have been tightened to the admin group (admin) on admin nodes.\nThe admin user (dba) created by the node.yml playbook has this permission by default. If your current user wants this permission, you need to explicitly add them to the admin group:\nsudo usermod -aG admin \u003cusername\u003e PGSQL Init Fails: Fail to wait for postgres/patroni primary There are multiple possible causes for this error. You need to check Ansible, Systemd / Patroni / PostgreSQL logs to find the real cause.\nPossibility 1: Cluster config error - find and fix the incorrect config items. Possibility 2: A cluster with the same name exists, or the previous same-named cluster primary was improperly removed. Possibility 3: Residual garbage metadata from a same-named cluster in DCS - decommissioning wasn’t completed properly. Use etcdctl del --prefix /pg/\u003ccls\u003e to manually delete residual data (be careful). Possibility 4: Your PostgreSQL or node-related RPM pkgs were not successfully installed. Possibility 5: Your Watchdog kernel module was not properly enabled/loaded. Possibility 6: The locale you specified during database init doesn’t exist (e.g., used en_US.UTF8 but English language pack or Locale support wasn’t installed). If you encounter other causes, please submit an Issue or ask the community for help. PGSQL Init Fails: Fail to wait for postgres/patroni replica There are several possible causes:\nImmediate failure: Usually due to config errors, network issues, corrupted DCS metadata, etc. You must check /pg/log to find the actual cause.\nFailure after a while: This might be due to source instance data corruption. See PGSQL FAQ: How to create a replica when data is corrupted?\nTimeout after a long time: If the wait for postgres replica task takes 30 minutes or longer and fails due to timeout, this is common for large clusters (e.g., 1TB+, may take hours to create a replica).\nIn this case, the underlying replica creation process is still ongoing. You can use pg list \u003ccls\u003e to check cluster status and wait for the replica to catch up with the primary. Then use the following command to continue with remaining tasks and complete the full replica init:\n./pgsql.yml -t pg_hba,pg_reload,pg_backup,pgbouncer,pg_vip,pg_dns,pg_service,pg_exporter,pg_register -l \u003cproblematic_replica\u003e PGSQL Init Fails: ABORT due to pg_safeguard enabled This means the PostgreSQL instance being cleaned has the deletion safeguard enabled. Disable pg_safeguard to remove the Postgres instance.\nIf the deletion safeguard pg_safeguard is enabled, you cannot remove running PGSQL instances using bin/pgsql-rm or the pgsql-rm.yml playbook.\nTo disable pg_safeguard, you can set pg_safeguard to false in the config inventory, or use the command param -e pg_safeguard=false when executing the playbook.\n./pgsql-rm.yml -e pg_safeguard=false -l \u003ccls_to_remove\u003e # Force override pg_safeguard How to Ensure No Data Loss During Failover? Use the crit.yml param template, set pg_rpo to 0, or config the cluster for sync commit mode.\nConsider using Sync Standby and Quorum Commit to ensure zero data loss during failover.\nFor more details, see the intro in Security Considerations - Availability.\nHow to Rescue When Disk is Full? If the disk is full and even Shell commands cannot execute, rm -rf /pg/dummy can release some emergency space.\nBy default, pg_dummy_filesize is set to 64MB. In prod envs, it’s recommended to increase it to 8GB or larger.\nIt will be placed at /pg/dummy path on the PGSQL main data disk. You can delete this file to free up some emergency space:\nAt least it will allow you to run some shell scripts on that node to further reclaim other space (e.g., logs/WAL, stale data, WAL archives and backups).\nHow to Create a Replica When Cluster Data is Corrupted? Pigsty sets the clonefrom: true tag in the patroni config of all instances, marking the instance as available for creating replicas.\nIf an instance has corrupted data files causing errors when creating new replicas, you can set clonefrom: false to avoid pulling data from the corrupted instance. Here’s how:\n$ vi /pg/bin/patroni.yml tags: nofailover: false clonefrom: true # ----------\u003e change to false noloadbalance: false nosync: false version: '15' spec: '4C.8G.50G' conf: 'oltp.yml' $ systemctl reload patroni # Reload Patroni config What is the Perf Overhead of PostgreSQL Monitoring? A regular PostgreSQL instance scrape takes about 200ms. The scrape interval defaults to 10 seconds, which is almost negligible for a prod multi-core database instance.\nNote that Pigsty enables in-database object monitoring by default, so if your database has hundreds of thousands of table/index objects, scraping may increase to several seconds.\nYou can modify Prometheus’s scrape frequency. Please ensure: the scrape cycle should be significantly longer than the duration of a single scrape.\nHow to Monitor an Existing PostgreSQL Instance? Detailed monitoring config instructions are provided in PGSQL Monitor.\nHow to Manually Remove PostgreSQL Monitoring Targets? ./pgsql-rm.yml -t rm_metrics -l \u003ccls\u003e # Remove all instances of cluster 'cls' from victoria bin/pgmon-rm \u003cins\u003e # Remove a single instance 'ins' monitoring object from Victoria, especially suitable for removing added external instances ","categories":["Reference"],"description":"Frequently asked questions about PostgreSQL","excerpt":"Frequently asked questions about PostgreSQL","ref":"/docs/pgsql/faq/","tags":"","title":"FAQ"},{"body":"Pigsty provides supplementary extension repositories, offering additional extension packages on top of the PGDG official repository.\nYUM Repository Applicable to EL 7/8/9/10 and compatible systems (RHEL, Rocky, AlmaLinux, CentOS, etc.).\nAdd Repository # Add GPG public key curl -fsSL https://repo.pigsty.io/key | sudo tee /etc/pki/rpm-gpg/RPM-GPG-KEY-pigsty \u003e/dev/null # Add repository configuration curl -fsSL https://repo.pigsty.io/yum/repo | sudo tee /etc/yum.repos.d/pigsty.repo \u003e/dev/null # Refresh cache sudo yum makecache China Mainland Mirror curl -fsSL https://repo.pigsty.cc/key | sudo tee /etc/pki/rpm-gpg/RPM-GPG-KEY-pigsty \u003e/dev/null curl -fsSL https://repo.pigsty.cc/yum/repo | sudo tee /etc/yum.repos.d/pigsty.repo \u003e/dev/null Repository URLs International: https://repo.pigsty.io/yum/ China: https://repo.pigsty.cc/yum/ APT Repository Applicable to Debian 11/12/13 and Ubuntu 22.04/24.04 and compatible systems.\nAdd Repository # Add GPG public key curl -fsSL https://repo.pigsty.io/key | sudo gpg --dearmor -o /etc/apt/keyrings/pigsty.gpg # Get distribution codename and add repository distro_codename=$(lsb_release -cs) sudo tee /etc/apt/sources.list.d/pigsty.list \u003e /dev/null \u003c\u003cEOF deb [signed-by=/etc/apt/keyrings/pigsty.gpg] https://repo.pigsty.io/apt/infra generic main deb [signed-by=/etc/apt/keyrings/pigsty.gpg] https://repo.pigsty.io/apt/pgsql ${distro_codename} main EOF # Refresh cache sudo apt update China Mainland Mirror curl -fsSL https://repo.pigsty.cc/key | sudo gpg --dearmor -o /etc/apt/keyrings/pigsty.gpg distro_codename=$(lsb_release -cs) sudo tee /etc/apt/sources.list.d/pigsty.list \u003e /dev/null \u003c\u003cEOF deb [signed-by=/etc/apt/keyrings/pigsty.gpg] https://repo.pigsty.cc/apt/infra generic main deb [signed-by=/etc/apt/keyrings/pigsty.gpg] https://repo.pigsty.cc/apt/pgsql/${distro_codename} ${distro_codename} main EOF Repository URLs International: https://repo.pigsty.io/apt/ China Mirror: https://repo.pigsty.cc/apt/ GPG Signature All packages are signed with GPG:\nFingerprint: 9592A7BC7A682E7333376E09E7935D8DB9BD8B20 Short ID: B9BD8B20 Repository Policy The Pigsty repository follows these principles:\nSupplementary: Only includes extensions not present in the PGDG repository Consistency: Once an extension enters the PGDG repository, the Pigsty repository will remove it or keep it consistent Compatibility: Supports multiple major versions of PostgreSQL 13-18 Multi-platform: Supports x86_64 and aarch64 architectures Related Resources Pigsty Extension Catalog: Browse all available extensions PGDG YUM Repository PGDG APT Repository ","categories":["Reference"],"description":"Pigsty extension software repository configuration","excerpt":"Pigsty extension software repository configuration","ref":"/docs/pgsql/ext/repo/","tags":["Extension"],"title":"Repository"},{"body":"","categories":["Reference"],"description":"Miscellaneous Topics","excerpt":"Miscellaneous Topics","ref":"/docs/pgsql/misc/","tags":["Security"],"title":"Misc"},{"body":"Pigsty natively supports Citus. This is a distributed horizontal scaling extension based on the native PostgreSQL kernel.\nInstallation Citus is a PostgreSQL extension plugin that can be installed and enabled on a native PostgreSQL cluster following the standard plugin installation process.\n./pgsql.yml -t pg_extension -e '{\"pg_extensions\":[\"citus\"]}' Configuration To define a citus cluster, you need to specify the following parameters:\npg_mode must be set to citus instead of the default pgsql You must define the shard name pg_shard and shard number pg_group on each shard cluster You must define pg_primary_db to specify the database managed by Patroni If you want to use postgres from pg_dbsu instead of the default pg_admin_username to execute admin commands, then pg_dbsu_password must be set to a non-empty plaintext password Additionally, you need extra hba rules to allow SSL access from localhost and other data nodes.\nYou can define each Citus cluster as a separate group, like standard PostgreSQL clusters, as shown in conf/dbms/citus.yml:\nall: children: pg-citus0: # citus shard 0 hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: { pg_cluster: pg-citus0 , pg_group: 0 } pg-citus1: # citus shard 1 hosts: { 10.10.10.11: { pg_seq: 1, pg_role: primary } } vars: { pg_cluster: pg-citus1 , pg_group: 1 } pg-citus2: # citus shard 2 hosts: { 10.10.10.12: { pg_seq: 1, pg_role: primary } } vars: { pg_cluster: pg-citus2 , pg_group: 2 } pg-citus3: # citus shard 3 hosts: 10.10.10.13: { pg_seq: 1, pg_role: primary } 10.10.10.14: { pg_seq: 2, pg_role: replica } vars: { pg_cluster: pg-citus3 , pg_group: 3 } vars: # Global parameters for all Citus clusters pg_mode: citus # pgsql cluster mode must be set to: citus pg_shard: pg-citus # citus horizontal shard name: pg-citus pg_primary_db: meta # citus database name: meta pg_dbsu_password: DBUser.Postgres # If using dbsu, you need to configure a password for it pg_users: [ { name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [ dbrole_admin ] } ] pg_databases: [ { name: meta ,extensions: [ { name: citus }, { name: postgis }, { name: timescaledb } ] } ] pg_hba_rules: - { user: 'all' ,db: all ,addr: 127.0.0.1/32 ,auth: ssl ,title: 'all user ssl access from localhost' } - { user: 'all' ,db: all ,addr: intra ,auth: ssl ,title: 'all user ssl access from intranet' } You can also specify identity parameters for all Citus cluster members within a single group, as shown in prod.yml:\n#==========================================================# # pg-citus: 10 node citus cluster (5 x primary-replica pair) #==========================================================# pg-citus: # citus group hosts: 10.10.10.50: { pg_group: 0, pg_cluster: pg-citus0 ,pg_vip_address: 10.10.10.60/24 ,pg_seq: 0, pg_role: primary } 10.10.10.51: { pg_group: 0, pg_cluster: pg-citus0 ,pg_vip_address: 10.10.10.60/24 ,pg_seq: 1, pg_role: replica } 10.10.10.52: { pg_group: 1, pg_cluster: pg-citus1 ,pg_vip_address: 10.10.10.61/24 ,pg_seq: 0, pg_role: primary } 10.10.10.53: { pg_group: 1, pg_cluster: pg-citus1 ,pg_vip_address: 10.10.10.61/24 ,pg_seq: 1, pg_role: replica } 10.10.10.54: { pg_group: 2, pg_cluster: pg-citus2 ,pg_vip_address: 10.10.10.62/24 ,pg_seq: 0, pg_role: primary } 10.10.10.55: { pg_group: 2, pg_cluster: pg-citus2 ,pg_vip_address: 10.10.10.62/24 ,pg_seq: 1, pg_role: replica } 10.10.10.56: { pg_group: 3, pg_cluster: pg-citus3 ,pg_vip_address: 10.10.10.63/24 ,pg_seq: 0, pg_role: primary } 10.10.10.57: { pg_group: 3, pg_cluster: pg-citus3 ,pg_vip_address: 10.10.10.63/24 ,pg_seq: 1, pg_role: replica } 10.10.10.58: { pg_group: 4, pg_cluster: pg-citus4 ,pg_vip_address: 10.10.10.64/24 ,pg_seq: 0, pg_role: primary } 10.10.10.59: { pg_group: 4, pg_cluster: pg-citus4 ,pg_vip_address: 10.10.10.64/24 ,pg_seq: 1, pg_role: replica } vars: pg_mode: citus # pgsql cluster mode: citus pg_shard: pg-citus # citus shard name: pg-citus pg_primary_db: test # primary database used by citus pg_dbsu_password: DBUser.Postgres # all dbsu password access for citus cluster pg_vip_enabled: true pg_vip_interface: eth1 pg_extensions: [ 'citus postgis timescaledb pgvector' ] pg_libs: 'citus, timescaledb, pg_stat_statements, auto_explain' # citus will be added by patroni automatically pg_users: [ { name: test ,password: test ,pgbouncer: true ,roles: [ dbrole_admin ] } ] pg_databases: [ { name: test ,owner: test ,extensions: [ { name: citus }, { name: postgis } ] } ] pg_hba_rules: - { user: 'all' ,db: all ,addr: 10.10.10.0/24 ,auth: trust ,title: 'trust citus cluster members' } - { user: 'all' ,db: all ,addr: 127.0.0.1/32 ,auth: ssl ,title: 'all user ssl access from localhost' } - { user: 'all' ,db: all ,addr: intra ,auth: ssl ,title: 'all user ssl access from intranet' } Usage You can access any node just like accessing a regular cluster:\npgbench -i postgres://test:test@pg-citus0/test pgbench -nv -P1 -T1000 -c 2 postgres://test:test@pg-citus0/test By default, changes you make to one Shard only occur on that cluster and are not synchronized to other Shards.\nIf you want to distribute writes across all Shards, you can use the API functions provided by Citus to mark tables as:\nDistributed tables (automatic partitioning, requires specifying partition key) Reference tables (full replication: does not require specifying partition key) Starting from Citus 11.2, any Citus database node can play the role of coordinator, meaning any primary node can write:\npsql -h pg-citus0 -d test -c \"SELECT create_distributed_table('pgbench_accounts', 'aid'); SELECT truncate_local_data_after_distributing_table('public.pgbench_accounts');\" psql -h pg-citus0 -d test -c \"SELECT create_reference_table('pgbench_branches') ; SELECT truncate_local_data_after_distributing_table('public.pgbench_branches');\" psql -h pg-citus0 -d test -c \"SELECT create_reference_table('pgbench_history') ; SELECT truncate_local_data_after_distributing_table('public.pgbench_history');\" psql -h pg-citus0 -d test -c \"SELECT create_reference_table('pgbench_tellers') ; SELECT truncate_local_data_after_distributing_table('public.pgbench_tellers');\" After distributing the tables, you can also access them on other nodes:\npsql -h pg-citus1 -d test -c '\\dt+' For example, a full table scan will show that the execution plan has become a distributed plan:\nvagrant@meta-1:~$ psql -h pg-citus3 -d test -c 'explain select * from pgbench_accounts' QUERY PLAN --------------------------------------------------------------------------------------------------------- Custom Scan (Citus Adaptive) (cost=0.00..0.00 rows=100000 width=352) Task Count: 32 Tasks Shown: One of 32 -\u003e Task Node: host=10.10.10.52 port=5432 dbname=test -\u003e Seq Scan on pgbench_accounts_102008 pgbench_accounts (cost=0.00..81.66 rows=3066 width=97) (6 rows) You can initiate writes from several different primary nodes:\npgbench -nv -P1 -T1000 -c 2 postgres://test:test@pg-citus1/test pgbench -nv -P1 -T1000 -c 2 postgres://test:test@pg-citus2/test pgbench -nv -P1 -T1000 -c 2 postgres://test:test@pg-citus3/test pgbench -nv -P1 -T1000 -c 2 postgres://test:test@pg-citus4/test When a node fails, the native high availability support provided by Patroni will promote the standby node and automatically take over.\ntest=# select * from pg_dist_node; nodeid | groupid | nodename | nodeport | noderack | hasmetadata | isactive | noderole | nodecluster | metadatasynced | shouldhaveshards --------+---------+-------------+----------+----------+-------------+----------+----------+-------------+----------------+------------------ 1 | 0 | 10.10.10.51 | 5432 | default | t | t | primary | default | t | f 2 | 2 | 10.10.10.54 | 5432 | default | t | t | primary | default | t | t 5 | 1 | 10.10.10.52 | 5432 | default | t | t | primary | default | t | t 3 | 4 | 10.10.10.58 | 5432 | default | t | t | primary | default | t | t 4 | 3 | 10.10.10.56 | 5432 | default | t | t | primary | default | t | t ","categories":["Concept"],"description":"Deploy native high-availability Citus horizontally sharded clusters with Pigsty, seamlessly scaling PostgreSQL across multiple shards and accelerating OLTP/OLAP queries.","excerpt":"Deploy native high-availability Citus horizontally sharded clusters …","ref":"/docs/pgsql/kernel/citus/","tags":["Citus"],"title":"Citus"},{"body":" Babelfish is an MSSQL (Microsoft SQL Server) compatibility solution based on PostgreSQL, open-sourced by AWS.\nOverview Pigsty allows users to create Microsoft SQL Server compatible PostgreSQL clusters using Babelfish and WiltonDB!\nBabelfish: An MSSQL (Microsoft SQL Server) compatibility extension plugin open-sourced by AWS WiltonDB: A PostgreSQL kernel distribution focusing on integrating Babelfish Babelfish is a PostgreSQL extension, but it only works on a slightly modified PostgreSQL kernel fork. WiltonDB provides compiled fork kernel binaries and extension binary packages on EL/Ubuntu systems.\nPigsty can replace the native PostgreSQL kernel with WiltonDB, providing an out-of-the-box MSSQL compatible cluster. Using and managing an MSSQL cluster is no different from a standard PostgreSQL 15 cluster. You can use all the features provided by Pigsty, such as high availability, backup, monitoring, etc.\nWiltonDB comes with several extension plugins including Babelfish, but cannot use native PostgreSQL extension plugins.\nAfter the MSSQL compatible cluster starts, in addition to listening on the PostgreSQL default port, it also listens on the MSSQL default port 1433, providing MSSQL services via the TDS Wire Protocol on this port. You can connect to the MSSQL service provided by Pigsty using any MSSQL client, such as SQL Server Management Studio, or using the sqlcmd command-line tool.\nInstallation WiltonDB conflicts with the native PostgreSQL kernel. Only one kernel can be installed on a node. Use the following command to install the WiltonDB kernel online.\n./node.yml -t node_install -e '{\"node_repo_modules\":\"local,mssql\",\"node_packages\":[\"wiltondb\"]}' Please note that WiltonDB is only available on EL and Ubuntu systems. Debian support is not currently provided.\nThe Pigsty Professional Edition provides offline installation packages for WiltonDB, which can be installed from local software sources.\nConfiguration When installing and deploying the MSSQL module, please pay special attention to the following:\nWiltonDB is available on EL (7/8/9) and Ubuntu (20.04/22.04), but not available on Debian systems. WiltonDB is currently compiled based on PostgreSQL 15, so you need to specify pg_version: 15. On EL systems, the wiltondb binary is installed by default in the /usr/bin/ directory, while on Ubuntu systems it is installed in the /usr/lib/postgresql/15/bin/ directory, which is different from the official PostgreSQL binary placement. In WiltonDB compatibility mode, the HBA password authentication rule needs to use md5 instead of scram-sha-256. Therefore, you need to override Pigsty’s default HBA rule set and insert the md5 authentication rule required by SQL Server before the dbrole_readonly wildcard authentication rule. WiltonDB can only be enabled for one primary database, and you should designate a user as the Babelfish superuser, allowing Babelfish to create databases and users. The default is mssql and dbuser_mssql. If you change this, please also modify the user in files/mssql.sql. The WiltonDB TDS wire protocol compatibility plugin babelfishpg_tds needs to be enabled in shared_preload_libraries. After enabling the WiltonDB extension, it listens on the MSSQL default port 1433. You can override Pigsty’s default service definitions to point the primary and replica services to port 1433 instead of 5432 / 6432. The following parameters need to be configured for the MSSQL database cluster:\n#----------------------------------# # PGSQL \u0026 MSSQL (Babelfish \u0026 Wilton) #----------------------------------# # PG Installation node_repo_modules: local,node,mssql # add mssql and os upstream repos pg_mode: mssql # Microsoft SQL Server Compatible Mode pg_libs: 'babelfishpg_tds, pg_stat_statements, auto_explain' # add timescaledb to shared_preload_libraries pg_version: 15 # The current WiltonDB major version is 15 pg_packages: - wiltondb # install forked version of postgresql with babelfishpg support - patroni pgbouncer pgbackrest pg_exporter pgbadger vip-manager pg_extensions: [] # do not install any vanilla postgresql extensions # PG Provision pg_default_hba_rules: # overwrite default HBA rules for babelfish cluster - {user: '${dbsu}' ,db: all ,addr: local ,auth: ident ,title: 'dbsu access via local os user ident' } - {user: '${dbsu}' ,db: replication ,addr: local ,auth: ident ,title: 'dbsu replication from local os ident' } - {user: '${repl}' ,db: replication ,addr: localhost ,auth: pwd ,title: 'replicator replication from localhost'} - {user: '${repl}' ,db: replication ,addr: intra ,auth: pwd ,title: 'replicator replication from intranet' } - {user: '${repl}' ,db: postgres ,addr: intra ,auth: pwd ,title: 'replicator postgres db from intranet' } - {user: '${monitor}' ,db: all ,addr: localhost ,auth: pwd ,title: 'monitor from localhost with password' } - {user: '${monitor}' ,db: all ,addr: infra ,auth: pwd ,title: 'monitor from infra host with password'} - {user: '${admin}' ,db: all ,addr: infra ,auth: ssl ,title: 'admin @ infra nodes with pwd \u0026 ssl' } - {user: '${admin}' ,db: all ,addr: world ,auth: ssl ,title: 'admin @ everywhere with ssl \u0026 pwd' } - {user: dbuser_mssql ,db: mssql ,addr: intra ,auth: md5 ,title: 'allow mssql dbsu intranet access' } # \u003c--- use md5 auth method for mssql user - {user: '+dbrole_readonly',db: all ,addr: localhost ,auth: pwd ,title: 'pgbouncer read/write via local socket'} - {user: '+dbrole_readonly',db: all ,addr: intra ,auth: pwd ,title: 'read/write biz user via password' } - {user: '+dbrole_offline' ,db: all ,addr: intra ,auth: pwd ,title: 'allow etl offline tasks from intranet'} pg_default_services: # route primary \u0026 replica service to mssql port 1433 - { name: primary ,port: 5433 ,dest: 1433 ,check: /primary ,selector: \"[]\" } - { name: replica ,port: 5434 ,dest: 1433 ,check: /read-only ,selector: \"[]\" , backup: \"[? pg_role == `primary` || pg_role == `offline` ]\" } - { name: default ,port: 5436 ,dest: postgres ,check: /primary ,selector: \"[]\" } - { name: offline ,port: 5438 ,dest: postgres ,check: /replica ,selector: \"[? pg_role == `offline` || pg_offline_query ]\" , backup: \"[? pg_role == `replica` \u0026\u0026 !pg_offline_query]\"} You can define MSSQL business databases and business users:\n#----------------------------------# # pgsql (singleton on current node) #----------------------------------# # this is an example single-node postgres cluster with postgis \u0026 timescaledb installed, with one biz database \u0026 two biz users pg-meta: hosts: 10.10.10.10: { pg_seq: 1, pg_role: primary } # \u003c---- primary instance with read-write capability vars: pg_cluster: pg-test pg_users: # create MSSQL superuser - {name: dbuser_mssql ,password: DBUser.MSSQL ,superuser: true, pgbouncer: true ,roles: [dbrole_admin], comment: superuser \u0026 owner for babelfish } pg_primary_db: mssql # use `mssql` as the primary sql server database pg_databases: - name: mssql baseline: mssql.sql # init babelfish database \u0026 user extensions: - { name: uuid-ossp } - { name: babelfishpg_common } - { name: babelfishpg_tsql } - { name: babelfishpg_tds } - { name: babelfishpg_money } - { name: pg_hint_plan } - { name: system_stats } - { name: tds_fdw } owner: dbuser_mssql parameters: { 'babelfishpg_tsql.migration_mode' : 'multi-db' } comment: babelfish cluster, a MSSQL compatible pg cluster Access You can use any SQL Server compatible client tool to access this database cluster.\nMicrosoft provides sqlcmd as the official command-line tool.\nIn addition, they also provide a Go version command-line tool go-sqlcmd.\nInstall go-sqlcmd:\ncurl -LO https://github.com/microsoft/go-sqlcmd/releases/download/v1.4.0/sqlcmd-v1.4.0-linux-amd64.tar.bz2 tar xjvf sqlcmd-v1.4.0-linux-amd64.tar.bz2 sudo mv sqlcmd* /usr/bin/ Quick start with go-sqlcmd:\n$ sqlcmd -S 10.10.10.10,1433 -U dbuser_mssql -P DBUser.MSSQL 1\u003e select @@version 2\u003e go version ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Babelfish for PostgreSQL with SQL Server Compatibility - 12.0.2000.8 Oct 22 2023 17:48:32 Copyright (c) Amazon Web Services PostgreSQL 15.4 (EL 1:15.4.wiltondb3.3_2-2.el8) on x86_64-redhat-linux-gnu (Babelfish 3.3.0) (1 row affected) Using the service mechanism provided by Pigsty, you can use ports 5433 / 5434 to always connect to port 1433 on the primary/replica.\n# Access port 5433 on any cluster member, pointing to port 1433 MSSQL port on the primary sqlcmd -S 10.10.10.11,5433 -U dbuser_mssql -P DBUser.MSSQL # Access port 5434 on any cluster member, pointing to port 1433 MSSQL port on any readable replica sqlcmd -S 10.10.10.11,5434 -U dbuser_mssql -P DBUser.MSSQL Extensions Most of the PGSQL module’s extension plugins (non-pure SQL class) cannot be directly used on the WiltonDB kernel of the MSSQL module and need to be recompiled.\nCurrently, WiltonDB comes with the following extension plugins. In addition to PostgreSQL Contrib extensions and the four BabelfishPG core extensions, it also provides three third-party extensions: pg_hint_plan, tds_fdw, and system_stats.\nExtension Name Version Description dblink 1.2 connect to other PostgreSQL databases from within a database adminpack 2.1 administrative functions for PostgreSQL dict_int 1.0 text search dictionary template for integers intagg 1.1 integer aggregator and enumerator (obsolete) dict_xsyn 1.0 text search dictionary template for extended synonym processing amcheck 1.3 functions for verifying relation integrity autoinc 1.0 functions for autoincrementing fields bloom 1.0 bloom access method - signature file based index fuzzystrmatch 1.1 determine similarities and distance between strings intarray 1.5 functions, operators, and index support for 1-D arrays of integers btree_gin 1.3 support for indexing common datatypes in GIN btree_gist 1.7 support for indexing common datatypes in GiST hstore 1.8 data type for storing sets of (key, value) pairs hstore_plperl 1.0 transform between hstore and plperl isn 1.2 data types for international product numbering standards hstore_plperlu 1.0 transform between hstore and plperlu jsonb_plperl 1.0 transform between jsonb and plperl citext 1.6 data type for case-insensitive character strings jsonb_plperlu 1.0 transform between jsonb and plperlu jsonb_plpython3u 1.0 transform between jsonb and plpython3u cube 1.5 data type for multidimensional cubes hstore_plpython3u 1.0 transform between hstore and plpython3u earthdistance 1.1 calculate great-circle distances on the surface of the Earth lo 1.1 Large Object maintenance file_fdw 1.0 foreign-data wrapper for flat file access insert_username 1.0 functions for tracking who changed a table ltree 1.2 data type for hierarchical tree-like structures ltree_plpython3u 1.0 transform between ltree and plpython3u pg_walinspect 1.0 functions to inspect contents of PostgreSQL Write-Ahead Log moddatetime 1.0 functions for tracking last modification time old_snapshot 1.0 utilities in support of old_snapshot_threshold pgcrypto 1.3 cryptographic functions pgrowlocks 1.2 show row-level locking information pageinspect 1.11 inspect the contents of database pages at a low level pg_surgery 1.0 extension to perform surgery on a damaged relation seg 1.4 data type for representing line segments or floating-point intervals pgstattuple 1.5 show tuple-level statistics pg_buffercache 1.3 examine the shared buffer cache pg_freespacemap 1.2 examine the free space map (FSM) postgres_fdw 1.1 foreign-data wrapper for remote PostgreSQL servers pg_prewarm 1.2 prewarm relation data tcn 1.0 Triggered change notifications pg_trgm 1.6 text similarity measurement and index searching based on trigrams xml2 1.1 XPath querying and XSLT refint 1.0 functions for implementing referential integrity (obsolete) pg_visibility 1.2 examine the visibility map (VM) and page-level visibility info pg_stat_statements 1.10 track planning and execution statistics of all SQL statements executed sslinfo 1.2 information about SSL certificates tablefunc 1.0 functions that manipulate whole tables, including crosstab tsm_system_rows 1.0 TABLESAMPLE method which accepts number of rows as a limit tsm_system_time 1.0 TABLESAMPLE method which accepts time in milliseconds as a limit unaccent 1.1 text search dictionary that removes accents uuid-ossp 1.1 generate universally unique identifiers (UUIDs) plpgsql 1.0 PL/pgSQL procedural language babelfishpg_money 1.1.0 babelfishpg_money system_stats 2.0 EnterpriseDB system statistics for PostgreSQL tds_fdw 2.0.3 Foreign data wrapper for querying a TDS database (Sybase or Microsoft SQL Server) babelfishpg_common 3.3.3 Transact SQL Datatype Support babelfishpg_tds 1.0.0 TDS protocol extension pg_hint_plan 1.5.1 babelfishpg_tsql 3.3.1 Transact SQL compatibility The Pigsty Professional Edition provides offline installation capabilities for MSSQL compatible modules Pigsty Professional Edition provides optional MSSQL compatible kernel extension porting and customization services, which can port extensions available in the PGSQL module to MSSQL clusters. ","categories":["Concept"],"description":"Create Microsoft SQL Server compatible PostgreSQL clusters using WiltonDB and Babelfish! (Wire protocol level compatibility)","excerpt":"Create Microsoft SQL Server compatible PostgreSQL clusters using …","ref":"/docs/pgsql/kernel/babelfish/","tags":["MSSQL"],"title":"Babelfish"},{"body":"IvorySQL is an open-source PostgreSQL kernel fork that aims to provide “Oracle compatibility” based on PG.\nOverview The IvorySQL kernel is supported in the Pigsty open-source version. Your server needs internet access to download relevant packages directly from IvorySQL’s official repository.\nPlease note that adding IvorySQL directly to Pigsty’s default software repository will affect the installation of the native PostgreSQL kernel. Pigsty Professional Edition provides offline installation solutions including the IvorySQL kernel.\nThe current latest version of IvorySQL is 5.0, corresponding to PostgreSQL version 18. Please note that IvorySQL is currently only available on EL8/EL9.\nThe last IvorySQL version supporting EL7 was 3.3, corresponding to PostgreSQL 16.3; the last version based on PostgreSQL 17 is IvorySQL 4.4\nInstallation If your environment has internet access, you can add the IvorySQL repository directly to the node using the following method, then execute the PGSQL playbook for installation:\n./node.yml -t node_repo -e '{\"node_repo_modules\":\"local,node,pgsql,ivory\"}' Configuration The following parameters need to be configured for IvorySQL database clusters:\n#----------------------------------# # Ivory SQL Configuration #----------------------------------# node_repo_modules: local,node,pgsql,ivory # add ivorysql upstream repo pg_mode: ivory # IvorySQL Oracle Compatible Mode pg_packages: [ 'ivorysql patroni pgbouncer pgbackrest pg_exporter pgbadger vip-manager' ] pg_libs: 'liboracle_parser, pg_stat_statements, auto_explain' pg_extensions: [ ] # do not install any vanilla postgresql extensions When using Oracle compatibility mode, you need to dynamically load the liboracle_parser extension plugin.\nClient Access IvorySQL is equivalent to PostgreSQL 16, and any client tool compatible with the PostgreSQL wire protocol can access IvorySQL clusters.\nExtension List Most of the PGSQL module’s extensions (non-pure SQL types) cannot be used directly on the IvorySQL kernel. If you need to use them, please recompile and install from source for the new kernel.\nCurrently, the IvorySQL kernel comes with the following 101 extension plugins.\n(The extension table remains unchanged as it’s already in English)\nPlease note that Pigsty does not assume any warranty responsibility for using the IvorySQL kernel. Any issues or requirements encountered when using this kernel should be addressed with the original vendor.\n","categories":["Concept"],"description":"Use HighGo's open-source IvorySQL kernel to achieve Oracle syntax/PLSQL compatibility based on PostgreSQL clusters.","excerpt":"Use HighGo's open-source IvorySQL kernel to achieve Oracle …","ref":"/docs/pgsql/kernel/ivorysql/","tags":["Oracle"],"title":"IvorySQL"},{"body":" Overview Pigsty allows you to create PostgreSQL clusters with “domestic innovation qualification” credentials using PolarDB!\nPolarDB for PostgreSQL is essentially equivalent to PostgreSQL 15. Any client tool compatible with the PostgreSQL wire protocol can access PolarDB clusters.\nPigsty’s PGSQL repository provides PolarDB PG open-source installation packages for EL7 / EL8, but they are not downloaded to the local software repository during Pigsty installation.\nIf you need offline installation support for PolarDB PG, please consider our professional subscription service\nInstallation If your environment has internet access, you can add the Pigsty PGSQL and dependency repositories to the node using the following method:\nnode_repo_modules: local,node,pgsql Then in pg_packages, replace the native postgresql package with polardb.\nConfiguration The following parameters need special configuration for PolarDB database clusters:\n#----------------------------------# # PGSQL \u0026 PolarDB #----------------------------------# pg_version: 15 pg_packages: [ 'polardb patroni pgbouncer pgbackrest pg_exporter pgbadger vip-manager' ] pg_extensions: [ ] # do not install any vanilla postgresql extensions pg_mode: polar # PolarDB Compatible Mode pg_default_roles: # default roles and users in postgres cluster - { name: dbrole_readonly ,login: false ,comment: role for global read-only access } - { name: dbrole_offline ,login: false ,comment: role for restricted read-only access } - { name: dbrole_readwrite ,login: false ,roles: [dbrole_readonly] ,comment: role for global read-write access } - { name: dbrole_admin ,login: false ,roles: [pg_monitor, dbrole_readwrite] ,comment: role for object creation } - { name: postgres ,superuser: true ,comment: system superuser } - { name: replicator ,superuser: true ,replication: true ,roles: [pg_monitor, dbrole_readonly] ,comment: system replicator } # \u003c- superuser is required for replication - { name: dbuser_dba ,superuser: true ,roles: [dbrole_admin] ,pgbouncer: true ,pool_mode: session, pool_connlimit: 16 ,comment: pgsql admin user } - { name: dbuser_monitor ,roles: [pg_monitor] ,pgbouncer: true ,parameters: {log_min_duration_statement: 1000 } ,pool_mode: session ,pool_connlimit: 8 ,comment: pgsql monitor user } Note particularly that PolarDB PG requires the replicator replication user to be a Superuser, unlike native PG.\nExtension List Most PGSQL module extension plugins (non-pure SQL types) cannot be used directly on the PolarDB kernel. If needed, please recompile and install from source for the new kernel.\nCurrently, the PolarDB kernel comes with the following 61 extension plugins. Apart from Contrib extensions, the additional extensions provided include:\npolar_csn 1.0 : polar_csn polar_monitor 1.2 : examine the polardb information polar_monitor_preload 1.1 : examine the polardb information polar_parameter_check 1.0 : kernel extension for parameter validation polar_px 1.0 : Parallel Execution extension polar_stat_env 1.0 : env stat functions for PolarDB polar_stat_sql 1.3 : Kernel statistics gathering, and sql plan nodes information gathering polar_tde_utils 1.0 : Internal extension for TDE polar_vfs 1.0 : polar_vfs polar_worker 1.0 : polar_worker timetravel 1.0 : functions for implementing time travel vector 0.5.1 : vector data type and ivfflat and hnsw access methods smlar 1.0 : compute similary of any one-dimensional arrays Complete list of available PolarDB plugins:\nname version comment hstore_plpython2u 1.0 transform between hstore and plpython2u dict_int 1.0 text search dictionary template for integers adminpack 2.0 administrative functions for PostgreSQL hstore_plpython3u 1.0 transform between hstore and plpython3u amcheck 1.1 functions for verifying relation integrity hstore_plpythonu 1.0 transform between hstore and plpythonu autoinc 1.0 functions for autoincrementing fields insert_username 1.0 functions for tracking who changed a table bloom 1.0 bloom access method - signature file based index file_fdw 1.0 foreign-data wrapper for flat file access dblink 1.2 connect to other PostgreSQL databases from within a database btree_gin 1.3 support for indexing common datatypes in GIN fuzzystrmatch 1.1 determine similarities and distance between strings lo 1.1 Large Object maintenance intagg 1.1 integer aggregator and enumerator (obsolete) btree_gist 1.5 support for indexing common datatypes in GiST hstore 1.5 data type for storing sets of (key, value) pairs intarray 1.2 functions, operators, and index support for 1-D arrays of integers citext 1.5 data type for case-insensitive character strings cube 1.4 data type for multidimensional cubes hstore_plperl 1.0 transform between hstore and plperl isn 1.2 data types for international product numbering standards jsonb_plperl 1.0 transform between jsonb and plperl dict_xsyn 1.0 text search dictionary template for extended synonym processing hstore_plperlu 1.0 transform between hstore and plperlu earthdistance 1.1 calculate great-circle distances on the surface of the Earth pg_prewarm 1.2 prewarm relation data jsonb_plperlu 1.0 transform between jsonb and plperlu pg_stat_statements 1.6 track execution statistics of all SQL statements executed jsonb_plpython2u 1.0 transform between jsonb and plpython2u jsonb_plpython3u 1.0 transform between jsonb and plpython3u jsonb_plpythonu 1.0 transform between jsonb and plpythonu pg_trgm 1.4 text similarity measurement and index searching based on trigrams pgstattuple 1.5 show tuple-level statistics ltree 1.1 data type for hierarchical tree-like structures ltree_plpython2u 1.0 transform between ltree and plpython2u pg_visibility 1.2 examine the visibility map (VM) and page-level visibility info ltree_plpython3u 1.0 transform between ltree and plpython3u ltree_plpythonu 1.0 transform between ltree and plpythonu seg 1.3 data type for representing line segments or floating-point intervals moddatetime 1.0 functions for tracking last modification time pgcrypto 1.3 cryptographic functions pgrowlocks 1.2 show row-level locking information pageinspect 1.7 inspect the contents of database pages at a low level pg_buffercache 1.3 examine the shared buffer cache pg_freespacemap 1.2 examine the free space map (FSM) tcn 1.0 Triggered change notifications plperl 1.0 PL/Perl procedural language uuid-ossp 1.1 generate universally unique identifiers (UUIDs) plperlu 1.0 PL/PerlU untrusted procedural language refint 1.0 functions for implementing referential integrity (obsolete) xml2 1.1 XPath querying and XSLT plpgsql 1.0 PL/pgSQL procedural language plpython3u 1.0 PL/Python3U untrusted procedural language pltcl 1.0 PL/Tcl procedural language pltclu 1.0 PL/TclU untrusted procedural language polar_csn 1.0 polar_csn sslinfo 1.2 information about SSL certificates polar_monitor 1.2 examine the polardb information polar_monitor_preload 1.1 examine the polardb information polar_parameter_check 1.0 kernel extension for parameter validation polar_px 1.0 Parallel Execution extension tablefunc 1.0 functions that manipulate whole tables, including crosstab polar_stat_env 1.0 env stat functions for PolarDB smlar 1.0 compute similary of any one-dimensional arrays timetravel 1.0 functions for implementing time travel tsm_system_rows 1.0 TABLESAMPLE method which accepts number of rows as a limit polar_stat_sql 1.3 Kernel statistics gathering, and sql plan nodes information gathering tsm_system_time 1.0 TABLESAMPLE method which accepts time in milliseconds as a limit polar_tde_utils 1.0 Internal extension for TDE polar_vfs 1.0 polar_vfs polar_worker 1.0 polar_worker unaccent 1.1 text search dictionary that removes accents postgres_fdw 1.0 foreign-data wrapper for remote PostgreSQL servers Pigsty Professional Edition provides PolarDB offline installation support, extension plugin compilation support, and monitoring and management support specifically adapted for PolarDB clusters. Pigsty collaborates with the Alibaba Cloud kernel team and can provide paid kernel backup support services. ","categories":["Concept"],"description":"Using Alibaba Cloud's open-source PolarDB for PostgreSQL kernel to provide domestic innovation qualification support, with Oracle RAC-like user experience.","excerpt":"Using Alibaba Cloud's open-source PolarDB for PostgreSQL kernel to …","ref":"/docs/pgsql/kernel/polardb/","tags":["PolarDB"],"title":"PolarDB PG"},{"body":"Pigsty allows you to create PolarDB for Oracle clusters with “domestic innovation qualification” credentials using PolarDB!\nAccording to the Security and Reliability Evaluation Results Announcement (No. 1, 2023), Appendix 3, Centralized Database. PolarDB v2.0 is an autonomous, controllable, secure, and reliable domestic innovation database.\nPolarDB for Oracle is an Oracle-compatible version developed based on PolarDB for PostgreSQL. Both share the same kernel, distinguished by the --compatibility-mode parameter.\nWe collaborate with the Alibaba Cloud kernel team to provide a complete database solution based on PolarDB v2.0 kernel and Pigsty v3.0 RDS. Please contact sales for inquiries, or purchase on Alibaba Cloud Marketplace.\nThe PolarDB for Oracle kernel is currently only available on EL systems.\nExtensions Currently, the PolarDB 2.0 (Oracle compatible) kernel comes with the following 188 extension plugins:\nname default_version comment cube 1.5 data type for multidimensional cubes ip4r 2.4 NULL adminpack 2.1 administrative functions for PostgreSQL dict_xsyn 1.0 text search dictionary template for extended synonym processing amcheck 1.4 functions for verifying relation integrity autoinc 1.0 functions for autoincrementing fields hstore 1.8 data type for storing sets of (key, value) pairs bloom 1.0 bloom access method - signature file based index earthdistance 1.1 calculate great-circle distances on the surface of the Earth hstore_plperl 1.0 transform between hstore and plperl bool_plperl 1.0 transform between bool and plperl file_fdw 1.0 foreign-data wrapper for flat file access bool_plperlu 1.0 transform between bool and plperlu fuzzystrmatch 1.1 determine similarities and distance between strings hstore_plperlu 1.0 transform between hstore and plperlu btree_gin 1.3 support for indexing common datatypes in GIN hstore_plpython2u 1.0 transform between hstore and plpython2u btree_gist 1.6 support for indexing common datatypes in GiST hll 2.17 type for storing hyperloglog data hstore_plpython3u 1.0 transform between hstore and plpython3u citext 1.6 data type for case-insensitive character strings hstore_plpythonu 1.0 transform between hstore and plpythonu hypopg 1.3.1 Hypothetical indexes for PostgreSQL insert_username 1.0 functions for tracking who changed a table dblink 1.2 connect to other PostgreSQL databases from within a database decoderbufs 0.1.0 Logical decoding plugin that delivers WAL stream changes using a Protocol Buffer format intagg 1.1 integer aggregator and enumerator (obsolete) dict_int 1.0 text search dictionary template for integers intarray 1.5 functions, operators, and index support for 1-D arrays of integers isn 1.2 data types for international product numbering standards jsonb_plperl 1.0 transform between jsonb and plperl jsonb_plperlu 1.0 transform between jsonb and plperlu jsonb_plpython2u 1.0 transform between jsonb and plpython2u jsonb_plpython3u 1.0 transform between jsonb and plpython3u jsonb_plpythonu 1.0 transform between jsonb and plpythonu lo 1.1 Large Object maintenance log_fdw 1.0 foreign-data wrapper for csvlog ltree 1.2 data type for hierarchical tree-like structures ltree_plpython2u 1.0 transform between ltree and plpython2u ltree_plpython3u 1.0 transform between ltree and plpython3u ltree_plpythonu 1.0 transform between ltree and plpythonu moddatetime 1.0 functions for tracking last modification time old_snapshot 1.0 utilities in support of old_snapshot_threshold oracle_fdw 1.2 foreign data wrapper for Oracle access oss_fdw 1.1 foreign-data wrapper for OSS access pageinspect 2.1 inspect the contents of database pages at a low level pase 0.0.1 ant ai similarity search pg_bigm 1.2 text similarity measurement and index searching based on bigrams pg_freespacemap 1.2 examine the free space map (FSM) pg_hint_plan 1.4 controls execution plan with hinting phrases in comment of special form pg_buffercache 1.5 examine the shared buffer cache pg_prewarm 1.2 prewarm relation data pg_repack 1.4.8-1 Reorganize tables in PostgreSQL databases with minimal locks pg_sphere 1.0 spherical objects with useful functions, operators and index support pg_cron 1.5 Job scheduler for PostgreSQL pg_jieba 1.1.0 a parser for full-text search of Chinese pg_stat_kcache 2.2.1 Kernel statistics gathering pg_stat_statements 1.9 track planning and execution statistics of all SQL statements executed pg_surgery 1.0 extension to perform surgery on a damaged relation pg_trgm 1.6 text similarity measurement and index searching based on trigrams pg_visibility 1.2 examine the visibility map (VM) and page-level visibility info pg_wait_sampling 1.1 sampling based statistics of wait events pgaudit 1.6.2 provides auditing functionality pgcrypto 1.3 cryptographic functions pgrowlocks 1.2 show row-level locking information pgstattuple 1.5 show tuple-level statistics pgtap 1.2.0 Unit testing for PostgreSQL pldbgapi 1.1 server-side support for debugging PL/pgSQL functions plperl 1.0 PL/Perl procedural language plperlu 1.0 PL/PerlU untrusted procedural language plpgsql 1.0 PL/pgSQL procedural language plpython2u 1.0 PL/Python2U untrusted procedural language plpythonu 1.0 PL/PythonU untrusted procedural language plsql 1.0 Oracle compatible PL/SQL procedural language pltcl 1.0 PL/Tcl procedural language pltclu 1.0 PL/TclU untrusted procedural language polar_bfile 1.0 The BFILE data type enables access to binary file LOBs that are stored in file systems outside Database polar_bpe 1.0 polar_bpe polar_builtin_cast 1.1 Internal extension for builtin casts polar_builtin_funcs 2.0 implement polar builtin functions polar_builtin_type 1.5 polar_builtin_type for PolarDB polar_builtin_view 1.5 polar_builtin_view polar_catalog 1.2 polardb pg extend catalog polar_channel 1.0 polar_channel polar_constraint 1.0 polar_constraint polar_csn 1.0 polar_csn polar_dba_views 1.0 polar_dba_views polar_dbms_alert 1.2 implement polar_dbms_alert - supports asynchronous notification of database events. polar_dbms_application_info 1.0 implement polar_dbms_application_info - record names of executing modules or transactions in the database. polar_dbms_pipe 1.1 implements polar_dbms_pipe - package lets two or more sessions in the same instance communicate. polar_dbms_aq 1.2 implement dbms_aq - provides an interface to Advanced Queuing. polar_dbms_lob 1.3 implement dbms_lob - provides subprograms to operate on BLOBs, CLOBs, and NCLOBs. polar_dbms_output 1.2 implement polar_dbms_output - enables you to send messages from stored procedures. polar_dbms_lock 1.0 implement polar_dbms_lock - provides an interface to Oracle Lock Management services. polar_dbms_aqadm 1.3 polar_dbms_aqadm - procedures to manage Advanced Queuing configuration and administration information. polar_dbms_assert 1.0 implement polar_dbms_assert - provide an interface to validate properties of the input value. polar_dbms_metadata 1.0 implement polar_dbms_metadata - provides a way for you to retrieve metadata from the database dictionary. polar_dbms_random 1.0 implement polar_dbms_random - a built-in random number generator, not intended for cryptography polar_dbms_crypto 1.1 implement dbms_crypto - provides an interface to encrypt and decrypt stored data. polar_dbms_redact 1.0 implement polar_dbms_redact - provides an interface to mask data from queries by an application. polar_dbms_debug 1.1 server-side support for debugging PL/SQL functions polar_dbms_job 1.0 polar_dbms_job polar_dbms_mview 1.1 implement polar_dbms_mview - enables to refresh materialized views. polar_dbms_job_preload 1.0 polar_dbms_job_preload polar_dbms_obfuscation_toolkit 1.1 implement polar_dbms_obfuscation_toolkit - enables an application to get data md5. polar_dbms_rls 1.1 implement polar_dbms_rls - a fine-grained access control administrative built-in package polar_multi_toast_utils 1.0 polar_multi_toast_utils polar_dbms_session 1.2 implement polar_dbms_session - support to set preferences and security levels. polar_odciconst 1.0 implement ODCIConst - Provide some built-in constants in Oracle. polar_dbms_sql 1.2 implement polar_dbms_sql - provides an interface to execute dynamic SQL. polar_osfs_toolkit 1.0 osfs library tools and functions extension polar_dbms_stats 14.0 stabilize plans by fixing statistics polar_monitor 1.5 monitor functions for PolarDB polar_osfs_utils 1.0 osfs library utils extension polar_dbms_utility 1.3 implement polar_dbms_utility - provides various utility subprograms. polar_parameter_check 1.0 kernel extension for parameter validation polar_dbms_xmldom 1.0 implement dbms_xmldom and dbms_xmlparser - support standard DOM interface and xml parser object polar_parameter_manager 1.1 Extension to select parameters for manger. polar_faults 1.0.0 simulate some database faults for end user or testing system. polar_monitor_preload 1.1 examine the polardb information polar_proxy_utils 1.0 Extension to provide operations about proxy. polar_feature_utils 1.2 PolarDB feature utilization polar_global_awr 1.0 PolarDB Global AWR Report polar_publication 1.0 support polardb pg logical replication polar_global_cache 1.0 polar_global_cache polar_px 1.0 Parallel Execution extension polar_serverless 1.0 polar serverless extension polar_resource_manager 1.0 a background process that forcibly frees user session process memory polar_sys_context 1.1 implement polar_sys_context - returns the value of parameter associated with the context namespace at the current instant. polar_gpc 1.3 polar_gpc polar_tde_utils 1.0 Internal extension for TDE polar_gtt 1.1 polar_gtt polar_utl_encode 1.2 implement polar_utl_encode - provides functions that encode RAW data into a standard encoded format polar_htap 1.1 extension for PolarDB HTAP polar_htap_db 1.0 extension for PolarDB HTAP database level operation polar_io_stat 1.0 polar io stat in multi dimension polar_utl_file 1.0 implement utl_file - support PL/SQL programs can read and write operating system text files polar_ivm 1.0 polar_ivm polar_sql_mapping 1.2 Record error sqls and mapping them to correct one polar_stat_sql 1.0 Kernel statistics gathering, and sql plan nodes information gathering tds_fdw 2.0.2 Foreign data wrapper for querying a TDS database (Sybase or Microsoft SQL Server) xml2 1.1 XPath querying and XSLT polar_upgrade_catalogs 1.1 Upgrade catalogs for old version instance polar_utl_i18n 1.1 polar_utl_i18n polar_utl_raw 1.0 implement utl_raw - provides SQL functions for manipulating RAW datatypes. timescaledb 2.9.2 Enables scalable inserts and complex queries for time-series data polar_vfs 1.0 polar virtual file system for different storage polar_worker 1.0 polar_worker postgres_fdw 1.1 foreign-data wrapper for remote PostgreSQL servers refint 1.0 functions for implementing referential integrity (obsolete) roaringbitmap 0.5 support for Roaring Bitmaps tsm_system_time 1.0 TABLESAMPLE method which accepts time in milliseconds as a limit vector 0.5.0 vector data type and ivfflat and hnsw access methods rum 1.3 RUM index access method unaccent 1.1 text search dictionary that removes accents seg 1.4 data type for representing line segments or floating-point intervals sequential_uuids 1.0.2 generator of sequential UUIDs uuid-ossp 1.1 generate universally unique identifiers (UUIDs) smlar 1.0 compute similary of any one-dimensional arrays varbitx 1.1 varbit functions pack sslinfo 1.2 information about SSL certificates tablefunc 1.0 functions that manipulate whole tables, including crosstab tcn 1.0 Triggered change notifications zhparser 1.0 a parser for full-text search of Chinese address_standardizer 3.3.2 Ganos PostGIS address standardizer address_standardizer_data_us 3.3.2 Ganos PostGIS address standardizer data us ganos_fdw 6.0 Ganos Spatial FDW extension for POLARDB ganos_geometry 6.0 Ganos geometry lite extension for POLARDB ganos_geometry_pyramid 6.0 Ganos Geometry Pyramid extension for POLARDB ganos_geometry_sfcgal 6.0 Ganos geometry lite sfcgal extension for POLARDB ganos_geomgrid 6.0 Ganos geometry grid extension for POLARDB ganos_importer 6.0 Ganos Spatial importer extension for POLARDB ganos_networking 6.0 Ganos networking ganos_pointcloud 6.0 Ganos pointcloud extension For POLARDB ganos_pointcloud_geometry 6.0 Ganos_pointcloud LIDAR data and ganos_geometry data for POLARDB ganos_raster 6.0 Ganos raster extension for POLARDB ganos_scene 6.0 Ganos scene extension for POLARDB ganos_sfmesh 6.0 Ganos surface mesh extension for POLARDB ganos_spatialref 6.0 Ganos spatial reference extension for POLARDB ganos_trajectory 6.0 Ganos trajectory extension for POLARDB ganos_vomesh 6.0 Ganos volumn mesh extension for POLARDB postgis_tiger_geocoder 3.3.2 Ganos PostGIS tiger geocoder postgis_topology 3.3.2 Ganos PostGIS topology ","categories":["Concept"],"description":"Using Alibaba Cloud's commercial PolarDB for Oracle kernel (closed source, PG14, only available in special enterprise edition customization)","excerpt":"Using Alibaba Cloud's commercial PolarDB for Oracle kernel (closed …","ref":"/docs/pgsql/kernel/polardb-o/","tags":["PolarDB"],"title":"PolarDB Oracle"},{"body":"PostgresML is a PostgreSQL extension that supports the latest large language models (LLM), vector operations, classical machine learning, and traditional Postgres application workloads.\nPostgresML (pgml) is a PostgreSQL extension written in Rust. You can run standalone Docker images, but this documentation is not a docker-compose template introduction, for reference only.\nPostgresML officially supports Ubuntu 22.04, but we also maintain RPM versions for EL 8/9, if you don’t need CUDA and NVIDIA-related features.\nYou need internet access on database nodes to download Python dependencies from PyPI and models from HuggingFace.\nPostgresML is Deprecated Because the company behind it has ceased operations.\nConfiguration PostgresML is an extension written in Rust, officially supporting Ubuntu. Pigsty maintains RPM versions of PostgresML on EL8 and EL9.\nCreating a New Cluster\nPostgresML 2.7.9 is available for PostgreSQL 15, supporting Ubuntu 22.04 (official), Debian 12, and EL 8/9 (maintained by Pigsty). To enable pgml, you first need to install the extension:\npg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta pg_users: - {name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [dbrole_admin] ,comment: pigsty admin user } - {name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [dbrole_readonly] ,comment: read-only viewer for meta database } pg_databases: - { name: meta ,baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [pigsty] ,extensions: [{name: postgis, schema: public}, {name: timescaledb}]} pg_hba_rules: - {user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes'} pg_libs: 'pgml, pg_stat_statements, auto_explain' pg_extensions: [ 'pgml_15 pgvector_15 wal2json_15 repack_15' ] # ubuntu #pg_extensions: [ 'postgresql-pgml-15 postgresql-15-pgvector postgresql-15-wal2json postgresql-15-repack' ] # ubuntu On EL 8/9, the extension name is pgml_15, corresponding to the Ubuntu/Debian name postgresql-pgml-15. You also need to add pgml to pg_libs.\nEnabling on an Existing Cluster\nTo enable pgml on an existing cluster, you can install it using Ansible’s package module:\nansible pg-meta -m package -b -a 'name=pgml_15' # ansible el8,el9 -m package -b -a 'name=pgml_15' # EL 8/9 # ansible u22 -m package -b -a 'name=postgresql-pgml-15' # Ubuntu 22.04 jammy Python Dependencies You also need to install PostgresML’s Python dependencies on cluster nodes. Official tutorial: Installation Guide\nInstall Python and PIP\nEnsure python3, pip, and venv are installed:\n# Ubuntu 22.04 (python3.10), need to install pip and venv using apt sudo apt install -y python3 python3-pip python3-venv For EL 8 / EL9 and compatible distributions, you can use python3.11:\n# EL 8/9, can upgrade the default pip and virtualenv sudo yum install -y python3.11 python3.11-pip # install latest python3.11 python3.11 -m pip install --upgrade pip virtualenv # use python3.11 on EL8 / EL9 Using PyPI Mirrors For users in mainland China, we recommend using Tsinghua University’s PyPI mirror.\npip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple # set global mirror (recommended) pip install -i https://pypi.tuna.tsinghua.edu.cn/simple some-package # use for single installation Install Dependencies\nCreate a Python virtual environment and use pip to install dependencies from requirements.txt and requirements-xformers.txt.\nIf you’re using EL 8/9, replace python3 with python3.11 in the following commands.\nsu - postgres; # create virtual environment as database superuser mkdir -p /data/pgml; cd /data/pgml; # create virtual environment directory python3 -m venv /data/pgml # create virtual environment directory (Ubuntu 22.04) source /data/pgml/bin/activate # activate virtual environment # write Python dependencies and install with pip cat \u003e /data/pgml/requirments.txt \u003c\u003cEOF accelerate==0.22.0 auto-gptq==0.4.2 bitsandbytes==0.41.1 catboost==1.2 ctransformers==0.2.27 datasets==2.14.5 deepspeed==0.10.3 huggingface-hub==0.17.1 InstructorEmbedding==1.0.1 lightgbm==4.1.0 orjson==3.9.7 pandas==2.1.0 rich==13.5.2 rouge==1.0.1 sacrebleu==2.3.1 sacremoses==0.0.53 scikit-learn==1.3.0 sentencepiece==0.1.99 sentence-transformers==2.2.2 tokenizers==0.13.3 torch==2.0.1 torchaudio==2.0.2 torchvision==0.15.2 tqdm==4.66.1 transformers==4.33.1 xgboost==2.0.0 langchain==0.0.287 einops==0.6.1 pynvml==11.5.0 EOF # install dependencies using pip in the virtual environment python3 -m pip install -r /data/pgml/requirments.txt python3 -m pip install xformers==0.0.21 --no-dependencies # additionally, 3 Python packages need to be installed globally using sudo! sudo python3 -m pip install xgboost lightgbm scikit-learn Enable PostgresML After installing the pgml extension and Python dependencies on all cluster nodes, you can enable pgml on the PostgreSQL cluster.\nUse the patronictl command to configure the cluster, add pgml to shared_preload_libraries, and specify your virtual environment directory in pgml.venv:\nshared_preload_libraries: pgml, timescaledb, pg_stat_statements, auto_explain pgml.venv: '/data/pgml' Then restart the database cluster and create the extension using SQL commands:\nCREATE EXTENSION vector; -- also recommend installing pgvector! CREATE EXTENSION pgml; -- create PostgresML in the current database SELECT pgml.version(); -- print PostgresML version information If everything is normal, you should see output similar to the following:\n# create extension pgml; INFO: Python version: 3.11.2 (main, Oct 5 2023, 16:06:03) [GCC 8.5.0 20210514 (Red Hat 8.5.0-18)] INFO: Scikit-learn 1.3.0, XGBoost 2.0.0, LightGBM 4.1.0, NumPy 1.26.1 CREATE EXTENSION # SELECT pgml.version(); -- print PostgresML version information version --------- 2.7.8 Done! For more details, please refer to the official PostgresML documentation: https://postgresml.org/docs/guides/use-cases/\n","categories":["Concept"],"description":"How to deploy PostgresML with Pigsty: ML, training, inference, Embedding, RAG inside DB.\n","excerpt":"How to deploy PostgresML with Pigsty: ML, training, inference, …","ref":"/docs/pgsql/kernel/pgml/","tags":["PostgresML","AI","ML"],"title":"PostgresML"},{"body":"Pigsty supports deploying Greenplum clusters and its derivative distribution YMatrixDB, and provides the capability to integrate existing Greenplum deployments into Pigsty monitoring.\nOverview Greenplum / YMatrix cluster deployment capabilities are only available in the professional/enterprise editions and are not currently open source.\nInstallation Pigsty provides installation packages for Greenplum 6 (@el7) and Greenplum 7 (@el8). Open source users can install and configure them manually.\n# EL 7 Only (Greenplum6) ./node.yml -t node_install -e '{\"node_repo_modules\":\"pgsql\",\"node_packages\":[\"open-source-greenplum-db-6\"]}' # EL 8 Only (Greenplum7) ./node.yml -t node_install -e '{\"node_repo_modules\":\"pgsql\",\"node_packages\":[\"open-source-greenplum-db-7\"]}' Configuration To define a Greenplum cluster, you need to use pg_mode = gpsql and additional identity parameters pg_shard and gp_role.\n#================================================================# # GPSQL Clusters # #================================================================# #----------------------------------# # cluster: mx-mdw (gp master) #----------------------------------# mx-mdw: hosts: 10.10.10.10: { pg_seq: 1, pg_role: primary , nodename: mx-mdw-1 } vars: gp_role: master # this cluster is used as greenplum master pg_shard: mx # pgsql sharding name \u0026 gpsql deployment name pg_cluster: mx-mdw # this master cluster name is mx-mdw pg_databases: - { name: matrixmgr , extensions: [ { name: matrixdbts } ] } - { name: meta } pg_users: - { name: meta , password: DBUser.Meta , pgbouncer: true } - { name: dbuser_monitor , password: DBUser.Monitor , roles: [ dbrole_readonly ], superuser: true } pgbouncer_enabled: true # enable pgbouncer for greenplum master pgbouncer_exporter_enabled: false # enable pgbouncer_exporter for greenplum master pg_exporter_params: 'host=127.0.0.1\u0026sslmode=disable' # use 127.0.0.1 as local monitor host #----------------------------------# # cluster: mx-sdw (gp master) #----------------------------------# mx-sdw: hosts: 10.10.10.11: nodename: mx-sdw-1 # greenplum segment node pg_instances: # greenplum segment instances 6000: { pg_cluster: mx-seg1, pg_seq: 1, pg_role: primary , pg_exporter_port: 9633 } 6001: { pg_cluster: mx-seg2, pg_seq: 2, pg_role: replica , pg_exporter_port: 9634 } 10.10.10.12: nodename: mx-sdw-2 pg_instances: 6000: { pg_cluster: mx-seg2, pg_seq: 1, pg_role: primary , pg_exporter_port: 9633 } 6001: { pg_cluster: mx-seg3, pg_seq: 2, pg_role: replica , pg_exporter_port: 9634 } 10.10.10.13: nodename: mx-sdw-3 pg_instances: 6000: { pg_cluster: mx-seg3, pg_seq: 1, pg_role: primary , pg_exporter_port: 9633 } 6001: { pg_cluster: mx-seg1, pg_seq: 2, pg_role: replica , pg_exporter_port: 9634 } vars: gp_role: segment # these are nodes for gp segments pg_shard: mx # pgsql sharding name \u0026 gpsql deployment name pg_cluster: mx-sdw # these segment clusters name is mx-sdw pg_preflight_skip: true # skip preflight check (since pg_seq \u0026 pg_role \u0026 pg_cluster not exists) pg_exporter_config: pg_exporter_basic.yml # use basic config to avoid segment server crash pg_exporter_params: 'options=-c%20gp_role%3Dutility\u0026sslmode=disable' # use gp_role = utility to connect to segments Additionally, PG Exporter requires extra connection parameters to connect to Greenplum Segment instances for metric collection.\n","categories":["Concept"],"description":"Deploy/Monitor Greenplum clusters with Pigsty, build Massively Parallel Processing (MPP) PostgreSQL data warehouse clusters!","excerpt":"Deploy/Monitor Greenplum clusters with Pigsty, build Massively …","ref":"/docs/pgsql/kernel/greenplum/","tags":["Greenplum"],"title":"Greenplum"},{"body":" Installation Pigsty provides installation packages for Greenplum 6 (@el7) and Greenplum 7 (@el8). Open source users can install and configure them manually.\n# EL 7 Only (Greenplum6) ./node.yml -t node_install -e '{\"node_repo_modules\":\"pgsql\",\"node_packages\":[\"cloudberrydb\"]}' # EL 8 Only (Greenplum7) ./node.yml -t node_install -e '{\"node_repo_modules\":\"pgsql\",\"node_packages\":[\"cloudberrydb\"]}' ","categories":["Concept"],"description":"Deploy/Monitor Cloudberry clusters with Pigsty, an MPP data warehouse cluster forked from Greenplum!","excerpt":"Deploy/Monitor Cloudberry clusters with Pigsty, an MPP data warehouse …","ref":"/docs/pgsql/kernel/cloudberry/","tags":["Cloudberry"],"title":"Cloudberry"},{"body":"Neon adopts a storage and compute separation architecture, providing seamless autoscaling, scale to zero, and unique database branching capabilities.\nNeon official website: https://neon.tech/\nThe compiled binaries of Neon are excessively large and are currently not available to open-source users. It is currently in the pilot stage. If you have requirements, please contact Pigsty sales.\n","categories":["Concept"],"description":"Use Neon's open-source Serverless PostgreSQL kernel to build flexible, scale-to-zero, forkable PG services.","excerpt":"Use Neon's open-source Serverless PostgreSQL kernel to build flexible, …","ref":"/docs/pgsql/kernel/neon/","tags":["Neon"],"title":"Neon"},{"body":"Configuration | Administration | Playbooks | Monitoring | Parameters\nOverview Every Pigsty deployment includes a set of infrastructure components that provide services for managed nodes and database clusters:\nComponent Port Domain Description Nginx 80/443 i.pigsty Web service portal, local repo, and unified entry point Grafana 3000 g.pigsty Visualization platform for monitoring dashboards and data apps VictoriaMetrics 8428 p.pigsty Time-series database with VMUI, compatible with Prometheus API VictoriaLogs 9428 - Centralized log database, receives structured logs from Vector VictoriaTraces 10428 - Tracing and event storage for slow SQL / request tracing VMAlert 8880 - Alert rule evaluator, triggers alerts based on VictoriaMetrics metrics AlertManager 9059 a.pigsty Alert aggregation and dispatch, receives notifications from VMAlert BlackboxExporter 9115 - ICMP/TCP/HTTP blackbox probing DNSMASQ 53 - DNS server for internal domain resolution Chronyd 123 - NTP time server PostgreSQL 5432 - CMDB and default database Ansible - - Runs playbooks, orchestrates all infrastructure In Pigsty, the PGSQL module uses some services on INFRA nodes, specifically:\nDatabase cluster/host node domains depend on DNSMASQ on INFRA nodes for resolution. Installing software on database nodes uses the local yum/apt repo hosted by Nginx on INFRA nodes. Database cluster/node monitoring metrics are scraped and stored by VictoriaMetrics on INFRA nodes, accessible via VMUI / PromQL. Database and node runtime logs are collected by Vector and pushed to VictoriaLogs on INFRA, searchable in Grafana. VMAlert evaluates alert rules based on metrics in VictoriaMetrics and forwards events to Alertmanager. Users initiate management of database nodes from Infra/Admin nodes using Ansible or other tools: Execute cluster creation, scaling, instance/cluster recycling Create business users, databases, modify services, HBA changes; Execute log collection, garbage cleanup, backup, inspections, etc. Database nodes sync time from the NTP server on INFRA/ADMIN nodes by default If no dedicated cluster exists, the HA component Patroni uses etcd on INFRA nodes as the HA DCS. If no dedicated cluster exists, the backup component pgbackrest uses MinIO on INFRA nodes as an optional centralized backup repository. Nginx Nginx is the access entry point for all WebUI services in Pigsty, using port 80 on the admin node by default.\nMany infrastructure components with WebUI are exposed through Nginx, such as Grafana, VictoriaMetrics (VMUI), AlertManager, and HAProxy traffic management pages. Additionally, static file resources like yum/apt repos are served through Nginx.\nNginx routes access requests to corresponding upstream components based on domain names according to infra_portal configuration. If you use other domains or public domains, you can modify them here:\ninfra_portal: # domain names and upstream servers home : { domain: i.pigsty } grafana : { domain: g.pigsty ,endpoint: \"${admin_ip}:3000\" , websocket: true } prometheus : { domain: p.pigsty ,endpoint: \"${admin_ip}:8428\" } # VMUI alertmanager : { domain: a.pigsty ,endpoint: \"${admin_ip}:9059\" } blackbox : { endpoint: \"${admin_ip}:9115\" } vmalert : { endpoint: \"${admin_ip}:8880\" } #logs : { domain: logs.pigsty ,endpoint: \"${admin_ip}:9428\" } #minio : { domain: sss.pigsty ,endpoint: \"${admin_ip}:9001\" ,scheme: https ,websocket: true } Pigsty strongly recommends using domain names to access Pigsty UI systems rather than direct IP+port access, for these reasons:\nUsing domains makes it easy to enable HTTPS traffic encryption, consolidate access to Nginx, audit all requests, and conveniently integrate authentication mechanisms. Some components only listen on 127.0.0.1 by default, so they can only be accessed through Nginx proxy. Domain names are easier to remember and provide additional configuration flexibility. If you don’t have available internet domains or local DNS resolution, you can add local static resolution records in /etc/hosts (MacOS/Linux) or C:\\Windows\\System32\\drivers\\etc\\hosts (Windows).\nNginx configuration parameters are at: Configuration: INFRA - NGINX\nLocal Software Repository Pigsty creates a local software repository during installation to accelerate subsequent software installation.\nThis repository is served by Nginx, located by default at /www/pigsty, accessible via http://i.pigsty/pigsty.\nPigsty’s offline package is the entire software repository directory (yum/apt) compressed. When Pigsty tries to build a local repo, if it finds the local repo directory /www/pigsty already exists with the /www/pigsty/repo_complete marker file, it considers the local repo already built and skips downloading software from upstream, eliminating internet dependency.\nThe repo definition file is at /www/pigsty.repo, accessible by default via http://${admin_ip}/pigsty.repo\ncurl -L http://i.pigsty/pigsty.repo -o /etc/yum.repos.d/pigsty.repo You can also use the file local repo directly without Nginx:\n[pigsty-local] name=Pigsty local $releasever - $basearch baseurl=file:///www/pigsty/ enabled=1 gpgcheck=0 Local repository configuration parameters are at: Configuration: INFRA - REPO\nVictoria Observability Suite Pigsty v4.0 uses the VictoriaMetrics family to replace Prometheus/Loki, providing unified monitoring, logging, and tracing capabilities:\nVictoriaMetrics listens on port 8428 by default, accessible via http://p.pigsty or https://i.pigsty/vmetrics/ for VMUI, compatible with Prometheus API. VMAlert evaluates alert rules in /infra/rules/*.yml, listens on port 8880, and sends alert events to Alertmanager. VictoriaLogs listens on port 9428, supports the https://i.pigsty/vlogs/ query interface. All nodes run Vector by default, pushing structured system logs, PostgreSQL logs, etc. to VictoriaLogs. VictoriaTraces listens on port 10428 for slow SQL / Trace collection, Grafana accesses it as a Jaeger datasource. Alertmanager listens on port 9059, accessible via http://a.pigsty or https://i.pigsty/alertmgr/ for managing alert notifications. After configuring SMTP, Webhook, etc., it can push messages. Blackbox Exporter listens on port 9115 by default for Ping/TCP/HTTP probing, accessible via https://i.pigsty/blackbox/. For more information, see: Configuration: INFRA - VICTORIA and Configuration: INFRA - PROMETHEUS.\nGrafana Grafana is the core of Pigsty’s WebUI, listening on port 3000 by default, accessible directly via IP:3000 or domain http://g.pigsty.\nPigsty comes with preconfigured datasources for VictoriaMetrics / Logs / Traces (vmetrics-*, vlogs-*, vtraces-*), and numerous dashboards with URL-based navigation for quick problem location.\nGrafana can also be used as a general low-code visualization platform, so Pigsty installs plugins like ECharts and victoriametrics-datasource by default for building monitoring dashboards or inspection reports.\nGrafana configuration parameters are at: Configuration: INFRA - GRAFANA.\nAnsible Pigsty installs Ansible on the meta node by default. Ansible is a popular operations tool with declarative configuration style and idempotent playbook design that greatly reduces system maintenance complexity.\nDNSMASQ DNSMASQ provides DNS resolution services within the environment. Domain names from other modules are registered with the DNSMASQ service on INFRA nodes.\nDNS records are placed by default in the /etc/hosts.d/ directory on all INFRA nodes.\nDNSMASQ configuration parameters are at: Configuration: INFRA - DNS\nChronyd NTP service synchronizes time across all nodes in the environment (optional)\nNTP configuration parameters are at: Configuration: NODES - NTP\nConfiguration To install the INFRA module on a node, first add it to the infra group in the config inventory and assign an instance number infra_seq\n# Configure single INFRA node infra: { hosts: { 10.10.10.10: { infra_seq: 1 } }} # Configure two INFRA nodes infra: hosts: 10.10.10.10: { infra_seq: 1 } 10.10.10.11: { infra_seq: 2 } Then use the infra.yml playbook to initialize the INFRA module on the nodes.\nAdministration Here are some administration tasks related to the INFRA module:\nInstall/Uninstall Infra Module ./infra.yml # Install INFRA module on infra group ./infra-rm.yml # Uninstall INFRA module from infra group Manage Local Software Repository You can use the following playbook subtasks to manage the local yum repo on Infra nodes:\n./infra.yml -t repo # Create local repo from internet or offline package ./infra.yml -t repo_dir # Create local repo directory ./infra.yml -t repo_check # Check if local repo already exists ./infra.yml -t repo_prepare # If exists, use existing local repo ./infra.yml -t repo_build # If not exists, build local repo from upstream ./infra.yml -t repo_upstream # Handle upstream repo files in /etc/yum.repos.d ./infra.yml -t repo_remove # If repo_remove == true, delete existing repo files ./infra.yml -t repo_add # Add upstream repo files to /etc/yum.repos.d (or /etc/apt/sources.list.d) ./infra.yml -t repo_url_pkg # Download packages from internet defined by repo_url_packages ./infra.yml -t repo_cache # Create upstream repo metadata cache with yum makecache / apt update ./infra.yml -t repo_boot_pkg # Install bootstrap packages like createrepo_c, yum-utils... (or dpkg-) ./infra.yml -t repo_pkg # Download packages \u0026 dependencies from upstream repos ./infra.yml -t repo_create # Create local repo with createrepo_c \u0026 modifyrepo_c ./infra.yml -t repo_use # Add newly built repo to /etc/yum.repos.d | /etc/apt/sources.list.d ./infra.yml -t repo_nginx # If no nginx serving, start nginx as web server The most commonly used commands are:\n./infra.yml -t repo_upstream # Add upstream repos defined in repo_upstream to INFRA nodes ./infra.yml -t repo_pkg # Download packages and dependencies from upstream repos ./infra.yml -t repo_create # Create/update local yum repo with createrepo_c \u0026 modifyrepo_c Manage Infrastructure Components You can use the following playbook subtasks to manage various infrastructure components on Infra nodes:\n./infra.yml -t infra # Configure infrastructure ./infra.yml -t infra_env # Configure environment variables on admin node: env_dir, env_pg, env_var ./infra.yml -t infra_pkg # Install software packages required by INFRA: infra_pkg_yum, infra_pkg_pip ./infra.yml -t infra_user # Setup infra OS user group ./infra.yml -t infra_cert # Issue certificates for infra components ./infra.yml -t dns # Configure DNSMasq: dns_config, dns_record, dns_launch ./infra.yml -t nginx # Configure Nginx: nginx_config, nginx_cert, nginx_static, nginx_launch, nginx_exporter ./infra.yml -t victoria # Configure VictoriaMetrics/Logs/Traces: vmetrics|vlogs|vtraces|vmalert ./infra.yml -t alertmanager # Configure AlertManager: alertmanager_config, alertmanager_launch ./infra.yml -t blackbox # Configure Blackbox Exporter: blackbox_launch ./infra.yml -t grafana # Configure Grafana: grafana_clean, grafana_config, grafana_plugin, grafana_launch, grafana_provision ./infra.yml -t infra_register # Register infra components to VictoriaMetrics / Grafana Other commonly used tasks include:\n./infra.yml -t nginx_index # Re-render Nginx homepage content ./infra.yml -t nginx_config,nginx_reload # Re-render Nginx portal config, expose new upstream services ./infra.yml -t vmetrics_config,vmetrics_launch # Regenerate VictoriaMetrics main config and restart service ./infra.yml -t vlogs_config,vlogs_launch # Re-render VictoriaLogs config ./infra.yml -t vmetrics_clean # Clean VictoriaMetrics storage data directory ./infra.yml -t grafana_plugin # Download Grafana plugins from internet Playbooks Pigsty provides three playbooks related to the INFRA module:\ninfra.yml: Initialize pigsty infrastructure on infra nodes infra-rm.yml: Remove infrastructure components from infra nodes deploy.yml: Complete one-time Pigsty installation on all nodes infra.yml The INFRA module playbook infra.yml initializes pigsty infrastructure on INFRA nodes\nExecuting this playbook completes the following tasks\nConfigure meta node directories and environment variables Download and build a local software repository to accelerate subsequent installation. (If using offline package, skip download phase) Add the current meta node as a regular node under Pigsty management Deploy infrastructure components including VictoriaMetrics/Logs/Traces, VMAlert, Grafana, Alertmanager, Blackbox Exporter, etc. This playbook executes on INFRA nodes by default\nPigsty uses the current node executing this playbook as Pigsty’s INFRA node and ADMIN node by default. During configuration, Pigsty marks the current node as Infra/Admin node and replaces the placeholder IP 10.10.10.10 in config templates with the current node’s primary IP address. Besides initiating management and hosting infrastructure, this node is no different from a regular managed node. In single-node installation, ETCD is also installed on this node to provide DCS service Notes about this playbook\nThis is an idempotent playbook; repeated execution will wipe infrastructure components on meta nodes. To preserve historical monitoring data, first set vmetrics_clean, vlogs_clean, vtraces_clean to false. When offline repo /www/pigsty/repo_complete exists, this playbook skips downloading software from internet. Full execution takes about 5-8 minutes depending on machine configuration. Downloading directly from upstream internet sources without offline package may take 10-20 minutes depending on your network conditions. infra-rm.yml The INFRA module playbook infra-rm.yml removes pigsty infrastructure from INFRA nodes\nCommon subtasks include:\n./infra-rm.yml # Remove INFRA module ./infra-rm.yml -t service # Stop infrastructure services on INFRA ./infra-rm.yml -t data # Remove remaining data on INFRA ./infra-rm.yml -t package # Uninstall software packages installed on INFRA deploy.yml The INFRA module playbook deploy.yml performs a complete one-time Pigsty installation on all nodes\nThis playbook is described in more detail in Playbook: One-Time Installation.\nMonitoring Pigsty Home: Pigsty monitoring system homepage\nPigsty Home Dashboard INFRA Overview: Pigsty infrastructure self-monitoring overview\nINFRA Overview Dashboard Nginx Instance: Nginx metrics and logs\nNginx Overview Dashboard Grafana Instance: Grafana metrics and logs\nGrafana Overview Dashboard VictoriaMetrics Instance: VictoriaMetrics scraping, querying, and storage metrics\nVMAlert Instance: Alert rule evaluation and queue status\nAlertmanager Instance: Alert aggregation, notification pipelines, and Silences\nVictoriaLogs Instance: Log ingestion rate, query load, and index hits\nVictoriaTraces Instance: Trace/KV storage and Jaeger interface\nLogs Instance: Node log search based on Vector + VictoriaLogs\nLogs Instance Dashboard CMDB Overview: CMDB visualization\nCMDB Overview Dashboard ETCD Overview: etcd metrics and logs\nETCD Overview Dashboard Parameters The INFRA module has the following 10 parameter groups.\nMETA: Pigsty metadata CA: Self-signed PKI/CA infrastructure INFRA_ID: Infrastructure portal, Nginx domains REPO: Local software repository INFRA_PACKAGE: Infrastructure software packages NGINX: Nginx web server DNS: DNSMASQ domain server VICTORIA: VictoriaMetrics / Logs / Traces suite PROMETHEUS: Alertmanager and Blackbox Exporter GRAFANA: Grafana observability suite Parameter Overview For the latest default values, types, and hierarchy, please refer to the Parameter Reference to stay consistent with the Pigsty version.\n","categories":["Reference"],"description":"Optional standalone infrastructure that provides NTP, DNS, observability and other foundational services for PostgreSQL.","excerpt":"Optional standalone infrastructure that provides NTP, DNS, …","ref":"/docs/infra/","tags":"","title":"Module: INFRA"},{"body":"Configuration Guide INFRA = primarily monitoring infrastructure, optional for PostgreSQL databases.\nUnless manually configured to depend on DNS/NTP services on INFRA nodes, INFRA module failures typically don’t affect PG cluster operations.\nSingle INFRA node suffices for most scenarios. Prod env recommends 2-3 INFRA nodes for HA.\nFor better resource utilization, ETCD module (required by PG HA) can share nodes with INFRA module.\nUsing more than 3 INFRA nodes provides little additional benefit, but more ETCD nodes (e.g., 5) can improve DCS availability.\nConfiguration Examples Add node IPs to infra group in config inventory, assign INFRA instance number infra_seq.\nDefault single INFRA node config:\nall: children: infra: { hosts: { 10.10.10.10: { infra_seq: 1 } }} By default, 10.10.10.10 placeholder replaced with current node’s primary IP during config.\nUse infra.yml playbook to init INFRA module on nodes.\nMore Nodes Two INFRA nodes config:\nall: children: infra: hosts: 10.10.10.10: { infra_seq: 1 } 10.10.10.11: { infra_seq: 2 } Three INFRA nodes config (with params):\nall: children: infra: hosts: 10.10.10.10: { infra_seq: 1 } 10.10.10.11: { infra_seq: 2, repo_enabled: false } 10.10.10.12: { infra_seq: 3, repo_enabled: false } vars: grafana_clean: false vmetrics_clean: false vlogs_clean: false vtraces_clean: false INFRA High Availability Most INFRA module components = “stateless/identical state”. For HA, focus on “load balancing”.\nHA achievable via Keepalived L2 VIP or HAProxy L4 load balancing. L2 VIP recommended for L2-reachable networks.\nConfig example:\ninfra: hosts: 10.10.10.10: { infra_seq: 1 } 10.10.10.11: { infra_seq: 2 } 10.10.10.12: { infra_seq: 3 } vars: vip_enabled: true vip_vrid: 128 vip_address: 10.10.10.8 vip_interface: eth1 infra_portal: home : { domain: i.pigsty } grafana : { domain: g.pigsty ,endpoint: \"10.10.10.8:3000\" , websocket: true } prometheus : { domain: p.pigsty ,endpoint: \"10.10.10.8:8428\" } alertmanager : { domain: a.pigsty ,endpoint: \"10.10.10.8:9059\" } blackbox : { endpoint: \"10.10.10.8:9115\" } vmalert : { endpoint: \"10.10.10.8:8880\" } Set VIP-related params and modify service endpoints in infra_portal.\nNginx Configuration See Nginx Parameter Config and Tutorial: Nginx.\nLocal Repo Configuration See Repo Parameter Config.\nDNS Configuration See DNS Parameter Config and Tutorial: DNS.\nNTP Configuration See NTP Parameter Config.\n","categories":["Reference"],"description":"How to configure INFRA nodes? Customize Nginx, local repo, DNS, NTP, monitoring components.","excerpt":"How to configure INFRA nodes? Customize Nginx, local repo, DNS, NTP, …","ref":"/docs/infra/config/","tags":"","title":"Configuration"},{"body":"The INFRA module is responsible for deploying Pigsty’s infrastructure components: local software repository, Nginx, DNSMasq, VictoriaMetrics, VictoriaLogs, Grafana, Alertmanager, Blackbox Exporter, and other monitoring and alerting infrastructure.\nPigsty v4.0 uses VictoriaMetrics to replace Prometheus and VictoriaLogs to replace Loki, providing a superior observability solution.\nSection Description META Pigsty metadata: version, admin IP, region, language, proxy CA Self-signed CA certificate management INFRA_ID Infrastructure node identity and service portal REPO Local software repository configuration INFRA_PACKAGE Infrastructure node package installation NGINX Nginx web server and reverse proxy configuration DNS DNSMasq DNS server configuration VICTORIA VictoriaMetrics/Logs/Traces observability stack PROMETHEUS Alertmanager and Blackbox Exporter GRAFANA Grafana visualization platform configuration Parameter Overview META parameters define Pigsty metadata, including version string, admin node IP, repository mirror region, default language, and proxy settings.\nParameter Type Level Description version string G Pigsty version string admin_ip ip G Admin node IP address region enum G Upstream mirror region: default,china,europe language enum G Default language: en or zh proxy_env dict G Global proxy environment variables CA parameters configure Pigsty’s self-signed CA certificate management, including CA creation, CA name, and certificate validity.\nParameter Type Level Description ca_create bool G Create CA if not exists? Default true ca_cn string G CA CN name, fixed as pigsty-ca cert_validity interval G Certificate validity, default 20 years INFRA_ID parameters define infrastructure node identity, including node sequence number, service portal configuration, and data directory.\nParameter Type Level Description infra_seq int I Infrastructure node sequence, REQUIRED infra_portal dict G Infrastructure services exposed via Nginx portal infra_data path G Infrastructure data directory, default /data/infra REPO parameters configure the local software repository, including repository enable switch, directory paths, upstream source definitions, and packages to download.\nParameter Type Level Description repo_enabled bool G/I Create local repo on this infra node? repo_home path G Repo home directory, default /www repo_name string G Repo name, default pigsty repo_endpoint url G Repo access endpoint: domain or ip:port repo_remove bool G/A Remove existing upstream repo definitions? repo_modules string G/A Enabled upstream repo modules, comma separated repo_upstream upstream[] G Upstream repo definitions repo_packages string[] G Packages to download from upstream repo_extra_packages string[] G/C/I Extra packages to download repo_url_packages string[] G Extra packages downloaded via URL INFRA_PACKAGE parameters define packages to install on infrastructure nodes, including RPM/DEB packages and PIP packages.\nParameter Type Level Description infra_packages string[] G Packages to install on infra nodes infra_packages_pip string G Pip packages to install on infra nodes NGINX parameters configure Nginx web server and reverse proxy, including enable switch, ports, SSL mode, certificates, and basic authentication.\nParameter Type Level Description nginx_enabled bool G/I Enable Nginx on this infra node? nginx_clean bool G/A Clean existing Nginx config during init? nginx_exporter_enabled bool G/I Enable nginx_exporter on this infra node? nginx_exporter_port port G nginx_exporter listen port, default 9113 nginx_sslmode enum G Nginx SSL mode: disable,enable,enforce nginx_cert_validity duration G Nginx self-signed cert validity, default 397d nginx_home path G Nginx content dir, default /www, symlink to nginx_data nginx_data path G Nginx actual data dir, default /data/nginx nginx_users dict G Nginx basic auth users: username-password dict nginx_port port G Nginx listen port, default 80 nginx_ssl_port port G Nginx SSL listen port, default 443 certbot_sign bool G/A Sign cert with certbot? certbot_email string G/A Certbot notification email address certbot_options string G/A Certbot extra command line options DNS parameters configure DNSMasq DNS server, including enable switch, listen port, and dynamic DNS records.\nParameter Type Level Description dns_enabled bool G/I Setup dnsmasq on this infra node? dns_port port G DNS server listen port, default 53 dns_records string[] G Dynamic DNS records resolved by dnsmasq VICTORIA parameters configure the VictoriaMetrics/Logs/Traces observability stack, including enable switches, ports, and data retention policies.\nParameter Type Level Description vmetrics_enabled bool G/I Enable VictoriaMetrics on this infra node? vmetrics_clean bool G/A Clean VictoriaMetrics data during init? vmetrics_port port G VictoriaMetrics listen port, default 8428 vmetrics_scrape_interval interval G Global scrape interval, default 10s vmetrics_scrape_timeout interval G Global scrape timeout, default 8s vmetrics_options arg G VictoriaMetrics extra CLI options vlogs_enabled bool G/I Enable VictoriaLogs on this infra node? vlogs_clean bool G/A Clean VictoriaLogs data during init? vlogs_port port G VictoriaLogs listen port, default 9428 vlogs_options arg G VictoriaLogs extra CLI options vtraces_enabled bool G/I Enable VictoriaTraces on this infra node? vtraces_clean bool G/A Clean VictoriaTraces data during init? vtraces_port port G VictoriaTraces listen port, default 10428 vtraces_options arg G VictoriaTraces extra CLI options vmalert_enabled bool G/I Enable VMAlert on this infra node? vmalert_port port G VMAlert listen port, default 8880 vmalert_options arg G VMAlert extra CLI options PROMETHEUS parameters configure Alertmanager and Blackbox Exporter, providing alert management and network probing capabilities.\nParameter Type Level Description blackbox_enabled bool G/I Setup blackbox_exporter on this infra node? blackbox_port port G blackbox_exporter listen port, default 9115 blackbox_options arg G blackbox_exporter extra CLI options alertmanager_enabled bool G/I Setup alertmanager on this infra node? alertmanager_port port G AlertManager listen port, default 9059 alertmanager_options arg G alertmanager extra CLI options exporter_metrics_path path G Exporter metrics path, default /metrics GRAFANA parameters configure the Grafana visualization platform, including enable switch, port, admin credentials, and data source configuration.\nParameter Type Level Description grafana_enabled bool G/I Enable Grafana on this infra node? grafana_port port G Grafana listen port, default 3000 grafana_clean bool G/A Clean Grafana data during init? grafana_admin_username username G Grafana admin username, default admin grafana_admin_password password G Grafana admin password, default pigsty grafana_auth_proxy bool G Enable Grafana auth proxy? grafana_pgurl url G External PostgreSQL URL for Grafana persistence grafana_view_password password G Grafana metadb PG datasource password META This section defines Pigsty deployment metadata: version string, admin node IP address, repository mirror region, default language, and HTTP(S) proxy for downloading packages.\nversion: v4.0.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default,china,europe language: en # default language: en or zh proxy_env: # global proxy env when downloading packages no_proxy: \"localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,*.pigsty,*.aliyun.com,mirrors.*,*.myqcloud.com,*.tsinghua.edu.cn\" # http_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com # https_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com # all_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com version name: version, type: string, level: G\nPigsty version string, default value is the current version: v4.0.0.\nPigsty uses this version string internally for feature control and content rendering. Do not modify this parameter arbitrarily.\nPigsty uses semantic versioning, and the version string typically starts with the character v, e.g., v4.0.0.\nadmin_ip name: admin_ip, type: ip, level: G\nAdmin node IP address, default is the placeholder IP address: 10.10.10.10\nThe node specified by this parameter will be treated as the admin node, typically pointing to the first node where Pigsty is installed, i.e., the control node.\nThe default value 10.10.10.10 is a placeholder that will be replaced with the actual admin node IP address during configure.\nMany parameters reference this parameter, such as:\ninfra_portal repo_endpoint repo_upstream dns_records node_default_etc_hosts node_etc_hosts In these parameters, the string ${admin_ip} will be replaced with the actual value of admin_ip. Using this mechanism, you can specify different admin nodes for different nodes.\nregion name: region, type: enum, level: G\nUpstream mirror region, available options: default, china, europe, default is default\nIf a region other than default is set, and there’s a corresponding entry in repo_upstream with a matching baseurl, it will be used instead of the default baseurl.\nFor example, if your region is set to china, Pigsty will attempt to use Chinese mirror sites to accelerate downloads. If an upstream repository doesn’t have a corresponding China region mirror, the default upstream mirror site will be used instead. Additionally, URLs defined in repo_url_packages will be replaced from repo.pigsty.io to repo.pigsty.cc to use domestic mirrors.\nlanguage name: language, type: enum, level: G\nDefault language setting, options are en (English) or zh (Chinese), default is en.\nThis parameter affects the language preference of some Pigsty-generated configurations and content, such as the initial language setting of Grafana dashboards.\nIf you are a Chinese user, it is recommended to set this parameter to zh for a better Chinese support experience.\nproxy_env name: proxy_env, type: dict, level: G\nGlobal proxy environment variables used when downloading packages, default value specifies no_proxy, which is the list of addresses that should not use a proxy:\nproxy_env: no_proxy: \"localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,*.pigsty,*.aliyun.com,mirrors.aliyuncs.com,mirrors.tuna.tsinghua.edu.cn,mirrors.zju.edu.cn\" #http_proxy: 'http://username:password@proxy.address.com' #https_proxy: 'http://username:password@proxy.address.com' #all_proxy: 'http://username:password@proxy.address.com' When installing from the Internet in mainland China, certain packages may be blocked. You can use a proxy to solve this problem.\nNote that if the Docker module is used, the proxy server configuration here will also be written to the Docker Daemon configuration file.\nNote that if the -x parameter is specified during ./configure, the proxy configuration information in the current environment will be automatically filled into the generated pigsty.yaml file.\nCA Pigsty uses self-signed CA certificates to support advanced security features such as HTTPS access, PostgreSQL SSL connections, etc.\nca_create: true # create CA if not exists? default true ca_cn: pigsty-ca # CA CN name, fixed as pigsty-ca cert_validity: 7300d # certificate validity, default 20 years ca_create name: ca_create, type: bool, level: G\nCreate CA if not exists? Default value is true.\nWhen set to true, if the CA public-private key pair does not exist in the files/pki/ca directory, Pigsty will automatically create a new CA.\nIf you already have a CA public-private key pair, you can copy them to the files/pki/ca directory:\nfiles/pki/ca/ca.crt: CA public key certificate files/pki/ca/ca.key: CA private key file Pigsty will use the existing CA key pair instead of creating a new one. If the CA does not exist and this parameter is set to false, an error will occur.\nBe sure to retain and backup the newly generated CA private key file during deployment, as it is crucial for issuing new certificates later.\nNote: Pigsty v3.x used the ca_method parameter (with values create/recreate/copy), v4.0 simplifies this to the boolean ca_create.\nca_cn name: ca_cn, type: string, level: G\nCA CN (Common Name), fixed as pigsty-ca, not recommended to modify.\nYou can use the following command to view the Pigsty CA certificate details on a node:\nopenssl x509 -text -in /etc/pki/ca.crt cert_validity name: cert_validity, type: interval, level: G\nCertificate validity period for issued certificates, default is 20 years, sufficient for most scenarios. Default value: 7300d\nThis parameter affects the validity of all certificates issued by the Pigsty CA, including:\nPostgreSQL server certificates Patroni API certificates etcd server/client certificates Other internal service certificates Note: The validity of HTTPS certificates used by Nginx is controlled separately by nginx_cert_validity, because modern browsers have stricter requirements for website certificate validity (maximum 397 days).\nINFRA_ID Infrastructure identity and portal definition.\n#infra_seq: 1 # infra node sequence, REQUIRED identity parameter infra_portal: # infrastructure services exposed via Nginx portal home : { domain: i.pigsty } # default home server definition infra_data: /data/infra # infrastructure default data directory infra_seq name: infra_seq, type: int, level: I\nInfrastructure node sequence number, REQUIRED identity parameter that must be explicitly specified on infrastructure nodes, so no default value is provided.\nThis parameter is used to uniquely identify each node in multi-infrastructure node deployments, typically using positive integers starting from 1.\nExample configuration:\ninfra: hosts: 10.10.10.10: { infra_seq: 1 } 10.10.10.11: { infra_seq: 2 } infra_portal name: infra_portal, type: dict, level: G\nInfrastructure services exposed via Nginx portal. The v4.0 default value is very concise:\ninfra_portal: home : { domain: i.pigsty } # default home server definition Pigsty will automatically configure the corresponding reverse proxies based on the actually enabled components. Users typically only need to define the home domain name.\nEach record consists of a Key and a Value dictionary, where name is the key representing the component name, and the value is an object that can configure the following parameters:\nname: REQUIRED, specifies the name of the Nginx server Default record: home is a fixed name, please do not modify it. Used as part of the Nginx configuration file name, corresponding to: /etc/nginx/conf.d/\u003cname\u003e.conf Nginx servers without a domain field will not generate configuration files but will be used as references. domain: OPTIONAL, when the service needs to be exposed via Nginx, this is a REQUIRED field specifying the domain name to use In Pigsty self-signed Nginx HTTPS certificates, the domain will be added to the SAN field of the Nginx SSL certificate Pigsty web page cross-references will use the default domain name here endpoint: Usually used as an alternative to path, specifies the upstream server address. Setting endpoint indicates this is a reverse proxy server ${admin_ip} can be used as a placeholder in the configuration and will be dynamically replaced with admin_ip during deployment Default reverse proxy servers use endpoint.conf as the configuration template Reverse proxy servers can also configure websocket and schema parameters path: Usually used as an alternative to endpoint, specifies the local file server path. Setting path indicates this is a local web server Local web servers use path.conf as the configuration template Local web servers can also configure the index parameter to enable file index pages certbot: Certbot certificate name; if configured, Certbot will be used to apply for certificates If multiple servers specify the same certbot, Pigsty will merge certificate applications; the final certificate name will be this certbot value cert: Certificate file path; if configured, will override the default certificate path key: Certificate key file path; if configured, will override the default certificate key path websocket: Whether to enable WebSocket support Only reverse proxy servers can configure this parameter; if enabled, upstream WebSocket connections will be allowed schema: Protocol used by the upstream server; if configured, will override the default protocol Default is http; if configured as https, it will force HTTPS connections to the upstream server index: Whether to enable file index pages Only local web servers can configure this parameter; if enabled, autoindex configuration will be enabled to automatically generate directory index pages log: Nginx log file path If specified, access logs will be written to this file; otherwise, the default log file will be used based on server type Reverse proxy servers use /var/log/nginx/\u003cname\u003e.log as the default log file path Local web servers use the default Access log conf: Nginx configuration file path Explicitly specifies the configuration template file to use, located in roles/infra/templates/nginx or templates/nginx directory If this parameter is not specified, the default configuration template will be used config: Nginx configuration code block Configuration text directly injected into the Nginx Server configuration block enforce_https: Redirect HTTP to HTTPS Global configuration can be specified via nginx_sslmode: enforce This configuration does not affect the default home server, which will always listen on both ports 80 and 443 to ensure compatibility infra_data name: infra_data, type: path, level: G\nInfrastructure data directory, default value is /data/infra.\nThis directory is used to store data files for infrastructure components, including:\nVictoriaMetrics time series database data VictoriaLogs log data VictoriaTraces trace data Other infrastructure component persistent data It is recommended to place this directory on a separate data disk for easier management and expansion.\nREPO This section is about local software repository configuration. Pigsty enables a local software repository (APT/YUM) on infrastructure nodes by default.\nDuring initialization, Pigsty downloads all packages and their dependencies (specified by repo_packages) from the Internet upstream repository (specified by repo_upstream) to {{ nginx_home }} / {{ repo_name }} (default /www/pigsty). The total size of all software and dependencies is approximately 1GB.\nWhen creating the local repository, if it already exists (determined by the presence of a marker file named repo_complete in the repository directory), Pigsty will consider the repository already built, skip the software download phase, and directly use the built repository.\nIf some packages download too slowly, you can set a download proxy using the proxy_env configuration to complete the initial download, or directly download the pre-packaged offline package, which is essentially a local software repository built on the same operating system.\nrepo_enabled: true # create local repo on this infra node? repo_home: /www # repo home directory, default /www repo_name: pigsty # repo name, default pigsty repo_endpoint: http://${admin_ip}:80 # repo access endpoint repo_remove: true # remove existing upstream repo definitions repo_modules: infra,node,pgsql # enabled upstream repo modules #repo_upstream: [] # upstream repo definitions (inherited from OS variables) #repo_packages: [] # packages to download (inherited from OS variables) #repo_extra_packages: [] # extra packages to download repo_url_packages: [] # extra packages downloaded via URL repo_enabled name: repo_enabled, type: bool, level: G/I\nCreate a local software repository on this infrastructure node? Default is true, meaning all Infra nodes will set up a local software repository.\nIf you have multiple infrastructure nodes, you can keep only 1-2 nodes as software repositories; other nodes can set this parameter to false to avoid duplicate software download builds.\nrepo_home name: repo_home, type: path, level: G\nLocal software repository home directory, defaults to Nginx’s root directory: /www.\nThis directory is actually a symlink pointing to nginx_data. It’s not recommended to modify this directory. If modified, it should be consistent with nginx_home.\nrepo_name name: repo_name, type: string, level: G\nLocal repository name, default is pigsty. Changing this repository name is not recommended.\nThe final repository path is {{ repo_home }}/{{ repo_name }}, defaulting to /www/pigsty.\nrepo_endpoint name: repo_endpoint, type: url, level: G\nEndpoint used by other nodes to access this repository, default value: http://${admin_ip}:80.\nPigsty starts Nginx on infrastructure nodes at ports 80/443 by default, providing local software repository (static files) service.\nIf you modify nginx_port or nginx_ssl_port, or use a different infrastructure node from the control node, adjust this parameter accordingly.\nIf you use a domain name, you can add resolution in node_default_etc_hosts, node_etc_hosts, or dns_records.\nrepo_remove name: repo_remove, type: bool, level: G/A\nRemove existing upstream repository definitions when building the local repository? Default value: true.\nWhen this parameter is enabled, all existing repository files in /etc/yum.repos.d will be moved and backed up to /etc/yum.repos.d/backup. On Debian systems, /etc/apt/sources.list and /etc/apt/sources.list.d are removed and backed up to /etc/apt/backup.\nSince existing OS sources have uncontrollable content, using Pigsty-validated upstream software sources can improve the success rate and speed of downloading packages from the Internet.\nIn certain situations (e.g., your OS is some EL/Deb compatible variant that uses private sources for many packages), you may need to keep existing upstream repository definitions. In such cases, set this parameter to false.\nrepo_modules name: repo_modules, type: string, level: G/A\nWhich upstream repository modules will be added to the local software source, default value: infra,node,pgsql\nWhen Pigsty attempts to add upstream repositories, it filters entries in repo_upstream based on this parameter’s value. Only entries whose module field matches this parameter’s value will be added to the local software source.\nModules are comma-separated. Available module lists can be found in the repo_upstream definitions; common modules include:\nlocal: Local Pigsty repository infra: Infrastructure packages (Nginx, Docker, etc.) node: OS base packages pgsql: PostgreSQL-related packages extra: Extra PostgreSQL extensions docker: Docker-related redis: Redis-related mongo: MongoDB-related mysql: MySQL-related etc… repo_upstream name: repo_upstream, type: upstream[], level: G\nWhere to download upstream packages when building the local repository? This parameter has no default value. If not explicitly specified by the user in the configuration file, it will be loaded from the repo_upstream_default variable defined in roles/node_id/vars based on the current node’s OS family.\nPigsty provides complete upstream repository definitions for different OS versions (EL8/9/10, Debian 11/12/13, Ubuntu 22/24), including:\nOS base repositories (BaseOS, AppStream, EPEL, etc.) PostgreSQL official PGDG repository Pigsty extension repository Various third-party software repositories (Docker, Nginx, Grafana, etc.) Each upstream repository definition contains the following fields:\n- name: pigsty-pgsql # repository name description: 'Pigsty PGSQL' # repository description module: pgsql # module it belongs to releases: [8,9,10] # supported OS versions arch: [x86_64, aarch64] # supported CPU architectures baseurl: # repository URL, configured by region default: 'https://repo.pigsty.io/yum/pgsql/el$releasever.$basearch' china: 'https://repo.pigsty.cc/yum/pgsql/el$releasever.$basearch' Users typically don’t need to modify this parameter unless they have special repository requirements. For detailed repository definitions, refer to the configuration files for corresponding operating systems in the roles/node_id/vars/ directory.\nrepo_packages name: repo_packages, type: string[], level: G\nString array type, where each line is a space-separated list of software packages, specifying packages (and their dependencies) to download using repotrack or apt download.\nThis parameter has no default value, meaning its default state is undefined. If not explicitly defined, Pigsty will load the default from the repo_packages_default variable defined in roles/node_id/vars:\n[ node-bootstrap, infra-package, infra-addons, node-package1, node-package2, pgsql-utility, extra-modules ] Each element in this parameter will be translated according to the package_map in the above files, based on the specific OS distro major version. For example, on EL systems it translates to:\nnode-bootstrap: \"ansible python3 python3-pip python3-virtualenv python3-requests python3-jmespath python3-cryptography dnf-utils modulemd-tools createrepo_c sshpass\" infra-package: \"nginx dnsmasq etcd haproxy vip-manager node_exporter keepalived_exporter pg_exporter pgbackrest_exporter redis_exporter redis minio mcli pig\" infra-addons: \"grafana grafana-plugins grafana-victoriametrics-ds grafana-victorialogs-ds victoria-metrics victoria-logs victoria-traces vlogscli vmutils vector alertmanager\" As a convention, repo_packages typically includes packages unrelated to the PostgreSQL major version (such as Infra, Node, and PGDG Common parts), while PostgreSQL major version-related packages (kernel, extensions) are usually specified in repo_extra_packages to facilitate switching PG major versions.\nrepo_extra_packages name: repo_extra_packages, type: string[], level: G/C/I\nUsed to specify additional packages to download without modifying repo_packages (typically PG major version-related packages), default value is an empty list.\nIf not explicitly defined, Pigsty will load the default from the repo_extra_packages_default variable defined in roles/node_id/vars:\n[ pgsql-main ] Elements in this parameter undergo package name translation, where $v will be replaced with pg_version, i.e., the current PG major version (default 18).\nThe pgsql-main here translates on EL systems to:\npostgresql$v postgresql$v-server postgresql$v-libs postgresql$v-contrib postgresql$v-plperl postgresql$v-plpython3 postgresql$v-pltcl postgresql$v-llvmjit pg_repack_$v* wal2json_$v* pgvector_$v* Users can typically specify PostgreSQL major version-related packages here without affecting the other PG version-independent packages defined in repo_packages.\nrepo_url_packages name: repo_url_packages, type: object[] | string[], level: G\nPackages downloaded directly from the Internet using URLs, default is an empty array: []\nYou can use URL strings directly as array elements in this parameter, or use object structures to explicitly specify URLs and filenames.\nNote that this parameter is affected by the region variable. If you’re in mainland China, Pigsty will automatically replace URLs, changing repo.pigsty.io to repo.pigsty.cc.\nINFRA_PACKAGE These packages are installed only on INFRA nodes, including regular RPM/DEB packages and PIP packages.\ninfra_packages name: infra_packages, type: string[], level: G\nString array type, where each line is a space-separated list of software packages, specifying packages to install on Infra nodes.\nThis parameter has no default value, meaning its default state is undefined. If not explicitly specified by the user in the configuration file, Pigsty will load the default from the infra_packages_default variable defined in roles/node_id/vars based on the current node’s OS family.\nv4.0 default value (EL operating systems):\ninfra_packages_default: - grafana,grafana-plugins,grafana-victorialogs-ds,grafana-victoriametrics-ds,victoria-metrics,victoria-logs,victoria-traces,vmutils,vlogscli,alertmanager - node_exporter,blackbox_exporter,nginx_exporter,pg_exporter,pev2,nginx,dnsmasq,ansible,etcd,python3-requests,redis,mcli,restic,certbot,python3-certbot-nginx Default value (Debian/Ubuntu):\ninfra_packages_default: - grafana,grafana-plugins,grafana-victorialogs-ds,grafana-victoriametrics-ds,victoria-metrics,victoria-logs,victoria-traces,vmutils,vlogscli,alertmanager - node-exporter,blackbox-exporter,nginx-exporter,pg-exporter,pev2,nginx,dnsmasq,ansible,etcd,python3-requests,redis,mcli,restic,certbot,python3-certbot-nginx Note: v4.0 uses the VictoriaMetrics suite to replace Prometheus and Loki, so the package list differs significantly from v3.x.\ninfra_packages_pip name: infra_packages_pip, type: string, level: G\nAdditional packages to install using pip on Infra nodes, package names separated by commas. Default value is an empty string, meaning no additional python packages are installed.\nExample:\ninfra_packages_pip: 'requests,boto3,awscli' NGINX Pigsty proxies all web service access through Nginx: Home Page, Grafana, VictoriaMetrics, etc., as well as other optional tools like PGWeb, Jupyter Lab, Pgadmin, Bytebase, and static resources and reports like pev, schemaspy, and pgbadger.\nMost importantly, Nginx also serves as the web server for the local software repository (Yum/Apt), used to store and distribute Pigsty packages.\nnginx_enabled: true # enable Nginx on this infra node? nginx_clean: false # clean existing Nginx config during init? nginx_exporter_enabled: true # enable nginx_exporter? nginx_exporter_port: 9113 # nginx_exporter listen port nginx_sslmode: enable # SSL mode: disable,enable,enforce nginx_cert_validity: 397d # self-signed cert validity nginx_home: /www # Nginx content directory (symlink) nginx_data: /data/nginx # Nginx actual data directory nginx_users: {} # basic auth users dictionary nginx_port: 80 # HTTP port nginx_ssl_port: 443 # HTTPS port certbot_sign: false # sign cert with certbot? certbot_email: your@email.com # certbot email certbot_options: '' # certbot extra options nginx_enabled name: nginx_enabled, type: bool, level: G/I\nEnable Nginx on this Infra node? Default value: true.\nNginx is a core component of Pigsty infrastructure, responsible for:\nProviding local software repository service Reverse proxying Grafana, VictoriaMetrics, and other web services Hosting static files and reports nginx_clean name: nginx_clean, type: bool, level: G/A\nClean existing Nginx configuration during initialization? Default value: false.\nWhen set to true, all existing configuration files under /etc/nginx/conf.d/ will be deleted during Nginx initialization, ensuring a clean start.\nIf you’re deploying for the first time or want to completely rebuild Nginx configuration, you can set this parameter to true.\nnginx_exporter_enabled name: nginx_exporter_enabled, type: bool, level: G/I\nEnable nginx_exporter on this infrastructure node? Default value: true.\nIf this option is disabled, the /nginx health check stub will also be disabled. Consider disabling this when your Nginx version doesn’t support this feature.\nnginx_exporter_port name: nginx_exporter_port, type: port, level: G\nnginx_exporter listen port, default value is 9113.\nnginx_exporter is used to collect Nginx operational metrics for VictoriaMetrics to scrape and monitor.\nnginx_sslmode name: nginx_sslmode, type: enum, level: G\nNginx SSL operating mode. Three options: disable, enable, enforce, default value is enable, meaning SSL is enabled but not enforced.\ndisable: Only listen on the port specified by nginx_port to serve HTTP requests. enable: Also listen on the port specified by nginx_ssl_port to serve HTTPS requests. enforce: All links will be rendered to use https:// by default Also redirect port 80 to port 443 for non-default servers in infra_portal nginx_cert_validity name: nginx_cert_validity, type: duration, level: G\nNginx self-signed certificate validity, default value is 397d (approximately 13 months).\nModern browsers require website certificate validity to be at most 397 days, hence this default value. Setting a longer validity is not recommended, as browsers may refuse to trust such certificates.\nnginx_home name: nginx_home, type: path, level: G\nNginx server static content directory, default: /www\nThis is a symlink that actually points to the nginx_data directory. This directory contains static resources and software repository files.\nIt’s best not to modify this parameter arbitrarily. If modified, it should be consistent with the repo_home parameter.\nnginx_data name: nginx_data, type: path, level: G\nNginx actual data directory, default is /data/nginx.\nThis is the actual storage location for Nginx static files; nginx_home is a symlink pointing to this directory.\nIt’s recommended to place this directory on a data disk for easier management of large package files.\nnginx_users name: nginx_users, type: dict, level: G\nNginx Basic Authentication user dictionary, default is an empty dictionary {}.\nFormat is { username: password } key-value pairs, for example:\nnginx_users: admin: pigsty viewer: readonly These users can be used to protect certain Nginx endpoints that require authentication.\nnginx_port name: nginx_port, type: port, level: G\nNginx default listening port (serving HTTP), default is port 80. It’s best not to modify this parameter.\nWhen your server’s port 80 is occupied, you can consider using another port, but you need to also modify repo_endpoint and keep node_repo_local_urls consistent with the port used here.\nnginx_ssl_port name: nginx_ssl_port, type: port, level: G\nNginx SSL default listening port, default is 443. It’s best not to modify this parameter.\ncertbot_sign name: certbot_sign, type: bool, level: G/A\nUse certbot to sign Nginx certificates during installation? Default value is false.\nWhen set to true, Pigsty will use certbot to automatically apply for free SSL certificates from Let’s Encrypt during the execution of infra.yml and deploy.yml playbooks (in the nginx role).\nFor domains defined in infra_portal, if a certbot parameter is defined, Pigsty will use certbot to apply for a certificate for that domain. The certificate name will be the value of the certbot parameter. If multiple servers/domains specify the same certbot parameter, Pigsty will merge and apply for certificates for these domains, using the certbot parameter value as the certificate name.\nEnabling this option requires:\nThe current node can be accessed through a public domain name, and DNS resolution is correctly pointed to the current node’s public IP The current node can access the Let’s Encrypt API interface This option is disabled by default. You can manually execute the make cert command after installation, which actually calls the rendered /etc/nginx/sign-cert script to update or apply for certificates using certbot.\ncertbot_email name: certbot_email, type: string, level: G/A\nEmail address for receiving certificate expiration reminder emails, default value is your@email.com.\nWhen certbot_sign is set to true, it’s recommended to provide this parameter. Let’s Encrypt will send reminder emails to this address when certificates are about to expire.\ncertbot_options name: certbot_options, type: string, level: G/A\nAdditional configuration parameters passed to certbot, default value is an empty string.\nYou can pass additional command-line options to certbot through this parameter, for example --dry-run, which makes certbot perform a preview and test without actually applying for certificates.\nDNS Pigsty enables DNSMASQ service on Infra nodes by default to resolve auxiliary domain names such as i.pigsty, m.pigsty, api.pigsty, etc., and optionally sss.pigsty for MinIO.\nResolution records are stored in the /etc/hosts.d/default file on Infra nodes. To use this DNS server, you must add nameserver \u003cip\u003e to /etc/resolv.conf. The node_dns_servers parameter handles this.\ndns_enabled: true # setup dnsmasq on this infra node? dns_port: 53 # DNS server listen port dns_records: # dynamic DNS records - \"${admin_ip} i.pigsty\" - \"${admin_ip} m.pigsty supa.pigsty api.pigsty adm.pigsty cli.pigsty ddl.pigsty\" dns_enabled name: dns_enabled, type: bool, level: G/I\nEnable DNSMASQ service on this Infra node? Default value: true.\nIf you don’t want to use the default DNS server (e.g., you already have an external DNS server, or your provider doesn’t allow you to use a DNS server), you can set this value to false to disable it, and use node_default_etc_hosts and node_etc_hosts static resolution records instead.\ndns_port name: dns_port, type: port, level: G\nDNSMASQ default listening port, default is 53. It’s not recommended to modify the default DNS service port.\ndns_records name: dns_records, type: string[], level: G\nDynamic DNS records resolved by dnsmasq, generally used to resolve auxiliary domain names to the admin node. These records are written to the /etc/hosts.d/default file on infrastructure nodes.\nv4.0 default value:\ndns_records: - \"${admin_ip} i.pigsty\" - \"${admin_ip} m.pigsty supa.pigsty api.pigsty adm.pigsty cli.pigsty ddl.pigsty\" The ${admin_ip} placeholder is used here and will be replaced with the actual admin_ip value during deployment.\nCommon domain name purposes:\ni.pigsty: Pigsty home page m.pigsty: VictoriaMetrics Web UI api.pigsty: API service adm.pigsty: Admin service Others customized based on actual deployment needs VICTORIA Pigsty v4.0 uses the VictoriaMetrics suite to replace Prometheus and Loki, providing a superior observability solution:\nVictoriaMetrics: Replaces Prometheus as the time series database for storing monitoring metrics VictoriaLogs: Replaces Loki as the log aggregation storage VictoriaTraces: Distributed trace storage VMAlert: Replaces Prometheus Alerting for alert rule evaluation vmetrics_enabled: true # enable VictoriaMetrics? vmetrics_clean: false # clean data during init? vmetrics_port: 8428 # listen port vmetrics_scrape_interval: 10s # global scrape interval vmetrics_scrape_timeout: 8s # global scrape timeout vmetrics_options: \u003e- -retentionPeriod=15d -promscrape.fileSDCheckInterval=5s vlogs_enabled: true # enable VictoriaLogs? vlogs_clean: false # clean data during init? vlogs_port: 9428 # listen port vlogs_options: \u003e- -retentionPeriod=15d -retention.maxDiskSpaceUsageBytes=50GiB -insert.maxLineSizeBytes=1MB -search.maxQueryDuration=120s vtraces_enabled: true # enable VictoriaTraces? vtraces_clean: false # clean data during init? vtraces_port: 10428 # listen port vtraces_options: \u003e- -retentionPeriod=15d -retention.maxDiskSpaceUsageBytes=50GiB vmalert_enabled: true # enable VMAlert? vmalert_port: 8880 # listen port vmalert_options: '' # extra CLI options vmetrics_enabled name: vmetrics_enabled, type: bool, level: G/I\nEnable VictoriaMetrics on this Infra node? Default value is true.\nVictoriaMetrics is the core monitoring component in Pigsty v4.0, replacing Prometheus as the time series database, responsible for:\nScraping monitoring metrics from various exporters Storing time series data Providing PromQL-compatible query interface Supporting Grafana data sources vmetrics_clean name: vmetrics_clean, type: bool, level: G/A\nClean existing VictoriaMetrics data during initialization? Default value is false.\nWhen set to true, existing time series data will be deleted during initialization. Use this option carefully unless you’re sure you want to rebuild monitoring data.\nvmetrics_port name: vmetrics_port, type: port, level: G\nVictoriaMetrics listen port, default value is 8428.\nThis port is used for:\nHTTP API access Web UI access Prometheus-compatible remote write/read Grafana data source connections vmetrics_scrape_interval name: vmetrics_scrape_interval, type: interval, level: G\nVictoriaMetrics global metrics scrape interval, default value is 10s.\nIn production environments, 10-30 seconds is a suitable scrape interval. If you need finer monitoring data granularity, you can adjust this parameter, but it will increase storage and CPU overhead.\nvmetrics_scrape_timeout name: vmetrics_scrape_timeout, type: interval, level: G\nVictoriaMetrics global scrape timeout, default is 8s.\nSetting a scrape timeout can effectively prevent avalanches caused by monitoring system queries. The principle is that this parameter must be less than and close to vmetrics_scrape_interval to ensure each scrape duration doesn’t exceed the scrape interval.\nvmetrics_options name: vmetrics_options, type: arg, level: G\nVictoriaMetrics extra command line options, default value:\nvmetrics_options: \u003e- -retentionPeriod=15d -promscrape.fileSDCheckInterval=5s Common parameter descriptions:\n-retentionPeriod=15d: Data retention period, default 15 days -promscrape.fileSDCheckInterval=5s: File service discovery refresh interval You can add other VictoriaMetrics-supported parameters as needed.\nvlogs_enabled name: vlogs_enabled, type: bool, level: G/I\nEnable VictoriaLogs on this Infra node? Default value is true.\nVictoriaLogs replaces Loki as the log aggregation storage, responsible for:\nReceiving log data from Vector Storing and indexing logs Providing log query interface Supporting Grafana VictoriaLogs data source vlogs_clean name: vlogs_clean, type: bool, level: G/A\nClean existing VictoriaLogs data during initialization? Default value is false.\nvlogs_port name: vlogs_port, type: port, level: G\nVictoriaLogs listen port, default value is 9428.\nvlogs_options name: vlogs_options, type: arg, level: G\nVictoriaLogs extra command line options, default value:\nvlogs_options: \u003e- -retentionPeriod=15d -retention.maxDiskSpaceUsageBytes=50GiB -insert.maxLineSizeBytes=1MB -search.maxQueryDuration=120s Common parameter descriptions:\n-retentionPeriod=15d: Log retention period, default 15 days -retention.maxDiskSpaceUsageBytes=50GiB: Maximum disk usage -insert.maxLineSizeBytes=1MB: Maximum single log line size -search.maxQueryDuration=120s: Maximum query execution time vtraces_enabled name: vtraces_enabled, type: bool, level: G/I\nEnable VictoriaTraces on this Infra node? Default value is true.\nVictoriaTraces is used for distributed trace data storage and query, supporting Jaeger, Zipkin, and other trace protocols.\nvtraces_clean name: vtraces_clean, type: bool, level: G/A\nClean existing VictoriaTraces data during initialization? Default value is false.\nvtraces_port name: vtraces_port, type: port, level: G\nVictoriaTraces listen port, default value is 10428.\nvtraces_options name: vtraces_options, type: arg, level: G\nVictoriaTraces extra command line options, default value:\nvtraces_options: \u003e- -retentionPeriod=15d -retention.maxDiskSpaceUsageBytes=50GiB vmalert_enabled name: vmalert_enabled, type: bool, level: G/I\nEnable VMAlert on this Infra node? Default value is true.\nVMAlert is responsible for alert rule evaluation, replacing Prometheus Alerting functionality, working with Alertmanager.\nvmalert_port name: vmalert_port, type: port, level: G\nVMAlert listen port, default value is 8880.\nvmalert_options name: vmalert_options, type: arg, level: G\nVMAlert extra command line options, default value is an empty string.\nPROMETHEUS This section now primarily contains Blackbox Exporter and Alertmanager configuration.\nNote: Pigsty v4.0 uses VictoriaMetrics to replace Prometheus. The original prometheus_* and pushgateway_* parameters have been moved to the VICTORIA section.\nblackbox_enabled: true # enable blackbox_exporter? blackbox_port: 9115 # blackbox_exporter listen port blackbox_options: '' # extra CLI options alertmanager_enabled: true # enable alertmanager? alertmanager_port: 9059 # alertmanager listen port alertmanager_options: '' # extra CLI options exporter_metrics_path: /metrics # exporter metrics path blackbox_enabled name: blackbox_enabled, type: bool, level: G/I\nEnable BlackboxExporter on this Infra node? Default value is true.\nBlackboxExporter sends ICMP packets to node IP addresses, VIP addresses, and PostgreSQL VIP addresses to test network connectivity. It can also perform HTTP, TCP, DNS, and other probes.\nblackbox_port name: blackbox_port, type: port, level: G\nBlackbox Exporter listen port, default value is 9115.\nblackbox_options name: blackbox_options, type: arg, level: G\nBlackboxExporter extra command line options, default value: empty string.\nalertmanager_enabled name: alertmanager_enabled, type: bool, level: G/I\nEnable AlertManager on this Infra node? Default value is true.\nAlertManager is responsible for receiving alert notifications from VMAlert and performing alert grouping, inhibition, silencing, routing, and other processing.\nalertmanager_port name: alertmanager_port, type: port, level: G\nAlertManager listen port, default value is 9059.\nIf you modify this port, ensure you update the alertmanager entry’s endpoint configuration in infra_portal accordingly (if defined).\nalertmanager_options name: alertmanager_options, type: arg, level: G\nAlertManager extra command line options, default value: empty string.\nexporter_metrics_path name: exporter_metrics_path, type: path, level: G\nHTTP endpoint path where monitoring exporters expose metrics, default: /metrics. Not recommended to modify this parameter.\nThis parameter defines the standard path for all exporters to expose monitoring metrics.\nGRAFANA Pigsty uses Grafana as the monitoring system frontend. It can also serve as a data analysis and visualization platform, or for low-code data application development and data application prototyping.\ngrafana_enabled: true # enable Grafana? grafana_port: 3000 # Grafana listen port grafana_clean: false # clean data during init? grafana_admin_username: admin # admin username grafana_admin_password: pigsty # admin password grafana_auth_proxy: false # enable auth proxy? grafana_pgurl: '' # external PostgreSQL URL grafana_view_password: DBUser.Viewer # PG datasource password grafana_enabled name: grafana_enabled, type: bool, level: G/I\nEnable Grafana on Infra node? Default value: true, meaning all infrastructure nodes will install and enable Grafana by default.\ngrafana_port name: grafana_port, type: port, level: G\nGrafana listen port, default value is 3000.\nIf you need to access Grafana directly (not through Nginx reverse proxy), you can use this port.\ngrafana_clean name: grafana_clean, type: bool, level: G/A\nClean Grafana data files during initialization? Default: false.\nThis operation removes /var/lib/grafana/grafana.db, ensuring a fresh Grafana installation.\nIf you want to preserve existing Grafana configuration (such as dashboards, users, data sources, etc.), set this parameter to false.\ngrafana_admin_username name: grafana_admin_username, type: username, level: G\nGrafana admin username, default is admin.\ngrafana_admin_password name: grafana_admin_password, type: password, level: G\nGrafana admin password, default is pigsty.\nIMPORTANT: Be sure to change this password parameter before deploying to production!\ngrafana_auth_proxy name: grafana_auth_proxy, type: bool, level: G\nEnable Grafana auth proxy? Default is false.\nWhen enabled, Grafana will trust user identity information passed by the reverse proxy (Nginx), enabling single sign-on (SSO) functionality.\nThis is typically used for integration with external identity authentication systems.\ngrafana_pgurl name: grafana_pgurl, type: url, level: G\nExternal PostgreSQL database URL for Grafana persistence storage. Default is an empty string.\nIf specified, Grafana will use this PostgreSQL database instead of the default SQLite database to store its configuration data.\nFormat example: postgres://grafana:password@pg-meta:5432/grafana?sslmode=disable\nThis is useful for scenarios requiring Grafana high availability deployment or data persistence.\ngrafana_view_password name: grafana_view_password, type: password, level: G\nRead-only user password used by Grafana metadb PG data source, default is DBUser.Viewer.\nThis password is used for Grafana to connect to the PostgreSQL CMDB data source to query metadata in read-only mode.\n","categories":["Reference"],"description":"INFRA module provides 10 sections with 70+ configurable parameters","excerpt":"INFRA module provides 10 sections with 70+ configurable parameters","ref":"/docs/infra/param/","tags":"","title":"Parameters"},{"body":"Pigsty provides four playbooks related to the INFRA module:\ndeploy.yml: Deploy all components on all nodes in one pass infra.yml: Initialize Pigsty infrastructure on infra nodes infra-rm.yml: Remove infrastructure components from infra nodes deploy.yml: Perform a complete one-time installation of Pigsty on all nodes deploy.yml Deploy all components on all nodes in one pass, resolving INFRA/NODE circular dependency issues.\nThis playbook interleaves subtasks from infra.yml and node.yml, completing deployment of all components in the following order:\nid: Generate node and PostgreSQL identities ca: Create self-signed CA on localhost repo: Create local software repository on infra nodes node-init: Initialize nodes, HAProxy, and Docker infra: Initialize Nginx, DNS, VictoriaMetrics, Grafana, etc. node-monitor: Initialize node-exporter, vector etcd: Initialize etcd (required for PostgreSQL HA) minio: Initialize MinIO (optional) pgsql: Initialize PostgreSQL clusters pgsql-monitor: Initialize PostgreSQL monitoring This playbook is equivalent to executing the following four playbooks sequentially:\n./infra.yml -l infra # Deploy infrastructure on infra group ./node.yml # Initialize all nodes ./etcd.yml # Initialize etcd cluster ./pgsql.yml # Initialize PostgreSQL clusters infra.yml Initialize the infrastructure module on Infra nodes defined in the infra group of your configuration file.\nThis playbook performs the following tasks:\nConfigures directories and environment variables on Infra nodes Downloads and creates a local software repository to accelerate subsequent installations Incorporates the current Infra node as a common node managed by Pigsty Deploys infrastructure components (VictoriaMetrics/Logs/Traces, VMAlert, Grafana, Alertmanager, Blackbox Exporter, etc.) Playbook notes:\nThis is an idempotent playbook - repeated execution will overwrite infrastructure components on Infra nodes To preserve historical monitoring data, set vmetrics_clean, vlogs_clean, vtraces_clean to false beforehand Unless grafana_clean is set to false, Grafana dashboards and configuration changes will be lost When the local software repository /www/pigsty/repo_complete exists, this playbook skips downloading software from the internet Complete execution takes approximately 1-3 minutes, depending on machine configuration and network conditions Available Tasks # ca: create self-signed CA on localhost files/pki # - ca_dir : create CA directory # - ca_private : generate ca private key: files/pki/ca/ca.key # - ca_cert : signing ca cert: files/pki/ca/ca.crt # # id: generate node identity # # repo: bootstrap a local yum repo from internet or offline packages # - repo_dir : create repo directory # - repo_check : check repo exists # - repo_prepare : use existing repo if exists # - repo_build : build repo from upstream if not exists # - repo_upstream : handle upstream repo files in /etc/yum.repos.d # - repo_remove : remove existing repo file if repo_remove == true # - repo_add : add upstream repo files to /etc/yum.repos.d # - repo_url_pkg : download packages from internet defined by repo_url_packages # - repo_cache : make upstream yum cache with yum makecache # - repo_boot_pkg : install bootstrap pkg such as createrepo_c,yum-utils,... # - repo_pkg : download packages \u0026 dependencies from upstream repo # - repo_create : create a local yum repo with createrepo_c \u0026 modifyrepo_c # - repo_use : add newly built repo into /etc/yum.repos.d # - repo_nginx : launch a nginx for repo if no nginx is serving # # node/haproxy/docker/monitor: setup infra node as a common node # - node_name, node_hosts, node_resolv, node_firewall, node_ca, node_repo, node_pkg # - node_feature, node_kernel, node_tune, node_sysctl, node_profile, node_ulimit # - node_data, node_admin, node_timezone, node_ntp, node_crontab, node_vip # - haproxy_install, haproxy_config, haproxy_launch, haproxy_reload # - docker_install, docker_admin, docker_config, docker_launch, docker_image # - haproxy_register, node_exporter, node_register, vector # # infra: setup infra components # - infra_env : env_dir, env_pg, env_pgadmin, env_var # - infra_pkg : infra_pkg_yum, infra_pkg_pip # - infra_user : setup infra os user group # - infra_cert : issue cert for infra components # - dns : dns_config, dns_record, dns_launch # - nginx : nginx_config, nginx_cert, nginx_static, nginx_launch, nginx_certbot, nginx_reload, nginx_exporter # - victoria : vmetrics_config, vmetrics_launch, vlogs_config, vlogs_launch, vtraces_config, vtraces_launch, vmalert_config, vmalert_launch # - alertmanager : alertmanager_config, alertmanager_launch # - blackbox : blackbox_config, blackbox_launch # - grafana : grafana_clean, grafana_config, grafana_launch, grafana_provision # - infra_register : register infra components to victoria infra-rm.yml Remove Pigsty infrastructure from Infra nodes defined in the infra group of your configuration file.\nCommon subtasks include:\n./infra-rm.yml # Remove the INFRA module ./infra-rm.yml -t service # Stop infrastructure services on INFRA ./infra-rm.yml -t data # Remove retained data on INFRA ./infra-rm.yml -t package # Uninstall packages installed on INFRA deploy.yml Perform a complete one-time installation of Pigsty on all nodes.\nThis playbook is described in more detail in Playbook: One-Pass Installation.\n","categories":["Task"],"description":"How to use built-in Ansible playbooks to manage the INFRA module, with a quick reference for common commands.","excerpt":"How to use built-in Ansible playbooks to manage the INFRA module, with …","ref":"/docs/infra/playbook/","tags":"","title":"Playbook"},{"body":"This document describes monitoring dashboards and alert rules for the INFRA module in Pigsty.\nDashboards Pigsty provides the following monitoring dashboards for the Infra module:\nDashboard Description Pigsty Home Pigsty monitoring system homepage INFRA Overview Pigsty infrastructure self-monitoring overview Nginx Instance Nginx metrics and logs Grafana Instance Grafana metrics and logs VictoriaMetrics Instance VictoriaMetrics scraping/query status VMAlert Instance Alert rule execution status Alertmanager Instance Alert aggregation and notifications VictoriaLogs Instance Log ingestion, querying, and indexing Logs Instance View log information on a single node VictoriaTraces Instance Trace storage and querying Inventory CMDB CMDB visualization ETCD Overview etcd cluster monitoring Alert Rules Pigsty provides the following two alert rules for the INFRA module:\nAlert Rule Description InfraDown Infrastructure component is down AgentDown Monitoring agent is down You can modify or add new infrastructure alert rules in files/victoria/rules/infra.yml.\nAlert Rule Configuration ################################################################ # Infrastructure Alert Rules # ################################################################ - name: infra-alert rules: #==============================================================# # Infra Aliveness # #==============================================================# # infra components (victoria,grafana) down for 1m triggers a P1 alert - alert: InfraDown expr: infra_up \u003c 1 for: 1m labels: { level: 0, severity: CRIT, category: infra } annotations: summary: \"CRIT InfraDown {{ $labels.type }}@{{ $labels.instance }}\" description: | infra_up[type={{ $labels.type }}, instance={{ $labels.instance }}] = {{ $value | printf \"%.2f\" }} \u003c 1 #==============================================================# # Agent Aliveness # #==============================================================# # agent aliveness are determined directly by exporter aliveness # including: node_exporter, pg_exporter, pgbouncer_exporter, haproxy_exporter - alert: AgentDown expr: agent_up \u003c 1 for: 1m labels: { level: 0, severity: CRIT, category: infra } annotations: summary: 'CRIT AgentDown {{ $labels.ins }}@{{ $labels.instance }}' description: | agent_up[ins={{ $labels.ins }}, instance={{ $labels.instance }}] = {{ $value | printf \"%.2f\" }} \u003c 1 ","categories":["Reference"],"description":"How to perform self-monitoring of infrastructure in Pigsty?","excerpt":"How to perform self-monitoring of infrastructure in Pigsty?","ref":"/docs/infra/monitor/","tags":"","title":"Monitoring"},{"body":" Note: Pigsty v4.0 has replaced Prometheus/Loki with VictoriaMetrics/Logs/Traces. The following metric list is still based on v3.x generation, for reference when troubleshooting older versions only. To get the latest metrics, query directly in https://p.pigsty (VMUI) or Grafana. Future versions will regenerate metric reference sheets consistent with the Victoria suite.\nINFRA Metrics The INFRA module has 964 available metrics.\nMetric Name Type Labels Description alertmanager_alerts gauge ins, instance, ip, job, cls, state How many alerts by state. alertmanager_alerts_invalid_total counter version, ins, instance, ip, job, cls The total number of received alerts that were invalid. alertmanager_alerts_received_total counter version, ins, instance, ip, status, job, cls The total number of received alerts. alertmanager_build_info gauge revision, version, ins, instance, ip, tags, goarch, goversion, job, cls, branch, goos A metric with a constant ‘1’ value labeled by version, revision, branch, goversion from which alertmanager was built, and the goos and goarch for the build. alertmanager_cluster_alive_messages_total counter ins, instance, ip, peer, job, cls Total number of received alive messages. alertmanager_cluster_enabled gauge ins, instance, ip, job, cls Indicates whether the clustering is enabled or not. alertmanager_cluster_failed_peers gauge ins, instance, ip, job, cls Number indicating the current number of failed peers in the cluster. alertmanager_cluster_health_score gauge ins, instance, ip, job, cls Health score of the cluster. Lower values are better and zero means ’totally healthy’. alertmanager_cluster_members gauge ins, instance, ip, job, cls Number indicating current number of members in cluster. alertmanager_cluster_messages_pruned_total counter ins, instance, ip, job, cls Total number of cluster messages pruned. alertmanager_cluster_messages_queued gauge ins, instance, ip, job, cls Number of cluster messages which are queued. alertmanager_cluster_messages_received_size_total counter ins, instance, ip, msg_type, job, cls Total size of cluster messages received. alertmanager_cluster_messages_received_total counter ins, instance, ip, msg_type, job, cls Total number of cluster messages received. alertmanager_cluster_messages_sent_size_total counter ins, instance, ip, msg_type, job, cls Total size of cluster messages sent. alertmanager_cluster_messages_sent_total counter ins, instance, ip, msg_type, job, cls Total number of cluster messages sent. alertmanager_cluster_peer_info gauge ins, instance, ip, peer, job, cls A metric with a constant ‘1’ value labeled by peer name. alertmanager_cluster_peers_joined_total counter ins, instance, ip, job, cls A counter of the number of peers that have joined. alertmanager_cluster_peers_left_total counter ins, instance, ip, job, cls A counter of the number of peers that have left. alertmanager_cluster_peers_update_total counter ins, instance, ip, job, cls A counter of the number of peers that have updated metadata. alertmanager_cluster_reconnections_failed_total counter ins, instance, ip, job, cls A counter of the number of failed cluster peer reconnection attempts. alertmanager_cluster_reconnections_total counter ins, instance, ip, job, cls A counter of the number of cluster peer reconnections. alertmanager_cluster_refresh_join_failed_total counter ins, instance, ip, job, cls A counter of the number of failed cluster peer joined attempts via refresh. alertmanager_cluster_refresh_join_total counter ins, instance, ip, job, cls A counter of the number of cluster peer joined via refresh. alertmanager_config_hash gauge ins, instance, ip, job, cls Hash of the currently loaded alertmanager configuration. alertmanager_config_last_reload_success_timestamp_seconds gauge ins, instance, ip, job, cls Timestamp of the last successful configuration reload. alertmanager_config_last_reload_successful gauge ins, instance, ip, job, cls Whether the last configuration reload attempt was successful. alertmanager_dispatcher_aggregation_groups gauge ins, instance, ip, job, cls Number of active aggregation groups alertmanager_dispatcher_alert_processing_duration_seconds_count Unknown ins, instance, ip, job, cls N/A alertmanager_dispatcher_alert_processing_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A alertmanager_http_concurrency_limit_exceeded_total counter ins, instance, method, ip, job, cls Total number of times an HTTP request failed because the concurrency limit was reached. alertmanager_http_request_duration_seconds_bucket Unknown ins, instance, method, ip, le, job, cls, handler N/A alertmanager_http_request_duration_seconds_count Unknown ins, instance, method, ip, job, cls, handler N/A alertmanager_http_request_duration_seconds_sum Unknown ins, instance, method, ip, job, cls, handler N/A alertmanager_http_requests_in_flight gauge ins, instance, method, ip, job, cls Current number of HTTP requests being processed. alertmanager_http_response_size_bytes_bucket Unknown ins, instance, method, ip, le, job, cls, handler N/A alertmanager_http_response_size_bytes_count Unknown ins, instance, method, ip, job, cls, handler N/A alertmanager_http_response_size_bytes_sum Unknown ins, instance, method, ip, job, cls, handler N/A alertmanager_integrations gauge ins, instance, ip, job, cls Number of configured integrations. alertmanager_marked_alerts gauge ins, instance, ip, job, cls, state How many alerts by state are currently marked in the Alertmanager regardless of their expiry. alertmanager_nflog_gc_duration_seconds_count Unknown ins, instance, ip, job, cls N/A alertmanager_nflog_gc_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A alertmanager_nflog_gossip_messages_propagated_total counter ins, instance, ip, job, cls Number of received gossip messages that have been further gossiped. alertmanager_nflog_maintenance_errors_total counter ins, instance, ip, job, cls How many maintenances were executed for the notification log that failed. alertmanager_nflog_maintenance_total counter ins, instance, ip, job, cls How many maintenances were executed for the notification log. alertmanager_nflog_queries_total counter ins, instance, ip, job, cls Number of notification log queries were received. alertmanager_nflog_query_duration_seconds_bucket Unknown ins, instance, ip, le, job, cls N/A alertmanager_nflog_query_duration_seconds_count Unknown ins, instance, ip, job, cls N/A alertmanager_nflog_query_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A alertmanager_nflog_query_errors_total counter ins, instance, ip, job, cls Number notification log received queries that failed. alertmanager_nflog_snapshot_duration_seconds_count Unknown ins, instance, ip, job, cls N/A alertmanager_nflog_snapshot_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A alertmanager_nflog_snapshot_size_bytes gauge ins, instance, ip, job, cls Size of the last notification log snapshot in bytes. alertmanager_notification_latency_seconds_bucket Unknown integration, ins, instance, ip, le, job, cls N/A alertmanager_notification_latency_seconds_count Unknown integration, ins, instance, ip, job, cls N/A alertmanager_notification_latency_seconds_sum Unknown integration, ins, instance, ip, job, cls N/A alertmanager_notification_requests_failed_total counter integration, ins, instance, ip, job, cls The total number of failed notification requests. alertmanager_notification_requests_total counter integration, ins, instance, ip, job, cls The total number of attempted notification requests. alertmanager_notifications_failed_total counter integration, ins, instance, ip, reason, job, cls The total number of failed notifications. alertmanager_notifications_total counter integration, ins, instance, ip, job, cls The total number of attempted notifications. alertmanager_oversize_gossip_message_duration_seconds_bucket Unknown ins, instance, ip, le, key, job, cls N/A alertmanager_oversize_gossip_message_duration_seconds_count Unknown ins, instance, ip, key, job, cls N/A alertmanager_oversize_gossip_message_duration_seconds_sum Unknown ins, instance, ip, key, job, cls N/A alertmanager_oversized_gossip_message_dropped_total counter ins, instance, ip, key, job, cls Number of oversized gossip messages that were dropped due to a full message queue. alertmanager_oversized_gossip_message_failure_total counter ins, instance, ip, key, job, cls Number of oversized gossip message sends that failed. alertmanager_oversized_gossip_message_sent_total counter ins, instance, ip, key, job, cls Number of oversized gossip message sent. alertmanager_peer_position gauge ins, instance, ip, job, cls Position the Alertmanager instance believes it’s in. The position determines a peer’s behavior in the cluster. alertmanager_receivers gauge ins, instance, ip, job, cls Number of configured receivers. alertmanager_silences gauge ins, instance, ip, job, cls, state How many silences by state. alertmanager_silences_gc_duration_seconds_count Unknown ins, instance, ip, job, cls N/A alertmanager_silences_gc_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A alertmanager_silences_gossip_messages_propagated_total counter ins, instance, ip, job, cls Number of received gossip messages that have been further gossiped. alertmanager_silences_maintenance_errors_total counter ins, instance, ip, job, cls How many maintenances were executed for silences that failed. alertmanager_silences_maintenance_total counter ins, instance, ip, job, cls How many maintenances were executed for silences. alertmanager_silences_queries_total counter ins, instance, ip, job, cls How many silence queries were received. alertmanager_silences_query_duration_seconds_bucket Unknown ins, instance, ip, le, job, cls N/A alertmanager_silences_query_duration_seconds_count Unknown ins, instance, ip, job, cls N/A alertmanager_silences_query_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A alertmanager_silences_query_errors_total counter ins, instance, ip, job, cls How many silence received queries did not succeed. alertmanager_silences_snapshot_duration_seconds_count Unknown ins, instance, ip, job, cls N/A alertmanager_silences_snapshot_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A alertmanager_silences_snapshot_size_bytes gauge ins, instance, ip, job, cls Size of the last silence snapshot in bytes. blackbox_exporter_build_info gauge revision, version, ins, instance, ip, tags, goarch, goversion, job, cls, branch, goos A metric with a constant ‘1’ value labeled by version, revision, branch, goversion from which blackbox_exporter was built, and the goos and goarch for the build. blackbox_exporter_config_last_reload_success_timestamp_seconds gauge ins, instance, ip, job, cls Timestamp of the last successful configuration reload. blackbox_exporter_config_last_reload_successful gauge ins, instance, ip, job, cls Blackbox exporter config loaded successfully. blackbox_module_unknown_total counter ins, instance, ip, job, cls Count of unknown modules requested by probes cortex_distributor_ingester_clients gauge ins, instance, ip, job, cls The current number of ingester clients. cortex_dns_failures_total Unknown ins, instance, ip, job, cls N/A cortex_dns_lookups_total Unknown ins, instance, ip, job, cls N/A cortex_frontend_query_range_duration_seconds_bucket Unknown ins, instance, method, ip, le, job, cls, status_code N/A cortex_frontend_query_range_duration_seconds_count Unknown ins, instance, method, ip, job, cls, status_code N/A cortex_frontend_query_range_duration_seconds_sum Unknown ins, instance, method, ip, job, cls, status_code N/A cortex_ingester_flush_queue_length gauge ins, instance, ip, job, cls The total number of series pending in the flush queue. cortex_kv_request_duration_seconds_bucket Unknown ins, instance, role, ip, le, kv_name, type, operation, job, cls, status_code N/A cortex_kv_request_duration_seconds_count Unknown ins, instance, role, ip, kv_name, type, operation, job, cls, status_code N/A cortex_kv_request_duration_seconds_sum Unknown ins, instance, role, ip, kv_name, type, operation, job, cls, status_code N/A cortex_member_consul_heartbeats_total Unknown ins, instance, ip, job, cls N/A cortex_prometheus_notifications_alertmanagers_discovered gauge ins, instance, ip, user, job, cls The number of alertmanagers discovered and active. cortex_prometheus_notifications_dropped_total Unknown ins, instance, ip, user, job, cls N/A cortex_prometheus_notifications_queue_capacity gauge ins, instance, ip, user, job, cls The capacity of the alert notifications queue. cortex_prometheus_notifications_queue_length gauge ins, instance, ip, user, job, cls The number of alert notifications in the queue. cortex_prometheus_rule_evaluation_duration_seconds summary ins, instance, ip, user, job, cls, quantile The duration for a rule to execute. cortex_prometheus_rule_evaluation_duration_seconds_count Unknown ins, instance, ip, user, job, cls N/A cortex_prometheus_rule_evaluation_duration_seconds_sum Unknown ins, instance, ip, user, job, cls N/A cortex_prometheus_rule_group_duration_seconds summary ins, instance, ip, user, job, cls, quantile The duration of rule group evaluations. cortex_prometheus_rule_group_duration_seconds_count Unknown ins, instance, ip, user, job, cls N/A cortex_prometheus_rule_group_duration_seconds_sum Unknown ins, instance, ip, user, job, cls N/A cortex_query_frontend_connected_schedulers gauge ins, instance, ip, job, cls Number of schedulers this frontend is connected to. cortex_query_frontend_queries_in_progress gauge ins, instance, ip, job, cls Number of queries in progress handled by this frontend. cortex_query_frontend_retries_bucket Unknown ins, instance, ip, le, job, cls N/A cortex_query_frontend_retries_count Unknown ins, instance, ip, job, cls N/A cortex_query_frontend_retries_sum Unknown ins, instance, ip, job, cls N/A cortex_query_scheduler_connected_frontend_clients gauge ins, instance, ip, job, cls Number of query-frontend worker clients currently connected to the query-scheduler. cortex_query_scheduler_connected_querier_clients gauge ins, instance, ip, job, cls Number of querier worker clients currently connected to the query-scheduler. cortex_query_scheduler_inflight_requests summary ins, instance, ip, job, cls, quantile Number of inflight requests (either queued or processing) sampled at a regular interval. Quantile buckets keep track of inflight requests over the last 60s. cortex_query_scheduler_inflight_requests_count Unknown ins, instance, ip, job, cls N/A cortex_query_scheduler_inflight_requests_sum Unknown ins, instance, ip, job, cls N/A cortex_query_scheduler_queue_duration_seconds_bucket Unknown ins, instance, ip, le, job, cls N/A cortex_query_scheduler_queue_duration_seconds_count Unknown ins, instance, ip, job, cls N/A cortex_query_scheduler_queue_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A cortex_query_scheduler_queue_length Unknown ins, instance, ip, user, job, cls N/A cortex_query_scheduler_running gauge ins, instance, ip, job, cls Value will be 1 if the scheduler is in the ReplicationSet and actively receiving/processing requests cortex_ring_member_heartbeats_total Unknown ins, instance, ip, job, cls N/A cortex_ring_member_tokens_owned gauge ins, instance, ip, job, cls The number of tokens owned in the ring. cortex_ring_member_tokens_to_own gauge ins, instance, ip, job, cls The number of tokens to own in the ring. cortex_ring_members gauge ins, instance, ip, job, cls, state Number of members in the ring cortex_ring_oldest_member_timestamp gauge ins, instance, ip, job, cls, state Timestamp of the oldest member in the ring. cortex_ring_tokens_total gauge ins, instance, ip, job, cls Number of tokens in the ring cortex_ruler_clients gauge ins, instance, ip, job, cls The current number of ruler clients in the pool. cortex_ruler_config_last_reload_successful gauge ins, instance, ip, user, job, cls Boolean set to 1 whenever the last configuration reload attempt was successful. cortex_ruler_config_last_reload_successful_seconds gauge ins, instance, ip, user, job, cls Timestamp of the last successful configuration reload. cortex_ruler_config_updates_total Unknown ins, instance, ip, user, job, cls N/A cortex_ruler_managers_total gauge ins, instance, ip, job, cls Total number of managers registered and running in the ruler cortex_ruler_ring_check_errors_total Unknown ins, instance, ip, job, cls N/A cortex_ruler_sync_rules_total Unknown ins, instance, ip, reason, job, cls N/A deprecated_flags_inuse_total Unknown ins, instance, ip, job, cls N/A go_cgo_go_to_c_calls_calls_total Unknown ins, instance, ip, job, cls N/A go_cpu_classes_gc_mark_assist_cpu_seconds_total Unknown ins, instance, ip, job, cls N/A go_cpu_classes_gc_mark_dedicated_cpu_seconds_total Unknown ins, instance, ip, job, cls N/A go_cpu_classes_gc_mark_idle_cpu_seconds_total Unknown ins, instance, ip, job, cls N/A go_cpu_classes_gc_pause_cpu_seconds_total Unknown ins, instance, ip, job, cls N/A go_cpu_classes_gc_total_cpu_seconds_total Unknown ins, instance, ip, job, cls N/A go_cpu_classes_idle_cpu_seconds_total Unknown ins, instance, ip, job, cls N/A go_cpu_classes_scavenge_assist_cpu_seconds_total Unknown ins, instance, ip, job, cls N/A go_cpu_classes_scavenge_background_cpu_seconds_total Unknown ins, instance, ip, job, cls N/A go_cpu_classes_scavenge_total_cpu_seconds_total Unknown ins, instance, ip, job, cls N/A go_cpu_classes_total_cpu_seconds_total Unknown ins, instance, ip, job, cls N/A go_cpu_classes_user_cpu_seconds_total Unknown ins, instance, ip, job, cls N/A go_gc_cycles_automatic_gc_cycles_total Unknown ins, instance, ip, job, cls N/A go_gc_cycles_forced_gc_cycles_total Unknown ins, instance, ip, job, cls N/A go_gc_cycles_total_gc_cycles_total Unknown ins, instance, ip, job, cls N/A go_gc_duration_seconds summary ins, instance, ip, job, cls, quantile A summary of the pause duration of garbage collection cycles. go_gc_duration_seconds_count Unknown ins, instance, ip, job, cls N/A go_gc_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A go_gc_gogc_percent gauge ins, instance, ip, job, cls Heap size target percentage configured by the user, otherwise 100. This value is set by the GOGC environment variable, and the runtime/debug.SetGCPercent function. go_gc_gomemlimit_bytes gauge ins, instance, ip, job, cls Go runtime memory limit configured by the user, otherwise math.MaxInt64. This value is set by the GOMEMLIMIT environment variable, and the runtime/debug.SetMemoryLimit function. go_gc_heap_allocs_by_size_bytes_bucket Unknown ins, instance, ip, le, job, cls N/A go_gc_heap_allocs_by_size_bytes_count Unknown ins, instance, ip, job, cls N/A go_gc_heap_allocs_by_size_bytes_sum Unknown ins, instance, ip, job, cls N/A go_gc_heap_allocs_bytes_total Unknown ins, instance, ip, job, cls N/A go_gc_heap_allocs_objects_total Unknown ins, instance, ip, job, cls N/A go_gc_heap_frees_by_size_bytes_bucket Unknown ins, instance, ip, le, job, cls N/A go_gc_heap_frees_by_size_bytes_count Unknown ins, instance, ip, job, cls N/A go_gc_heap_frees_by_size_bytes_sum Unknown ins, instance, ip, job, cls N/A go_gc_heap_frees_bytes_total Unknown ins, instance, ip, job, cls N/A go_gc_heap_frees_objects_total Unknown ins, instance, ip, job, cls N/A go_gc_heap_goal_bytes gauge ins, instance, ip, job, cls Heap size target for the end of the GC cycle. go_gc_heap_live_bytes gauge ins, instance, ip, job, cls Heap memory occupied by live objects that were marked by the previous GC. go_gc_heap_objects_objects gauge ins, instance, ip, job, cls Number of objects, live or unswept, occupying heap memory. go_gc_heap_tiny_allocs_objects_total Unknown ins, instance, ip, job, cls N/A go_gc_limiter_last_enabled_gc_cycle gauge ins, instance, ip, job, cls GC cycle the last time the GC CPU limiter was enabled. This metric is useful for diagnosing the root cause of an out-of-memory error, because the limiter trades memory for CPU time when the GC’s CPU time gets too high. This is most likely to occur with use of SetMemoryLimit. The first GC cycle is cycle 1, so a value of 0 indicates that it was never enabled. go_gc_pauses_seconds_bucket Unknown ins, instance, ip, le, job, cls N/A go_gc_pauses_seconds_count Unknown ins, instance, ip, job, cls N/A go_gc_pauses_seconds_sum Unknown ins, instance, ip, job, cls N/A go_gc_scan_globals_bytes gauge ins, instance, ip, job, cls The total amount of global variable space that is scannable. go_gc_scan_heap_bytes gauge ins, instance, ip, job, cls The total amount of heap space that is scannable. go_gc_scan_stack_bytes gauge ins, instance, ip, job, cls The number of bytes of stack that were scanned last GC cycle. go_gc_scan_total_bytes gauge ins, instance, ip, job, cls The total amount space that is scannable. Sum of all metrics in /gc/scan. go_gc_stack_starting_size_bytes gauge ins, instance, ip, job, cls The stack size of new goroutines. go_godebug_non_default_behavior_execerrdot_events_total Unknown ins, instance, ip, job, cls N/A go_godebug_non_default_behavior_gocachehash_events_total Unknown ins, instance, ip, job, cls N/A go_godebug_non_default_behavior_gocachetest_events_total Unknown ins, instance, ip, job, cls N/A go_godebug_non_default_behavior_gocacheverify_events_total Unknown ins, instance, ip, job, cls N/A go_godebug_non_default_behavior_http2client_events_total Unknown ins, instance, ip, job, cls N/A go_godebug_non_default_behavior_http2server_events_total Unknown ins, instance, ip, job, cls N/A go_godebug_non_default_behavior_installgoroot_events_total Unknown ins, instance, ip, job, cls N/A go_godebug_non_default_behavior_jstmpllitinterp_events_total Unknown ins, instance, ip, job, cls N/A go_godebug_non_default_behavior_multipartmaxheaders_events_total Unknown ins, instance, ip, job, cls N/A go_godebug_non_default_behavior_multipartmaxparts_events_total Unknown ins, instance, ip, job, cls N/A go_godebug_non_default_behavior_multipathtcp_events_total Unknown ins, instance, ip, job, cls N/A go_godebug_non_default_behavior_panicnil_events_total Unknown ins, instance, ip, job, cls N/A go_godebug_non_default_behavior_randautoseed_events_total Unknown ins, instance, ip, job, cls N/A go_godebug_non_default_behavior_tarinsecurepath_events_total Unknown ins, instance, ip, job, cls N/A go_godebug_non_default_behavior_tlsmaxrsasize_events_total Unknown ins, instance, ip, job, cls N/A go_godebug_non_default_behavior_x509sha1_events_total Unknown ins, instance, ip, job, cls N/A go_godebug_non_default_behavior_x509usefallbackroots_events_total Unknown ins, instance, ip, job, cls N/A go_godebug_non_default_behavior_zipinsecurepath_events_total Unknown ins, instance, ip, job, cls N/A go_goroutines gauge ins, instance, ip, job, cls Number of goroutines that currently exist. go_info gauge version, ins, instance, ip, job, cls Information about the Go environment. go_memory_classes_heap_free_bytes gauge ins, instance, ip, job, cls Memory that is completely free and eligible to be returned to the underlying system, but has not been. This metric is the runtime’s estimate of free address space that is backed by physical memory. go_memory_classes_heap_objects_bytes gauge ins, instance, ip, job, cls Memory occupied by live objects and dead objects that have not yet been marked free by the garbage collector. go_memory_classes_heap_released_bytes gauge ins, instance, ip, job, cls Memory that is completely free and has been returned to the underlying system. This metric is the runtime’s estimate of free address space that is still mapped into the process, but is not backed by physical memory. go_memory_classes_heap_stacks_bytes gauge ins, instance, ip, job, cls Memory allocated from the heap that is reserved for stack space, whether or not it is currently in-use. Currently, this represents all stack memory for goroutines. It also includes all OS thread stacks in non-cgo programs. Note that stacks may be allocated differently in the future, and this may change. go_memory_classes_heap_unused_bytes gauge ins, instance, ip, job, cls Memory that is reserved for heap objects but is not currently used to hold heap objects. go_memory_classes_metadata_mcache_free_bytes gauge ins, instance, ip, job, cls Memory that is reserved for runtime mcache structures, but not in-use. go_memory_classes_metadata_mcache_inuse_bytes gauge ins, instance, ip, job, cls Memory that is occupied by runtime mcache structures that are currently being used. go_memory_classes_metadata_mspan_free_bytes gauge ins, instance, ip, job, cls Memory that is reserved for runtime mspan structures, but not in-use. go_memory_classes_metadata_mspan_inuse_bytes gauge ins, instance, ip, job, cls Memory that is occupied by runtime mspan structures that are currently being used. go_memory_classes_metadata_other_bytes gauge ins, instance, ip, job, cls Memory that is reserved for or used to hold runtime metadata. go_memory_classes_os_stacks_bytes gauge ins, instance, ip, job, cls Stack memory allocated by the underlying operating system. In non-cgo programs this metric is currently zero. This may change in the future.In cgo programs this metric includes OS thread stacks allocated directly from the OS. Currently, this only accounts for one stack in c-shared and c-archive build modes, and other sources of stacks from the OS are not measured. This too may change in the future. go_memory_classes_other_bytes gauge ins, instance, ip, job, cls Memory used by execution trace buffers, structures for debugging the runtime, finalizer and profiler specials, and more. go_memory_classes_profiling_buckets_bytes gauge ins, instance, ip, job, cls Memory that is used by the stack trace hash map used for profiling. go_memory_classes_total_bytes gauge ins, instance, ip, job, cls All memory mapped by the Go runtime into the current process as read-write. Note that this does not include memory mapped by code called via cgo or via the syscall package. Sum of all metrics in /memory/classes. go_memstats_alloc_bytes counter ins, instance, ip, job, cls Total number of bytes allocated, even if freed. go_memstats_alloc_bytes_total counter ins, instance, ip, job, cls Total number of bytes allocated, even if freed. go_memstats_buck_hash_sys_bytes gauge ins, instance, ip, job, cls Number of bytes used by the profiling bucket hash table. go_memstats_frees_total counter ins, instance, ip, job, cls Total number of frees. go_memstats_gc_sys_bytes gauge ins, instance, ip, job, cls Number of bytes used for garbage collection system metadata. go_memstats_heap_alloc_bytes gauge ins, instance, ip, job, cls Number of heap bytes allocated and still in use. go_memstats_heap_idle_bytes gauge ins, instance, ip, job, cls Number of heap bytes waiting to be used. go_memstats_heap_inuse_bytes gauge ins, instance, ip, job, cls Number of heap bytes that are in use. go_memstats_heap_objects gauge ins, instance, ip, job, cls Number of allocated objects. go_memstats_heap_released_bytes gauge ins, instance, ip, job, cls Number of heap bytes released to OS. go_memstats_heap_sys_bytes gauge ins, instance, ip, job, cls Number of heap bytes obtained from system. go_memstats_last_gc_time_seconds gauge ins, instance, ip, job, cls Number of seconds since 1970 of last garbage collection. go_memstats_lookups_total counter ins, instance, ip, job, cls Total number of pointer lookups. go_memstats_mallocs_total counter ins, instance, ip, job, cls Total number of mallocs. go_memstats_mcache_inuse_bytes gauge ins, instance, ip, job, cls Number of bytes in use by mcache structures. go_memstats_mcache_sys_bytes gauge ins, instance, ip, job, cls Number of bytes used for mcache structures obtained from system. go_memstats_mspan_inuse_bytes gauge ins, instance, ip, job, cls Number of bytes in use by mspan structures. go_memstats_mspan_sys_bytes gauge ins, instance, ip, job, cls Number of bytes used for mspan structures obtained from system. go_memstats_next_gc_bytes gauge ins, instance, ip, job, cls Number of heap bytes when next garbage collection will take place. go_memstats_other_sys_bytes gauge ins, instance, ip, job, cls Number of bytes used for other system allocations. go_memstats_stack_inuse_bytes gauge ins, instance, ip, job, cls Number of bytes in use by the stack allocator. go_memstats_stack_sys_bytes gauge ins, instance, ip, job, cls Number of bytes obtained from system for stack allocator. go_memstats_sys_bytes gauge ins, instance, ip, job, cls Number of bytes obtained from system. go_sched_gomaxprocs_threads gauge ins, instance, ip, job, cls The current runtime.GOMAXPROCS setting, or the number of operating system threads that can execute user-level Go code simultaneously. go_sched_goroutines_goroutines gauge ins, instance, ip, job, cls Count of live goroutines. go_sched_latencies_seconds_bucket Unknown ins, instance, ip, le, job, cls N/A go_sched_latencies_seconds_count Unknown ins, instance, ip, job, cls N/A go_sched_latencies_seconds_sum Unknown ins, instance, ip, job, cls N/A go_sql_stats_connections_blocked_seconds unknown ins, instance, db_name, ip, job, cls The total time blocked waiting for a new connection. go_sql_stats_connections_closed_max_idle unknown ins, instance, db_name, ip, job, cls The total number of connections closed due to SetMaxIdleConns. go_sql_stats_connections_closed_max_idle_time unknown ins, instance, db_name, ip, job, cls The total number of connections closed due to SetConnMaxIdleTime. go_sql_stats_connections_closed_max_lifetime unknown ins, instance, db_name, ip, job, cls The total number of connections closed due to SetConnMaxLifetime. go_sql_stats_connections_idle gauge ins, instance, db_name, ip, job, cls The number of idle connections. go_sql_stats_connections_in_use gauge ins, instance, db_name, ip, job, cls The number of connections currently in use. go_sql_stats_connections_max_open gauge ins, instance, db_name, ip, job, cls Maximum number of open connections to the database. go_sql_stats_connections_open gauge ins, instance, db_name, ip, job, cls The number of established connections both in use and idle. go_sql_stats_connections_waited_for unknown ins, instance, db_name, ip, job, cls The total number of connections waited for. go_sync_mutex_wait_total_seconds_total Unknown ins, instance, ip, job, cls N/A go_threads gauge ins, instance, ip, job, cls Number of OS threads created. grafana_access_evaluation_count unknown ins, instance, ip, job, cls number of evaluation calls grafana_access_evaluation_duration_bucket Unknown ins, instance, ip, le, job, cls N/A grafana_access_evaluation_duration_count Unknown ins, instance, ip, job, cls N/A grafana_access_evaluation_duration_sum Unknown ins, instance, ip, job, cls N/A grafana_access_permissions_duration_bucket Unknown ins, instance, ip, le, job, cls N/A grafana_access_permissions_duration_count Unknown ins, instance, ip, job, cls N/A grafana_access_permissions_duration_sum Unknown ins, instance, ip, job, cls N/A grafana_aggregator_discovery_aggregation_count_total Unknown ins, instance, ip, job, cls N/A grafana_alerting_active_alerts gauge ins, instance, ip, job, cls amount of active alerts grafana_alerting_active_configurations gauge ins, instance, ip, job, cls The number of active Alertmanager configurations. grafana_alerting_alertmanager_config_match gauge ins, instance, ip, job, cls The total number of match grafana_alerting_alertmanager_config_match_re gauge ins, instance, ip, job, cls The total number of matchRE grafana_alerting_alertmanager_config_matchers gauge ins, instance, ip, job, cls The total number of matchers grafana_alerting_alertmanager_config_object_matchers gauge ins, instance, ip, job, cls The total number of object_matchers grafana_alerting_discovered_configurations gauge ins, instance, ip, job, cls The number of organizations we’ve discovered that require an Alertmanager configuration. grafana_alerting_dispatcher_aggregation_groups gauge ins, instance, ip, job, cls Number of active aggregation groups grafana_alerting_dispatcher_alert_processing_duration_seconds_count Unknown ins, instance, ip, job, cls N/A grafana_alerting_dispatcher_alert_processing_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A grafana_alerting_execution_time_milliseconds summary ins, instance, ip, job, cls, quantile summary of alert execution duration grafana_alerting_execution_time_milliseconds_count Unknown ins, instance, ip, job, cls N/A grafana_alerting_execution_time_milliseconds_sum Unknown ins, instance, ip, job, cls N/A grafana_alerting_nflog_gc_duration_seconds_count Unknown ins, instance, ip, job, cls N/A grafana_alerting_nflog_gc_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A grafana_alerting_nflog_gossip_messages_propagated_total Unknown ins, instance, ip, job, cls N/A grafana_alerting_nflog_queries_total Unknown ins, instance, ip, job, cls N/A grafana_alerting_nflog_query_duration_seconds_bucket Unknown ins, instance, ip, le, job, cls N/A grafana_alerting_nflog_query_duration_seconds_count Unknown ins, instance, ip, job, cls N/A grafana_alerting_nflog_query_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A grafana_alerting_nflog_query_errors_total Unknown ins, instance, ip, job, cls N/A grafana_alerting_nflog_snapshot_duration_seconds_count Unknown ins, instance, ip, job, cls N/A grafana_alerting_nflog_snapshot_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A grafana_alerting_nflog_snapshot_size_bytes gauge ins, instance, ip, job, cls Size of the last notification log snapshot in bytes. grafana_alerting_notification_latency_seconds_bucket Unknown ins, instance, ip, le, job, cls N/A grafana_alerting_notification_latency_seconds_count Unknown ins, instance, ip, job, cls N/A grafana_alerting_notification_latency_seconds_sum Unknown ins, instance, ip, job, cls N/A grafana_alerting_schedule_alert_rules gauge ins, instance, ip, job, cls The number of alert rules that could be considered for evaluation at the next tick. grafana_alerting_schedule_alert_rules_hash gauge ins, instance, ip, job, cls A hash of the alert rules that could be considered for evaluation at the next tick. grafana_alerting_schedule_periodic_duration_seconds_bucket Unknown ins, instance, ip, le, job, cls N/A grafana_alerting_schedule_periodic_duration_seconds_count Unknown ins, instance, ip, job, cls N/A grafana_alerting_schedule_periodic_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A grafana_alerting_schedule_query_alert_rules_duration_seconds_bucket Unknown ins, instance, ip, le, job, cls N/A grafana_alerting_schedule_query_alert_rules_duration_seconds_count Unknown ins, instance, ip, job, cls N/A grafana_alerting_schedule_query_alert_rules_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A grafana_alerting_scheduler_behind_seconds gauge ins, instance, ip, job, cls The total number of seconds the scheduler is behind. grafana_alerting_silences_gc_duration_seconds_count Unknown ins, instance, ip, job, cls N/A grafana_alerting_silences_gc_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A grafana_alerting_silences_gossip_messages_propagated_total Unknown ins, instance, ip, job, cls N/A grafana_alerting_silences_queries_total Unknown ins, instance, ip, job, cls N/A grafana_alerting_silences_query_duration_seconds_bucket Unknown ins, instance, ip, le, job, cls N/A grafana_alerting_silences_query_duration_seconds_count Unknown ins, instance, ip, job, cls N/A grafana_alerting_silences_query_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A grafana_alerting_silences_query_errors_total Unknown ins, instance, ip, job, cls N/A grafana_alerting_silences_snapshot_duration_seconds_count Unknown ins, instance, ip, job, cls N/A grafana_alerting_silences_snapshot_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A grafana_alerting_silences_snapshot_size_bytes gauge ins, instance, ip, job, cls Size of the last silence snapshot in bytes. grafana_alerting_state_calculation_duration_seconds_bucket Unknown ins, instance, ip, le, job, cls N/A grafana_alerting_state_calculation_duration_seconds_count Unknown ins, instance, ip, job, cls N/A grafana_alerting_state_calculation_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A grafana_alerting_state_history_writes_bytes_total Unknown ins, instance, ip, job, cls N/A grafana_alerting_ticker_interval_seconds gauge ins, instance, ip, job, cls Interval at which the ticker is meant to tick. grafana_alerting_ticker_last_consumed_tick_timestamp_seconds gauge ins, instance, ip, job, cls Timestamp of the last consumed tick in seconds. grafana_alerting_ticker_next_tick_timestamp_seconds gauge ins, instance, ip, job, cls Timestamp of the next tick in seconds before it is consumed. grafana_api_admin_user_created_total Unknown ins, instance, ip, job, cls N/A grafana_api_dashboard_get_milliseconds summary ins, instance, ip, job, cls, quantile summary for dashboard get duration grafana_api_dashboard_get_milliseconds_count Unknown ins, instance, ip, job, cls N/A grafana_api_dashboard_get_milliseconds_sum Unknown ins, instance, ip, job, cls N/A grafana_api_dashboard_save_milliseconds summary ins, instance, ip, job, cls, quantile summary for dashboard save duration grafana_api_dashboard_save_milliseconds_count Unknown ins, instance, ip, job, cls N/A grafana_api_dashboard_save_milliseconds_sum Unknown ins, instance, ip, job, cls N/A grafana_api_dashboard_search_milliseconds summary ins, instance, ip, job, cls, quantile summary for dashboard search duration grafana_api_dashboard_search_milliseconds_count Unknown ins, instance, ip, job, cls N/A grafana_api_dashboard_search_milliseconds_sum Unknown ins, instance, ip, job, cls N/A grafana_api_dashboard_snapshot_create_total Unknown ins, instance, ip, job, cls N/A grafana_api_dashboard_snapshot_external_total Unknown ins, instance, ip, job, cls N/A grafana_api_dashboard_snapshot_get_total Unknown ins, instance, ip, job, cls N/A grafana_api_dataproxy_request_all_milliseconds summary ins, instance, ip, job, cls, quantile summary for dataproxy request duration grafana_api_dataproxy_request_all_milliseconds_count Unknown ins, instance, ip, job, cls N/A grafana_api_dataproxy_request_all_milliseconds_sum Unknown ins, instance, ip, job, cls N/A grafana_api_login_oauth_total Unknown ins, instance, ip, job, cls N/A grafana_api_login_post_total Unknown ins, instance, ip, job, cls N/A grafana_api_login_saml_total Unknown ins, instance, ip, job, cls N/A grafana_api_models_dashboard_insert_total Unknown ins, instance, ip, job, cls N/A grafana_api_org_create_total Unknown ins, instance, ip, job, cls N/A grafana_api_response_status_total Unknown ins, instance, ip, job, cls, code N/A grafana_api_user_signup_completed_total Unknown ins, instance, ip, job, cls N/A grafana_api_user_signup_invite_total Unknown ins, instance, ip, job, cls N/A grafana_api_user_signup_started_total Unknown ins, instance, ip, job, cls N/A grafana_apiserver_audit_event_total Unknown ins, instance, ip, job, cls N/A grafana_apiserver_audit_requests_rejected_total Unknown ins, instance, ip, job, cls N/A grafana_apiserver_client_certificate_expiration_seconds_bucket Unknown ins, instance, ip, le, job, cls N/A grafana_apiserver_client_certificate_expiration_seconds_count Unknown ins, instance, ip, job, cls N/A grafana_apiserver_client_certificate_expiration_seconds_sum Unknown ins, instance, ip, job, cls N/A grafana_apiserver_envelope_encryption_dek_cache_fill_percent gauge ins, instance, ip, job, cls [ALPHA] Percent of the cache slots currently occupied by cached DEKs. grafana_apiserver_flowcontrol_seat_fair_frac gauge ins, instance, ip, job, cls [ALPHA] Fair fraction of server’s concurrency to allocate to each priority level that can use it grafana_apiserver_storage_data_key_generation_duration_seconds_bucket Unknown ins, instance, ip, le, job, cls N/A grafana_apiserver_storage_data_key_generation_duration_seconds_count Unknown ins, instance, ip, job, cls N/A grafana_apiserver_storage_data_key_generation_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A grafana_apiserver_storage_data_key_generation_failures_total Unknown ins, instance, ip, job, cls N/A grafana_apiserver_storage_envelope_transformation_cache_misses_total Unknown ins, instance, ip, job, cls N/A grafana_apiserver_tls_handshake_errors_total Unknown ins, instance, ip, job, cls N/A grafana_apiserver_webhooks_x509_insecure_sha1_total Unknown ins, instance, ip, job, cls N/A grafana_apiserver_webhooks_x509_missing_san_total Unknown ins, instance, ip, job, cls N/A grafana_authn_authn_failed_authentication_total Unknown ins, instance, ip, job, cls N/A grafana_authn_authn_successful_authentication_total Unknown ins, instance, ip, client, job, cls N/A grafana_authn_authn_successful_login_total Unknown ins, instance, ip, client, job, cls N/A grafana_aws_cloudwatch_get_metric_data_total Unknown ins, instance, ip, job, cls N/A grafana_aws_cloudwatch_get_metric_statistics_total Unknown ins, instance, ip, job, cls N/A grafana_aws_cloudwatch_list_metrics_total Unknown ins, instance, ip, job, cls N/A grafana_build_info gauge revision, version, ins, instance, edition, ip, goversion, job, cls, branch A metric with a constant ‘1’ value labeled by version, revision, branch, and goversion from which Grafana was built grafana_build_timestamp gauge revision, version, ins, instance, edition, ip, goversion, job, cls, branch A metric exposing when the binary was built in epoch grafana_cardinality_enforcement_unexpected_categorizations_total Unknown ins, instance, ip, job, cls N/A grafana_database_conn_idle gauge ins, instance, ip, job, cls The number of idle connections grafana_database_conn_in_use gauge ins, instance, ip, job, cls The number of connections currently in use grafana_database_conn_max_idle_closed_seconds unknown ins, instance, ip, job, cls The total number of connections closed due to SetConnMaxIdleTime grafana_database_conn_max_idle_closed_total Unknown ins, instance, ip, job, cls N/A grafana_database_conn_max_lifetime_closed_total Unknown ins, instance, ip, job, cls N/A grafana_database_conn_max_open gauge ins, instance, ip, job, cls Maximum number of open connections to the database grafana_database_conn_open gauge ins, instance, ip, job, cls The number of established connections both in use and idle grafana_database_conn_wait_count_total Unknown ins, instance, ip, job, cls N/A grafana_database_conn_wait_duration_seconds unknown ins, instance, ip, job, cls The total time blocked waiting for a new connection grafana_datasource_request_duration_seconds_bucket Unknown datasource, ins, instance, method, ip, le, datasource_type, job, cls, code N/A grafana_datasource_request_duration_seconds_count Unknown datasource, ins, instance, method, ip, datasource_type, job, cls, code N/A grafana_datasource_request_duration_seconds_sum Unknown datasource, ins, instance, method, ip, datasource_type, job, cls, code N/A grafana_datasource_request_in_flight gauge datasource, ins, instance, ip, datasource_type, job, cls A gauge of outgoing data source requests currently being sent by Grafana grafana_datasource_request_total Unknown datasource, ins, instance, method, ip, datasource_type, job, cls, code N/A grafana_datasource_response_size_bytes_bucket Unknown datasource, ins, instance, ip, le, datasource_type, job, cls N/A grafana_datasource_response_size_bytes_count Unknown datasource, ins, instance, ip, datasource_type, job, cls N/A grafana_datasource_response_size_bytes_sum Unknown datasource, ins, instance, ip, datasource_type, job, cls N/A grafana_db_datasource_query_by_id_total Unknown ins, instance, ip, job, cls N/A grafana_disabled_metrics_total Unknown ins, instance, ip, job, cls N/A grafana_emails_sent_failed unknown ins, instance, ip, job, cls Number of emails Grafana failed to send grafana_emails_sent_total Unknown ins, instance, ip, job, cls N/A grafana_encryption_cache_reads_total Unknown ins, instance, method, ip, hit, job, cls N/A grafana_encryption_ops_total Unknown ins, instance, ip, success, operation, job, cls N/A grafana_environment_info gauge version, ins, instance, ip, job, cls, commit A metric with a constant ‘1’ value labeled by environment information about the running instance. grafana_feature_toggles_info gauge ins, instance, ip, job, cls info metric that exposes what feature toggles are enabled or not grafana_frontend_boot_css_time_seconds_bucket Unknown ins, instance, ip, le, job, cls N/A grafana_frontend_boot_css_time_seconds_count Unknown ins, instance, ip, job, cls N/A grafana_frontend_boot_css_time_seconds_sum Unknown ins, instance, ip, job, cls N/A grafana_frontend_boot_first_contentful_paint_time_seconds_bucket Unknown ins, instance, ip, le, job, cls N/A grafana_frontend_boot_first_contentful_paint_time_seconds_count Unknown ins, instance, ip, job, cls N/A grafana_frontend_boot_first_contentful_paint_time_seconds_sum Unknown ins, instance, ip, job, cls N/A grafana_frontend_boot_first_paint_time_seconds_bucket Unknown ins, instance, ip, le, job, cls N/A grafana_frontend_boot_first_paint_time_seconds_count Unknown ins, instance, ip, job, cls N/A grafana_frontend_boot_first_paint_time_seconds_sum Unknown ins, instance, ip, job, cls N/A grafana_frontend_boot_js_done_time_seconds_bucket Unknown ins, instance, ip, le, job, cls N/A grafana_frontend_boot_js_done_time_seconds_count Unknown ins, instance, ip, job, cls N/A grafana_frontend_boot_js_done_time_seconds_sum Unknown ins, instance, ip, job, cls N/A grafana_frontend_boot_load_time_seconds_bucket Unknown ins, instance, ip, le, job, cls N/A grafana_frontend_boot_load_time_seconds_count Unknown ins, instance, ip, job, cls N/A grafana_frontend_boot_load_time_seconds_sum Unknown ins, instance, ip, job, cls N/A grafana_frontend_plugins_preload_ms_bucket Unknown ins, instance, ip, le, job, cls N/A grafana_frontend_plugins_preload_ms_count Unknown ins, instance, ip, job, cls N/A grafana_frontend_plugins_preload_ms_sum Unknown ins, instance, ip, job, cls N/A grafana_hidden_metrics_total Unknown ins, instance, ip, job, cls N/A grafana_http_request_duration_seconds_bucket Unknown ins, instance, method, ip, le, job, cls, status_code, handler N/A grafana_http_request_duration_seconds_count Unknown ins, instance, method, ip, job, cls, status_code, handler N/A grafana_http_request_duration_seconds_sum Unknown ins, instance, method, ip, job, cls, status_code, handler N/A grafana_http_request_in_flight gauge ins, instance, ip, job, cls A gauge of requests currently being served by Grafana. grafana_idforwarding_idforwarding_failed_token_signing_total Unknown ins, instance, ip, job, cls N/A grafana_idforwarding_idforwarding_token_signing_duration_seconds_bucket Unknown ins, instance, ip, le, job, cls N/A grafana_idforwarding_idforwarding_token_signing_duration_seconds_count Unknown ins, instance, ip, job, cls N/A grafana_idforwarding_idforwarding_token_signing_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A grafana_idforwarding_idforwarding_token_signing_from_cache_total Unknown ins, instance, ip, job, cls N/A grafana_idforwarding_idforwarding_token_signing_total Unknown ins, instance, ip, job, cls N/A grafana_instance_start_total Unknown ins, instance, ip, job, cls N/A grafana_ldap_users_sync_execution_time summary ins, instance, ip, job, cls, quantile summary for LDAP users sync execution duration grafana_ldap_users_sync_execution_time_count Unknown ins, instance, ip, job, cls N/A grafana_ldap_users_sync_execution_time_sum Unknown ins, instance, ip, job, cls N/A grafana_live_client_command_duration_seconds summary ins, instance, method, ip, job, cls, quantile Client command duration summary. grafana_live_client_command_duration_seconds_count Unknown ins, instance, method, ip, job, cls N/A grafana_live_client_command_duration_seconds_sum Unknown ins, instance, method, ip, job, cls N/A grafana_live_client_num_reply_errors unknown ins, instance, method, ip, job, cls, code Number of errors in replies sent to clients. grafana_live_client_num_server_disconnects unknown ins, instance, ip, job, cls, code Number of server initiated disconnects. grafana_live_client_recover unknown ins, instance, ip, recovered, job, cls Count of recover operations. grafana_live_node_action_count unknown action, ins, instance, ip, job, cls Number of node actions called. grafana_live_node_build gauge version, ins, instance, ip, job, cls Node build info. grafana_live_node_messages_received_count unknown ins, instance, ip, type, job, cls Number of messages received. grafana_live_node_messages_sent_count unknown ins, instance, ip, type, job, cls Number of messages sent. grafana_live_node_num_channels gauge ins, instance, ip, job, cls Number of channels with one or more subscribers. grafana_live_node_num_clients gauge ins, instance, ip, job, cls Number of clients connected. grafana_live_node_num_nodes gauge ins, instance, ip, job, cls Number of nodes in cluster. grafana_live_node_num_subscriptions gauge ins, instance, ip, job, cls Number of subscriptions. grafana_live_node_num_users gauge ins, instance, ip, job, cls Number of unique users connected. grafana_live_transport_connect_count unknown ins, instance, ip, transport, job, cls Number of connections to specific transport. grafana_live_transport_messages_sent unknown ins, instance, ip, transport, job, cls Number of messages sent over specific transport. grafana_loki_plugin_parse_response_duration_seconds_bucket Unknown endpoint, ins, instance, ip, le, status, job, cls N/A grafana_loki_plugin_parse_response_duration_seconds_count Unknown endpoint, ins, instance, ip, status, job, cls N/A grafana_loki_plugin_parse_response_duration_seconds_sum Unknown endpoint, ins, instance, ip, status, job, cls N/A grafana_page_response_status_total Unknown ins, instance, ip, job, cls, code N/A grafana_plugin_build_info gauge version, signature_status, ins, instance, plugin_type, ip, plugin_id, job, cls A metric with a constant ‘1’ value labeled by pluginId, pluginType and version from which Grafana plugin was built grafana_plugin_request_duration_milliseconds_bucket Unknown endpoint, ins, instance, target, ip, le, plugin_id, job, cls N/A grafana_plugin_request_duration_milliseconds_count Unknown endpoint, ins, instance, target, ip, plugin_id, job, cls N/A grafana_plugin_request_duration_milliseconds_sum Unknown endpoint, ins, instance, target, ip, plugin_id, job, cls N/A grafana_plugin_request_duration_seconds_bucket Unknown endpoint, ins, instance, target, ip, le, status, plugin_id, source, job, cls N/A grafana_plugin_request_duration_seconds_count Unknown endpoint, ins, instance, target, ip, status, plugin_id, source, job, cls N/A grafana_plugin_request_duration_seconds_sum Unknown endpoint, ins, instance, target, ip, status, plugin_id, source, job, cls N/A grafana_plugin_request_size_bytes_bucket Unknown endpoint, ins, instance, target, ip, le, plugin_id, source, job, cls N/A grafana_plugin_request_size_bytes_count Unknown endpoint, ins, instance, target, ip, plugin_id, source, job, cls N/A grafana_plugin_request_size_bytes_sum Unknown endpoint, ins, instance, target, ip, plugin_id, source, job, cls N/A grafana_plugin_request_total Unknown endpoint, ins, instance, target, ip, status, plugin_id, job, cls N/A grafana_process_cpu_seconds_total Unknown ins, instance, ip, job, cls N/A grafana_process_max_fds gauge ins, instance, ip, job, cls Maximum number of open file descriptors. grafana_process_open_fds gauge ins, instance, ip, job, cls Number of open file descriptors. grafana_process_resident_memory_bytes gauge ins, instance, ip, job, cls Resident memory size in bytes. grafana_process_start_time_seconds gauge ins, instance, ip, job, cls Start time of the process since unix epoch in seconds. grafana_process_virtual_memory_bytes gauge ins, instance, ip, job, cls Virtual memory size in bytes. grafana_process_virtual_memory_max_bytes gauge ins, instance, ip, job, cls Maximum amount of virtual memory available in bytes. grafana_prometheus_plugin_backend_request_count unknown endpoint, ins, instance, ip, status, errorSource, job, cls The total amount of prometheus backend plugin requests grafana_proxy_response_status_total Unknown ins, instance, ip, job, cls, code N/A grafana_public_dashboard_request_count unknown ins, instance, ip, job, cls counter for public dashboards requests grafana_registered_metrics_total Unknown ins, instance, ip, stability_level, deprecated_version, job, cls N/A grafana_rendering_queue_size gauge ins, instance, ip, job, cls size of rendering queue grafana_search_dashboard_search_failures_duration_seconds_bucket Unknown ins, instance, ip, le, job, cls N/A grafana_search_dashboard_search_failures_duration_seconds_count Unknown ins, instance, ip, job, cls N/A grafana_search_dashboard_search_failures_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A grafana_search_dashboard_search_successes_duration_seconds_bucket Unknown ins, instance, ip, le, job, cls N/A grafana_search_dashboard_search_successes_duration_seconds_count Unknown ins, instance, ip, job, cls N/A grafana_search_dashboard_search_successes_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A grafana_stat_active_users gauge ins, instance, ip, job, cls number of active users grafana_stat_total_orgs gauge ins, instance, ip, job, cls total amount of orgs grafana_stat_total_playlists gauge ins, instance, ip, job, cls total amount of playlists grafana_stat_total_service_account_tokens gauge ins, instance, ip, job, cls total amount of service account tokens grafana_stat_total_service_accounts gauge ins, instance, ip, job, cls total amount of service accounts grafana_stat_total_service_accounts_role_none gauge ins, instance, ip, job, cls total amount of service accounts with no role grafana_stat_total_teams gauge ins, instance, ip, job, cls total amount of teams grafana_stat_total_users gauge ins, instance, ip, job, cls total amount of users grafana_stat_totals_active_admins gauge ins, instance, ip, job, cls total amount of active admins grafana_stat_totals_active_editors gauge ins, instance, ip, job, cls total amount of active editors grafana_stat_totals_active_viewers gauge ins, instance, ip, job, cls total amount of active viewers grafana_stat_totals_admins gauge ins, instance, ip, job, cls total amount of admins grafana_stat_totals_alert_rules gauge ins, instance, ip, job, cls total amount of alert rules in the database grafana_stat_totals_annotations gauge ins, instance, ip, job, cls total amount of annotations in the database grafana_stat_totals_correlations gauge ins, instance, ip, job, cls total amount of correlations grafana_stat_totals_dashboard gauge ins, instance, ip, job, cls total amount of dashboards grafana_stat_totals_dashboard_versions gauge ins, instance, ip, job, cls total amount of dashboard versions in the database grafana_stat_totals_data_keys gauge ins, instance, ip, job, cls, active total amount of data keys in the database grafana_stat_totals_datasource gauge ins, instance, ip, plugin_id, job, cls total number of defined datasources, labeled by pluginId grafana_stat_totals_editors gauge ins, instance, ip, job, cls total amount of editors grafana_stat_totals_folder gauge ins, instance, ip, job, cls total amount of folders grafana_stat_totals_library_panels gauge ins, instance, ip, job, cls total amount of library panels in the database grafana_stat_totals_library_variables gauge ins, instance, ip, job, cls total amount of library variables in the database grafana_stat_totals_public_dashboard gauge ins, instance, ip, job, cls total amount of public dashboards grafana_stat_totals_rule_groups gauge ins, instance, ip, job, cls total amount of alert rule groups in the database grafana_stat_totals_viewers gauge ins, instance, ip, job, cls total amount of viewers infra_up Unknown ins, instance, ip, job, cls N/A jaeger_tracer_baggage_restrictions_updates_total Unknown result, ins, instance, ip, job, cls N/A jaeger_tracer_baggage_truncations_total Unknown ins, instance, ip, job, cls N/A jaeger_tracer_baggage_updates_total Unknown result, ins, instance, ip, job, cls N/A jaeger_tracer_finished_spans_total Unknown ins, instance, ip, sampled, job, cls N/A jaeger_tracer_reporter_queue_length gauge ins, instance, ip, job, cls Current number of spans in the reporter queue jaeger_tracer_reporter_spans_total Unknown result, ins, instance, ip, job, cls N/A jaeger_tracer_sampler_queries_total Unknown result, ins, instance, ip, job, cls N/A jaeger_tracer_sampler_updates_total Unknown result, ins, instance, ip, job, cls N/A jaeger_tracer_span_context_decoding_errors_total Unknown ins, instance, ip, job, cls N/A jaeger_tracer_started_spans_total Unknown ins, instance, ip, sampled, job, cls N/A jaeger_tracer_throttled_debug_spans_total Unknown ins, instance, ip, job, cls N/A jaeger_tracer_throttler_updates_total Unknown result, ins, instance, ip, job, cls N/A jaeger_tracer_traces_total Unknown ins, instance, ip, sampled, job, cls, state N/A kv_request_duration_seconds_bucket Unknown ins, instance, role, ip, le, kv_name, type, operation, job, cls, status_code N/A kv_request_duration_seconds_count Unknown ins, instance, role, ip, kv_name, type, operation, job, cls, status_code N/A kv_request_duration_seconds_sum Unknown ins, instance, role, ip, kv_name, type, operation, job, cls, status_code N/A legacy_grafana_alerting_ticker_interval_seconds gauge ins, instance, ip, job, cls Interval at which the ticker is meant to tick. legacy_grafana_alerting_ticker_last_consumed_tick_timestamp_seconds gauge ins, instance, ip, job, cls Timestamp of the last consumed tick in seconds. legacy_grafana_alerting_ticker_next_tick_timestamp_seconds gauge ins, instance, ip, job, cls Timestamp of the next tick in seconds before it is consumed. logql_query_duration_seconds_bucket Unknown ins, instance, query_type, ip, le, job, cls N/A logql_query_duration_seconds_count Unknown ins, instance, query_type, ip, job, cls N/A logql_query_duration_seconds_sum Unknown ins, instance, query_type, ip, job, cls N/A loki_azure_blob_egress_bytes_total Unknown ins, instance, ip, job, cls N/A loki_boltdb_shipper_apply_retention_last_successful_run_timestamp_seconds gauge ins, instance, ip, job, cls Unix timestamp of the last successful retention run loki_boltdb_shipper_compact_tables_operation_duration_seconds gauge ins, instance, ip, job, cls Time (in seconds) spent in compacting all the tables loki_boltdb_shipper_compact_tables_operation_last_successful_run_timestamp_seconds gauge ins, instance, ip, job, cls Unix timestamp of the last successful compaction run loki_boltdb_shipper_compact_tables_operation_total Unknown ins, instance, ip, status, job, cls N/A loki_boltdb_shipper_compactor_running gauge ins, instance, ip, job, cls Value will be 1 if compactor is currently running on this instance loki_boltdb_shipper_open_existing_file_failures_total Unknown ins, instance, ip, component, job, cls N/A loki_boltdb_shipper_query_time_table_download_duration_seconds unknown ins, instance, ip, component, job, cls, table Time (in seconds) spent in downloading of files per table at query time loki_boltdb_shipper_request_duration_seconds_bucket Unknown ins, instance, ip, le, component, operation, job, cls, status_code N/A loki_boltdb_shipper_request_duration_seconds_count Unknown ins, instance, ip, component, operation, job, cls, status_code N/A loki_boltdb_shipper_request_duration_seconds_sum Unknown ins, instance, ip, component, operation, job, cls, status_code N/A loki_boltdb_shipper_tables_download_operation_duration_seconds gauge ins, instance, ip, component, job, cls Time (in seconds) spent in downloading updated files for all the tables loki_boltdb_shipper_tables_sync_operation_total Unknown ins, instance, ip, status, component, job, cls N/A loki_boltdb_shipper_tables_upload_operation_total Unknown ins, instance, ip, status, component, job, cls N/A loki_build_info gauge revision, version, ins, instance, ip, tags, goarch, goversion, job, cls, branch, goos A metric with a constant ‘1’ value labeled by version, revision, branch, goversion from which loki was built, and the goos and goarch for the build. loki_bytes_per_line_bucket Unknown ins, instance, ip, le, job, cls N/A loki_bytes_per_line_count Unknown ins, instance, ip, job, cls N/A loki_bytes_per_line_sum Unknown ins, instance, ip, job, cls N/A loki_cache_corrupt_chunks_total Unknown ins, instance, ip, job, cls N/A loki_cache_fetched_keys unknown ins, instance, ip, job, cls Total count of keys requested from cache. loki_cache_hits unknown ins, instance, ip, job, cls Total count of keys found in cache. loki_cache_request_duration_seconds_bucket Unknown ins, instance, method, ip, le, job, cls, status_code N/A loki_cache_request_duration_seconds_count Unknown ins, instance, method, ip, job, cls, status_code N/A loki_cache_request_duration_seconds_sum Unknown ins, instance, method, ip, job, cls, status_code N/A loki_cache_value_size_bytes_bucket Unknown ins, instance, method, ip, le, job, cls N/A loki_cache_value_size_bytes_count Unknown ins, instance, method, ip, job, cls N/A loki_cache_value_size_bytes_sum Unknown ins, instance, method, ip, job, cls N/A loki_chunk_fetcher_cache_dequeued_total Unknown ins, instance, ip, job, cls N/A loki_chunk_fetcher_cache_enqueued_total Unknown ins, instance, ip, job, cls N/A loki_chunk_fetcher_cache_skipped_buffer_full_total Unknown ins, instance, ip, job, cls N/A loki_chunk_fetcher_fetched_size_bytes_bucket Unknown ins, instance, ip, le, source, job, cls N/A loki_chunk_fetcher_fetched_size_bytes_count Unknown ins, instance, ip, source, job, cls N/A loki_chunk_fetcher_fetched_size_bytes_sum Unknown ins, instance, ip, source, job, cls N/A loki_chunk_store_chunks_per_query_bucket Unknown ins, instance, ip, le, job, cls N/A loki_chunk_store_chunks_per_query_count Unknown ins, instance, ip, job, cls N/A loki_chunk_store_chunks_per_query_sum Unknown ins, instance, ip, job, cls N/A loki_chunk_store_deduped_bytes_total Unknown ins, instance, ip, job, cls N/A loki_chunk_store_deduped_chunks_total Unknown ins, instance, ip, job, cls N/A loki_chunk_store_fetched_chunk_bytes_total Unknown ins, instance, ip, user, job, cls N/A loki_chunk_store_fetched_chunks_total Unknown ins, instance, ip, user, job, cls N/A loki_chunk_store_index_entries_per_chunk_bucket Unknown ins, instance, ip, le, job, cls N/A loki_chunk_store_index_entries_per_chunk_count Unknown ins, instance, ip, job, cls N/A loki_chunk_store_index_entries_per_chunk_sum Unknown ins, instance, ip, job, cls N/A loki_chunk_store_index_lookups_per_query_bucket Unknown ins, instance, ip, le, job, cls N/A loki_chunk_store_index_lookups_per_query_count Unknown ins, instance, ip, job, cls N/A loki_chunk_store_index_lookups_per_query_sum Unknown ins, instance, ip, job, cls N/A loki_chunk_store_series_post_intersection_per_query_bucket Unknown ins, instance, ip, le, job, cls N/A loki_chunk_store_series_post_intersection_per_query_count Unknown ins, instance, ip, job, cls N/A loki_chunk_store_series_post_intersection_per_query_sum Unknown ins, instance, ip, job, cls N/A loki_chunk_store_series_pre_intersection_per_query_bucket Unknown ins, instance, ip, le, job, cls N/A loki_chunk_store_series_pre_intersection_per_query_count Unknown ins, instance, ip, job, cls N/A loki_chunk_store_series_pre_intersection_per_query_sum Unknown ins, instance, ip, job, cls N/A loki_chunk_store_stored_chunk_bytes_total Unknown ins, instance, ip, user, job, cls N/A loki_chunk_store_stored_chunks_total Unknown ins, instance, ip, user, job, cls N/A loki_consul_request_duration_seconds_bucket Unknown ins, instance, ip, le, kv_name, operation, job, cls, status_code N/A loki_consul_request_duration_seconds_count Unknown ins, instance, ip, kv_name, operation, job, cls, status_code N/A loki_consul_request_duration_seconds_sum Unknown ins, instance, ip, kv_name, operation, job, cls, status_code N/A loki_delete_request_lookups_failed_total Unknown ins, instance, ip, job, cls N/A loki_delete_request_lookups_total Unknown ins, instance, ip, job, cls N/A loki_discarded_bytes_total Unknown ins, instance, ip, reason, job, cls, tenant N/A loki_discarded_samples_total Unknown ins, instance, ip, reason, job, cls, tenant N/A loki_distributor_bytes_received_total Unknown ins, instance, retention_hours, ip, job, cls, tenant N/A loki_distributor_ingester_appends_total Unknown ins, instance, ip, ingester, job, cls N/A loki_distributor_lines_received_total Unknown ins, instance, ip, job, cls, tenant N/A loki_distributor_replication_factor gauge ins, instance, ip, job, cls The configured replication factor. loki_distributor_structured_metadata_bytes_received_total Unknown ins, instance, retention_hours, ip, job, cls, tenant N/A loki_experimental_features_in_use_total Unknown ins, instance, ip, job, cls N/A loki_index_chunk_refs_total Unknown ins, instance, ip, status, job, cls N/A loki_index_request_duration_seconds_bucket Unknown ins, instance, ip, le, component, operation, job, cls, status_code N/A loki_index_request_duration_seconds_count Unknown ins, instance, ip, component, operation, job, cls, status_code N/A loki_index_request_duration_seconds_sum Unknown ins, instance, ip, component, operation, job, cls, status_code N/A loki_inflight_requests gauge ins, instance, method, ip, route, job, cls Current number of inflight requests. loki_ingester_autoforget_unhealthy_ingesters_total Unknown ins, instance, ip, job, cls N/A loki_ingester_blocks_per_chunk_bucket Unknown ins, instance, ip, le, job, cls N/A loki_ingester_blocks_per_chunk_count Unknown ins, instance, ip, job, cls N/A loki_ingester_blocks_per_chunk_sum Unknown ins, instance, ip, job, cls N/A loki_ingester_checkpoint_creations_failed_total Unknown ins, instance, ip, job, cls N/A loki_ingester_checkpoint_creations_total Unknown ins, instance, ip, job, cls N/A loki_ingester_checkpoint_deletions_failed_total Unknown ins, instance, ip, job, cls N/A loki_ingester_checkpoint_deletions_total Unknown ins, instance, ip, job, cls N/A loki_ingester_checkpoint_duration_seconds summary ins, instance, ip, job, cls, quantile Time taken to create a checkpoint. loki_ingester_checkpoint_duration_seconds_count Unknown ins, instance, ip, job, cls N/A loki_ingester_checkpoint_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A loki_ingester_checkpoint_logged_bytes_total Unknown ins, instance, ip, job, cls N/A loki_ingester_chunk_age_seconds_bucket Unknown ins, instance, ip, le, job, cls N/A loki_ingester_chunk_age_seconds_count Unknown ins, instance, ip, job, cls N/A loki_ingester_chunk_age_seconds_sum Unknown ins, instance, ip, job, cls N/A loki_ingester_chunk_bounds_hours_bucket Unknown ins, instance, ip, le, job, cls N/A loki_ingester_chunk_bounds_hours_count Unknown ins, instance, ip, job, cls N/A loki_ingester_chunk_bounds_hours_sum Unknown ins, instance, ip, job, cls N/A loki_ingester_chunk_compression_ratio_bucket Unknown ins, instance, ip, le, job, cls N/A loki_ingester_chunk_compression_ratio_count Unknown ins, instance, ip, job, cls N/A loki_ingester_chunk_compression_ratio_sum Unknown ins, instance, ip, job, cls N/A loki_ingester_chunk_encode_time_seconds_bucket Unknown ins, instance, ip, le, job, cls N/A loki_ingester_chunk_encode_time_seconds_count Unknown ins, instance, ip, job, cls N/A loki_ingester_chunk_encode_time_seconds_sum Unknown ins, instance, ip, job, cls N/A loki_ingester_chunk_entries_bucket Unknown ins, instance, ip, le, job, cls N/A loki_ingester_chunk_entries_count Unknown ins, instance, ip, job, cls N/A loki_ingester_chunk_entries_sum Unknown ins, instance, ip, job, cls N/A loki_ingester_chunk_size_bytes_bucket Unknown ins, instance, ip, le, job, cls N/A loki_ingester_chunk_size_bytes_count Unknown ins, instance, ip, job, cls N/A loki_ingester_chunk_size_bytes_sum Unknown ins, instance, ip, job, cls N/A loki_ingester_chunk_stored_bytes_total Unknown ins, instance, ip, job, cls, tenant N/A loki_ingester_chunk_utilization_bucket Unknown ins, instance, ip, le, job, cls N/A loki_ingester_chunk_utilization_count Unknown ins, instance, ip, job, cls N/A loki_ingester_chunk_utilization_sum Unknown ins, instance, ip, job, cls N/A loki_ingester_chunks_created_total Unknown ins, instance, ip, job, cls N/A loki_ingester_chunks_flushed_total Unknown ins, instance, ip, reason, job, cls N/A loki_ingester_chunks_stored_total Unknown ins, instance, ip, job, cls, tenant N/A loki_ingester_client_request_duration_seconds_bucket Unknown ins, instance, ip, le, operation, job, cls, status_code N/A loki_ingester_client_request_duration_seconds_count Unknown ins, instance, ip, operation, job, cls, status_code N/A loki_ingester_client_request_duration_seconds_sum Unknown ins, instance, ip, operation, job, cls, status_code N/A loki_ingester_limiter_enabled gauge ins, instance, ip, job, cls Whether the ingester’s limiter is enabled loki_ingester_memory_chunks gauge ins, instance, ip, job, cls The total number of chunks in memory. loki_ingester_memory_streams gauge ins, instance, ip, job, cls, tenant The total number of streams in memory per tenant. loki_ingester_memory_streams_labels_bytes gauge ins, instance, ip, job, cls Total bytes of labels of the streams in memory. loki_ingester_received_chunks unknown ins, instance, ip, job, cls The total number of chunks received by this ingester whilst joining. loki_ingester_samples_per_chunk_bucket Unknown ins, instance, ip, le, job, cls N/A loki_ingester_samples_per_chunk_count Unknown ins, instance, ip, job, cls N/A loki_ingester_samples_per_chunk_sum Unknown ins, instance, ip, job, cls N/A loki_ingester_sent_chunks unknown ins, instance, ip, job, cls The total number of chunks sent by this ingester whilst leaving. loki_ingester_shutdown_marker gauge ins, instance, ip, job, cls 1 if prepare shutdown has been called, 0 otherwise loki_ingester_streams_created_total Unknown ins, instance, ip, job, cls, tenant N/A loki_ingester_streams_removed_total Unknown ins, instance, ip, job, cls, tenant N/A loki_ingester_wal_bytes_in_use gauge ins, instance, ip, job, cls Total number of bytes in use by the WAL recovery process. loki_ingester_wal_disk_full_failures_total Unknown ins, instance, ip, job, cls N/A loki_ingester_wal_duplicate_entries_total Unknown ins, instance, ip, job, cls N/A loki_ingester_wal_logged_bytes_total Unknown ins, instance, ip, job, cls N/A loki_ingester_wal_records_logged_total Unknown ins, instance, ip, job, cls N/A loki_ingester_wal_recovered_bytes_total Unknown ins, instance, ip, job, cls N/A loki_ingester_wal_recovered_chunks_total Unknown ins, instance, ip, job, cls N/A loki_ingester_wal_recovered_entries_total Unknown ins, instance, ip, job, cls N/A loki_ingester_wal_recovered_streams_total Unknown ins, instance, ip, job, cls N/A loki_ingester_wal_replay_active gauge ins, instance, ip, job, cls Whether the WAL is replaying loki_ingester_wal_replay_duration_seconds gauge ins, instance, ip, job, cls Time taken to replay the checkpoint and the WAL. loki_ingester_wal_replay_flushing gauge ins, instance, ip, job, cls Whether the wal replay is in a flushing phase due to backpressure loki_internal_log_messages_total Unknown ins, instance, ip, level, job, cls N/A loki_kv_request_duration_seconds_bucket Unknown ins, instance, role, ip, le, kv_name, type, operation, job, cls, status_code N/A loki_kv_request_duration_seconds_count Unknown ins, instance, role, ip, kv_name, type, operation, job, cls, status_code N/A loki_kv_request_duration_seconds_sum Unknown ins, instance, role, ip, kv_name, type, operation, job, cls, status_code N/A loki_log_flushes_bucket Unknown ins, instance, ip, le, job, cls N/A loki_log_flushes_count Unknown ins, instance, ip, job, cls N/A loki_log_flushes_sum Unknown ins, instance, ip, job, cls N/A loki_log_messages_total Unknown ins, instance, ip, level, job, cls N/A loki_logql_querystats_bytes_processed_per_seconds_bucket Unknown ins, instance, range, ip, le, sharded, type, job, cls, status_code, latency_type N/A loki_logql_querystats_bytes_processed_per_seconds_count Unknown ins, instance, range, ip, sharded, type, job, cls, status_code, latency_type N/A loki_logql_querystats_bytes_processed_per_seconds_sum Unknown ins, instance, range, ip, sharded, type, job, cls, status_code, latency_type N/A loki_logql_querystats_chunk_download_latency_seconds_bucket Unknown ins, instance, range, ip, le, type, job, cls, status_code N/A loki_logql_querystats_chunk_download_latency_seconds_count Unknown ins, instance, range, ip, type, job, cls, status_code N/A loki_logql_querystats_chunk_download_latency_seconds_sum Unknown ins, instance, range, ip, type, job, cls, status_code N/A loki_logql_querystats_downloaded_chunk_total Unknown ins, instance, range, ip, type, job, cls, status_code N/A loki_logql_querystats_duplicates_total Unknown ins, instance, ip, job, cls N/A loki_logql_querystats_ingester_sent_lines_total Unknown ins, instance, ip, job, cls N/A loki_logql_querystats_latency_seconds_bucket Unknown ins, instance, range, ip, le, type, job, cls, status_code N/A loki_logql_querystats_latency_seconds_count Unknown ins, instance, range, ip, type, job, cls, status_code N/A loki_logql_querystats_latency_seconds_sum Unknown ins, instance, range, ip, type, job, cls, status_code N/A loki_panic_total Unknown ins, instance, ip, job, cls N/A loki_querier_index_cache_corruptions_total Unknown ins, instance, ip, job, cls N/A loki_querier_index_cache_encode_errors_total Unknown ins, instance, ip, job, cls N/A loki_querier_index_cache_gets_total Unknown ins, instance, ip, job, cls N/A loki_querier_index_cache_hits_total Unknown ins, instance, ip, job, cls N/A loki_querier_index_cache_puts_total Unknown ins, instance, ip, job, cls N/A loki_querier_query_frontend_clients gauge ins, instance, ip, job, cls The current number of clients connected to query-frontend. loki_querier_query_frontend_request_duration_seconds_bucket Unknown ins, instance, ip, le, operation, job, cls, status_code N/A loki_querier_query_frontend_request_duration_seconds_count Unknown ins, instance, ip, operation, job, cls, status_code N/A loki_querier_query_frontend_request_duration_seconds_sum Unknown ins, instance, ip, operation, job, cls, status_code N/A loki_querier_tail_active gauge ins, instance, ip, job, cls Number of active tailers loki_querier_tail_active_streams gauge ins, instance, ip, job, cls Number of active streams being tailed loki_querier_tail_bytes_total Unknown ins, instance, ip, job, cls N/A loki_querier_worker_concurrency gauge ins, instance, ip, job, cls Number of concurrent querier workers loki_querier_worker_inflight_queries gauge ins, instance, ip, job, cls Number of queries being processed by the querier workers loki_query_frontend_log_result_cache_hit_total Unknown ins, instance, ip, job, cls N/A loki_query_frontend_log_result_cache_miss_total Unknown ins, instance, ip, job, cls N/A loki_query_frontend_partitions_bucket Unknown ins, instance, ip, le, job, cls N/A loki_query_frontend_partitions_count Unknown ins, instance, ip, job, cls N/A loki_query_frontend_partitions_sum Unknown ins, instance, ip, job, cls N/A loki_query_frontend_shard_factor_bucket Unknown ins, instance, ip, le, mapper, job, cls N/A loki_query_frontend_shard_factor_count Unknown ins, instance, ip, mapper, job, cls N/A loki_query_frontend_shard_factor_sum Unknown ins, instance, ip, mapper, job, cls N/A loki_query_scheduler_enqueue_count Unknown ins, instance, ip, level, user, job, cls N/A loki_rate_store_expired_streams_total Unknown ins, instance, ip, job, cls N/A loki_rate_store_max_stream_rate_bytes gauge ins, instance, ip, job, cls The maximum stream rate for any stream reported by ingesters during a sync operation. Sharded Streams are combined. loki_rate_store_max_stream_shards gauge ins, instance, ip, job, cls The number of shards for a single stream reported by ingesters during a sync operation. loki_rate_store_max_unique_stream_rate_bytes gauge ins, instance, ip, job, cls The maximum stream rate for any stream reported by ingesters during a sync operation. Sharded Streams are considered separate. loki_rate_store_stream_rate_bytes_bucket Unknown ins, instance, ip, le, job, cls N/A loki_rate_store_stream_rate_bytes_count Unknown ins, instance, ip, job, cls N/A loki_rate_store_stream_rate_bytes_sum Unknown ins, instance, ip, job, cls N/A loki_rate_store_stream_shards_bucket Unknown ins, instance, ip, le, job, cls N/A loki_rate_store_stream_shards_count Unknown ins, instance, ip, job, cls N/A loki_rate_store_stream_shards_sum Unknown ins, instance, ip, job, cls N/A loki_rate_store_streams gauge ins, instance, ip, job, cls The number of unique streams reported by all ingesters. Sharded streams are combined loki_request_duration_seconds_bucket Unknown ins, instance, method, ip, le, ws, route, job, cls, status_code N/A loki_request_duration_seconds_count Unknown ins, instance, method, ip, ws, route, job, cls, status_code N/A loki_request_duration_seconds_sum Unknown ins, instance, method, ip, ws, route, job, cls, status_code N/A loki_request_message_bytes_bucket Unknown ins, instance, method, ip, le, route, job, cls N/A loki_request_message_bytes_count Unknown ins, instance, method, ip, route, job, cls N/A loki_request_message_bytes_sum Unknown ins, instance, method, ip, route, job, cls N/A loki_response_message_bytes_bucket Unknown ins, instance, method, ip, le, route, job, cls N/A loki_response_message_bytes_count Unknown ins, instance, method, ip, route, job, cls N/A loki_response_message_bytes_sum Unknown ins, instance, method, ip, route, job, cls N/A loki_results_cache_version_comparisons_total Unknown ins, instance, ip, job, cls N/A loki_store_chunks_downloaded_total Unknown ins, instance, ip, status, job, cls N/A loki_store_chunks_per_batch_bucket Unknown ins, instance, ip, le, status, job, cls N/A loki_store_chunks_per_batch_count Unknown ins, instance, ip, status, job, cls N/A loki_store_chunks_per_batch_sum Unknown ins, instance, ip, status, job, cls N/A loki_store_series_total Unknown ins, instance, ip, status, job, cls N/A loki_stream_sharding_count unknown ins, instance, ip, job, cls Total number of times the distributor has sharded streams loki_tcp_connections gauge ins, instance, ip, protocol, job, cls Current number of accepted TCP connections. loki_tcp_connections_limit gauge ins, instance, ip, protocol, job, cls The max number of TCP connections that can be accepted (0 means no limit). net_conntrack_dialer_conn_attempted_total counter ins, instance, ip, dialer_name, job, cls Total number of connections attempted by the given dialer a given name. net_conntrack_dialer_conn_closed_total counter ins, instance, ip, dialer_name, job, cls Total number of connections closed which originated from the dialer of a given name. net_conntrack_dialer_conn_established_total counter ins, instance, ip, dialer_name, job, cls Total number of connections successfully established by the given dialer a given name. net_conntrack_dialer_conn_failed_total counter ins, instance, ip, dialer_name, reason, job, cls Total number of connections failed to dial by the dialer a given name. net_conntrack_listener_conn_accepted_total counter ins, instance, ip, listener_name, job, cls Total number of connections opened to the listener of a given name. net_conntrack_listener_conn_closed_total counter ins, instance, ip, listener_name, job, cls Total number of connections closed that were made to the listener of a given name. nginx_connections_accepted counter ins, instance, ip, job, cls Accepted client connections nginx_connections_active gauge ins, instance, ip, job, cls Active client connections nginx_connections_handled counter ins, instance, ip, job, cls Handled client connections nginx_connections_reading gauge ins, instance, ip, job, cls Connections where NGINX is reading the request header nginx_connections_waiting gauge ins, instance, ip, job, cls Idle client connections nginx_connections_writing gauge ins, instance, ip, job, cls Connections where NGINX is writing the response back to the client nginx_exporter_build_info gauge revision, version, ins, instance, ip, tags, goarch, goversion, job, cls, branch, goos A metric with a constant ‘1’ value labeled by version, revision, branch, goversion from which nginx_exporter was built, and the goos and goarch for the build. nginx_http_requests_total counter ins, instance, ip, job, cls Total http requests nginx_up gauge ins, instance, ip, job, cls Status of the last metric scrape plugins_active_instances gauge ins, instance, ip, job, cls The number of active plugin instances plugins_datasource_instances_total Unknown ins, instance, ip, job, cls N/A process_cpu_seconds_total counter ins, instance, ip, job, cls Total user and system CPU time spent in seconds. process_max_fds gauge ins, instance, ip, job, cls Maximum number of open file descriptors. process_open_fds gauge ins, instance, ip, job, cls Number of open file descriptors. process_resident_memory_bytes gauge ins, instance, ip, job, cls Resident memory size in bytes. process_start_time_seconds gauge ins, instance, ip, job, cls Start time of the process since unix epoch in seconds. process_virtual_memory_bytes gauge ins, instance, ip, job, cls Virtual memory size in bytes. process_virtual_memory_max_bytes gauge ins, instance, ip, job, cls Maximum amount of virtual memory available in bytes. prometheus_api_remote_read_queries gauge ins, instance, ip, job, cls The current number of remote read queries being executed or waiting. prometheus_build_info gauge revision, version, ins, instance, ip, tags, goarch, goversion, job, cls, branch, goos A metric with a constant ‘1’ value labeled by version, revision, branch, goversion from which prometheus was built, and the goos and goarch for the build. prometheus_config_last_reload_success_timestamp_seconds gauge ins, instance, ip, job, cls Timestamp of the last successful configuration reload. prometheus_config_last_reload_successful gauge ins, instance, ip, job, cls Whether the last configuration reload attempt was successful. prometheus_engine_queries gauge ins, instance, ip, job, cls The current number of queries being executed or waiting. prometheus_engine_queries_concurrent_max gauge ins, instance, ip, job, cls The max number of concurrent queries. prometheus_engine_query_duration_seconds summary ins, instance, ip, job, cls, quantile, slice Query timings prometheus_engine_query_duration_seconds_count Unknown ins, instance, ip, job, cls, slice N/A prometheus_engine_query_duration_seconds_sum Unknown ins, instance, ip, job, cls, slice N/A prometheus_engine_query_log_enabled gauge ins, instance, ip, job, cls State of the query log. prometheus_engine_query_log_failures_total counter ins, instance, ip, job, cls The number of query log failures. prometheus_engine_query_samples_total counter ins, instance, ip, job, cls The total number of samples loaded by all queries. prometheus_http_request_duration_seconds_bucket Unknown ins, instance, ip, le, job, cls, handler N/A prometheus_http_request_duration_seconds_count Unknown ins, instance, ip, job, cls, handler N/A prometheus_http_request_duration_seconds_sum Unknown ins, instance, ip, job, cls, handler N/A prometheus_http_requests_total counter ins, instance, ip, job, cls, code, handler Counter of HTTP requests. prometheus_http_response_size_bytes_bucket Unknown ins, instance, ip, le, job, cls, handler N/A prometheus_http_response_size_bytes_count Unknown ins, instance, ip, job, cls, handler N/A prometheus_http_response_size_bytes_sum Unknown ins, instance, ip, job, cls, handler N/A prometheus_notifications_alertmanagers_discovered gauge ins, instance, ip, job, cls The number of alertmanagers discovered and active. prometheus_notifications_dropped_total counter ins, instance, ip, job, cls Total number of alerts dropped due to errors when sending to Alertmanager. prometheus_notifications_errors_total counter ins, instance, ip, alertmanager, job, cls Total number of errors sending alert notifications. prometheus_notifications_latency_seconds summary ins, instance, ip, alertmanager, job, cls, quantile Latency quantiles for sending alert notifications. prometheus_notifications_latency_seconds_count Unknown ins, instance, ip, alertmanager, job, cls N/A prometheus_notifications_latency_seconds_sum Unknown ins, instance, ip, alertmanager, job, cls N/A prometheus_notifications_queue_capacity gauge ins, instance, ip, job, cls The capacity of the alert notifications queue. prometheus_notifications_queue_length gauge ins, instance, ip, job, cls The number of alert notifications in the queue. prometheus_notifications_sent_total counter ins, instance, ip, alertmanager, job, cls Total number of alerts sent. prometheus_ready gauge ins, instance, ip, job, cls Whether Prometheus startup was fully completed and the server is ready for normal operation. prometheus_remote_storage_exemplars_in_total counter ins, instance, ip, job, cls Exemplars in to remote storage, compare to exemplars out for queue managers. prometheus_remote_storage_highest_timestamp_in_seconds gauge ins, instance, ip, job, cls Highest timestamp that has come into the remote storage via the Appender interface, in seconds since epoch. prometheus_remote_storage_histograms_in_total counter ins, instance, ip, job, cls HistogramSamples in to remote storage, compare to histograms out for queue managers. prometheus_remote_storage_samples_in_total counter ins, instance, ip, job, cls Samples in to remote storage, compare to samples out for queue managers. prometheus_remote_storage_string_interner_zero_reference_releases_total counter ins, instance, ip, job, cls The number of times release has been called for strings that are not interned. prometheus_rule_evaluation_duration_seconds summary ins, instance, ip, job, cls, quantile The duration for a rule to execute. prometheus_rule_evaluation_duration_seconds_count Unknown ins, instance, ip, job, cls N/A prometheus_rule_evaluation_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A prometheus_rule_evaluation_failures_total counter ins, instance, ip, job, cls, rule_group The total number of rule evaluation failures. prometheus_rule_evaluations_total counter ins, instance, ip, job, cls, rule_group The total number of rule evaluations. prometheus_rule_group_duration_seconds summary ins, instance, ip, job, cls, quantile The duration of rule group evaluations. prometheus_rule_group_duration_seconds_count Unknown ins, instance, ip, job, cls N/A prometheus_rule_group_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A prometheus_rule_group_interval_seconds gauge ins, instance, ip, job, cls, rule_group The interval of a rule group. prometheus_rule_group_iterations_missed_total counter ins, instance, ip, job, cls, rule_group The total number of rule group evaluations missed due to slow rule group evaluation. prometheus_rule_group_iterations_total counter ins, instance, ip, job, cls, rule_group The total number of scheduled rule group evaluations, whether executed or missed. prometheus_rule_group_last_duration_seconds gauge ins, instance, ip, job, cls, rule_group The duration of the last rule group evaluation. prometheus_rule_group_last_evaluation_samples gauge ins, instance, ip, job, cls, rule_group The number of samples returned during the last rule group evaluation. prometheus_rule_group_last_evaluation_timestamp_seconds gauge ins, instance, ip, job, cls, rule_group The timestamp of the last rule group evaluation in seconds. prometheus_rule_group_rules gauge ins, instance, ip, job, cls, rule_group The number of rules. prometheus_sd_azure_cache_hit_total counter ins, instance, ip, job, cls Number of cache hit during refresh. prometheus_sd_azure_failures_total counter ins, instance, ip, job, cls Number of Azure service discovery refresh failures. prometheus_sd_consul_rpc_duration_seconds summary endpoint, ins, instance, ip, job, cls, call, quantile The duration of a Consul RPC call in seconds. prometheus_sd_consul_rpc_duration_seconds_count Unknown endpoint, ins, instance, ip, job, cls, call N/A prometheus_sd_consul_rpc_duration_seconds_sum Unknown endpoint, ins, instance, ip, job, cls, call N/A prometheus_sd_consul_rpc_failures_total counter ins, instance, ip, job, cls The number of Consul RPC call failures. prometheus_sd_discovered_targets gauge ins, instance, ip, config, job, cls Current number of discovered targets. prometheus_sd_dns_lookup_failures_total counter ins, instance, ip, job, cls The number of DNS-SD lookup failures. prometheus_sd_dns_lookups_total counter ins, instance, ip, job, cls The number of DNS-SD lookups. prometheus_sd_failed_configs gauge ins, instance, ip, job, cls Current number of service discovery configurations that failed to load. prometheus_sd_file_mtime_seconds gauge ins, instance, ip, filename, job, cls Timestamp (mtime) of files read by FileSD. Timestamp is set at read time. prometheus_sd_file_read_errors_total counter ins, instance, ip, job, cls The number of File-SD read errors. prometheus_sd_file_scan_duration_seconds summary ins, instance, ip, job, cls, quantile The duration of the File-SD scan in seconds. prometheus_sd_file_scan_duration_seconds_count Unknown ins, instance, ip, job, cls N/A prometheus_sd_file_scan_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A prometheus_sd_file_watcher_errors_total counter ins, instance, ip, job, cls The number of File-SD errors caused by filesystem watch failures. prometheus_sd_http_failures_total counter ins, instance, ip, job, cls Number of HTTP service discovery refresh failures. prometheus_sd_kubernetes_events_total counter event, ins, instance, role, ip, job, cls The number of Kubernetes events handled. prometheus_sd_kuma_fetch_duration_seconds summary ins, instance, ip, job, cls, quantile The duration of a Kuma MADS fetch call. prometheus_sd_kuma_fetch_duration_seconds_count Unknown ins, instance, ip, job, cls N/A prometheus_sd_kuma_fetch_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A prometheus_sd_kuma_fetch_failures_total counter ins, instance, ip, job, cls The number of Kuma MADS fetch call failures. prometheus_sd_kuma_fetch_skipped_updates_total counter ins, instance, ip, job, cls The number of Kuma MADS fetch calls that result in no updates to the targets. prometheus_sd_linode_failures_total counter ins, instance, ip, job, cls Number of Linode service discovery refresh failures. prometheus_sd_nomad_failures_total counter ins, instance, ip, job, cls Number of nomad service discovery refresh failures. prometheus_sd_received_updates_total counter ins, instance, ip, job, cls Total number of update events received from the SD providers. prometheus_sd_updates_total counter ins, instance, ip, job, cls Total number of update events sent to the SD consumers. prometheus_target_interval_length_seconds summary ins, instance, interval, ip, job, cls, quantile Actual intervals between scrapes. prometheus_target_interval_length_seconds_count Unknown ins, instance, interval, ip, job, cls N/A prometheus_target_interval_length_seconds_sum Unknown ins, instance, interval, ip, job, cls N/A prometheus_target_metadata_cache_bytes gauge ins, instance, ip, scrape_job, job, cls The number of bytes that are currently used for storing metric metadata in the cache prometheus_target_metadata_cache_entries gauge ins, instance, ip, scrape_job, job, cls Total number of metric metadata entries in the cache prometheus_target_scrape_pool_exceeded_label_limits_total counter ins, instance, ip, job, cls Total number of times scrape pools hit the label limits, during sync or config reload. prometheus_target_scrape_pool_exceeded_target_limit_total counter ins, instance, ip, job, cls Total number of times scrape pools hit the target limit, during sync or config reload. prometheus_target_scrape_pool_reloads_failed_total counter ins, instance, ip, job, cls Total number of failed scrape pool reloads. prometheus_target_scrape_pool_reloads_total counter ins, instance, ip, job, cls Total number of scrape pool reloads. prometheus_target_scrape_pool_sync_total counter ins, instance, ip, scrape_job, job, cls Total number of syncs that were executed on a scrape pool. prometheus_target_scrape_pool_target_limit gauge ins, instance, ip, scrape_job, job, cls Maximum number of targets allowed in this scrape pool. prometheus_target_scrape_pool_targets gauge ins, instance, ip, scrape_job, job, cls Current number of targets in this scrape pool. prometheus_target_scrape_pools_failed_total counter ins, instance, ip, job, cls Total number of scrape pool creations that failed. prometheus_target_scrape_pools_total counter ins, instance, ip, job, cls Total number of scrape pool creation attempts. prometheus_target_scrapes_cache_flush_forced_total counter ins, instance, ip, job, cls How many times a scrape cache was flushed due to getting big while scrapes are failing. prometheus_target_scrapes_exceeded_body_size_limit_total counter ins, instance, ip, job, cls Total number of scrapes that hit the body size limit prometheus_target_scrapes_exceeded_native_histogram_bucket_limit_total counter ins, instance, ip, job, cls Total number of scrapes that hit the native histogram bucket limit and were rejected. prometheus_target_scrapes_exceeded_sample_limit_total counter ins, instance, ip, job, cls Total number of scrapes that hit the sample limit and were rejected. prometheus_target_scrapes_exemplar_out_of_order_total counter ins, instance, ip, job, cls Total number of exemplar rejected due to not being out of the expected order. prometheus_target_scrapes_sample_duplicate_timestamp_total counter ins, instance, ip, job, cls Total number of samples rejected due to duplicate timestamps but different values. prometheus_target_scrapes_sample_out_of_bounds_total counter ins, instance, ip, job, cls Total number of samples rejected due to timestamp falling outside of the time bounds. prometheus_target_scrapes_sample_out_of_order_total counter ins, instance, ip, job, cls Total number of samples rejected due to not being out of the expected order. prometheus_target_sync_failed_total counter ins, instance, ip, scrape_job, job, cls Total number of target sync failures. prometheus_target_sync_length_seconds summary ins, instance, ip, scrape_job, job, cls, quantile Actual interval to sync the scrape pool. prometheus_target_sync_length_seconds_count Unknown ins, instance, ip, scrape_job, job, cls N/A prometheus_target_sync_length_seconds_sum Unknown ins, instance, ip, scrape_job, job, cls N/A prometheus_template_text_expansion_failures_total counter ins, instance, ip, job, cls The total number of template text expansion failures. prometheus_template_text_expansions_total counter ins, instance, ip, job, cls The total number of template text expansions. prometheus_treecache_watcher_goroutines gauge ins, instance, ip, job, cls The current number of watcher goroutines. prometheus_treecache_zookeeper_failures_total counter ins, instance, ip, job, cls The total number of ZooKeeper failures. prometheus_tsdb_blocks_loaded gauge ins, instance, ip, job, cls Number of currently loaded data blocks prometheus_tsdb_checkpoint_creations_failed_total counter ins, instance, ip, job, cls Total number of checkpoint creations that failed. prometheus_tsdb_checkpoint_creations_total counter ins, instance, ip, job, cls Total number of checkpoint creations attempted. prometheus_tsdb_checkpoint_deletions_failed_total counter ins, instance, ip, job, cls Total number of checkpoint deletions that failed. prometheus_tsdb_checkpoint_deletions_total counter ins, instance, ip, job, cls Total number of checkpoint deletions attempted. prometheus_tsdb_clean_start gauge ins, instance, ip, job, cls -1: lockfile is disabled. 0: a lockfile from a previous execution was replaced. 1: lockfile creation was clean prometheus_tsdb_compaction_chunk_range_seconds_bucket Unknown ins, instance, ip, le, job, cls N/A prometheus_tsdb_compaction_chunk_range_seconds_count Unknown ins, instance, ip, job, cls N/A prometheus_tsdb_compaction_chunk_range_seconds_sum Unknown ins, instance, ip, job, cls N/A prometheus_tsdb_compaction_chunk_samples_bucket Unknown ins, instance, ip, le, job, cls N/A prometheus_tsdb_compaction_chunk_samples_count Unknown ins, instance, ip, job, cls N/A prometheus_tsdb_compaction_chunk_samples_sum Unknown ins, instance, ip, job, cls N/A prometheus_tsdb_compaction_chunk_size_bytes_bucket Unknown ins, instance, ip, le, job, cls N/A prometheus_tsdb_compaction_chunk_size_bytes_count Unknown ins, instance, ip, job, cls N/A prometheus_tsdb_compaction_chunk_size_bytes_sum Unknown ins, instance, ip, job, cls N/A prometheus_tsdb_compaction_duration_seconds_bucket Unknown ins, instance, ip, le, job, cls N/A prometheus_tsdb_compaction_duration_seconds_count Unknown ins, instance, ip, job, cls N/A prometheus_tsdb_compaction_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A prometheus_tsdb_compaction_populating_block gauge ins, instance, ip, job, cls Set to 1 when a block is currently being written to the disk. prometheus_tsdb_compactions_failed_total counter ins, instance, ip, job, cls Total number of compactions that failed for the partition. prometheus_tsdb_compactions_skipped_total counter ins, instance, ip, job, cls Total number of skipped compactions due to disabled auto compaction. prometheus_tsdb_compactions_total counter ins, instance, ip, job, cls Total number of compactions that were executed for the partition. prometheus_tsdb_compactions_triggered_total counter ins, instance, ip, job, cls Total number of triggered compactions for the partition. prometheus_tsdb_data_replay_duration_seconds gauge ins, instance, ip, job, cls Time taken to replay the data on disk. prometheus_tsdb_exemplar_exemplars_appended_total counter ins, instance, ip, job, cls Total number of appended exemplars. prometheus_tsdb_exemplar_exemplars_in_storage gauge ins, instance, ip, job, cls Number of exemplars currently in circular storage. prometheus_tsdb_exemplar_last_exemplars_timestamp_seconds gauge ins, instance, ip, job, cls The timestamp of the oldest exemplar stored in circular storage. Useful to check for what timerange the current exemplar buffer limit allows. This usually means the last timestampfor all exemplars for a typical setup. This is not true though if one of the series timestamp is in future compared to rest series. prometheus_tsdb_exemplar_max_exemplars gauge ins, instance, ip, job, cls Total number of exemplars the exemplar storage can store, resizeable. prometheus_tsdb_exemplar_out_of_order_exemplars_total counter ins, instance, ip, job, cls Total number of out of order exemplar ingestion failed attempts. prometheus_tsdb_exemplar_series_with_exemplars_in_storage gauge ins, instance, ip, job, cls Number of series with exemplars currently in circular storage. prometheus_tsdb_head_active_appenders gauge ins, instance, ip, job, cls Number of currently active appender transactions prometheus_tsdb_head_chunks gauge ins, instance, ip, job, cls Total number of chunks in the head block. prometheus_tsdb_head_chunks_created_total counter ins, instance, ip, job, cls Total number of chunks created in the head prometheus_tsdb_head_chunks_removed_total counter ins, instance, ip, job, cls Total number of chunks removed in the head prometheus_tsdb_head_chunks_storage_size_bytes gauge ins, instance, ip, job, cls Size of the chunks_head directory. prometheus_tsdb_head_gc_duration_seconds_count Unknown ins, instance, ip, job, cls N/A prometheus_tsdb_head_gc_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A prometheus_tsdb_head_max_time gauge ins, instance, ip, job, cls Maximum timestamp of the head block. The unit is decided by the library consumer. prometheus_tsdb_head_max_time_seconds gauge ins, instance, ip, job, cls Maximum timestamp of the head block. prometheus_tsdb_head_min_time gauge ins, instance, ip, job, cls Minimum time bound of the head block. The unit is decided by the library consumer. prometheus_tsdb_head_min_time_seconds gauge ins, instance, ip, job, cls Minimum time bound of the head block. prometheus_tsdb_head_out_of_order_samples_appended_total counter ins, instance, ip, job, cls Total number of appended out of order samples. prometheus_tsdb_head_samples_appended_total counter ins, instance, ip, type, job, cls Total number of appended samples. prometheus_tsdb_head_series gauge ins, instance, ip, job, cls Total number of series in the head block. prometheus_tsdb_head_series_created_total counter ins, instance, ip, job, cls Total number of series created in the head prometheus_tsdb_head_series_not_found_total counter ins, instance, ip, job, cls Total number of requests for series that were not found. prometheus_tsdb_head_series_removed_total counter ins, instance, ip, job, cls Total number of series removed in the head prometheus_tsdb_head_truncations_failed_total counter ins, instance, ip, job, cls Total number of head truncations that failed. prometheus_tsdb_head_truncations_total counter ins, instance, ip, job, cls Total number of head truncations attempted. prometheus_tsdb_isolation_high_watermark gauge ins, instance, ip, job, cls The highest TSDB append ID that has been given out. prometheus_tsdb_isolation_low_watermark gauge ins, instance, ip, job, cls The lowest TSDB append ID that is still referenced. prometheus_tsdb_lowest_timestamp gauge ins, instance, ip, job, cls Lowest timestamp value stored in the database. The unit is decided by the library consumer. prometheus_tsdb_lowest_timestamp_seconds gauge ins, instance, ip, job, cls Lowest timestamp value stored in the database. prometheus_tsdb_mmap_chunk_corruptions_total counter ins, instance, ip, job, cls Total number of memory-mapped chunk corruptions. prometheus_tsdb_mmap_chunks_total counter ins, instance, ip, job, cls Total number of chunks that were memory-mapped. prometheus_tsdb_out_of_bound_samples_total counter ins, instance, ip, type, job, cls Total number of out of bound samples ingestion failed attempts with out of order support disabled. prometheus_tsdb_out_of_order_samples_total counter ins, instance, ip, type, job, cls Total number of out of order samples ingestion failed attempts due to out of order being disabled. prometheus_tsdb_reloads_failures_total counter ins, instance, ip, job, cls Number of times the database failed to reloadBlocks block data from disk. prometheus_tsdb_reloads_total counter ins, instance, ip, job, cls Number of times the database reloaded block data from disk. prometheus_tsdb_retention_limit_bytes gauge ins, instance, ip, job, cls Max number of bytes to be retained in the tsdb blocks, configured 0 means disabled prometheus_tsdb_retention_limit_seconds gauge ins, instance, ip, job, cls How long to retain samples in storage. prometheus_tsdb_size_retentions_total counter ins, instance, ip, job, cls The number of times that blocks were deleted because the maximum number of bytes was exceeded. prometheus_tsdb_snapshot_replay_error_total counter ins, instance, ip, job, cls Total number snapshot replays that failed. prometheus_tsdb_storage_blocks_bytes gauge ins, instance, ip, job, cls The number of bytes that are currently used for local storage by all blocks. prometheus_tsdb_symbol_table_size_bytes gauge ins, instance, ip, job, cls Size of symbol table in memory for loaded blocks prometheus_tsdb_time_retentions_total counter ins, instance, ip, job, cls The number of times that blocks were deleted because the maximum time limit was exceeded. prometheus_tsdb_tombstone_cleanup_seconds_bucket Unknown ins, instance, ip, le, job, cls N/A prometheus_tsdb_tombstone_cleanup_seconds_count Unknown ins, instance, ip, job, cls N/A prometheus_tsdb_tombstone_cleanup_seconds_sum Unknown ins, instance, ip, job, cls N/A prometheus_tsdb_too_old_samples_total counter ins, instance, ip, type, job, cls Total number of out of order samples ingestion failed attempts with out of support enabled, but sample outside of time window. prometheus_tsdb_vertical_compactions_total counter ins, instance, ip, job, cls Total number of compactions done on overlapping blocks. prometheus_tsdb_wal_completed_pages_total counter ins, instance, ip, job, cls Total number of completed pages. prometheus_tsdb_wal_corruptions_total counter ins, instance, ip, job, cls Total number of WAL corruptions. prometheus_tsdb_wal_fsync_duration_seconds summary ins, instance, ip, job, cls, quantile Duration of write log fsync. prometheus_tsdb_wal_fsync_duration_seconds_count Unknown ins, instance, ip, job, cls N/A prometheus_tsdb_wal_fsync_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A prometheus_tsdb_wal_page_flushes_total counter ins, instance, ip, job, cls Total number of page flushes. prometheus_tsdb_wal_segment_current gauge ins, instance, ip, job, cls Write log segment index that TSDB is currently writing to. prometheus_tsdb_wal_storage_size_bytes gauge ins, instance, ip, job, cls Size of the write log directory. prometheus_tsdb_wal_truncate_duration_seconds_count Unknown ins, instance, ip, job, cls N/A prometheus_tsdb_wal_truncate_duration_seconds_sum Unknown ins, instance, ip, job, cls N/A prometheus_tsdb_wal_truncations_failed_total counter ins, instance, ip, job, cls Total number of write log truncations that failed. prometheus_tsdb_wal_truncations_total counter ins, instance, ip, job, cls Total number of write log truncations attempted. prometheus_tsdb_wal_writes_failed_total counter ins, instance, ip, job, cls Total number of write log writes that failed. prometheus_web_federation_errors_total counter ins, instance, ip, job, cls Total number of errors that occurred while sending federation responses. prometheus_web_federation_warnings_total counter ins, instance, ip, job, cls Total number of warnings that occurred while sending federation responses. promhttp_metric_handler_requests_in_flight gauge ins, instance, ip, job, cls Current number of scrapes being served. promhttp_metric_handler_requests_total counter ins, instance, ip, job, cls, code Total number of scrapes by HTTP status code. pushgateway_build_info gauge revision, version, ins, instance, ip, tags, goarch, goversion, job, cls, branch, goos A metric with a constant ‘1’ value labeled by version, revision, branch, goversion from which pushgateway was built, and the goos and goarch for the build. pushgateway_http_requests_total counter ins, instance, method, ip, job, cls, code, handler Total HTTP requests processed by the Pushgateway, excluding scrapes. querier_cache_added_new_total Unknown ins, instance, ip, job, cache, cls N/A querier_cache_added_total Unknown ins, instance, ip, job, cache, cls N/A querier_cache_entries gauge ins, instance, ip, job, cache, cls The total number of entries querier_cache_evicted_total Unknown ins, instance, ip, job, reason, cache, cls N/A querier_cache_gets_total Unknown ins, instance, ip, job, cache, cls N/A querier_cache_memory_bytes gauge ins, instance, ip, job, cache, cls The current cache size in bytes querier_cache_misses_total Unknown ins, instance, ip, job, cache, cls N/A querier_cache_stale_gets_total Unknown ins, instance, ip, job, cache, cls N/A ring_member_heartbeats_total Unknown ins, instance, ip, job, cls N/A ring_member_tokens_owned gauge ins, instance, ip, job, cls The number of tokens owned in the ring. ring_member_tokens_to_own gauge ins, instance, ip, job, cls The number of tokens to own in the ring. scrape_duration_seconds Unknown ins, instance, ip, job, cls N/A scrape_samples_post_metric_relabeling Unknown ins, instance, ip, job, cls N/A scrape_samples_scraped Unknown ins, instance, ip, job, cls N/A scrape_series_added Unknown ins, instance, ip, job, cls N/A up Unknown ins, instance, ip, job, cls N/A PING Metrics PING job has 54 metrics, provided by blackbox_exporter.\nMetric Name Type Labels Description agent_up Unknown ins, ip, job, instance, cls N/A probe_dns_lookup_time_seconds gauge ins, ip, job, instance, cls Returns the time taken for probe dns lookup in seconds probe_duration_seconds gauge ins, ip, job, instance, cls Returns how long the probe took to complete in seconds probe_icmp_duration_seconds gauge ins, ip, job, phase, instance, cls Duration of icmp request by phase probe_icmp_reply_hop_limit gauge ins, ip, job, instance, cls Replied packet hop limit (TTL for ipv4) probe_ip_addr_hash gauge ins, ip, job, instance, cls Specifies the hash of IP address. It’s useful to detect if the IP address changes. probe_ip_protocol gauge ins, ip, job, instance, cls Specifies whether probe ip protocol is IP4 or IP6 probe_success gauge ins, ip, job, instance, cls Displays whether or not the probe was a success scrape_duration_seconds Unknown ins, ip, job, instance, cls N/A scrape_samples_post_metric_relabeling Unknown ins, ip, job, instance, cls N/A scrape_samples_scraped Unknown ins, ip, job, instance, cls N/A scrape_series_added Unknown ins, ip, job, instance, cls N/A up Unknown ins, ip, job, instance, cls N/A PUSH Metrics PushGateway provides 44 metrics.\nMetric Name Type Labels Description agent_up Unknown job, cls, instance, ins, ip N/A go_gc_duration_seconds summary job, cls, instance, ins, quantile, ip A summary of the pause duration of garbage collection cycles. go_gc_duration_seconds_count Unknown job, cls, instance, ins, ip N/A go_gc_duration_seconds_sum Unknown job, cls, instance, ins, ip N/A go_goroutines gauge job, cls, instance, ins, ip Number of goroutines that currently exist. go_info gauge job, cls, instance, ins, ip, version Information about the Go environment. go_memstats_alloc_bytes counter job, cls, instance, ins, ip Total number of bytes allocated, even if freed. go_memstats_alloc_bytes_total counter job, cls, instance, ins, ip Total number of bytes allocated, even if freed. go_memstats_buck_hash_sys_bytes gauge job, cls, instance, ins, ip Number of bytes used by the profiling bucket hash table. go_memstats_frees_total counter job, cls, instance, ins, ip Total number of frees. go_memstats_gc_sys_bytes gauge job, cls, instance, ins, ip Number of bytes used for garbage collection system metadata. go_memstats_heap_alloc_bytes gauge job, cls, instance, ins, ip Number of heap bytes allocated and still in use. go_memstats_heap_idle_bytes gauge job, cls, instance, ins, ip Number of heap bytes waiting to be used. go_memstats_heap_inuse_bytes gauge job, cls, instance, ins, ip Number of heap bytes that are in use. go_memstats_heap_objects gauge job, cls, instance, ins, ip Number of allocated objects. go_memstats_heap_released_bytes gauge job, cls, instance, ins, ip Number of heap bytes released to OS. go_memstats_heap_sys_bytes gauge job, cls, instance, ins, ip Number of heap bytes obtained from system. go_memstats_last_gc_time_seconds gauge job, cls, instance, ins, ip Number of seconds since 1970 of last garbage collection. go_memstats_lookups_total counter job, cls, instance, ins, ip Total number of pointer lookups. go_memstats_mallocs_total counter job, cls, instance, ins, ip Total number of mallocs. go_memstats_mcache_inuse_bytes gauge job, cls, instance, ins, ip Number of bytes in use by mcache structures. go_memstats_mcache_sys_bytes gauge job, cls, instance, ins, ip Number of bytes used for mcache structures obtained from system. go_memstats_mspan_inuse_bytes gauge job, cls, instance, ins, ip Number of bytes in use by mspan structures. go_memstats_mspan_sys_bytes gauge job, cls, instance, ins, ip Number of bytes used for mspan structures obtained from system. go_memstats_next_gc_bytes gauge job, cls, instance, ins, ip Number of heap bytes when next garbage collection will take place. go_memstats_other_sys_bytes gauge job, cls, instance, ins, ip Number of bytes used for other system allocations. go_memstats_stack_inuse_bytes gauge job, cls, instance, ins, ip Number of bytes in use by the stack allocator. go_memstats_stack_sys_bytes gauge job, cls, instance, ins, ip Number of bytes obtained from system for stack allocator. go_memstats_sys_bytes gauge job, cls, instance, ins, ip Number of bytes obtained from system. go_threads gauge job, cls, instance, ins, ip Number of OS threads created. process_cpu_seconds_total counter job, cls, instance, ins, ip Total user and system CPU time spent in seconds. process_max_fds gauge job, cls, instance, ins, ip Maximum number of open file descriptors. process_open_fds gauge job, cls, instance, ins, ip Number of open file descriptors. process_resident_memory_bytes gauge job, cls, instance, ins, ip Resident memory size in bytes. process_start_time_seconds gauge job, cls, instance, ins, ip Start time of the process since unix epoch in seconds. process_virtual_memory_bytes gauge job, cls, instance, ins, ip Virtual memory size in bytes. process_virtual_memory_max_bytes gauge job, cls, instance, ins, ip Maximum amount of virtual memory available in bytes. pushgateway_build_info gauge job, goversion, cls, branch, instance, tags, revision, goarch, ins, ip, version, goos A metric with a constant ‘1’ value labeled by version, revision, branch, goversion from which pushgateway was built, and the goos and goarch for the build. pushgateway_http_requests_total counter job, cls, method, code, handler, instance, ins, ip Total HTTP requests processed by the Pushgateway, excluding scrapes. scrape_duration_seconds Unknown job, cls, instance, ins, ip N/A scrape_samples_post_metric_relabeling Unknown job, cls, instance, ins, ip N/A scrape_samples_scraped Unknown job, cls, instance, ins, ip N/A scrape_series_added Unknown job, cls, instance, ins, ip N/A up Unknown job, cls, instance, ins, ip N/A ","categories":["Reference"],"description":"Complete list of monitoring metrics provided by the Pigsty INFRA module","excerpt":"Complete list of monitoring metrics provided by the Pigsty INFRA …","ref":"/docs/infra/metric/","tags":"","title":"Metrics"},{"body":" What components are included in the INFRA module? Ansible: Used for automation configuration, deployment, and daily operations. Nginx: Exposes WebUIs like Grafana, VictoriaMetrics (VMUI), Alertmanager, and hosts local YUM/APT repositories. Self-signed CA: Issues SSL/TLS certificates for components like Nginx, Patroni, pgBackRest. VictoriaMetrics Suite: Replaces Prometheus/Loki, including VictoriaMetrics (TSDB), VMAlert (alert evaluation), VictoriaLogs (centralized logs), VictoriaTraces (tracing). Vector: Node-side log collector, pushes system/database logs to VictoriaLogs. AlertManager: Aggregates and dispatches alert notifications. Grafana: Monitoring/visualization platform with numerous preconfigured dashboards and datasources. Chronyd: Provides NTP time synchronization. DNSMasq: Provides DNS registration and resolution. ETCD: Acts as PostgreSQL HA DCS (can also be deployed on dedicated cluster). PostgreSQL: Acts as CMDB on the admin node (optional). Docker: Runs stateless tools or applications on nodes (optional). How to re-register monitoring targets to VictoriaMetrics? VictoriaMetrics uses static service discovery through the /infra/targets/\u003cjob\u003e/*.yml directory. If target files are accidentally deleted, use the following commands to re-register:\n./infra.yml -t infra_register # Re-render infra self-monitoring targets ./node.yml -t node_register # Re-render node / HAProxy / Vector targets ./etcd.yml -t etcd_register # Re-render etcd targets ./minio.yml -t minio_register # Re-render MinIO targets ./pgsql.yml -t pg_register # Re-render PGSQL/Patroni targets ./redis.yml -t redis_register # Re-render Redis targets Other modules (like pg_monitor.yml, mongo.yml, mysql.yml) also provide corresponding *_register tags that can be executed as needed.\nHow to re-register PostgreSQL datasources to Grafana? PGSQL databases defined in pg_databases are registered as Grafana datasources by default (for use by PGCAT applications).\nIf you accidentally delete postgres datasources registered in Grafana, you can register them again using the following command:\n# Register all pgsql databases (defined in pg_databases) as grafana datasources ./pgsql.yml -t register_grafana How to re-register node HAProxy admin pages to Nginx? If you accidentally delete the registered haproxy proxy settings in /etc/nginx/conf.d/haproxy, you can restore them using the following command:\n./node.yml -t register_nginx # Register all haproxy admin page proxy settings to nginx on infra nodes How to restore DNS registration records in DNSMASQ? PGSQL cluster/instance domains are registered by default to /etc/hosts.d/\u003cname\u003e on infra nodes. You can restore them using the following command:\n./pgsql.yml -t pg_dns # Register pg DNS names to dnsmasq on infra nodes How to expose new upstream services via Nginx? Although you can access services directly via IP:Port, we still recommend consolidating access entry points by using domain names and accessing various WebUI services through Nginx proxy. This helps consolidate access, reduce exposed ports, and facilitate access control and auditing.\nIf you want to expose new WebUI services through the Nginx portal, you can add service definitions to the infra_portal parameter. For example, here’s the Infra portal configuration used by Pigsty’s official demo, exposing several additional services:\ninfra_portal: home : { domain: home.pigsty.cc } grafana : { domain: demo.pigsty.io ,endpoint: \"${admin_ip}:3000\" ,websocket: true } prometheus : { domain: p.pigsty.cc ,endpoint: \"${admin_ip}:8428\" } alertmanager : { domain: a.pigsty.cc ,endpoint: \"${admin_ip}:9059\" } blackbox : { endpoint: \"${admin_ip}:9115\" } vmalert : { endpoint: \"${admin_ip}:8880\" } # Additional web portals minio : { domain: sss.pigsty ,endpoint: \"${admin_ip}:9001\" ,scheme: https ,websocket: true } postgrest : { domain: api.pigsty.cc ,endpoint: \"127.0.0.1:8884\" } pgadmin : { domain: adm.pigsty.cc ,endpoint: \"127.0.0.1:8885\" } pgweb : { domain: cli.pigsty.cc ,endpoint: \"127.0.0.1:8886\" } bytebase : { domain: ddl.pigsty.cc ,endpoint: \"127.0.0.1:8887\" } gitea : { domain: git.pigsty.cc ,endpoint: \"127.0.0.1:8889\" } wiki : { domain: wiki.pigsty.cc ,endpoint: \"127.0.0.1:9002\" } noco : { domain: noco.pigsty.cc ,endpoint: \"127.0.0.1:9003\" } supa : { domain: supa.pigsty.cc ,endpoint: \"127.0.0.1:8000\", websocket: true } After completing the Nginx upstream service definition, use the following configuration and commands to register new services to Nginx.\n./infra.yml -t nginx_config # Regenerate Nginx configuration files ./infra.yml -t nginx_launch # Update and apply Nginx configuration # You can also manually reload Nginx config with Ansible ansible infra -b -a 'nginx -s reload' # Reload Nginx config If you want HTTPS access, you must delete files/pki/csr/pigsty.csr and files/pki/nginx/pigsty.{key,crt} to force regeneration of Nginx SSL/TLS certificates to include new upstream domains. If you want to use certificates issued by an authoritative CA instead of Pigsty self-signed CA certificates, you can place them in the /etc/nginx/conf.d/cert/ directory and modify the corresponding configuration: /etc/nginx/conf.d/\u003cname\u003e.conf.\nHow to manually add upstream repo files to nodes? Pigsty has a built-in wrapper script bin/repo-add that calls the ansible playbook node.yml to add repo files to corresponding nodes.\nbin/repo-add \u003cselector\u003e [modules] bin/repo-add 10.10.10.10 # Add node repo for node 10.10.10.10 bin/repo-add infra node,infra # Add node and infra repos for infra group bin/repo-add infra node,local # Add node repo and local pigsty repo for infra group bin/repo-add pg-test node,pgsql # Add node and pgsql repos for pg-test group ","categories":["Reference"],"description":"Frequently asked questions about the Pigsty INFRA infrastructure module","excerpt":"Frequently asked questions about the Pigsty INFRA infrastructure …","ref":"/docs/infra/faq/","tags":"","title":"FAQ"},{"body":"This section covers daily administration and operations for Pigsty deployments.\nCreate INFRA Module Use infra.yml playbook to install INFRA module on infra group:\n./infra.yml # Install INFRA module on infra group Uninstall INFRA Module Use dedicated infra-rm.yml playbook to remove INFRA module from infra group:\n./infra-rm.yml # Remove INFRA module from infra group Manage Local Repository Pigsty includes local yum/apt repo for software packages. Manage repo configuration:\nRepo Variables Variable Description repo_enabled Enable local repo on node repo_upstream Upstream repos to include repo_remove Remove upstream repos if true repo_url_pkg Extra packages to download repo_clean Clean repo cache (makecache) repo_pkg Packages to include Repo Tasks ./infra.yml -t repo # Create or update repo Repo location: /www/pigsty served by Nginx.\nMore: Configuration: INFRA - REPO\n","categories":["Task","Reference"],"description":"Infrastructure components and INFRA cluster administration SOP: create, destroy, scale out, scale in, certificates, repositories...","excerpt":"Infrastructure components and INFRA cluster administration SOP: …","ref":"/docs/infra/admin/","tags":"","title":"Administration"},{"body":"Ansible is installed by default on all INFRA nodes and can be used to manage the entire deployment.\nPigsty implements automation based on Ansible, following the Infrastructure-as-Code philosophy.\nAnsible knowledge is useful for managing databases and infrastructure, but not required. You only need to know how to execute Playbooks - YAML files that define a series of automated tasks.\nInstallation Pigsty automatically installs ansible and its dependencies during the bootstrap process. For manual installation, use the following commands:\n# Debian / Ubuntu sudo apt install -y ansible python3-jmespath # EL 10 sudo dnf install -y ansible python-jmespath # EL 8/9 sudo dnf install -y ansible python3.12-jmespath # EL 7 sudo yum install -y ansible python-jmespath macOS macOS users can install using Homebrew:\nbrew install ansible pip3 install jmespath Basic Usage To run a playbook, simply execute ./path/to/playbook.yml. Here are the most commonly used Ansible command-line parameters:\nPurpose Parameter Description Where -l / --limit \u003cpattern\u003e Limit target hosts/groups/patterns What -t / --tags \u003ctags\u003e Only run tasks with specified tags How -e / --extra-vars \u003cvars\u003e Pass extra command-line variables Config -i / --inventory \u003cpath\u003e Specify inventory file path Limiting Hosts Use -l|--limit \u003cpattern\u003e to limit execution to specific groups, hosts, or patterns:\n./node.yml # Execute on all nodes ./pgsql.yml -l pg-test # Only execute on pg-test cluster ./pgsql.yml -l pg-* # Execute on all clusters starting with pg- ./pgsql.yml -l 10.10.10.10 # Only execute on specific IP host Running playbooks without host limits can be very dangerous! By default, most playbooks execute on all hosts. Use with caution!\nLimiting Tasks Use -t|--tags \u003ctags\u003e to only execute task subsets with specified tags:\n./infra.yml -t repo # Only execute tasks to create local repo ./infra.yml -t repo_upstream # Only execute tasks to add upstream repos ./node.yml -t node_pkg # Only execute tasks to install node packages ./pgsql.yml -t pg_hba # Only execute tasks to render pg_hba.conf Passing Variables Use -e|--extra-vars \u003ckey=value\u003e to override variables at runtime:\n./pgsql.yml -e pg_clean=true # Force clean existing PG instances ./pgsql-rm.yml -e pg_rm_pkg=false # Keep packages when uninstalling ./node.yml -e '{\"node_tune\":\"tiny\"}' # Pass variables in JSON format ./pgsql.yml -e @/path/to/config.yml # Load variables from YAML file Specifying Inventory By default, Ansible uses pigsty.yml in the current directory as the inventory. Use -i|--inventory \u003cpath\u003e to specify a different config file:\n./pgsql.yml -i files/pigsty/full.yml -l pg-test [!NOTE]\nTo permanently change the default config file path, modify the inventory parameter in ansible.cfg.\n","categories":["Task"],"description":"Using Ansible to run administration commands","excerpt":"Using Ansible to run administration commands","ref":"/docs/infra/admin/ansible/","tags":"","title":"Ansible"},{"body":"Pigsty uses idempotent Ansible playbooks for management and control. Running playbooks requires ansible-playbook to be in the system PATH; users must first install Ansible before executing playbooks.\nAvailable Playbooks Module Playbook Purpose INFRA deploy.yml One-click Pigsty installation INFRA infra.yml Initialize Pigsty infrastructure on infra nodes INFRA infra-rm.yml Remove infrastructure components from infra nodes INFRA cache.yml Create offline installation packages from target nodes INFRA cert.yml Issue certificates using Pigsty self-signed CA NODE node.yml Initialize nodes, configure to desired state NODE node-rm.yml Remove nodes from Pigsty PGSQL pgsql.yml Initialize HA PostgreSQL cluster, or add new replica PGSQL pgsql-rm.yml Remove PostgreSQL cluster, or remove replica PGSQL pgsql-db.yml Add new business database to existing cluster PGSQL pgsql-user.yml Add new business user to existing cluster PGSQL pgsql-pitr.yml Perform point-in-time recovery (PITR) on cluster PGSQL pgsql-monitor.yml Monitor remote PostgreSQL using local exporters PGSQL pgsql-migration.yml Generate migration manual and scripts for PostgreSQL PGSQL slim.yml Install Pigsty with minimal components REDIS redis.yml Initialize Redis cluster/node/instance REDIS redis-rm.yml Remove Redis cluster/node/instance ETCD etcd.yml Initialize ETCD cluster, or add new member ETCD etcd-rm.yml Remove ETCD cluster, or remove existing member MINIO minio.yml Initialize MinIO cluster MINIO minio-rm.yml Remove MinIO cluster DOCKER docker.yml Install Docker on nodes DOCKER app.yml Install applications using Docker Compose FERRET mongo.yml Install Mongo/FerretDB on nodes Deployment Strategy The deploy.yml playbook orchestrates specialized playbooks in the following group order for complete deployment:\ninfra: infra.yml (-l infra) nodes: node.yml etcd: etcd.yml (-l etcd) minio: minio.yml (-l minio) pgsql: pgsql.yml Circular Dependency Note: There is a weak circular dependency between NODE and INFRA: to register NODE to INFRA, INFRA must already exist; while INFRA module depends on NODE to work. The solution is to initialize infra nodes first, then add other nodes. To complete all deployment at once, use deploy.yml.\nSafety Notes Most playbooks are idempotent, which means some deployment playbooks may wipe existing databases and create new ones when protection options are not enabled. Use extra caution with pgsql, minio, and infra playbooks. Read the documentation carefully and proceed with caution.\nBest Practices Read playbook documentation carefully before execution Press Ctrl-C immediately to stop when anomalies occur Test in non-production environments first Use -l parameter to limit target hosts, avoiding unintended hosts Use -t parameter to specify tags, executing only specific tasks Dry-Run Mode Use --check --diff options to preview changes without actually executing:\n# Preview changes without execution ./pgsql.yml -l pg-test --check --diff # Check specific tasks with tags ./pgsql.yml -l pg-test -t pg_config --check --diff ","categories":["Task"],"description":"Built-in Ansible playbooks in Pigsty","excerpt":"Built-in Ansible playbooks in Pigsty","ref":"/docs/infra/admin/playbook/","tags":"","title":"Playbooks"},{"body":"Pigsty installs Nginx on INFRA nodes as the entry point for all web services, listening on standard ports 80/443.\nIn Pigsty, you can configure Nginx to provide various services through inventory:\nExpose web interfaces for monitoring components like Grafana, VictoriaMetrics (VMUI), Alertmanager, and VictoriaLogs Serve static files (software repos, documentation sites, websites, etc.) Proxy custom application services (internal apps, database management UIs, Docker application interfaces, etc.) Automatically issue self-signed HTTPS certificates, or use Certbot to obtain free Let’s Encrypt certificates Expose services through a single port using different subdomains for unified access Basic Configuration Customize Nginx behavior via infra_portal parameter:\ninfra_portal: home: { domain: i.pigsty } infra_portal is a dictionary where each key defines a service and the value is the service configuration. Only services with a domain defined will generate corresponding Nginx config files.\nhome: Special default server for homepage and built-in monitoring component reverse proxies Proxy services: Specify upstream service address via endpoint for reverse proxy Static services: Specify local directory via path for static file serving Server Parameters Basic Parameters Parameter Description domain Optional proxy domain endpoint Upstream service address (IP:PORT or socket) path Local directory for static content scheme Protocol type (http/https), default http domains Additional domain list (aliases) SSL/TLS Options Parameter Description certbot Enable Let’s Encrypt cert management, value is cert name cert Custom certificate file path key Custom private key file path enforce_https Force HTTPS redirect (301) Advanced Settings Parameter Description config Custom Nginx config snippet index Enable directory listing (for static) log Custom log file name websocket Enable WebSocket support auth Enable Basic Auth realm Basic Auth prompt message Configuration Examples Reverse Proxy Services grafana: { domain: g.pigsty, endpoint: \"${admin_ip}:3000\", websocket: true } pgadmin: { domain: adm.pigsty, endpoint: \"127.0.0.1:8885\" } Static Files and Directory Listing repo: { domain: repo.pigsty.io, path: \"/www/repo\", index: true } Custom SSL Certificate secure_app: domain: secure.pigsty.io endpoint: \"${admin_ip}:8443\" cert: \"/etc/ssl/certs/custom.crt\" key: \"/etc/ssl/private/custom.key\" Using Let’s Encrypt Certificates grafana: domain: demo.pigsty.io endpoint: \"${admin_ip}:3000\" websocket: true certbot: pigsty.demo # Cert name, multiple domains can share one cert Force HTTPS Redirect web.io: domain: en.pigsty.io path: \"/www/web.io\" certbot: pigsty.doc enforce_https: true Custom Config Snippet web.cc: domain: pigsty.io path: \"/www/web.io\" domains: [ en.pigsty.io ] certbot: pigsty.doc config: | # rewrite /en/ to / location /en/ { rewrite ^/en/(.*)$ /$1 permanent; } Management Commands ./infra.yml -t nginx # Full Nginx reconfiguration ./infra.yml -t nginx_config # Regenerate config files ./infra.yml -t nginx_launch # Restart Nginx service ./infra.yml -t nginx_cert # Regenerate SSL certificates ./infra.yml -t nginx_certbot # Sign certificates with certbot ./infra.yml -t nginx_reload # Reload Nginx configuration Domain Resolution Three ways to resolve domains to Pigsty servers:\nPublic domains: Configure via DNS provider Internal DNS server: Configure internal DNS resolution Local hosts file: Modify /etc/hosts For local development, add to /etc/hosts:\n\u003cyour_public_ip_address\u003e i.pigsty g.pigsty p.pigsty a.pigsty Pigsty includes dnsmasq service, configurable via dns_records parameter for internal DNS resolution.\nHTTPS Configuration Configure HTTPS via nginx_sslmode parameter:\nMode Description disable Listen HTTP only (nginx_port) enable Also listen HTTPS (nginx_ssl_port), default self-signed cert enforce Force redirect to HTTPS, all port 80 requests get 301 redirect For self-signed certificates, several access options:\nTrust the self-signed CA in browser (download at http://\u003cip\u003e/ca.crt) Use browser security bypass (type “thisisunsafe” in Chrome) Configure proper CA-signed certs or Let’s Encrypt for production Certbot Certificates Pigsty supports using Certbot to request free Let’s Encrypt certificates.\nEnable Certbot Add certbot parameter to services in infra_portal, specifying cert name Configure certbot_email with a valid email Set certbot_sign to true for auto-signing during deployment certbot_sign: true certbot_email: your@email.com Manual Certificate Signing ./infra.yml -t nginx_certbot # Sign Let's Encrypt certificates Or run the scripts directly on the server:\n/etc/nginx/sign-cert # Sign certificates /etc/nginx/link-cert # Link certificates to Nginx config directory For more info, see Certbot: Request and Renew HTTPS Certificates\nDefault Homepage Pigsty’s default home server provides these built-in routes:\nPath Description / Homepage navigation /ui/ Grafana monitoring dashboards /vmetrics/ VictoriaMetrics VMUI /vlogs/ VictoriaLogs log query /vtraces/ VictoriaTraces tracing /vmalert/ VMAlert alerting rules /alertmgr/ AlertManager alert management /blackbox/ Blackbox Exporter /pev PostgreSQL Explain visualization /haproxy/\u003ccluster\u003e/ HAProxy admin interface (if any) These routes allow accessing all monitoring components through a single entry point, no need for multiple domain configurations.\nBest Practices Use domain names instead of IP:PORT for service access Properly configure DNS resolution or hosts file Enable WebSocket for real-time apps (e.g., Grafana, Jupyter) Enable HTTPS for production Use meaningful subdomains to organize services Monitor Let’s Encrypt certificate expiration Use config parameter for custom Nginx configurations Full Example Here’s the Nginx configuration used by Pigsty’s public demo site demo.pigsty.io:\ninfra_portal: home : { domain: i.pigsty } io : { domain: pigsty.io ,path: \"/www/pigsty.io\" ,cert: /etc/cert/pigsty.io.crt ,key: /etc/cert/pigsty.io.key } minio : { domain: m.pigsty.io ,endpoint: \"${admin_ip}:9001\" ,scheme: https ,websocket: true } postgrest : { domain: api.pigsty.io ,endpoint: \"127.0.0.1:8884\" } pgadmin : { domain: adm.pigsty.io ,endpoint: \"127.0.0.1:8885\" } pgweb : { domain: cli.pigsty.io ,endpoint: \"127.0.0.1:8886\" } bytebase : { domain: ddl.pigsty.io ,endpoint: \"127.0.0.1:8887\" } jupyter : { domain: lab.pigsty.io ,endpoint: \"127.0.0.1:8888\" ,websocket: true } gitea : { domain: git.pigsty.io ,endpoint: \"127.0.0.1:8889\" } wiki : { domain: wiki.pigsty.io ,endpoint: \"127.0.0.1:9002\" } noco : { domain: noco.pigsty.io ,endpoint: \"127.0.0.1:9003\" } supa : { domain: supa.pigsty.io ,endpoint: \"10.10.10.10:8000\" ,websocket: true } dify : { domain: dify.pigsty.io ,endpoint: \"10.10.10.10:8001\" ,websocket: true } odoo : { domain: odoo.pigsty.io ,endpoint: \"127.0.0.1:8069\" ,websocket: true } mm : { domain: mm.pigsty.io ,endpoint: \"10.10.10.10:8065\" ,websocket: true } ","categories":["Task"],"description":"Nginx management, web portal configuration, web server, upstream services","excerpt":"Nginx management, web portal configuration, web server, upstream …","ref":"/docs/infra/admin/portal/","tags":"","title":"Nginx Management"},{"body":"Pigsty supports creating and managing local APT/YUM software repositories for offline deployment or accelerated package installation.\nQuick Start To add packages to the local repository:\nAdd packages to repo_packages (default packages) Add packages to repo_extra_packages (extra packages) Run the build command: ./infra.yml -t repo_build # Build local repo from upstream ./node.yml -t node_repo # Refresh node repository cache Package Aliases Pigsty predefines common package combinations for batch installation:\nEL Systems (RHEL/CentOS/Rocky) Alias Description node-bootstrap Ansible, Python3 tools, SSH related infra-package Nginx, etcd, HAProxy, monitoring exporters, MinIO pgsql-utility Patroni, pgBouncer, pgBackRest, PG tools pgsql Full PostgreSQL (server, client, extensions) pgsql-mini Minimal PostgreSQL installation Debian/Ubuntu Systems Alias Description node-bootstrap Ansible, development tools infra-package Infrastructure components (Debian naming) pgsql-client PostgreSQL client pgsql-server PostgreSQL server and related packages Playbook Tasks Main Tasks Task Description repo Create local repo from internet or offline packages repo_build Build from upstream if not exists repo_upstream Add upstream repository files repo_pkg Download packages and dependencies repo_create Create/update YUM or APT repository repo_nginx Start Nginx file server Complete Task List ./infra.yml -t repo_dir # Create local repository directory ./infra.yml -t repo_check # Check if local repo exists ./infra.yml -t repo_prepare # Use existing repo directly ./infra.yml -t repo_build # Build repo from upstream ./infra.yml -t repo_upstream # Add upstream repositories ./infra.yml -t repo_remove # Delete existing repo files ./infra.yml -t repo_add # Add repo to system directory ./infra.yml -t repo_url_pkg # Download packages from internet ./infra.yml -t repo_cache # Create metadata cache ./infra.yml -t repo_boot_pkg # Install bootstrap packages ./infra.yml -t repo_pkg # Download packages and dependencies ./infra.yml -t repo_create # Create local repository ./infra.yml -t repo_use # Add new repo to system ./infra.yml -t repo_nginx # Start Nginx file server Common Operations Add New Packages # 1. Configure upstream repositories ./infra.yml -t repo_upstream # 2. Download packages and dependencies ./infra.yml -t repo_pkg # 3. Build local repository metadata ./infra.yml -t repo_create Refresh Node Repositories ./node.yml -t node_repo # Refresh repository cache on all nodes Complete Repository Rebuild ./infra.yml -t repo # Create repo from internet or offline packages ","categories":["Task"],"description":"Managing local APT/YUM software repositories","excerpt":"Managing local APT/YUM software repositories","ref":"/docs/infra/admin/repo/","tags":"","title":"Software Repository"},{"body":"Use domain names instead of IP addresses to access Pigsty’s various web services.\nQuick Start Add the following static resolution records to /etc/hosts:\n10.10.10.10 i.pigsty g.pigsty p.pigsty a.pigsty Replace IP address with your actual Pigsty node’s IP.\nWhy Use Domain Names Easier to remember than IP addresses Flexible pointing to different IPs Unified service management through Nginx Support for HTTPS encryption Prevent ISP hijacking in some regions Allow access to internally bound services via proxy DNS Mechanism DNS Protocol: Resolves domain names to IP addresses. Multiple domains can point to same IP.\nHTTP Protocol: Uses Host header to route requests to different sites on same port (80/443).\nDefault Domains Pigsty predefines the following default domains:\nDomain Service Port Purpose i.pigsty Nginx 80/443 Default homepage, local repo, unified entry g.pigsty Grafana 3000 Monitoring and visualization p.pigsty VictoriaMetrics 8428 VMUI/PromQL entry a.pigsty AlertManager 9059 Alert routing m.pigsty MinIO 9001 Object storage console Resolution Methods Local Static Resolution Add entries to /etc/hosts on the client machine:\n# Linux/macOS sudo vim /etc/hosts # Windows notepad C:\\Windows\\System32\\drivers\\etc\\hosts Add content:\n10.10.10.10 i.pigsty g.pigsty p.pigsty a.pigsty m.pigsty Internal Dynamic Resolution Pigsty includes dnsmasq as an internal DNS server. Configure managed nodes to use INFRA node as DNS server:\nnode_dns_servers: ['${admin_ip}'] # Use INFRA node as DNS server node_dns_method: add # Add to existing DNS server list Configure domain records resolved by dnsmasq via dns_records:\ndns_records: - \"${admin_ip} i.pigsty\" - \"${admin_ip} m.pigsty sss.pigsty api.pigsty adm.pigsty cli.pigsty ddl.pigsty\" Public Domain Names Purchase a domain and add DNS A record pointing to public IP:\nPurchase domain from registrar (e.g., example.com) Configure A record pointing to server public IP Use real domain in infra_portal Built-in DNS Service Pigsty runs dnsmasq on INFRA nodes as a DNS server.\nRelated Parameters Parameter Default Description dns_enabled true Enable DNS service dns_port 53 DNS listen port dns_records See below Default DNS records Default DNS records:\ndns_records: - \"${admin_ip} i.pigsty\" - \"${admin_ip} m.pigsty sss.pigsty api.pigsty adm.pigsty cli.pigsty ddl.pigsty\" Dynamic DNS Registration Pigsty automatically registers DNS records for PostgreSQL clusters and instances:\nInstance-level DNS: \u003cpg_instance\u003e points to instance IP (e.g., pg-meta-1) Cluster-level DNS: \u003cpg_cluster\u003e points to primary IP or VIP (e.g., pg-meta) Cluster-level DNS target controlled by pg_dns_target:\nValue Description auto Auto-select: use VIP if available, else primary IP primary Always point to primary IP vip Always point to VIP (requires VIP enabled) none Don’t register cluster DNS \u003cip\u003e Specify fixed IP address Add suffix to cluster DNS via pg_dns_suffix.\nNode DNS Configuration Pigsty manages DNS configuration on managed nodes.\nStatic hosts Records Configure static /etc/hosts records via node_etc_hosts:\nnode_etc_hosts: - \"${admin_ip} i.pigsty sss.pigsty\" - \"10.10.10.20 db.example.com\" DNS Server Configuration Parameter Default Description node_dns_method add DNS config method node_dns_servers ['${admin_ip}'] DNS server list node_dns_options See below resolv.conf options node_dns_method options:\nValue Description add Prepend to existing DNS server list overwrite Completely overwrite DNS config none Don’t modify DNS config Default DNS options:\nnode_dns_options: - options single-request-reopen timeout:1 HTTPS Certificates Pigsty uses self-signed certificates by default. Options include:\nIgnore warnings, use HTTP Trust self-signed CA certificate (download at http://\u003cip\u003e/ca.crt) Use real CA or get free public domain certs via Certbot See CA and Certificates documentation for details.\nExtended Domains Pigsty reserves the following domains for various application services:\nDomain Purpose adm.pigsty PgAdmin interface ddl.pigsty Bytebase DDL management cli.pigsty PgWeb CLI interface api.pigsty PostgREST API service lab.pigsty Jupyter environment git.pigsty Gitea Git service wiki.pigsty Wiki.js docs noco.pigsty NocoDB supa.pigsty Supabase dify.pigsty Dify AI odoo.pigsty Odoo ERP mm.pigsty Mattermost Using these domains requires configuring corresponding services in infra_portal.\nManagement Commands ./infra.yml -t dns # Full DNS service configuration ./infra.yml -t dns_config # Regenerate dnsmasq config ./infra.yml -t dns_record # Update default DNS records ./infra.yml -t dns_launch # Restart dnsmasq service ./node.yml -t node_hosts # Configure node /etc/hosts ./node.yml -t node_resolv # Configure node DNS resolver ./pgsql.yml -t pg_dns # Register PostgreSQL DNS records ./pgsql.yml -t pg_dns_ins # Register instance-level DNS only ./pgsql.yml -t pg_dns_cls # Register cluster-level DNS only ","categories":["Task"],"description":"Configure local or public domain names to access Pigsty services.","excerpt":"Configure local or public domain names to access Pigsty services.","ref":"/docs/infra/admin/domain/","tags":"","title":"Domain Management"},{"body":"This document covers daily management operations for the INFRA module, including installation, uninstallation, scaling, and component maintenance.\nInstall INFRA Module Use the infra.yml playbook to install the INFRA module on the infra group:\n./infra.yml # Install INFRA module on infra group Uninstall INFRA Module Use the infra-rm.yml playbook to uninstall the INFRA module from the infra group:\n./infra-rm.yml # Uninstall INFRA module from infra group Scale Out INFRA Module Assign infra_seq to new nodes and add them to the infra group in the inventory:\nall: children: infra: hosts: 10.10.10.10: { infra_seq: 1 } # Existing node 10.10.10.11: { infra_seq: 2 } # New node Use the -l limit option to execute the playbook on the new node only:\n./infra.yml -l 10.10.10.11 # Install INFRA module on new node Manage Local Repository Local repository management tasks:\n./infra.yml -t repo # Create repo from internet or offline packages ./infra.yml -t repo_upstream # Add upstream repositories ./infra.yml -t repo_pkg # Download packages and dependencies ./infra.yml -t repo_create # Create local yum/apt repository Complete subtask list:\n./infra.yml -t repo_dir # Create local repository directory ./infra.yml -t repo_check # Check if local repo exists ./infra.yml -t repo_prepare # Use existing repo directly ./infra.yml -t repo_build # Build repo from upstream ./infra.yml -t repo_upstream # Add upstream repositories ./infra.yml -t repo_remove # Delete existing repo files ./infra.yml -t repo_add # Add repo to system directory ./infra.yml -t repo_url_pkg # Download packages from internet ./infra.yml -t repo_cache # Create metadata cache ./infra.yml -t repo_boot_pkg # Install bootstrap packages ./infra.yml -t repo_pkg # Download packages and dependencies ./infra.yml -t repo_create # Create local repository ./infra.yml -t repo_use # Add new repo to system ./infra.yml -t repo_nginx # Start Nginx file server Manage Nginx Nginx management tasks:\n./infra.yml -t nginx # Reset Nginx component ./infra.yml -t nginx_index # Re-render homepage ./infra.yml -t nginx_config,nginx_reload # Re-render config and reload Request HTTPS certificate:\n./infra.yml -t nginx_certbot,nginx_reload -e certbot_sign=true Manage Infrastructure Components Management commands for various infrastructure components:\n./infra.yml -t infra # Configure infrastructure ./infra.yml -t infra_env # Configure environment variables ./infra.yml -t infra_pkg # Install packages ./infra.yml -t infra_user # Set up OS user ./infra.yml -t infra_cert # Issue certificates ./infra.yml -t dns # Configure DNSMasq ./infra.yml -t nginx # Configure Nginx ./infra.yml -t victoria # Configure VictoriaMetrics/Logs/Traces ./infra.yml -t alertmanager # Configure AlertManager ./infra.yml -t blackbox # Configure Blackbox Exporter ./infra.yml -t grafana # Configure Grafana ./infra.yml -t infra_register # Register to VictoriaMetrics/Grafana Common maintenance commands:\n./infra.yml -t nginx_index # Re-render homepage ./infra.yml -t nginx_config,nginx_reload # Reconfigure and reload ./infra.yml -t vmetrics_config,vmetrics_launch # Regenerate VictoriaMetrics config and restart ./infra.yml -t vlogs_config,vlogs_launch # Update VictoriaLogs config ./infra.yml -t grafana_plugin # Download Grafana plugins ","categories":["Task"],"description":"INFRA module management SOP: define, create, destroy, scale out, scale in","excerpt":"INFRA module management SOP: define, create, destroy, scale out, scale …","ref":"/docs/infra/admin/sop/","tags":"","title":"Module Management"},{"body":"Pigsty uses a self-signed Certificate Authority (CA) by default for internal SSL/TLS encryption. This document covers:\nSelf-Signed CA: Default PKI infrastructure Issue Certificates: Using cert.yml to issue additional certificates Trust CA Certificate: Installing CA on client machines Let’s Encrypt: Using real certificates for public-facing services Self-Signed CA Pigsty automatically creates a self-signed CA during infrastructure initialization (infra.yml). The CA signs certificates for:\nPostgreSQL server/client SSL Patroni REST API etcd cluster communication MinIO cluster communication Nginx HTTPS (fallback) Infrastructure services PKI Directory Structure files/pki/ ├── ca/ │ ├── ca.key # CA private key (keep secure!) │ └── ca.crt # CA certificate ├── csr/ # Certificate signing requests │ ├── misc/ # Miscellaneous certificates (cert.yml output) │ ├── etcd/ # ETCD certificates │ ├── pgsql/ # PostgreSQL certificates │ ├── minio/ # MinIO certificates │ ├── nginx/ # Nginx certificates │ └── mongo/ # FerretDB certificates └── infra/ # Infrastructure certificates CA Variables Variable Default Description ca_create true Create CA if not exists, or abort ca_cn pigsty-ca CA certificate common name cert_validity 7300d Default validity for issued certificates Variable Default :—————- ————– —————————————- CA Certificate 100 years Hardcoded (36500 days) Server/Client 20 years cert_validity (7300d) Nginx HTTPS ~1 year nginx_cert_validity (397d) \u003e Note: Browser vendors limit trust to 398-day certificates. Nginx uses shorter validity for browser compatibility. Using External CA To use your own enterprise CA instead of auto-generated one:\n1. Set ca_create: false in your configuration.\n2. Place your CA files before running playbook:\nmkdir -p files/pki/ca cp /path/to/your/ca.key files/pki/ca/ca.key cp /path/to/your/ca.crt files/pki/ca/ca.crt chmod 600 files/pki/ca/ca.key chmod 644 files/pki/ca/ca.crt 3. Run ./infra.yml\nBackup CA Files The CA private key is critical. Back it up securely:\n# Backup with timestamp tar -czvf pigsty-ca-$(date +%Y%m%d).tar.gz files/pki/ca/ Warning: If you lose CA private key, all certificates signed by it become unverifiable. You’ll need to regenerate everything.\nIssue Certificates Use cert.yml to issue additional certificates signed by Pigsty CA.\nBasic Usage # Issue certificate for database user (client cert) ./cert.yml -e cn=dbuser_dba # Issue certificate for monitor user ./cert.yml -e cn=dbuser_monitor Certificates generated in files/pki/misc/\u003ccn\u003e.{key,crt} by default.\nParameters Parameter Default Description cn pigsty Common Name (required) san [DNS:localhost, IP:127.0.0.1] Subject Alternative Names org pigsty Organization name unit pigsty Organizational unit name expire 7300d Certificate validity (20 years) key files/pki/misc/\u003ccn\u003e.key Private key output path crt files/pki/misc/\u003ccn\u003e.crt Certificate output path Advanced Examples # Issue certificate with custom SAN (DNS and IP) ./cert.yml -e cn=myservice -e san=DNS:myservice,IP:10.2.82.163 (File has more lines. Use ‘offset’ parameter to read beyond line 130)\n","categories":["Task"],"description":"Using self-signed CA or real HTTPS certificates","excerpt":"Using self-signed CA or real HTTPS certificates","ref":"/docs/infra/admin/cert/","tags":"","title":"CA and Certificates"},{"body":"Tune nodes into the desired state and monitor it, manage node, VIP, HAProxy, and exporters.\n","categories":["Reference"],"description":"Tune nodes into the desired state and monitor it, manage node, VIP, HAProxy, and exporters.","excerpt":"Tune nodes into the desired state and monitor it, manage node, VIP, …","ref":"/docs/node/","tags":"","title":"Module: NODE"},{"body":"Pigsty uses IP address as the unique identifier for nodes. This IP should be the internal IP address on which the database instance listens and provides external services.\nnode-test: hosts: 10.10.10.11: { nodename: node-test-1 } 10.10.10.12: { nodename: node-test-2 } 10.10.10.13: { nodename: node-test-3 } vars: node_cluster: node-test This IP address must be the address on which the database instance listens and provides external services, but should not be a public IP address. That said, you don’t necessarily have to connect to the database via this IP. For example, managing target nodes indirectly through SSH tunnels or jump hosts is also feasible. However, when identifying database nodes, the primary IPv4 address remains the node’s core identifier. This is critical, and you should ensure this during configuration.\nThe IP address is the inventory_hostname in the inventory, represented as the key in the \u003ccluster\u003e.hosts object. In addition, each node has two optional identity parameters:\nName Type Level Necessity Comment inventory_hostname ip - Required Node IP nodename string I Optional Node Name node_cluster string C Optional Node cluster name The parameters nodename and node_cluster are optional. If not provided, the node’s existing hostname and the fixed value nodes will be used as defaults. In Pigsty’s monitoring system, these two will be used as the node’s cluster identifier (cls) and instance identifier (ins).\nFor PGSQL nodes, because Pigsty defaults to a 1:1 exclusive deployment of PG to node, you can use the node_id_from_pg parameter to borrow the PostgreSQL instance’s identity parameters (pg_cluster and pg_seq) for the node’s ins and cls labels. This allows database and node monitoring metrics to share the same labels for cross-analysis.\n#nodename: # [instance] # node instance identity, uses existing hostname if missing, optional node_cluster: nodes # [cluster] # node cluster identity, uses 'nodes' if missing, optional nodename_overwrite: true # overwrite node's hostname with nodename? nodename_exchange: false # exchange nodename among play hosts? node_id_from_pg: true # borrow postgres identity as node identity if applicable? You can also configure rich functionality for host clusters. For example, use HAProxy on the node cluster for load balancing and service exposure, or bind an L2 VIP to the cluster.\n","categories":["Reference"],"description":"Configure node identity, cluster, and identity borrowing from PostgreSQL","excerpt":"Configure node identity, cluster, and identity borrowing from …","ref":"/docs/node/config/","tags":"","title":"Configuration"},{"body":"The NODE module tunes target nodes into the desired state and integrates them into the Pigsty monitoring system.\nParameter Section Description NODE_ID Node identity parameters NODE_DNS Node DNS resolution NODE_PACKAGE Upstream repo \u0026 package install NODE_TUNE Node tuning \u0026 kernel features NODE_SEC Node security configurations NODE_ADMIN Admin user \u0026 SSH keys NODE_TIME Timezone, NTP, crontab NODE_VIP Optional L2 VIP for cluster HAPROXY HAProxy load balancer NODE_EXPORTER Node monitoring exporter VECTOR Vector log collector Parameters Overview NODE_ID section defines node identity parameters, including node name, cluster name, and whether to borrow identity from PostgreSQL.\nParameter Type Level Description nodename string I node instance identity, use hostname if missing node_cluster string C node cluster identity, use ’nodes’ if missing nodename_overwrite bool C overwrite node’s hostname with nodename? nodename_exchange bool C exchange nodename among play hosts? node_id_from_pg bool C use postgres identity as node identity if applicable? NODE_DNS section configures node DNS resolution, including static hosts records and dynamic DNS servers.\nParameter Type Level Description node_write_etc_hosts bool G/C/I modify /etc/hosts on target node? node_default_etc_hosts string[] G static dns records in /etc/hosts node_etc_hosts string[] C extra static dns records in /etc/hosts node_dns_method enum C how to handle dns servers: add,none,overwrite node_dns_servers string[] C dynamic nameserver in /etc/resolv.conf node_dns_options string[] C dns resolv options in /etc/resolv.conf NODE_PACKAGE section configures node software repositories and package installation.\nParameter Type Level Description node_repo_modules enum C which repo modules to enable on node? local default node_repo_remove bool C remove existing repo on node when configuring? node_packages string[] C packages to be installed on current nodes node_default_packages string[] G default packages to be installed on all nodes NODE_TUNE section configures node kernel parameters, feature toggles, and tuning templates.\nParameter Type Level Description node_disable_numa bool C disable node numa, reboot required node_disable_swap bool C disable node swap, use with caution node_static_network bool C preserve dns resolver settings after reboot node_disk_prefetch bool C setup disk prefetch on HDD to increase performance node_kernel_modules string[] C kernel modules to be enabled on this node node_hugepage_count int C number of 2MB hugepage, take precedence over ratio node_hugepage_ratio float C node mem hugepage ratio, 0 disable it by default node_overcommit_ratio float C node mem overcommit ratio (50-100), 0 disable it node_tune enum C node tuned profile: none,oltp,olap,crit,tiny node_sysctl_params dict C extra sysctl parameters in k:v format NODE_SEC section configures node security options, including SELinux and firewall.\nParameter Type Level Description node_selinux_mode enum C SELinux mode: disabled, permissive, enforcing node_firewall_mode enum C firewall mode: off, none, zone node_firewall_intranet cidr[] C intranet CIDR list for firewall rules node_firewall_public_port port[] C public exposed port list, default [22, 80, 443, 5432] NODE_ADMIN section configures admin user, data directory, and shell aliases.\nParameter Type Level Description node_data path C node main data directory, /data by default node_admin_enabled bool C create a admin user on target node? node_admin_uid int C uid and gid for node admin user node_admin_username username C name of node admin user, dba by default node_admin_sudo enum C admin sudo privilege: limited, nopass, all, none node_admin_ssh_exchange bool C exchange admin ssh key among node cluster node_admin_pk_current bool C add current user’s ssh pk to admin authorized_keys node_admin_pk_list string[] C ssh public keys to be added to admin user node_aliases dict C shell aliases in K:V dict format NODE_TIME section configures timezone, NTP time sync, and crontab.\nParameter Type Level Description node_timezone string C setup node timezone, empty string to skip node_ntp_enabled bool C enable chronyd time sync service? node_ntp_servers string[] C ntp servers in /etc/chrony.conf node_crontab_overwrite bool C overwrite or append to /etc/crontab? node_crontab string[] C crontab entries in /etc/crontab NODE_VIP section configures L2 VIP for node cluster, implemented by keepalived.\nParameter Type Level Description vip_enabled bool C enable L2 vip on this node cluster? vip_address ip C node vip address in ipv4 format, required if enabled vip_vrid int C integer 1-254, should be unique in same VLAN vip_role enum I optional, master/backup, backup by default vip_preempt bool C/I optional, true/false, enable vip preemption vip_interface string C/I node vip network interface, eth0 by default vip_dns_suffix string C node vip dns name suffix, empty string by default vip_auth_pass password C vrrp authentication password, auto-generated if empty vip_exporter_port port C keepalived exporter listen port, 9650 by default HAPROXY section configures HAProxy load balancer and service exposure.\nParameter Type Level Description haproxy_enabled bool C enable haproxy on this node? haproxy_clean bool G/C/A cleanup all existing haproxy config? haproxy_reload bool A reload haproxy after config? haproxy_auth_enabled bool G enable authentication for admin page haproxy_admin_username username G haproxy admin username, admin default haproxy_admin_password password G haproxy admin password, pigsty default haproxy_exporter_port port C haproxy exporter port, 9101 by default haproxy_client_timeout interval C client connection timeout, 24h default haproxy_server_timeout interval C server connection timeout, 24h default haproxy_services service[] C list of haproxy services to expose NODE_EXPORTER section configures node monitoring exporter.\nParameter Type Level Description node_exporter_enabled bool C setup node_exporter on this node? node_exporter_port port C node exporter listen port, 9100 default node_exporter_options arg C extra server options for node_exporter VECTOR section configures Vector log collector.\nParameter Type Level Description vector_enabled bool C enable vector log collector? vector_clean bool G/A purge vector data dir during init? vector_data path C vector data directory, /data/vector default vector_port port C vector metrics listen port, 9598 default vector_read_from enum C read log from beginning or end vector_log_endpoint string[] C log endpoint, default send to infra group NODE_ID Each node has identity parameters that are configured through the parameters in \u003ccluster\u003e.hosts and \u003ccluster\u003e.vars.\nPigsty uses IP address as the unique identifier for database nodes. This IP address must be the one that the database instance listens on and provides services, but should not be a public IP address. However, users don’t have to connect to the database via this IP address. For example, managing target nodes indirectly through SSH tunnels or jump servers is feasible. When identifying database nodes, the primary IPv4 address remains the core identifier. This is very important, and users should ensure this when configuring. The IP address is the inventory_hostname in the inventory, which is the key of the \u003ccluster\u003e.hosts object.\nnode-test: hosts: 10.10.10.11: { nodename: node-test-1 } 10.10.10.12: { nodename: node-test-2 } 10.10.10.13: { nodename: node-test-3 } vars: node_cluster: node-test In addition, nodes have two important identity parameters in the Pigsty monitoring system: nodename and node_cluster, which are used as the instance identity (ins) and cluster identity (cls) in the monitoring system.\nnode_load1{cls=\"pg-meta\", ins=\"pg-meta-1\", ip=\"10.10.10.10\", job=\"nodes\"} node_load1{cls=\"pg-test\", ins=\"pg-test-1\", ip=\"10.10.10.11\", job=\"nodes\"} node_load1{cls=\"pg-test\", ins=\"pg-test-2\", ip=\"10.10.10.12\", job=\"nodes\"} node_load1{cls=\"pg-test\", ins=\"pg-test-3\", ip=\"10.10.10.13\", job=\"nodes\"} When executing the default PostgreSQL deployment, since Pigsty uses exclusive 1:1 deployment by default, you can borrow the database instance’s identity parameters (pg_cluster) to the node’s ins and cls labels through the node_id_from_pg parameter.\nName Type Level Required Description inventory_hostname ip - Required Node IP Address nodename string I Optional Node Name node_cluster string C Optional Node Cluster Name #nodename: # [instance] # node instance identity, use hostname if missing, optional node_cluster: nodes # [cluster] # node cluster identity, use 'nodes' if missing, optional nodename_overwrite: true # overwrite node's hostname with nodename? nodename_exchange: false # exchange nodename among play hosts? node_id_from_pg: true # use postgres identity as node identity if applicable? nodename name: nodename, type: string, level: I\nNode instance identity parameter. If not explicitly set, the existing hostname will be used as the node name. This parameter is optional since it has a reasonable default value.\nIf node_id_from_pg is enabled (default), and nodename is not explicitly specified, nodename will try to use ${pg_cluster}-${pg_seq} as the instance identity. If the PGSQL module is not defined on this cluster, it will fall back to the default, which is the node’s HOSTNAME.\nnode_cluster name: node_cluster, type: string, level: C\nThis option allows explicitly specifying a cluster name for the node, which is only meaningful when defined at the node cluster level. Using the default empty value will use the fixed value nodes as the node cluster identity.\nIf node_id_from_pg is enabled (default), and node_cluster is not explicitly specified, node_cluster will try to use ${pg_cluster} as the cluster identity. If the PGSQL module is not defined on this cluster, it will fall back to the default nodes.\nnodename_overwrite name: nodename_overwrite, type: bool, level: C\nOverwrite node’s hostname with nodename? Default is true. In this case, if you set a non-empty nodename, it will be used as the current host’s HOSTNAME.\nWhen nodename is empty, if node_id_from_pg is true (default), Pigsty will try to borrow the identity parameters of the PostgreSQL instance defined 1:1 on the node as the node name, i.e., {{ pg_cluster }}-{{ pg_seq }}. If the PGSQL module is not installed on this node, it will fall back to not doing anything.\nTherefore, if you leave nodename empty and don’t enable node_id_from_pg, Pigsty will not make any changes to the existing hostname.\nnodename_exchange name: nodename_exchange, type: bool, level: C\nExchange nodename among play hosts? Default is false.\nWhen enabled, nodes executing the node.yml playbook in the same batch will exchange node names with each other, writing them to /etc/hosts.\nnode_id_from_pg name: node_id_from_pg, type: bool, level: C\nBorrow identity parameters from the PostgreSQL instance/cluster deployed 1:1 on the node? Default is true.\nPostgreSQL instances and nodes in Pigsty use 1:1 deployment by default, so you can “borrow” identity parameters from the database instance. This parameter is enabled by default, meaning that if a PostgreSQL cluster has no special configuration, the host node cluster and instance identity parameters will default to matching the database identity parameters. This provides extra convenience for problem analysis and monitoring data processing.\nNODE_DNS Pigsty configures static DNS records and dynamic DNS servers for nodes.\nIf your node provider has already configured DNS servers for you, you can set node_dns_method to none to skip DNS setup.\nnode_write_etc_hosts: true # modify `/etc/hosts` on target node? node_default_etc_hosts: # static dns records in `/etc/hosts` - \"${admin_ip} i.pigsty\" node_etc_hosts: [] # extra static dns records in `/etc/hosts` node_dns_method: add # how to handle dns servers: add,none,overwrite node_dns_servers: ['${admin_ip}'] # dynamic nameserver in `/etc/resolv.conf` node_dns_options: # dns resolv options in `/etc/resolv.conf` - options single-request-reopen timeout:1 node_write_etc_hosts name: node_write_etc_hosts, type: bool, level: G|C|I\nModify /etc/hosts on target node? For example, in container environments, this file usually cannot be modified.\nnode_default_etc_hosts name: node_default_etc_hosts, type: string[], level: G\nStatic DNS records to be written to all nodes’ /etc/hosts. Default value:\n[\"${admin_ip} i.pigsty\"] node_default_etc_hosts is an array. Each element is a DNS record with format \u003cip\u003e \u003cname\u003e. You can specify multiple domain names separated by spaces.\nThis parameter is used to configure global static DNS records. If you want to configure specific static DNS records for individual clusters and instances, use the node_etc_hosts parameter.\nnode_etc_hosts name: node_etc_hosts, type: string[], level: C\nExtra static DNS records to write to node’s /etc/hosts. Default is [] (empty array).\nSame format as node_default_etc_hosts, but suitable for configuration at the cluster/instance level.\nnode_dns_method name: node_dns_method, type: enum, level: C\nHow to configure DNS servers? Three options: add, none, overwrite. Default is add.\nadd: Append the records in node_dns_servers to /etc/resolv.conf and keep existing DNS servers. (default) overwrite: Overwrite /etc/resolv.conf with the records in node_dns_servers none: Skip DNS server configuration. If your environment already has DNS servers configured, you can skip DNS configuration directly. node_dns_servers name: node_dns_servers, type: string[], level: C\nConfigure the dynamic DNS server list in /etc/resolv.conf. Default is [\"${admin_ip}\"], using the admin node as the primary DNS server.\nnode_dns_options name: node_dns_options, type: string[], level: C\nDNS resolution options in /etc/resolv.conf. Default value:\n- \"options single-request-reopen timeout:1\" If node_dns_method is configured as add or overwrite, the records in this configuration will be written to /etc/resolv.conf first. Refer to Linux documentation for /etc/resolv.conf format details.\nNODE_PACKAGE Pigsty configures software repositories and installs packages on managed nodes.\nnode_repo_modules: local # upstream repo to be added on node, local by default. node_repo_remove: true # remove existing repo on node? node_packages: [openssh-server] # packages to be installed current nodes with latest version #node_default_packages: # default packages to be installed on all nodes node_repo_modules name: node_repo_modules, type: string, level: C/A\nList of software repository modules to be added on the node, same format as repo_modules. Default is local, using the local software repository specified in repo_upstream.\nWhen Pigsty manages nodes, it filters entries in repo_upstream based on this parameter value. Only entries whose module field matches this parameter value will be added to the node’s software sources.\nnode_repo_remove name: node_repo_remove, type: bool, level: C/A\nRemove existing software repository definitions on the node? Default is true.\nWhen enabled, Pigsty will remove existing configuration files in /etc/yum.repos.d on the node and back them up to /etc/yum.repos.d/backup. On Debian/Ubuntu systems, it backs up /etc/apt/sources.list(.d) to /etc/apt/backup.\nnode_packages name: node_packages, type: string[], level: C\nList of software packages to install and upgrade on the current node. Default is [openssh-server], which upgrades sshd to the latest version during installation (to avoid security vulnerabilities).\nEach array element is a string of comma-separated package names. Same format as node_default_packages. This parameter is usually used to specify additional packages to install at the node/cluster level.\nPackages specified in this parameter will be upgraded to the latest available version. If you need to keep existing node software versions unchanged (just ensure they exist), use the node_default_packages parameter.\nnode_default_packages name: node_default_packages, type: string[], level: G\nDefault packages to be installed on all nodes. Default value is a common RPM package list for EL 7/8/9. Array where each element is a space-separated package list string.\nPackages specified in this variable only require existence, not latest. If you need to install the latest version, use the node_packages parameter.\nThis parameter has no default value (undefined state). If users don’t explicitly specify this parameter in the configuration file, Pigsty will load default values from the node_packages_default variable defined in roles/node_id/vars based on the current node’s OS family.\nDefault value (EL-based systems):\n- lz4,unzip,bzip2,pv,jq,git,ncdu,make,patch,bash,lsof,wget,uuid,tuned,nvme-cli,numactl,sysstat,iotop,htop,rsync,tcpdump - python3,python3-pip,socat,lrzsz,net-tools,ipvsadm,telnet,ca-certificates,openssl,keepalived,etcd,haproxy,chrony,pig - zlib,yum,audit,bind-utils,readline,vim-minimal,node_exporter,grubby,openssh-server,openssh-clients,chkconfig,vector Default value (Debian/Ubuntu):\n- lz4,unzip,bzip2,pv,jq,git,ncdu,make,patch,bash,lsof,wget,uuid,tuned,nvme-cli,numactl,sysstat,iotop,htop,rsync - python3,python3-pip,socat,lrzsz,net-tools,ipvsadm,telnet,ca-certificates,openssl,keepalived,etcd,haproxy,chrony,pig - zlib1g,acl,dnsutils,libreadline-dev,vim-tiny,node-exporter,openssh-server,openssh-client,vector Same format as node_packages, but this parameter is usually used to specify default packages that must be installed on all nodes at the global level.\nNODE_TUNE Host node features, kernel modules, and tuning templates.\nnode_disable_numa: false # disable node numa, reboot required node_disable_swap: false # disable node swap, use with caution node_static_network: true # preserve dns resolver settings after reboot node_disk_prefetch: false # setup disk prefetch on HDD to increase performance node_kernel_modules: [ softdog, ip_vs, ip_vs_rr, ip_vs_wrr, ip_vs_sh ] node_hugepage_count: 0 # number of 2MB hugepage, take precedence over ratio node_hugepage_ratio: 0 # node mem hugepage ratio, 0 disable it by default node_overcommit_ratio: 0 # node mem overcommit ratio, 0 disable it by default node_tune: oltp # node tuned profile: none,oltp,olap,crit,tiny node_sysctl_params: { } # sysctl parameters in k:v format in addition to tuned node_disable_numa name: node_disable_numa, type: bool, level: C\nDisable NUMA? Default is false (NUMA not disabled).\nNote that disabling NUMA requires a machine reboot to take effect! If you don’t know how to set CPU affinity, it’s recommended to disable NUMA when using databases in production environments.\nnode_disable_swap name: node_disable_swap, type: bool, level: C\nDisable SWAP? Default is false (SWAP not disabled).\nDisabling SWAP is generally not recommended. The exception is if you have enough memory for exclusive PostgreSQL deployment, you can disable SWAP to improve performance.\nException: SWAP should be disabled when your node is used for Kubernetes deployments.\nnode_static_network name: node_static_network, type: bool, level: C\nUse static DNS servers? Default is true (enabled).\nEnabling static networking means your DNS Resolv configuration won’t be overwritten by machine reboots or NIC changes. Recommended to enable, or have network engineers handle the configuration.\nnode_disk_prefetch name: node_disk_prefetch, type: bool, level: C\nEnable disk prefetch? Default is false (not enabled).\nCan optimize performance for HDD-deployed instances. Recommended to enable when using mechanical hard drives.\nnode_kernel_modules name: node_kernel_modules, type: string[], level: C\nWhich kernel modules to enable? Default enables the following kernel modules:\nnode_kernel_modules: [ softdog, ip_vs, ip_vs_rr, ip_vs_wrr, ip_vs_sh ] An array of kernel module names declaring the kernel modules that need to be installed on the node.\nnode_hugepage_count name: node_hugepage_count, type: int, level: C\nNumber of 2MB hugepages to allocate on the node. Default is 0. Related parameter is node_hugepage_ratio.\nIf both node_hugepage_count and node_hugepage_ratio are 0 (default), hugepages will be completely disabled. This parameter has higher priority than node_hugepage_ratio because it’s more precise.\nIf a non-zero value is set, it will be written to /etc/sysctl.d/hugepage.conf to take effect. Negative values won’t work, and numbers higher than 90% of node memory will be capped at 90% of node memory.\nIf not zero, it should be slightly larger than the corresponding pg_shared_buffer_ratio value so PostgreSQL can use hugepages.\nnode_hugepage_ratio name: node_hugepage_ratio, type: float, level: C\nRatio of node memory for hugepages. Default is 0. Valid range: 0 ~ 0.40.\nThis memory ratio will be allocated as hugepages and reserved for PostgreSQL. node_hugepage_count is the higher priority and more precise version of this parameter.\nDefault: 0, which sets vm.nr_hugepages=0 and completely disables hugepages.\nThis parameter should equal or be slightly larger than pg_shared_buffer_ratio if not zero.\nFor example, if you allocate 25% of memory for Postgres shared buffers by default, you can set this value to 0.27 ~ 0.30, and use /pg/bin/pg-tune-hugepage after initialization to precisely reclaim wasted hugepages.\nnode_overcommit_ratio name: node_overcommit_ratio, type: int, level: C\nNode memory overcommit ratio. Default is 0. This is an integer from 0 to 100+.\nDefault: 0, which sets vm.overcommit_memory=0. Otherwise, vm.overcommit_memory=2 will be used with this value as vm.overcommit_ratio.\nRecommended to set vm.overcommit_ratio on dedicated pgsql nodes to avoid memory overcommit.\nnode_tune name: node_tune, type: enum, level: C\nPreset tuning profiles for machines, provided through tuned. Four preset modes:\ntiny: Micro virtual machine oltp: Regular OLTP template, optimizes latency (default) olap: Regular OLAP template, optimizes throughput crit: Core financial business template, optimizes dirty page count Typically, the database tuning template pg_conf should match the machine tuning template.\nnode_sysctl_params name: node_sysctl_params, type: dict, level: C\nSysctl kernel parameters in K:V format, added to the tuned profile. Default is {} (empty object).\nThis is a KV dictionary parameter where Key is the kernel sysctl parameter name and Value is the parameter value. You can also consider defining extra sysctl parameters directly in the tuned templates in roles/node/templates.\nNODE_SEC Node security related parameters, including SELinux and firewall configuration.\nnode_selinux_mode: permissive # selinux mode: disabled, permissive, enforcing node_firewall_mode: zone # firewall mode: disabled, zone, rules node_firewall_intranet: # which intranet cidr considered as internal network - 10.0.0.0/8 - 192.168.0.0/16 - 172.16.0.0/12 node_firewall_public_port: # expose these ports to public network in (zone, strict) mode - 22 # enable ssh access - 80 # enable http access - 443 # enable https access - 5432 # enable postgresql access (think twice before exposing it!) node_selinux_mode name: node_selinux_mode, type: enum, level: C\nSELinux running mode. Default is permissive.\nOptions:\ndisabled: Completely disable SELinux (equivalent to old version’s node_disable_selinux: true) permissive: Permissive mode, logs violations but doesn’t block (recommended, default) enforcing: Enforcing mode, strictly enforces SELinux policies If you don’t have professional OS/security experts, it’s recommended to use permissive or disabled mode.\nNote that SELinux is only enabled by default on EL-based systems. If you want to enable SELinux on Debian/Ubuntu systems, you need to install and enable SELinux configuration yourself. Also, SELinux mode changes may require a system reboot to fully take effect.\nnode_firewall_mode name: node_firewall_mode, type: enum, level: C\nFirewall running mode. Default is zone.\nOptions:\noff: Turn off and disable firewall (equivalent to old version’s node_disable_firewall: true) none: Do nothing, maintain existing firewall rules unchanged zone: Use firewalld / ufw to configure firewall rules: trust intranet, only open specified ports to public Uses firewalld service on EL systems, ufw service on Debian/Ubuntu systems.\nIf you’re deploying in a completely trusted intranet environment, or using cloud provider security groups for access control, you can choose none mode to keep existing firewall configuration, or set to off to completely disable the firewall.\nProduction environments recommend using zone mode with node_firewall_intranet and node_firewall_public_port for fine-grained access control.\nNote that zone mode won’t automatically enable the firewall for you.\nnode_firewall_intranet name: node_firewall_intranet, type: cidr[], level: C\nIntranet CIDR address list. Introduced in v4.0. Default value:\nnode_firewall_intranet: - 10.0.0.0/8 - 172.16.0.0/12 - 192.168.0.0/16 This parameter defines IP address ranges considered as “internal network”. Traffic from these networks will be allowed to access all service ports without separate open rules.\nHosts within these CIDR ranges will be treated as trusted intranet hosts with more relaxed firewall rules. Also, in PG/PGB HBA rules, the intranet ranges defined here will be treated as “intranet”.\nnode_firewall_public_port name: node_firewall_public_port, type: port[], level: C\nPublic exposed port list. Default is [22, 80, 443, 5432].\nThis parameter defines ports exposed to public network (non-intranet CIDR). Default exposed ports include:\n22: SSH service port 80: HTTP service port 443: HTTPS service port 5432: PostgreSQL database port You can adjust this list according to actual needs. For example, if you don’t need to expose the database port externally, remove 5432:\nnode_firewall_public_port: [22, 80, 443] PostgreSQL default security policy in Pigsty only allows administrators to access the database port from public networks. If you want other users to access the database from public networks, make sure to correctly configure corresponding access permissions in PG/PGB HBA rules.\nIf you want to expose other service ports to public networks, you can also add them to this list. If you want to tighten firewall rules, you can remove the 5432 database port to ensure only truly needed service ports are exposed.\nNote that this parameter only takes effect when node_firewall_mode is set to zone.\nNODE_ADMIN This section is about administrators on host nodes - who can log in and how.\nnode_data: /data # node main data directory, `/data` by default node_admin_enabled: true # create a admin user on target node? node_admin_uid: 88 # uid and gid for node admin user node_admin_username: dba # name of node admin user, `dba` by default node_admin_sudo: nopass # admin user's sudo privilege: limited, nopass, all, none node_admin_ssh_exchange: true # exchange admin ssh key among node cluster node_admin_pk_current: true # add current user's ssh pk to admin authorized_keys node_admin_pk_list: [] # ssh public keys to be added to admin user node_aliases: {} # alias name -\u003e IP address dict for `/etc/hosts` node_data name: node_data, type: path, level: C\nNode’s main data directory. Default is /data.\nIf this directory doesn’t exist, it will be created. This directory should be owned by root with 777 permissions.\nnode_admin_enabled name: node_admin_enabled, type: bool, level: C\nCreate a dedicated admin user on this node? Default is true.\nPigsty creates an admin user on each node by default (with password-free sudo and ssh). The default admin is named dba (uid=88), which can access other nodes in the environment from the admin node via password-free SSH and execute password-free sudo.\nnode_admin_uid name: node_admin_uid, type: int, level: C\nAdmin user UID. Default is 88.\nPlease ensure the UID is the same across all nodes whenever possible to avoid unnecessary permission issues.\nIf the default UID 88 is already taken, you can choose another UID. Be careful about UID namespace conflicts when manually assigning.\nnode_admin_username name: node_admin_username, type: username, level: C\nAdmin username. Default is dba.\nnode_admin_sudo name: node_admin_sudo, type: enum, level: C\nAdmin user’s sudo privilege level. Default is nopass (password-free sudo).\nOptions:\nnone: No sudo privileges limited: Limited sudo privileges (only allowed to execute specific commands) nopass: Password-free sudo privileges (default, allows all commands without password) all: Full sudo privileges (requires password) Pigsty uses nopass mode by default, allowing admin users to execute any sudo command without password, which is very convenient for automated operations.\nIn production environments with high security requirements, you may need to adjust this parameter to limited or all to restrict admin privileges.\nnode_admin_ssh_exchange name: node_admin_ssh_exchange, type: bool, level: C\nExchange node admin SSH keys between node clusters. Default is true.\nWhen enabled, Pigsty will exchange SSH public keys between members during playbook execution, allowing admin node_admin_username to access each other from different nodes.\nnode_admin_pk_current name: node_admin_pk_current, type: bool, level: C\nAdd current node \u0026 user’s public key to admin account? Default is true.\nWhen enabled, the SSH public key (~/.ssh/id_rsa.pub) of the admin user executing this playbook on the current node will be copied to the target node admin user’s authorized_keys.\nWhen deploying in production environments, please pay attention to this parameter, as it will install the default public key of the user currently executing the command to the admin user on all machines.\nnode_admin_pk_list name: node_admin_pk_list, type: string[], level: C\nList of public keys for admins who can log in. Default is [] (empty array).\nEach array element is a string containing the public key to be written to the admin user’s ~/.ssh/authorized_keys. Users with the corresponding private key can log in as admin.\nWhen deploying in production environments, please pay attention to this parameter and only add trusted keys to this list.\nnode_aliases name: node_aliases, type: dict, level: C\nShell aliases to be written to host’s /etc/profile.d/node.alias.sh. Default is {} (empty dict).\nThis parameter allows you to configure convenient shell aliases for the host’s shell environment. The K:V dict defined here will be written to the target node’s profile.d file in the format alias k=v.\nFor example, the following declares an alias named dp for quickly executing docker compose pull:\nnode_alias: dp: 'docker compose pull' NODE_TIME Configuration related to host time/timezone/NTP/scheduled tasks.\nTime synchronization is very important for database services. Please ensure the system chronyd time service is running properly.\nnode_timezone: '' # setup node timezone, empty string to skip node_ntp_enabled: true # enable chronyd time sync service? node_ntp_servers: # ntp servers in `/etc/chrony.conf` - pool pool.ntp.org iburst node_crontab_overwrite: true # overwrite or append to `/etc/crontab`? node_crontab: [ ] # crontab entries in `/etc/crontab` node_timezone name: node_timezone, type: string, level: C\nSet node timezone. Empty string means skip. Default is empty string, which won’t modify the default timezone (usually UTC).\nWhen using in China region, it’s recommended to set to Asia/Hong_Kong / Asia/Shanghai.\nnode_ntp_enabled name: node_ntp_enabled, type: bool, level: C\nEnable chronyd time sync service? Default is true.\nPigsty will override the node’s /etc/chrony.conf with the NTP server list specified in node_ntp_servers.\nIf your node already has NTP servers configured, you can set this parameter to false to skip time sync configuration.\nnode_ntp_servers name: node_ntp_servers, type: string[], level: C\nNTP server list used in /etc/chrony.conf. Default: [\"pool pool.ntp.org iburst\"]\nThis parameter is an array where each element is a string representing one line of NTP server configuration. Only takes effect when node_ntp_enabled is enabled.\nPigsty uses the global NTP server pool.ntp.org by default. You can modify this parameter according to your network environment, e.g., cn.pool.ntp.org iburst, or internal time services.\nYou can also use the ${admin_ip} placeholder in the configuration to use the time server on the admin node.\nnode_ntp_servers: [ 'pool ${admin_ip} iburst' ] node_crontab_overwrite name: node_crontab_overwrite, type: bool, level: C\nWhen handling scheduled tasks in node_crontab, append or overwrite? Default is true (overwrite).\nIf you want to append scheduled tasks on the node, set this parameter to false, and Pigsty will append rather than overwrite all scheduled tasks on the node’s crontab.\nnode_crontab name: node_crontab, type: string[], level: C\nScheduled tasks defined in node’s /etc/crontab. Default is [] (empty array).\nEach array element is a string representing one scheduled task line. Use standard cron format for definition.\nFor example, the following configuration will execute a system task as root at 3am every day:\nnode_crontab: - '00 03 * * * root /usr/bin/some-system-task' Note: For PostgreSQL backup tasks and other postgres user cron jobs, use the pg_crontab parameter instead of node_crontab. Because node_crontab is written to /etc/crontab during NODE initialization, the postgres user may not exist yet, which will cause cron to report bad username and ignore the entire crontab file.\nWhen node_crontab_overwrite is true (default), the default /etc/crontab will be restored when removing the node.\nNODE_VIP You can bind an optional L2 VIP to a node cluster. This feature is disabled by default. L2 VIP only makes sense for a group of node clusters. The VIP will switch between nodes in the cluster according to configured priorities, ensuring high availability of node services.\nNote that L2 VIP can only be used within the same L2 network segment, which may impose additional restrictions on your network topology. If you don’t want this restriction, you can consider using DNS LB or HAProxy for similar functionality.\nWhen enabling this feature, you need to explicitly assign available vip_address and vip_vrid for this L2 VIP. Users should ensure both are unique within the same network segment.\nNote that NODE VIP is different from PG VIP. PG VIP is a VIP serving PostgreSQL instances, managed by vip-manager and bound to the PG cluster primary. NODE VIP is managed by Keepalived and bound to node clusters. It can be in master-backup mode or load-balanced mode, and both can coexist.\nvip_enabled: false # enable vip on this node cluster? # vip_address: [IDENTITY] # node vip address in ipv4 format, required if vip is enabled # vip_vrid: [IDENTITY] # required, integer, 1-254, should be unique among same VLAN vip_role: backup # optional, `master/backup`, backup by default, use as init role vip_preempt: false # optional, `true/false`, false by default, enable vip preemption vip_interface: eth0 # node vip network interface to listen, `eth0` by default vip_dns_suffix: '' # node vip dns name suffix, empty string by default vip_auth_pass: '' # vrrp auth password, empty to use `\u003ccls\u003e-\u003cvrid\u003e` as default vip_exporter_port: 9650 # keepalived exporter listen port, 9650 by default vip_enabled name: vip_enabled, type: bool, level: C\nEnable an L2 VIP managed by Keepalived on this node cluster? Default is false.\nvip_address name: vip_address, type: ip, level: C\nNode VIP address in IPv4 format (without CIDR suffix). This is a required parameter when vip_enabled is enabled.\nThis parameter has no default value, meaning you must explicitly assign a unique VIP address for the node cluster.\nvip_vrid name: vip_vrid, type: int, level: C\nVRID is a positive integer from 1 to 254 used to identify a VIP in the network. This is a required parameter when vip_enabled is enabled.\nThis parameter has no default value, meaning you must explicitly assign a unique ID within the network segment for the node cluster.\nvip_role name: vip_role, type: enum, level: I\nNode VIP role. Options are master or backup. Default is backup.\nThis parameter value will be set as keepalived’s initial state.\nvip_preempt name: vip_preempt, type: bool, level: C/I\nEnable VIP preemption? Optional parameter. Default is false (no preemption).\nPreemption means when a backup node has higher priority than the currently alive and working master node, should it preempt the VIP?\nvip_interface name: vip_interface, type: string, level: C/I\nNetwork interface for node VIP to listen on. Default is eth0.\nYou should use the same interface name as the node’s primary IP address (the IP address you put in the inventory).\nIf your nodes have different interface names, you can override it at the instance/node level.\nvip_dns_suffix name: vip_dns_suffix, type: string, level: C/I\nDNS name for node cluster L2 VIP. Default is empty string, meaning the cluster name itself is used as the DNS name.\nvip_auth_pass name: vip_auth_pass, type: password, level: C\nVRRP authentication password for keepalived. Default is empty string.\nWhen empty, Pigsty will auto-generate a password using the pattern \u003ccluster_name\u003e-\u003cvrid\u003e. For production environments with security requirements, set an explicit strong password.\nvip_exporter_port name: vip_exporter_port, type: port, level: C/I\nKeepalived exporter listen port. Default is 9650.\nHAPROXY HAProxy is installed and enabled on all nodes by default, exposing services in a manner similar to Kubernetes NodePort.\nThe PGSQL module uses HAProxy for services.\nhaproxy_enabled: true # enable haproxy on this node? haproxy_clean: false # cleanup all existing haproxy config? haproxy_reload: true # reload haproxy after config? haproxy_auth_enabled: true # enable authentication for haproxy admin page haproxy_admin_username: admin # haproxy admin username, `admin` by default haproxy_admin_password: pigsty # haproxy admin password, `pigsty` by default haproxy_exporter_port: 9101 # haproxy admin/exporter port, 9101 by default haproxy_client_timeout: 24h # client connection timeout, 24h by default haproxy_server_timeout: 24h # server connection timeout, 24h by default haproxy_services: [] # list of haproxy services to be exposed on node haproxy_enabled name: haproxy_enabled, type: bool, level: C\nEnable haproxy on this node? Default is true.\nhaproxy_clean name: haproxy_clean, type: bool, level: G/C/A\nCleanup all existing haproxy config? Default is false.\nhaproxy_reload name: haproxy_reload, type: bool, level: A\nReload haproxy after config? Default is true, will reload haproxy after config changes.\nIf you want to check before applying, you can disable this option with command arguments, check, then apply.\nhaproxy_auth_enabled name: haproxy_auth_enabled, type: bool, level: G\nEnable authentication for haproxy admin page. Default is true, which requires HTTP basic auth for the admin page.\nNot recommended to disable authentication, as your traffic control page will be exposed, which is risky.\nhaproxy_admin_username name: haproxy_admin_username, type: username, level: G\nHAProxy admin username. Default is admin.\nhaproxy_admin_password name: haproxy_admin_password, type: password, level: G\nHAProxy admin password. Default is pigsty.\nPLEASE CHANGE THIS PASSWORD IN YOUR PRODUCTION ENVIRONMENT!\nhaproxy_exporter_port name: haproxy_exporter_port, type: port, level: C\nHAProxy traffic management/metrics exposed port. Default is 9101.\nhaproxy_client_timeout name: haproxy_client_timeout, type: interval, level: C\nClient connection timeout. Default is 24h.\nSetting a timeout can avoid long-lived connections that are difficult to clean up. If you really need long connections, you can set it to a longer time.\nhaproxy_server_timeout name: haproxy_server_timeout, type: interval, level: C\nServer connection timeout. Default is 24h.\nSetting a timeout can avoid long-lived connections that are difficult to clean up. If you really need long connections, you can set it to a longer time.\nhaproxy_services name: haproxy_services, type: service[], level: C\nList of services to expose via HAProxy on this node. Default is [] (empty array).\nEach array element is a service definition. Here’s an example service definition:\nhaproxy_services: # list of haproxy service # expose pg-test read only replicas - name: pg-test-ro # [REQUIRED] service name, unique port: 5440 # [REQUIRED] service port, unique ip: \"*\" # [OPTIONAL] service listen addr, \"*\" by default protocol: tcp # [OPTIONAL] service protocol, 'tcp' by default balance: leastconn # [OPTIONAL] load balance algorithm, roundrobin by default (or leastconn) maxconn: 20000 # [OPTIONAL] max allowed front-end connection, 20000 by default default: 'inter 3s fastinter 1s downinter 5s rise 3 fall 3 on-marked-down shutdown-sessions slowstart 30s maxconn 3000 maxqueue 128 weight 100' options: - option httpchk - option http-keep-alive - http-check send meth OPTIONS uri /read-only - http-check expect status 200 servers: - { name: pg-test-1 ,ip: 10.10.10.11 , port: 5432 , options: check port 8008 , backup: true } - { name: pg-test-2 ,ip: 10.10.10.12 , port: 5432 , options: check port 8008 } - { name: pg-test-3 ,ip: 10.10.10.13 , port: 5432 , options: check port 8008 } Each service definition will be rendered to /etc/haproxy/\u003cservice.name\u003e.cfg configuration file and take effect after HAProxy reload.\nNODE_EXPORTER node_exporter_enabled: true # setup node_exporter on this node? node_exporter_port: 9100 # node exporter listen port, 9100 by default node_exporter_options: '--no-collector.softnet --no-collector.nvme --collector.tcpstat --collector.processes' node_exporter_enabled name: node_exporter_enabled, type: bool, level: C\nEnable node metrics collector on current node? Default is true.\nnode_exporter_port name: node_exporter_port, type: port, level: C\nPort used to expose node metrics. Default is 9100.\nnode_exporter_options name: node_exporter_options, type: arg, level: C\nCommand line arguments for node metrics collector. Default value:\n--no-collector.softnet --no-collector.nvme --collector.tcpstat --collector.processes\nThis option enables/disables some metrics collectors. Please adjust according to your needs.\nVECTOR Vector is the log collection component used in Pigsty v4.0. It collects logs from various modules and sends them to VictoriaLogs service on infrastructure nodes.\nINFRA: Infrastructure component logs, collected only on Infra nodes.\nnginx-access: /var/log/nginx/access.log nginx-error: /var/log/nginx/error.log grafana: /var/log/grafana/grafana.log NODES: Host-related logs, collection enabled on all nodes.\nsyslog: /var/log/messages (/var/log/syslog on Debian) dmesg: /var/log/dmesg cron: /var/log/cron PGSQL: PostgreSQL-related logs, collection enabled only when node has PGSQL module configured.\npostgres: /pg/log/postgres/* patroni: /pg/log/patroni.log pgbouncer: /pg/log/pgbouncer/pgbouncer.log pgbackrest: /pg/log/pgbackrest/*.log REDIS: Redis-related logs, collection enabled only when node has REDIS module configured.\nredis: /var/log/redis/*.log Log directories are automatically adjusted according to these parameter configurations: pg_log_dir, patroni_log_dir, pgbouncer_log_dir, pgbackrest_log_dir\nvector_enabled: true # enable vector log collector? vector_clean: false # purge vector data dir during init? vector_data: /data/vector # vector data directory, /data/vector by default vector_port: 9598 # vector metrics port, 9598 by default vector_read_from: beginning # read log from beginning or end vector_log_endpoint: [ infra ] # log endpoint, default send to infra group vector_enabled name: vector_enabled, type: bool, level: C\nEnable Vector log collection service? Default is true.\nVector is the log collection agent used in Pigsty v4.0, replacing Promtail from previous versions. It collects node and service logs and sends them to VictoriaLogs.\nvector_clean name: vector_clean, type: bool, level: G/A\nClean existing data directory when installing Vector? Default is false.\nBy default, it won’t clean. When you choose to clean, Pigsty will remove the existing data directory vector_data when deploying Vector. This means Vector will re-collect all logs on the current node and send them to VictoriaLogs.\nvector_data name: vector_data, type: path, level: C\nVector data directory path. Default is /data/vector.\nVector stores log read offsets and buffered data in this directory.\nvector_port name: vector_port, type: port, level: C\nVector metrics listen port. Default is 9598.\nThis port is used to expose Vector’s own monitoring metrics, which can be scraped by VictoriaMetrics.\nvector_read_from name: vector_read_from, type: enum, level: C\nVector log reading start position. Default is beginning.\nOptions are beginning (start from beginning) or end (start from end). beginning reads the entire content of existing log files, end only reads newly generated logs.\nvector_log_endpoint name: vector_log_endpoint, type: string[], level: C\nLog destination endpoint list. Default is [ infra ].\nSpecifies which node group’s VictoriaLogs service to send logs to. Default sends to nodes in the infra group.\n","categories":["Reference"],"description":"NODE module provides 11 sections with 83 parameters","excerpt":"NODE module provides 11 sections with 83 parameters","ref":"/docs/node/param/","tags":"","title":"Parameters"},{"body":"Pigsty provides two playbooks related to the NODE module:\nnode.yml: Add nodes to Pigsty and configure them to the desired state node-rm.yml: Remove managed nodes from Pigsty Two wrapper scripts are also provided: bin/node-add and bin/node-rm, for quickly invoking these playbooks.\nnode.yml The node.yml playbook for adding nodes to Pigsty contains the following subtasks:\nnode-id : generate node identity node_name : setup hostname node_hosts : setup /etc/hosts records node_resolv : setup DNS resolver /etc/resolv.conf node_firewall : setup firewall \u0026 selinux node_ca : add \u0026 trust CA certificate node_repo : add upstream software repository node_pkg : install rpm/deb packages node_feature : setup numa, grub, static network node_kernel : enable kernel modules node_tune : setup tuned profile node_sysctl : setup additional sysctl parameters node_profile : write /etc/profile.d/node.sh node_ulimit : setup resource limits node_data : setup data directory node_admin : setup admin user and ssh key node_timezone : setup timezone node_ntp : setup NTP server/client node_crontab : add/overwrite crontab tasks node_vip : setup optional L2 VIP for node cluster haproxy : setup haproxy on node to expose services monitor : setup node monitoring: node_exporter \u0026 vector node-rm.yml The node-rm.yml playbook for removing nodes from Pigsty contains the following subtasks:\nregister : remove registration from prometheus \u0026 nginx - prometheus : remove registered prometheus monitoring target - nginx : remove nginx proxy record for haproxy admin vip : remove keepalived \u0026 L2 VIP (if VIP enabled) haproxy : remove haproxy load balancer node_exporter : remove node monitoring: Node Exporter vip_exporter : remove keepalived_exporter (if VIP enabled) vector : remove log collection agent vector profile : remove /etc/profile.d/node.sh Quick Reference # Basic node management ./node.yml -l \u003ccls|ip|group\u003e # Add node to Pigsty ./node-rm.yml -l \u003ccls|ip|group\u003e # Remove node from Pigsty # Node management shortcuts bin/node-add node-test # Initialize node cluster 'node-test' bin/node-add 10.10.10.10 # Initialize node '10.10.10.10' bin/node-rm node-test # Remove node cluster 'node-test' bin/node-rm 10.10.10.10 # Remove node '10.10.10.10' # Node main initialization ./node.yml -t node # Complete node main init (excludes haproxy, monitor) ./node.yml -t haproxy # Setup haproxy on node ./node.yml -t monitor # Setup node monitoring: node_exporter \u0026 vector # VIP management ./node.yml -t node_vip # Setup optional L2 VIP for node cluster ./node.yml -t vip_config,vip_reload # Refresh node L2 VIP configuration # HAProxy management ./node.yml -t haproxy_config,haproxy_reload # Refresh service definitions on node # Registration management ./node.yml -t register_prometheus # Re-register node to Prometheus ./node.yml -t register_nginx # Re-register node haproxy admin to Nginx # Specific tasks ./node.yml -t node-id # Generate node identity ./node.yml -t node_name # Setup hostname ./node.yml -t node_hosts # Setup node /etc/hosts records ./node.yml -t node_resolv # Setup node DNS resolver /etc/resolv.conf ./node.yml -t node_firewall # Setup firewall \u0026 selinux ./node.yml -t node_ca # Setup node CA certificate ./node.yml -t node_repo # Setup node upstream software repository ./node.yml -t node_pkg # Install yum packages on node ./node.yml -t node_feature # Setup numa, grub, static network ./node.yml -t node_kernel # Enable kernel modules ./node.yml -t node_tune # Setup tuned profile ./node.yml -t node_sysctl # Setup additional sysctl parameters ./node.yml -t node_profile # Setup node environment: /etc/profile.d/node.sh ./node.yml -t node_ulimit # Setup node resource limits ./node.yml -t node_data # Setup node primary data directory ./node.yml -t node_admin # Setup admin user and ssh key ./node.yml -t node_timezone # Setup node timezone ./node.yml -t node_ntp # Setup node NTP server/client ./node.yml -t node_crontab # Add/overwrite crontab tasks ","categories":["Task"],"description":"How to use built-in Ansible playbooks to manage NODE clusters, with a quick reference for common commands.","excerpt":"How to use built-in Ansible playbooks to manage NODE clusters, with a …","ref":"/docs/node/playbook/","tags":"","title":"Playbook"},{"body":"Here are common administration operations for the NODE module:\nAdd Node Remove Node Create Admin Bind VIP Add Node Monitoring Other Tasks For more questions, see FAQ: NODE\nAdd Node To add a node to Pigsty, you need passwordless ssh/sudo access to that node.\nYou can also add an entire cluster at once, or use wildcards to match nodes in the inventory to add to Pigsty.\n# ./node.yml -l \u003ccls|ip|group\u003e # actual playbook to add nodes to Pigsty # bin/node-add \u003cselector|ip...\u003e # add node to Pigsty bin/node-add node-test # init node cluster 'node-test' bin/node-add 10.10.10.10 # init node '10.10.10.10' Example: Add three nodes of PG cluster pg-test to Pigsty management\nRemove Node To remove a node from Pigsty, you can use the following commands:\n# ./node-rm.yml -l \u003ccls|ip|group\u003e # actual playbook to remove node from Pigsty # bin/node-rm \u003ccls|ip|selector\u003e ... # remove node from Pigsty bin/node-rm node-test # remove node cluster 'node-test' bin/node-rm 10.10.10.10 # remove node '10.10.10.10' You can also remove an entire cluster at once, or use wildcards to match nodes in the inventory to remove from Pigsty.\nCreate Admin If the current user doesn’t have passwordless ssh/sudo access to the node, you can use another admin user to bootstrap it:\nnode.yml -t node_admin -k -K -e ansible_user=\u003canother admin\u003e # enter ssh/sudo password for another admin to complete this task Bind VIP You can bind an optional L2 VIP on a node cluster using the vip_enabled parameter.\nproxy: hosts: 10.10.10.29: { nodename: proxy-1 } # you can explicitly specify initial VIP role: MASTER / BACKUP 10.10.10.30: { nodename: proxy-2 } # , vip_role: master } vars: node_cluster: proxy vip_enabled: true vip_vrid: 128 vip_address: 10.10.10.99 vip_interface: eth1 ./node.yml -l proxy -t node_vip # enable VIP for the first time ./node.yml -l proxy -t vip_refresh # refresh VIP config (e.g., designate master) Add Node Monitoring If you want to add or reconfigure monitoring on existing nodes, use the following commands:\n./node.yml -t node_exporter,node_register # configure monitoring and register ./node.yml -t vector # configure log collection Other Tasks # Play ./node.yml -t node # complete node initialization (excludes haproxy, monitoring) ./node.yml -t haproxy # setup haproxy on node ./node.yml -t monitor # configure node monitoring: node_exporter \u0026 vector ./node.yml -t node_vip # install, configure, enable L2 VIP for clusters without VIP ./node.yml -t vip_config,vip_reload # refresh node L2 VIP configuration ./node.yml -t haproxy_config,haproxy_reload # refresh service definitions on node ./node.yml -t register_prometheus # re-register node with Prometheus ./node.yml -t register_nginx # re-register node haproxy admin page with Nginx # Task ./node.yml -t node-id # generate node identity ./node.yml -t node_name # setup hostname ./node.yml -t node_hosts # configure node /etc/hosts records ./node.yml -t node_resolv # configure node DNS resolver /etc/resolv.conf ./node.yml -t node_firewall # configure firewall \u0026 selinux ./node.yml -t node_ca # configure node CA certificate ./node.yml -t node_repo # configure node upstream software repository ./node.yml -t node_pkg # install yum packages on node ./node.yml -t node_feature # configure numa, grub, static network, etc. ./node.yml -t node_kernel # configure OS kernel modules ./node.yml -t node_tune # configure tuned profile ./node.yml -t node_sysctl # set additional sysctl parameters ./node.yml -t node_profile # configure node environment variables: /etc/profile.d/node.sh ./node.yml -t node_ulimit # configure node resource limits ./node.yml -t node_data # configure node primary data directory ./node.yml -t node_admin # configure admin user and ssh keys ./node.yml -t node_timezone # configure node timezone ./node.yml -t node_ntp # configure node NTP server/client ./node.yml -t node_crontab # add/overwrite crontab entries ./node.yml -t node_vip # setup optional L2 VIP for node cluster ","categories":["Task"],"description":"Node cluster management SOP - create, destroy, expand, shrink, and handle node/disk failures","excerpt":"Node cluster management SOP - create, destroy, expand, shrink, and …","ref":"/docs/node/admin/","tags":"","title":"Administration"},{"body":"The NODE module in Pigsty provides 6 monitoring dashboards and comprehensive alerting rules.\nDashboards The NODE module provides 6 monitoring dashboards:\nNODE Overview Displays an overall overview of all host nodes in the current environment.\nNODE Cluster Shows detailed monitoring data for a specific host cluster.\nNode Instance Presents detailed monitoring information for a single host node.\nNODE Alert Centrally displays alert information for all hosts in the environment.\nNODE VIP Monitors detailed status of L2 virtual IPs.\nNode Haproxy Tracks the operational status of HAProxy load balancers.\nAlert Rules Pigsty implements the following alerting rules for NODE:\nAvailability Alerts Rule Level Description NodeDown CRIT Node is offline HaproxyDown CRIT HAProxy service is offline VectorDown WARN Log collecting agent offline (Vector) DockerDown WARN Container engine offline KeepalivedDown WARN Keepalived daemon offline CPU Alerts Rule Level Description NodeCpuHigh WARN CPU usage exceeds 70% Scheduling Alerts Rule Level Description NodeLoadHigh WARN Normalized load exceeds 100% Memory Alerts Rule Level Description NodeOutOfMem WARN Available memory less than 10% NodeMemSwapped WARN Swap usage exceeds 1% Filesystem Alerts Rule Level Description NodeFsSpaceFull WARN Disk usage exceeds 90% NodeFsFilesFull WARN Inode usage exceeds 90% NodeFdFull WARN File descriptor usage exceeds 90% Disk Alerts Rule Level Description NodeDiskSlow WARN Read/write latency exceeds 32ms Network Protocol Alerts Rule Level Description NodeTcpErrHigh WARN TCP error rate exceeds 1/min NodeTcpRetransHigh WARN TCP retransmission rate exceeds 1% Time Synchronization Alerts Rule Level Description NodeTimeDrift WARN System time not synchronized ","categories":["Reference"],"description":"Monitor NODE in Pigsty with dashboards and alerting rules","excerpt":"Monitor NODE in Pigsty with dashboards and alerting rules","ref":"/docs/node/monitor/","tags":"","title":"Monitoring"},{"body":"The NODE module has 747 available metrics.\nMetric Name Type Labels Description ALERTS Unknown alertname, ip, level, severity, ins, job, alertstate, category, instance, cls N/A ALERTS_FOR_STATE Unknown alertname, ip, level, severity, ins, job, category, instance, cls N/A deprecated_flags_inuse_total Unknown instance, ins, job, ip, cls N/A go_gc_duration_seconds summary quantile, instance, ins, job, ip, cls A summary of the pause duration of garbage collection cycles. go_gc_duration_seconds_count Unknown instance, ins, job, ip, cls N/A go_gc_duration_seconds_sum Unknown instance, ins, job, ip, cls N/A go_goroutines gauge instance, ins, job, ip, cls Number of goroutines that currently exist. go_info gauge version, instance, ins, job, ip, cls Information about the Go environment. go_memstats_alloc_bytes gauge instance, ins, job, ip, cls Number of bytes allocated and still in use. go_memstats_alloc_bytes_total counter instance, ins, job, ip, cls Total number of bytes allocated, even if freed. go_memstats_buck_hash_sys_bytes gauge instance, ins, job, ip, cls Number of bytes used by the profiling bucket hash table. go_memstats_frees_total counter instance, ins, job, ip, cls Total number of frees. go_memstats_gc_sys_bytes gauge instance, ins, job, ip, cls Number of bytes used for garbage collection system metadata. go_memstats_heap_alloc_bytes gauge instance, ins, job, ip, cls Number of heap bytes allocated and still in use. go_memstats_heap_idle_bytes gauge instance, ins, job, ip, cls Number of heap bytes waiting to be used. go_memstats_heap_inuse_bytes gauge instance, ins, job, ip, cls Number of heap bytes that are in use. go_memstats_heap_objects gauge instance, ins, job, ip, cls Number of allocated objects. go_memstats_heap_released_bytes gauge instance, ins, job, ip, cls Number of heap bytes released to OS. go_memstats_heap_sys_bytes gauge instance, ins, job, ip, cls Number of heap bytes obtained from system. go_memstats_last_gc_time_seconds gauge instance, ins, job, ip, cls Number of seconds since 1970 of last garbage collection. go_memstats_lookups_total counter instance, ins, job, ip, cls Total number of pointer lookups. go_memstats_mallocs_total counter instance, ins, job, ip, cls Total number of mallocs. go_memstats_mcache_inuse_bytes gauge instance, ins, job, ip, cls Number of bytes in use by mcache structures. go_memstats_mcache_sys_bytes gauge instance, ins, job, ip, cls Number of bytes used for mcache structures obtained from system. go_memstats_mspan_inuse_bytes gauge instance, ins, job, ip, cls Number of bytes in use by mspan structures. go_memstats_mspan_sys_bytes gauge instance, ins, job, ip, cls Number of bytes used for mspan structures obtained from system. go_memstats_next_gc_bytes gauge instance, ins, job, ip, cls Number of heap bytes when next garbage collection will take place. go_memstats_other_sys_bytes gauge instance, ins, job, ip, cls Number of bytes used for other system allocations. go_memstats_stack_inuse_bytes gauge instance, ins, job, ip, cls Number of bytes in use by the stack allocator. go_memstats_stack_sys_bytes gauge instance, ins, job, ip, cls Number of bytes obtained from system for stack allocator. go_memstats_sys_bytes gauge instance, ins, job, ip, cls Number of bytes obtained from system. go_threads gauge instance, ins, job, ip, cls Number of OS threads created. haproxy:cls:usage Unknown job, cls N/A haproxy:ins:uptime Unknown instance, ins, job, ip, cls N/A haproxy:ins:usage Unknown instance, ins, job, ip, cls N/A haproxy_backend_active_servers gauge proxy, instance, ins, job, ip, cls Total number of active UP servers with a non-zero weight haproxy_backend_agg_check_status gauge state, proxy, instance, ins, job, ip, cls Backend’s aggregated gauge of servers’ state check status haproxy_backend_agg_server_check_status gauge state, proxy, instance, ins, job, ip, cls [DEPRECATED] Backend’s aggregated gauge of servers’ status haproxy_backend_agg_server_status gauge state, proxy, instance, ins, job, ip, cls Backend’s aggregated gauge of servers’ status haproxy_backend_backup_servers gauge proxy, instance, ins, job, ip, cls Total number of backup UP servers with a non-zero weight haproxy_backend_bytes_in_total counter proxy, instance, ins, job, ip, cls Total number of request bytes since process started haproxy_backend_bytes_out_total counter proxy, instance, ins, job, ip, cls Total number of response bytes since process started haproxy_backend_check_last_change_seconds gauge proxy, instance, ins, job, ip, cls How long ago the last server state changed, in seconds haproxy_backend_check_up_down_total counter proxy, instance, ins, job, ip, cls Total number of failed checks causing UP to DOWN server transitions, per server/backend, since the worker process started haproxy_backend_client_aborts_total counter proxy, instance, ins, job, ip, cls Total number of requests or connections aborted by the client since the worker process started haproxy_backend_connect_time_average_seconds gauge proxy, instance, ins, job, ip, cls Avg. connect time for last 1024 successful connections. haproxy_backend_connection_attempts_total counter proxy, instance, ins, job, ip, cls Total number of outgoing connection attempts on this backend/server since the worker process started haproxy_backend_connection_errors_total counter proxy, instance, ins, job, ip, cls Total number of failed connections to server since the worker process started haproxy_backend_connection_reuses_total counter proxy, instance, ins, job, ip, cls Total number of reused connection on this backend/server since the worker process started haproxy_backend_current_queue gauge proxy, instance, ins, job, ip, cls Number of current queued connections haproxy_backend_current_sessions gauge proxy, instance, ins, job, ip, cls Number of current sessions on the frontend, backend or server haproxy_backend_downtime_seconds_total counter proxy, instance, ins, job, ip, cls Total time spent in DOWN state, for server or backend haproxy_backend_failed_header_rewriting_total counter proxy, instance, ins, job, ip, cls Total number of failed HTTP header rewrites since the worker process started haproxy_backend_http_cache_hits_total counter proxy, instance, ins, job, ip, cls Total number of HTTP requests not found in the cache on this frontend/backend since the worker process started haproxy_backend_http_cache_lookups_total counter proxy, instance, ins, job, ip, cls Total number of HTTP requests looked up in the cache on this frontend/backend since the worker process started haproxy_backend_http_comp_bytes_bypassed_total counter proxy, instance, ins, job, ip, cls Total number of bytes that bypassed HTTP compression for this object since the worker process started (CPU/memory/bandwidth limitation) haproxy_backend_http_comp_bytes_in_total counter proxy, instance, ins, job, ip, cls Total number of bytes submitted to the HTTP compressor for this object since the worker process started haproxy_backend_http_comp_bytes_out_total counter proxy, instance, ins, job, ip, cls Total number of bytes emitted by the HTTP compressor for this object since the worker process started haproxy_backend_http_comp_responses_total counter proxy, instance, ins, job, ip, cls Total number of HTTP responses that were compressed for this object since the worker process started haproxy_backend_http_requests_total counter proxy, instance, ins, job, ip, cls Total number of HTTP requests processed by this object since the worker process started haproxy_backend_http_responses_total counter ip, proxy, ins, code, job, instance, cls Total number of HTTP responses with status 100-199 returned by this object since the worker process started haproxy_backend_internal_errors_total counter proxy, instance, ins, job, ip, cls Total number of internal errors since process started haproxy_backend_last_session_seconds gauge proxy, instance, ins, job, ip, cls How long ago some traffic was seen on this object on this worker process, in seconds haproxy_backend_limit_sessions gauge proxy, instance, ins, job, ip, cls Frontend/listener/server’s maxconn, backend’s fullconn haproxy_backend_loadbalanced_total counter proxy, instance, ins, job, ip, cls Total number of requests routed by load balancing since the worker process started (ignores queue pop and stickiness) haproxy_backend_max_connect_time_seconds gauge proxy, instance, ins, job, ip, cls Maximum observed time spent waiting for a connection to complete haproxy_backend_max_queue gauge proxy, instance, ins, job, ip, cls Highest value of queued connections encountered since process started haproxy_backend_max_queue_time_seconds gauge proxy, instance, ins, job, ip, cls Maximum observed time spent in the queue haproxy_backend_max_response_time_seconds gauge proxy, instance, ins, job, ip, cls Maximum observed time spent waiting for a server response haproxy_backend_max_session_rate gauge proxy, instance, ins, job, ip, cls Highest value of sessions per second observed since the worker process started haproxy_backend_max_sessions gauge proxy, instance, ins, job, ip, cls Highest value of current sessions encountered since process started haproxy_backend_max_total_time_seconds gauge proxy, instance, ins, job, ip, cls Maximum observed total request+response time (request+queue+connect+response+processing) haproxy_backend_queue_time_average_seconds gauge proxy, instance, ins, job, ip, cls Avg. queue time for last 1024 successful connections. haproxy_backend_redispatch_warnings_total counter proxy, instance, ins, job, ip, cls Total number of server redispatches due to connection failures since the worker process started haproxy_backend_requests_denied_total counter proxy, instance, ins, job, ip, cls Total number of denied requests since process started haproxy_backend_response_errors_total counter proxy, instance, ins, job, ip, cls Total number of invalid responses since the worker process started haproxy_backend_response_time_average_seconds gauge proxy, instance, ins, job, ip, cls Avg. response time for last 1024 successful connections. haproxy_backend_responses_denied_total counter proxy, instance, ins, job, ip, cls Total number of denied responses since process started haproxy_backend_retry_warnings_total counter proxy, instance, ins, job, ip, cls Total number of server connection retries since the worker process started haproxy_backend_server_aborts_total counter proxy, instance, ins, job, ip, cls Total number of requests or connections aborted by the server since the worker process started haproxy_backend_sessions_total counter proxy, instance, ins, job, ip, cls Total number of sessions since process started haproxy_backend_status gauge state, proxy, instance, ins, job, ip, cls Current status of the service, per state label value. haproxy_backend_total_time_average_seconds gauge proxy, instance, ins, job, ip, cls Avg. total time for last 1024 successful connections. haproxy_backend_uweight gauge proxy, instance, ins, job, ip, cls Server’s user weight, or sum of active servers’ user weights for a backend haproxy_backend_weight gauge proxy, instance, ins, job, ip, cls Server’s effective weight, or sum of active servers’ effective weights for a backend haproxy_frontend_bytes_in_total counter proxy, instance, ins, job, ip, cls Total number of request bytes since process started haproxy_frontend_bytes_out_total counter proxy, instance, ins, job, ip, cls Total number of response bytes since process started haproxy_frontend_connections_rate_max gauge proxy, instance, ins, job, ip, cls Highest value of connections per second observed since the worker process started haproxy_frontend_connections_total counter proxy, instance, ins, job, ip, cls Total number of new connections accepted on this frontend since the worker process started haproxy_frontend_current_sessions gauge proxy, instance, ins, job, ip, cls Number of current sessions on the frontend, backend or server haproxy_frontend_denied_connections_total counter proxy, instance, ins, job, ip, cls Total number of incoming connections blocked on a listener/frontend by a tcp-request connection rule since the worker process started haproxy_frontend_denied_sessions_total counter proxy, instance, ins, job, ip, cls Total number of incoming sessions blocked on a listener/frontend by a tcp-request connection rule since the worker process started haproxy_frontend_failed_header_rewriting_total counter proxy, instance, ins, job, ip, cls Total number of failed HTTP header rewrites since the worker process started haproxy_frontend_http_cache_hits_total counter proxy, instance, ins, job, ip, cls Total number of HTTP requests not found in the cache on this frontend/backend since the worker process started haproxy_frontend_http_cache_lookups_total counter proxy, instance, ins, job, ip, cls Total number of HTTP requests looked up in the cache on this frontend/backend since the worker process started haproxy_frontend_http_comp_bytes_bypassed_total counter proxy, instance, ins, job, ip, cls Total number of bytes that bypassed HTTP compression for this object since the worker process started (CPU/memory/bandwidth limitation) haproxy_frontend_http_comp_bytes_in_total counter proxy, instance, ins, job, ip, cls Total number of bytes submitted to the HTTP compressor for this object since the worker process started haproxy_frontend_http_comp_bytes_out_total counter proxy, instance, ins, job, ip, cls Total number of bytes emitted by the HTTP compressor for this object since the worker process started haproxy_frontend_http_comp_responses_total counter proxy, instance, ins, job, ip, cls Total number of HTTP responses that were compressed for this object since the worker process started haproxy_frontend_http_requests_rate_max gauge proxy, instance, ins, job, ip, cls Highest value of http requests observed since the worker process started haproxy_frontend_http_requests_total counter proxy, instance, ins, job, ip, cls Total number of HTTP requests processed by this object since the worker process started haproxy_frontend_http_responses_total counter ip, proxy, ins, code, job, instance, cls Total number of HTTP responses with status 100-199 returned by this object since the worker process started haproxy_frontend_intercepted_requests_total counter proxy, instance, ins, job, ip, cls Total number of HTTP requests intercepted on the frontend (redirects/stats/services) since the worker process started haproxy_frontend_internal_errors_total counter proxy, instance, ins, job, ip, cls Total number of internal errors since process started haproxy_frontend_limit_session_rate gauge proxy, instance, ins, job, ip, cls Limit on the number of sessions accepted in a second (frontend only, ‘rate-limit sessions’ setting) haproxy_frontend_limit_sessions gauge proxy, instance, ins, job, ip, cls Frontend/listener/server’s maxconn, backend’s fullconn haproxy_frontend_max_session_rate gauge proxy, instance, ins, job, ip, cls Highest value of sessions per second observed since the worker process started haproxy_frontend_max_sessions gauge proxy, instance, ins, job, ip, cls Highest value of current sessions encountered since process started haproxy_frontend_request_errors_total counter proxy, instance, ins, job, ip, cls Total number of invalid requests since process started haproxy_frontend_requests_denied_total counter proxy, instance, ins, job, ip, cls Total number of denied requests since process started haproxy_frontend_responses_denied_total counter proxy, instance, ins, job, ip, cls Total number of denied responses since process started haproxy_frontend_sessions_total counter proxy, instance, ins, job, ip, cls Total number of sessions since process started haproxy_frontend_status gauge state, proxy, instance, ins, job, ip, cls Current status of the service, per state label value. haproxy_process_active_peers gauge instance, ins, job, ip, cls Current number of verified active peers connections on the current worker process haproxy_process_build_info gauge version, instance, ins, job, ip, cls Build info haproxy_process_busy_polling_enabled gauge instance, ins, job, ip, cls 1 if busy-polling is currently in use on the worker process, otherwise zero (config.busy-polling) haproxy_process_bytes_out_rate gauge instance, ins, job, ip, cls Number of bytes emitted by current worker process over the last second haproxy_process_bytes_out_total counter instance, ins, job, ip, cls Total number of bytes emitted by current worker process since started haproxy_process_connected_peers gauge instance, ins, job, ip, cls Current number of peers having passed the connection step on the current worker process haproxy_process_connections_total counter instance, ins, job, ip, cls Total number of connections on this worker process since started haproxy_process_current_backend_ssl_key_rate gauge instance, ins, job, ip, cls Number of SSL keys created on backends in this worker process over the last second haproxy_process_current_connection_rate gauge instance, ins, job, ip, cls Number of front connections created on this worker process over the last second haproxy_process_current_connections gauge instance, ins, job, ip, cls Current number of connections on this worker process haproxy_process_current_frontend_ssl_key_rate gauge instance, ins, job, ip, cls Number of SSL keys created on frontends in this worker process over the last second haproxy_process_current_run_queue gauge instance, ins, job, ip, cls Total number of active tasks+tasklets in the current worker process haproxy_process_current_session_rate gauge instance, ins, job, ip, cls Number of sessions created on this worker process over the last second haproxy_process_current_ssl_connections gauge instance, ins, job, ip, cls Current number of SSL endpoints on this worker process (front+back) haproxy_process_current_ssl_rate gauge instance, ins, job, ip, cls Number of SSL connections created on this worker process over the last second haproxy_process_current_tasks gauge instance, ins, job, ip, cls Total number of tasks in the current worker process (active + sleeping) haproxy_process_current_zlib_memory gauge instance, ins, job, ip, cls Amount of memory currently used by HTTP compression on the current worker process (in bytes) haproxy_process_dropped_logs_total counter instance, ins, job, ip, cls Total number of dropped logs for current worker process since started haproxy_process_failed_resolutions counter instance, ins, job, ip, cls Total number of failed DNS resolutions in current worker process since started haproxy_process_frontend_ssl_reuse gauge instance, ins, job, ip, cls Percent of frontend SSL connections which did not require a new key haproxy_process_hard_max_connections gauge instance, ins, job, ip, cls Hard limit on the number of per-process connections (imposed by Memmax_MB or Ulimit-n) haproxy_process_http_comp_bytes_in_total counter instance, ins, job, ip, cls Number of bytes submitted to the HTTP compressor in this worker process over the last second haproxy_process_http_comp_bytes_out_total counter instance, ins, job, ip, cls Number of bytes emitted by the HTTP compressor in this worker process over the last second haproxy_process_idle_time_percent gauge instance, ins, job, ip, cls Percentage of last second spent waiting in the current worker thread haproxy_process_jobs gauge instance, ins, job, ip, cls Current number of active jobs on the current worker process (frontend connections, master connections, listeners) haproxy_process_limit_connection_rate gauge instance, ins, job, ip, cls Hard limit for ConnRate (global.maxconnrate) haproxy_process_limit_http_comp gauge instance, ins, job, ip, cls Limit of CompressBpsOut beyond which HTTP compression is automatically disabled haproxy_process_limit_session_rate gauge instance, ins, job, ip, cls Hard limit for SessRate (global.maxsessrate) haproxy_process_limit_ssl_rate gauge instance, ins, job, ip, cls Hard limit for SslRate (global.maxsslrate) haproxy_process_listeners gauge instance, ins, job, ip, cls Current number of active listeners on the current worker process haproxy_process_max_backend_ssl_key_rate gauge instance, ins, job, ip, cls Highest SslBackendKeyRate reached on this worker process since started (in SSL keys per second) haproxy_process_max_connection_rate gauge instance, ins, job, ip, cls Highest ConnRate reached on this worker process since started (in connections per second) haproxy_process_max_connections gauge instance, ins, job, ip, cls Hard limit on the number of per-process connections (configured or imposed by Ulimit-n) haproxy_process_max_fds gauge instance, ins, job, ip, cls Hard limit on the number of per-process file descriptors haproxy_process_max_frontend_ssl_key_rate gauge instance, ins, job, ip, cls Highest SslFrontendKeyRate reached on this worker process since started (in SSL keys per second) haproxy_process_max_memory_bytes gauge instance, ins, job, ip, cls Worker process’s hard limit on memory usage in byes (-m on command line) haproxy_process_max_pipes gauge instance, ins, job, ip, cls Hard limit on the number of pipes for splicing, 0=unlimited haproxy_process_max_session_rate gauge instance, ins, job, ip, cls Highest SessRate reached on this worker process since started (in sessions per second) haproxy_process_max_sockets gauge instance, ins, job, ip, cls Hard limit on the number of per-process sockets haproxy_process_max_ssl_connections gauge instance, ins, job, ip, cls Hard limit on the number of per-process SSL endpoints (front+back), 0=unlimited haproxy_process_max_ssl_rate gauge instance, ins, job, ip, cls Highest SslRate reached on this worker process since started (in connections per second) haproxy_process_max_zlib_memory gauge instance, ins, job, ip, cls Limit on the amount of memory used by HTTP compression above which it is automatically disabled (in bytes, see global.maxzlibmem) haproxy_process_nbproc gauge instance, ins, job, ip, cls Number of started worker processes (historical, always 1) haproxy_process_nbthread gauge instance, ins, job, ip, cls Number of started threads (global.nbthread) haproxy_process_pipes_free_total counter instance, ins, job, ip, cls Current number of allocated and available pipes in this worker process haproxy_process_pipes_used_total counter instance, ins, job, ip, cls Current number of pipes in use in this worker process haproxy_process_pool_allocated_bytes gauge instance, ins, job, ip, cls Amount of memory allocated in pools (in bytes) haproxy_process_pool_failures_total counter instance, ins, job, ip, cls Number of failed pool allocations since this worker was started haproxy_process_pool_used_bytes gauge instance, ins, job, ip, cls Amount of pool memory currently used (in bytes) haproxy_process_recv_logs_total counter instance, ins, job, ip, cls Total number of log messages received by log-forwarding listeners on this worker process since started haproxy_process_relative_process_id gauge instance, ins, job, ip, cls Relative worker process number (1) haproxy_process_requests_total counter instance, ins, job, ip, cls Total number of requests on this worker process since started haproxy_process_spliced_bytes_out_total counter instance, ins, job, ip, cls Total number of bytes emitted by current worker process through a kernel pipe since started haproxy_process_ssl_cache_lookups_total counter instance, ins, job, ip, cls Total number of SSL session ID lookups in the SSL session cache on this worker since started haproxy_process_ssl_cache_misses_total counter instance, ins, job, ip, cls Total number of SSL session ID lookups that didn’t find a session in the SSL session cache on this worker since started haproxy_process_ssl_connections_total counter instance, ins, job, ip, cls Total number of SSL endpoints on this worker process since started (front+back) haproxy_process_start_time_seconds gauge instance, ins, job, ip, cls Start time in seconds haproxy_process_stopping gauge instance, ins, job, ip, cls 1 if the worker process is currently stopping, otherwise zero haproxy_process_unstoppable_jobs gauge instance, ins, job, ip, cls Current number of unstoppable jobs on the current worker process (master connections) haproxy_process_uptime_seconds gauge instance, ins, job, ip, cls How long ago this worker process was started (seconds) haproxy_server_bytes_in_total counter proxy, instance, ins, job, server, ip, cls Total number of request bytes since process started haproxy_server_bytes_out_total counter proxy, instance, ins, job, server, ip, cls Total number of response bytes since process started haproxy_server_check_code gauge proxy, instance, ins, job, server, ip, cls layer5-7 code, if available of the last health check. haproxy_server_check_duration_seconds gauge proxy, instance, ins, job, server, ip, cls Total duration of the latest server health check, in seconds. haproxy_server_check_failures_total counter proxy, instance, ins, job, server, ip, cls Total number of failed individual health checks per server/backend, since the worker process started haproxy_server_check_last_change_seconds gauge proxy, instance, ins, job, server, ip, cls How long ago the last server state changed, in seconds haproxy_server_check_status gauge state, proxy, instance, ins, job, server, ip, cls Status of last health check, per state label value. haproxy_server_check_up_down_total counter proxy, instance, ins, job, server, ip, cls Total number of failed checks causing UP to DOWN server transitions, per server/backend, since the worker process started haproxy_server_client_aborts_total counter proxy, instance, ins, job, server, ip, cls Total number of requests or connections aborted by the client since the worker process started haproxy_server_connect_time_average_seconds gauge proxy, instance, ins, job, server, ip, cls Avg. connect time for last 1024 successful connections. haproxy_server_connection_attempts_total counter proxy, instance, ins, job, server, ip, cls Total number of outgoing connection attempts on this backend/server since the worker process started haproxy_server_connection_errors_total counter proxy, instance, ins, job, server, ip, cls Total number of failed connections to server since the worker process started haproxy_server_connection_reuses_total counter proxy, instance, ins, job, server, ip, cls Total number of reused connection on this backend/server since the worker process started haproxy_server_current_queue gauge proxy, instance, ins, job, server, ip, cls Number of current queued connections haproxy_server_current_sessions gauge proxy, instance, ins, job, server, ip, cls Number of current sessions on the frontend, backend or server haproxy_server_current_throttle gauge proxy, instance, ins, job, server, ip, cls Throttling ratio applied to a server’s maxconn and weight during the slowstart period (0 to 100%) haproxy_server_downtime_seconds_total counter proxy, instance, ins, job, server, ip, cls Total time spent in DOWN state, for server or backend haproxy_server_failed_header_rewriting_total counter proxy, instance, ins, job, server, ip, cls Total number of failed HTTP header rewrites since the worker process started haproxy_server_idle_connections_current gauge proxy, instance, ins, job, server, ip, cls Current number of idle connections available for reuse on this server haproxy_server_idle_connections_limit gauge proxy, instance, ins, job, server, ip, cls Limit on the number of available idle connections on this server (server ‘pool_max_conn’ directive) haproxy_server_internal_errors_total counter proxy, instance, ins, job, server, ip, cls Total number of internal errors since process started haproxy_server_last_session_seconds gauge proxy, instance, ins, job, server, ip, cls How long ago some traffic was seen on this object on this worker process, in seconds haproxy_server_limit_sessions gauge proxy, instance, ins, job, server, ip, cls Frontend/listener/server’s maxconn, backend’s fullconn haproxy_server_loadbalanced_total counter proxy, instance, ins, job, server, ip, cls Total number of requests routed by load balancing since the worker process started (ignores queue pop and stickiness) haproxy_server_max_connect_time_seconds gauge proxy, instance, ins, job, server, ip, cls Maximum observed time spent waiting for a connection to complete haproxy_server_max_queue gauge proxy, instance, ins, job, server, ip, cls Highest value of queued connections encountered since process started haproxy_server_max_queue_time_seconds gauge proxy, instance, ins, job, server, ip, cls Maximum observed time spent in the queue haproxy_server_max_response_time_seconds gauge proxy, instance, ins, job, server, ip, cls Maximum observed time spent waiting for a server response haproxy_server_max_session_rate gauge proxy, instance, ins, job, server, ip, cls Highest value of sessions per second observed since the worker process started haproxy_server_max_sessions gauge proxy, instance, ins, job, server, ip, cls Highest value of current sessions encountered since process started haproxy_server_max_total_time_seconds gauge proxy, instance, ins, job, server, ip, cls Maximum observed total request+response time (request+queue+connect+response+processing) haproxy_server_need_connections_current gauge proxy, instance, ins, job, server, ip, cls Estimated needed number of connections haproxy_server_queue_limit gauge proxy, instance, ins, job, server, ip, cls Limit on the number of connections in queue, for servers only (maxqueue argument) haproxy_server_queue_time_average_seconds gauge proxy, instance, ins, job, server, ip, cls Avg. queue time for last 1024 successful connections. haproxy_server_redispatch_warnings_total counter proxy, instance, ins, job, server, ip, cls Total number of server redispatches due to connection failures since the worker process started haproxy_server_response_errors_total counter proxy, instance, ins, job, server, ip, cls Total number of invalid responses since the worker process started haproxy_server_response_time_average_seconds gauge proxy, instance, ins, job, server, ip, cls Avg. response time for last 1024 successful connections. haproxy_server_responses_denied_total counter proxy, instance, ins, job, server, ip, cls Total number of denied responses since process started haproxy_server_retry_warnings_total counter proxy, instance, ins, job, server, ip, cls Total number of server connection retries since the worker process started haproxy_server_safe_idle_connections_current gauge proxy, instance, ins, job, server, ip, cls Current number of safe idle connections haproxy_server_server_aborts_total counter proxy, instance, ins, job, server, ip, cls Total number of requests or connections aborted by the server since the worker process started haproxy_server_sessions_total counter proxy, instance, ins, job, server, ip, cls Total number of sessions since process started haproxy_server_status gauge state, proxy, instance, ins, job, server, ip, cls Current status of the service, per state label value. haproxy_server_total_time_average_seconds gauge proxy, instance, ins, job, server, ip, cls Avg. total time for last 1024 successful connections. haproxy_server_unsafe_idle_connections_current gauge proxy, instance, ins, job, server, ip, cls Current number of unsafe idle connections haproxy_server_used_connections_current gauge proxy, instance, ins, job, server, ip, cls Current number of connections in use haproxy_server_uweight gauge proxy, instance, ins, job, server, ip, cls Server’s user weight, or sum of active servers’ user weights for a backend haproxy_server_weight gauge proxy, instance, ins, job, server, ip, cls Server’s effective weight, or sum of active servers’ effective weights for a backend haproxy_up Unknown instance, ins, job, ip, cls N/A inflight_requests gauge instance, ins, job, route, ip, cls, method Current number of inflight requests. jaeger_tracer_baggage_restrictions_updates_total Unknown instance, ins, job, result, ip, cls N/A jaeger_tracer_baggage_truncations_total Unknown instance, ins, job, ip, cls N/A jaeger_tracer_baggage_updates_total Unknown instance, ins, job, result, ip, cls N/A jaeger_tracer_finished_spans_total Unknown instance, ins, job, sampled, ip, cls N/A jaeger_tracer_reporter_queue_length gauge instance, ins, job, ip, cls Current number of spans in the reporter queue jaeger_tracer_reporter_spans_total Unknown instance, ins, job, result, ip, cls N/A jaeger_tracer_sampler_queries_total Unknown instance, ins, job, result, ip, cls N/A jaeger_tracer_sampler_updates_total Unknown instance, ins, job, result, ip, cls N/A jaeger_tracer_span_context_decoding_errors_total Unknown instance, ins, job, ip, cls N/A jaeger_tracer_started_spans_total Unknown instance, ins, job, sampled, ip, cls N/A jaeger_tracer_throttled_debug_spans_total Unknown instance, ins, job, ip, cls N/A jaeger_tracer_throttler_updates_total Unknown instance, ins, job, result, ip, cls N/A jaeger_tracer_traces_total Unknown state, instance, ins, job, sampled, ip, cls N/A loki_experimental_features_in_use_total Unknown instance, ins, job, ip, cls N/A loki_internal_log_messages_total Unknown level, instance, ins, job, ip, cls N/A loki_log_flushes_bucket Unknown instance, ins, job, le, ip, cls N/A loki_log_flushes_count Unknown instance, ins, job, ip, cls N/A loki_log_flushes_sum Unknown instance, ins, job, ip, cls N/A loki_log_messages_total Unknown level, instance, ins, job, ip, cls N/A loki_logql_querystats_duplicates_total Unknown instance, ins, job, ip, cls N/A loki_logql_querystats_ingester_sent_lines_total Unknown instance, ins, job, ip, cls N/A loki_querier_index_cache_corruptions_total Unknown instance, ins, job, ip, cls N/A loki_querier_index_cache_encode_errors_total Unknown instance, ins, job, ip, cls N/A loki_querier_index_cache_gets_total Unknown instance, ins, job, ip, cls N/A loki_querier_index_cache_hits_total Unknown instance, ins, job, ip, cls N/A loki_querier_index_cache_puts_total Unknown instance, ins, job, ip, cls N/A net_conntrack_dialer_conn_attempted_total counter ip, ins, job, instance, cls, dialer_name Total number of connections attempted by the given dialer a given name. net_conntrack_dialer_conn_closed_total counter ip, ins, job, instance, cls, dialer_name Total number of connections closed which originated from the dialer of a given name. net_conntrack_dialer_conn_established_total counter ip, ins, job, instance, cls, dialer_name Total number of connections successfully established by the given dialer a given name. net_conntrack_dialer_conn_failed_total counter ip, ins, job, reason, instance, cls, dialer_name Total number of connections failed to dial by the dialer a given name. node:cls:avail_bytes Unknown job, cls N/A node:cls:cpu_count Unknown job, cls N/A node:cls:cpu_usage Unknown job, cls N/A node:cls:cpu_usage_15m Unknown job, cls N/A node:cls:cpu_usage_1m Unknown job, cls N/A node:cls:cpu_usage_5m Unknown job, cls N/A node:cls:disk_io_bytes_rate1m Unknown job, cls N/A node:cls:disk_iops_1m Unknown job, cls N/A node:cls:disk_mreads_rate1m Unknown job, cls N/A node:cls:disk_mreads_ratio1m Unknown job, cls N/A node:cls:disk_mwrites_rate1m Unknown job, cls N/A node:cls:disk_mwrites_ratio1m Unknown job, cls N/A node:cls:disk_read_bytes_rate1m Unknown job, cls N/A node:cls:disk_reads_rate1m Unknown job, cls N/A node:cls:disk_write_bytes_rate1m Unknown job, cls N/A node:cls:disk_writes_rate1m Unknown job, cls N/A node:cls:free_bytes Unknown job, cls N/A node:cls:mem_usage Unknown job, cls N/A node:cls:network_io_bytes_rate1m Unknown job, cls N/A node:cls:network_rx_bytes_rate1m Unknown job, cls N/A node:cls:network_rx_pps1m Unknown job, cls N/A node:cls:network_tx_bytes_rate1m Unknown job, cls N/A node:cls:network_tx_pps1m Unknown job, cls N/A node:cls:size_bytes Unknown job, cls N/A node:cls:space_usage Unknown job, cls N/A node:cls:space_usage_max Unknown job, cls N/A node:cls:stdload1 Unknown job, cls N/A node:cls:stdload15 Unknown job, cls N/A node:cls:stdload5 Unknown job, cls N/A node:cls:time_drift_max Unknown job, cls N/A node:cpu:idle_time_irate1m Unknown ip, ins, job, cpu, instance, cls N/A node:cpu:sched_timeslices_rate1m Unknown ip, ins, job, cpu, instance, cls N/A node:cpu:sched_wait_rate1m Unknown ip, ins, job, cpu, instance, cls N/A node:cpu:time_irate1m Unknown ip, mode, ins, job, cpu, instance, cls N/A node:cpu:total_time_irate1m Unknown ip, ins, job, cpu, instance, cls N/A node:cpu:usage Unknown ip, ins, job, cpu, instance, cls N/A node:cpu:usage_avg15m Unknown ip, ins, job, cpu, instance, cls N/A node:cpu:usage_avg1m Unknown ip, ins, job, cpu, instance, cls N/A node:cpu:usage_avg5m Unknown ip, ins, job, cpu, instance, cls N/A node:dev:disk_avg_queue_size Unknown ip, device, ins, job, instance, cls N/A node:dev:disk_io_batch_1m Unknown ip, device, ins, job, instance, cls N/A node:dev:disk_io_bytes_rate1m Unknown ip, device, ins, job, instance, cls N/A node:dev:disk_io_rt_1m Unknown ip, device, ins, job, instance, cls N/A node:dev:disk_io_time_rate1m Unknown ip, device, ins, job, instance, cls N/A node:dev:disk_iops_1m Unknown ip, device, ins, job, instance, cls N/A node:dev:disk_mreads_rate1m Unknown ip, device, ins, job, instance, cls N/A node:dev:disk_mreads_ratio1m Unknown ip, device, ins, job, instance, cls N/A node:dev:disk_mwrites_rate1m Unknown ip, device, ins, job, instance, cls N/A node:dev:disk_mwrites_ratio1m Unknown ip, device, ins, job, instance, cls N/A node:dev:disk_read_batch_1m Unknown ip, device, ins, job, instance, cls N/A node:dev:disk_read_bytes_rate1m Unknown ip, device, ins, job, instance, cls N/A node:dev:disk_read_rt_1m Unknown ip, device, ins, job, instance, cls N/A node:dev:disk_read_time_rate1m Unknown ip, device, ins, job, instance, cls N/A node:dev:disk_reads_rate1m Unknown ip, device, ins, job, instance, cls N/A node:dev:disk_util_1m Unknown ip, device, ins, job, instance, cls N/A node:dev:disk_write_batch_1m Unknown ip, device, ins, job, instance, cls N/A node:dev:disk_write_bytes_rate1m Unknown ip, device, ins, job, instance, cls N/A node:dev:disk_write_rt_1m Unknown ip, device, ins, job, instance, cls N/A node:dev:disk_write_time_rate1m Unknown ip, device, ins, job, instance, cls N/A node:dev:disk_writes_rate1m Unknown ip, device, ins, job, instance, cls N/A node:dev:network_io_bytes_rate1m Unknown ip, device, ins, job, instance, cls N/A node:dev:network_rx_bytes_rate1m Unknown ip, device, ins, job, instance, cls N/A node:dev:network_rx_pps1m Unknown ip, device, ins, job, instance, cls N/A node:dev:network_tx_bytes_rate1m Unknown ip, device, ins, job, instance, cls N/A node:dev:network_tx_pps1m Unknown ip, device, ins, job, instance, cls N/A node:env:avail_bytes Unknown job N/A node:env:cpu_count Unknown job N/A node:env:cpu_usage Unknown job N/A node:env:cpu_usage_15m Unknown job N/A node:env:cpu_usage_1m Unknown job N/A node:env:cpu_usage_5m Unknown job N/A node:env:device_space_usage_max Unknown device, mountpoint, job, fstype N/A node:env:free_bytes Unknown job N/A node:env:mem_avail Unknown job N/A node:env:mem_total Unknown job N/A node:env:mem_usage Unknown job N/A node:env:size_bytes Unknown job N/A node:env:space_usage Unknown job N/A node:env:stdload1 Unknown job N/A node:env:stdload15 Unknown job N/A node:env:stdload5 Unknown job N/A node:fs:avail_bytes Unknown ip, device, mountpoint, ins, cls, job, instance, fstype N/A node:fs:free_bytes Unknown ip, device, mountpoint, ins, cls, job, instance, fstype N/A node:fs:inode_free Unknown ip, device, mountpoint, ins, cls, job, instance, fstype N/A node:fs:inode_total Unknown ip, device, mountpoint, ins, cls, job, instance, fstype N/A node:fs:inode_usage Unknown ip, device, mountpoint, ins, cls, job, instance, fstype N/A node:fs:inode_used Unknown ip, device, mountpoint, ins, cls, job, instance, fstype N/A node:fs:size_bytes Unknown ip, device, mountpoint, ins, cls, job, instance, fstype N/A node:fs:space_deriv1h Unknown ip, device, mountpoint, ins, cls, job, instance, fstype N/A node:fs:space_exhaust Unknown ip, device, mountpoint, ins, cls, job, instance, fstype N/A node:fs:space_predict_1d Unknown ip, device, mountpoint, ins, cls, job, instance, fstype N/A node:fs:space_usage Unknown ip, device, mountpoint, ins, cls, job, instance, fstype N/A node:ins Unknown id, ip, ins, job, nodename, instance, cls N/A node:ins:avail_bytes Unknown instance, ins, job, ip, cls N/A node:ins:cpu_count Unknown instance, ins, job, ip, cls N/A node:ins:cpu_usage Unknown instance, ins, job, ip, cls N/A node:ins:cpu_usage_15m Unknown instance, ins, job, ip, cls N/A node:ins:cpu_usage_1m Unknown instance, ins, job, ip, cls N/A node:ins:cpu_usage_5m Unknown instance, ins, job, ip, cls N/A node:ins:ctx_switch_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:disk_io_bytes_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:disk_iops_1m Unknown instance, ins, job, ip, cls N/A node:ins:disk_mreads_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:disk_mreads_ratio1m Unknown instance, ins, job, ip, cls N/A node:ins:disk_mwrites_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:disk_mwrites_ratio1m Unknown instance, ins, job, ip, cls N/A node:ins:disk_read_bytes_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:disk_reads_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:disk_write_bytes_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:disk_writes_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:fd_alloc_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:fd_usage Unknown instance, ins, job, ip, cls N/A node:ins:forks_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:free_bytes Unknown instance, ins, job, ip, cls N/A node:ins:inode_usage Unknown instance, ins, job, ip, cls N/A node:ins:interrupt_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:mem_avail Unknown instance, ins, job, ip, cls N/A node:ins:mem_commit_ratio Unknown instance, ins, job, ip, cls N/A node:ins:mem_kernel Unknown instance, ins, job, ip, cls N/A node:ins:mem_rss Unknown instance, ins, job, ip, cls N/A node:ins:mem_usage Unknown instance, ins, job, ip, cls N/A node:ins:network_io_bytes_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:network_rx_bytes_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:network_rx_pps1m Unknown instance, ins, job, ip, cls N/A node:ins:network_tx_bytes_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:network_tx_pps1m Unknown instance, ins, job, ip, cls N/A node:ins:pagefault_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:pagein_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:pageout_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:pgmajfault_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:sched_wait_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:size_bytes Unknown instance, ins, job, ip, cls N/A node:ins:space_usage_max Unknown instance, ins, job, ip, cls N/A node:ins:stdload1 Unknown instance, ins, job, ip, cls N/A node:ins:stdload15 Unknown instance, ins, job, ip, cls N/A node:ins:stdload5 Unknown instance, ins, job, ip, cls N/A node:ins:swap_usage Unknown instance, ins, job, ip, cls N/A node:ins:swapin_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:swapout_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:tcp_active_opens_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:tcp_dropped_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:tcp_error Unknown instance, ins, job, ip, cls N/A node:ins:tcp_error_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:tcp_insegs_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:tcp_outsegs_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:tcp_overflow_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:tcp_passive_opens_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:tcp_retrans_ratio1m Unknown instance, ins, job, ip, cls N/A node:ins:tcp_retranssegs_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:tcp_segs_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:time_drift Unknown instance, ins, job, ip, cls N/A node:ins:udp_in_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:udp_out_rate1m Unknown instance, ins, job, ip, cls N/A node:ins:uptime Unknown instance, ins, job, ip, cls N/A node_arp_entries gauge ip, device, ins, job, instance, cls ARP entries by device node_boot_time_seconds gauge instance, ins, job, ip, cls Node boot time, in unixtime. node_context_switches_total counter instance, ins, job, ip, cls Total number of context switches. node_cooling_device_cur_state gauge instance, ins, job, type, ip, cls Current throttle state of the cooling device node_cooling_device_max_state gauge instance, ins, job, type, ip, cls Maximum throttle state of the cooling device node_cpu_guest_seconds_total counter ip, mode, ins, job, cpu, instance, cls Seconds the CPUs spent in guests (VMs) for each mode. node_cpu_seconds_total counter ip, mode, ins, job, cpu, instance, cls Seconds the CPUs spent in each mode. node_disk_discard_time_seconds_total counter ip, device, ins, job, instance, cls This is the total number of seconds spent by all discards. node_disk_discarded_sectors_total counter ip, device, ins, job, instance, cls The total number of sectors discarded successfully. node_disk_discards_completed_total counter ip, device, ins, job, instance, cls The total number of discards completed successfully. node_disk_discards_merged_total counter ip, device, ins, job, instance, cls The total number of discards merged. node_disk_filesystem_info gauge ip, usage, version, device, uuid, ins, type, job, instance, cls Info about disk filesystem. node_disk_info gauge minor, ip, major, revision, device, model, serial, path, ins, job, instance, cls Info of /sys/block/\u003cblock_device\u003e. node_disk_io_now gauge ip, device, ins, job, instance, cls The number of I/Os currently in progress. node_disk_io_time_seconds_total counter ip, device, ins, job, instance, cls Total seconds spent doing I/Os. node_disk_io_time_weighted_seconds_total counter ip, device, ins, job, instance, cls The weighted # of seconds spent doing I/Os. node_disk_read_bytes_total counter ip, device, ins, job, instance, cls The total number of bytes read successfully. node_disk_read_time_seconds_total counter ip, device, ins, job, instance, cls The total number of seconds spent by all reads. node_disk_reads_completed_total counter ip, device, ins, job, instance, cls The total number of reads completed successfully. node_disk_reads_merged_total counter ip, device, ins, job, instance, cls The total number of reads merged. node_disk_write_time_seconds_total counter ip, device, ins, job, instance, cls This is the total number of seconds spent by all writes. node_disk_writes_completed_total counter ip, device, ins, job, instance, cls The total number of writes completed successfully. node_disk_writes_merged_total counter ip, device, ins, job, instance, cls The number of writes merged. node_disk_written_bytes_total counter ip, device, ins, job, instance, cls The total number of bytes written successfully. node_dmi_info gauge bios_vendor, ip, product_family, product_version, product_uuid, system_vendor, bios_version, ins, bios_date, cls, job, product_name, instance, chassis_version, chassis_vendor, product_serial A metric with a constant ‘1’ value labeled by bios_date, bios_release, bios_vendor, bios_version, board_asset_tag, board_name, board_serial, board_vendor, board_version, chassis_asset_tag, chassis_serial, chassis_vendor, chassis_version, product_family, product_name, product_serial, product_sku, product_uuid, product_version, system_vendor if provided by DMI. node_entropy_available_bits gauge instance, ins, job, ip, cls Bits of available entropy. node_entropy_pool_size_bits gauge instance, ins, job, ip, cls Bits of entropy pool. node_exporter_build_info gauge ip, version, revision, goversion, branch, ins, goarch, job, tags, instance, cls, goos A metric with a constant ‘1’ value labeled by version, revision, branch, goversion from which node_exporter was built, and the goos and goarch for the build. node_filefd_allocated gauge instance, ins, job, ip, cls File descriptor statistics: allocated. node_filefd_maximum gauge instance, ins, job, ip, cls File descriptor statistics: maximum. node_filesystem_avail_bytes gauge ip, device, mountpoint, ins, cls, job, instance, fstype Filesystem space available to non-root users in bytes. node_filesystem_device_error gauge ip, device, mountpoint, ins, cls, job, instance, fstype Whether an error occurred while getting statistics for the given device. node_filesystem_files gauge ip, device, mountpoint, ins, cls, job, instance, fstype Filesystem total file nodes. node_filesystem_files_free gauge ip, device, mountpoint, ins, cls, job, instance, fstype Filesystem total free file nodes. node_filesystem_free_bytes gauge ip, device, mountpoint, ins, cls, job, instance, fstype Filesystem free space in bytes. node_filesystem_readonly gauge ip, device, mountpoint, ins, cls, job, instance, fstype Filesystem read-only status. node_filesystem_size_bytes gauge ip, device, mountpoint, ins, cls, job, instance, fstype Filesystem size in bytes. node_forks_total counter instance, ins, job, ip, cls Total number of forks. node_hwmon_chip_names gauge chip_name, ip, ins, chip, job, instance, cls Annotation metric for human-readable chip names node_hwmon_energy_joule_total counter sensor, ip, ins, chip, job, instance, cls Hardware monitor for joules used so far (input) node_hwmon_sensor_label gauge sensor, ip, ins, chip, job, label, instance, cls Label for given chip and sensor node_intr_total counter instance, ins, job, ip, cls Total number of interrupts serviced. node_ipvs_connections_total counter instance, ins, job, ip, cls The total number of connections made. node_ipvs_incoming_bytes_total counter instance, ins, job, ip, cls The total amount of incoming data. node_ipvs_incoming_packets_total counter instance, ins, job, ip, cls The total number of incoming packets. node_ipvs_outgoing_bytes_total counter instance, ins, job, ip, cls The total amount of outgoing data. node_ipvs_outgoing_packets_total counter instance, ins, job, ip, cls The total number of outgoing packets. node_load1 gauge instance, ins, job, ip, cls 1m load average. node_load15 gauge instance, ins, job, ip, cls 15m load average. node_load5 gauge instance, ins, job, ip, cls 5m load average. node_memory_Active_anon_bytes gauge instance, ins, job, ip, cls Memory information field Active_anon_bytes. node_memory_Active_bytes gauge instance, ins, job, ip, cls Memory information field Active_bytes. node_memory_Active_file_bytes gauge instance, ins, job, ip, cls Memory information field Active_file_bytes. node_memory_AnonHugePages_bytes gauge instance, ins, job, ip, cls Memory information field AnonHugePages_bytes. node_memory_AnonPages_bytes gauge instance, ins, job, ip, cls Memory information field AnonPages_bytes. node_memory_Bounce_bytes gauge instance, ins, job, ip, cls Memory information field Bounce_bytes. node_memory_Buffers_bytes gauge instance, ins, job, ip, cls Memory information field Buffers_bytes. node_memory_Cached_bytes gauge instance, ins, job, ip, cls Memory information field Cached_bytes. node_memory_CommitLimit_bytes gauge instance, ins, job, ip, cls Memory information field CommitLimit_bytes. node_memory_Committed_AS_bytes gauge instance, ins, job, ip, cls Memory information field Committed_AS_bytes. node_memory_DirectMap1G_bytes gauge instance, ins, job, ip, cls Memory information field DirectMap1G_bytes. node_memory_DirectMap2M_bytes gauge instance, ins, job, ip, cls Memory information field DirectMap2M_bytes. node_memory_DirectMap4k_bytes gauge instance, ins, job, ip, cls Memory information field DirectMap4k_bytes. node_memory_Dirty_bytes gauge instance, ins, job, ip, cls Memory information field Dirty_bytes. node_memory_FileHugePages_bytes gauge instance, ins, job, ip, cls Memory information field FileHugePages_bytes. node_memory_FilePmdMapped_bytes gauge instance, ins, job, ip, cls Memory information field FilePmdMapped_bytes. node_memory_HardwareCorrupted_bytes gauge instance, ins, job, ip, cls Memory information field HardwareCorrupted_bytes. node_memory_HugePages_Free gauge instance, ins, job, ip, cls Memory information field HugePages_Free. node_memory_HugePages_Rsvd gauge instance, ins, job, ip, cls Memory information field HugePages_Rsvd. node_memory_HugePages_Surp gauge instance, ins, job, ip, cls Memory information field HugePages_Surp. node_memory_HugePages_Total gauge instance, ins, job, ip, cls Memory information field HugePages_Total. node_memory_Hugepagesize_bytes gauge instance, ins, job, ip, cls Memory information field Hugepagesize_bytes. node_memory_Hugetlb_bytes gauge instance, ins, job, ip, cls Memory information field Hugetlb_bytes. node_memory_Inactive_anon_bytes gauge instance, ins, job, ip, cls Memory information field Inactive_anon_bytes. node_memory_Inactive_bytes gauge instance, ins, job, ip, cls Memory information field Inactive_bytes. node_memory_Inactive_file_bytes gauge instance, ins, job, ip, cls Memory information field Inactive_file_bytes. node_memory_KReclaimable_bytes gauge instance, ins, job, ip, cls Memory information field KReclaimable_bytes. node_memory_KernelStack_bytes gauge instance, ins, job, ip, cls Memory information field KernelStack_bytes. node_memory_Mapped_bytes gauge instance, ins, job, ip, cls Memory information field Mapped_bytes. node_memory_MemAvailable_bytes gauge instance, ins, job, ip, cls Memory information field MemAvailable_bytes. node_memory_MemFree_bytes gauge instance, ins, job, ip, cls Memory information field MemFree_bytes. node_memory_MemTotal_bytes gauge instance, ins, job, ip, cls Memory information field MemTotal_bytes. node_memory_Mlocked_bytes gauge instance, ins, job, ip, cls Memory information field Mlocked_bytes. node_memory_NFS_Unstable_bytes gauge instance, ins, job, ip, cls Memory information field NFS_Unstable_bytes. node_memory_PageTables_bytes gauge instance, ins, job, ip, cls Memory information field PageTables_bytes. node_memory_Percpu_bytes gauge instance, ins, job, ip, cls Memory information field Percpu_bytes. node_memory_SReclaimable_bytes gauge instance, ins, job, ip, cls Memory information field SReclaimable_bytes. node_memory_SUnreclaim_bytes gauge instance, ins, job, ip, cls Memory information field SUnreclaim_bytes. node_memory_ShmemHugePages_bytes gauge instance, ins, job, ip, cls Memory information field ShmemHugePages_bytes. node_memory_ShmemPmdMapped_bytes gauge instance, ins, job, ip, cls Memory information field ShmemPmdMapped_bytes. node_memory_Shmem_bytes gauge instance, ins, job, ip, cls Memory information field Shmem_bytes. node_memory_Slab_bytes gauge instance, ins, job, ip, cls Memory information field Slab_bytes. node_memory_SwapCached_bytes gauge instance, ins, job, ip, cls Memory information field SwapCached_bytes. node_memory_SwapFree_bytes gauge instance, ins, job, ip, cls Memory information field SwapFree_bytes. node_memory_SwapTotal_bytes gauge instance, ins, job, ip, cls Memory information field SwapTotal_bytes. node_memory_Unevictable_bytes gauge instance, ins, job, ip, cls Memory information field Unevictable_bytes. node_memory_VmallocChunk_bytes gauge instance, ins, job, ip, cls Memory information field VmallocChunk_bytes. node_memory_VmallocTotal_bytes gauge instance, ins, job, ip, cls Memory information field VmallocTotal_bytes. node_memory_VmallocUsed_bytes gauge instance, ins, job, ip, cls Memory information field VmallocUsed_bytes. node_memory_WritebackTmp_bytes gauge instance, ins, job, ip, cls Memory information field WritebackTmp_bytes. node_memory_Writeback_bytes gauge instance, ins, job, ip, cls Memory information field Writeback_bytes. node_netstat_Icmp6_InErrors unknown instance, ins, job, ip, cls Statistic Icmp6InErrors. node_netstat_Icmp6_InMsgs unknown instance, ins, job, ip, cls Statistic Icmp6InMsgs. node_netstat_Icmp6_OutMsgs unknown instance, ins, job, ip, cls Statistic Icmp6OutMsgs. node_netstat_Icmp_InErrors unknown instance, ins, job, ip, cls Statistic IcmpInErrors. node_netstat_Icmp_InMsgs unknown instance, ins, job, ip, cls Statistic IcmpInMsgs. node_netstat_Icmp_OutMsgs unknown instance, ins, job, ip, cls Statistic IcmpOutMsgs. node_netstat_Ip6_InOctets unknown instance, ins, job, ip, cls Statistic Ip6InOctets. node_netstat_Ip6_OutOctets unknown instance, ins, job, ip, cls Statistic Ip6OutOctets. node_netstat_IpExt_InOctets unknown instance, ins, job, ip, cls Statistic IpExtInOctets. node_netstat_IpExt_OutOctets unknown instance, ins, job, ip, cls Statistic IpExtOutOctets. node_netstat_Ip_Forwarding unknown instance, ins, job, ip, cls Statistic IpForwarding. node_netstat_TcpExt_ListenDrops unknown instance, ins, job, ip, cls Statistic TcpExtListenDrops. node_netstat_TcpExt_ListenOverflows unknown instance, ins, job, ip, cls Statistic TcpExtListenOverflows. node_netstat_TcpExt_SyncookiesFailed unknown instance, ins, job, ip, cls Statistic TcpExtSyncookiesFailed. node_netstat_TcpExt_SyncookiesRecv unknown instance, ins, job, ip, cls Statistic TcpExtSyncookiesRecv. node_netstat_TcpExt_SyncookiesSent unknown instance, ins, job, ip, cls Statistic TcpExtSyncookiesSent. node_netstat_TcpExt_TCPSynRetrans unknown instance, ins, job, ip, cls Statistic TcpExtTCPSynRetrans. node_netstat_TcpExt_TCPTimeouts unknown instance, ins, job, ip, cls Statistic TcpExtTCPTimeouts. node_netstat_Tcp_ActiveOpens unknown instance, ins, job, ip, cls Statistic TcpActiveOpens. node_netstat_Tcp_CurrEstab unknown instance, ins, job, ip, cls Statistic TcpCurrEstab. node_netstat_Tcp_InErrs unknown instance, ins, job, ip, cls Statistic TcpInErrs. node_netstat_Tcp_InSegs unknown instance, ins, job, ip, cls Statistic TcpInSegs. node_netstat_Tcp_OutRsts unknown instance, ins, job, ip, cls Statistic TcpOutRsts. node_netstat_Tcp_OutSegs unknown instance, ins, job, ip, cls Statistic TcpOutSegs. node_netstat_Tcp_PassiveOpens unknown instance, ins, job, ip, cls Statistic TcpPassiveOpens. node_netstat_Tcp_RetransSegs unknown instance, ins, job, ip, cls Statistic TcpRetransSegs. node_netstat_Udp6_InDatagrams unknown instance, ins, job, ip, cls Statistic Udp6InDatagrams. node_netstat_Udp6_InErrors unknown instance, ins, job, ip, cls Statistic Udp6InErrors. node_netstat_Udp6_NoPorts unknown instance, ins, job, ip, cls Statistic Udp6NoPorts. node_netstat_Udp6_OutDatagrams unknown instance, ins, job, ip, cls Statistic Udp6OutDatagrams. node_netstat_Udp6_RcvbufErrors unknown instance, ins, job, ip, cls Statistic Udp6RcvbufErrors. node_netstat_Udp6_SndbufErrors unknown instance, ins, job, ip, cls Statistic Udp6SndbufErrors. node_netstat_UdpLite6_InErrors unknown instance, ins, job, ip, cls Statistic UdpLite6InErrors. node_netstat_UdpLite_InErrors unknown instance, ins, job, ip, cls Statistic UdpLiteInErrors. node_netstat_Udp_InDatagrams unknown instance, ins, job, ip, cls Statistic UdpInDatagrams. node_netstat_Udp_InErrors unknown instance, ins, job, ip, cls Statistic UdpInErrors. node_netstat_Udp_NoPorts unknown instance, ins, job, ip, cls Statistic UdpNoPorts. node_netstat_Udp_OutDatagrams unknown instance, ins, job, ip, cls Statistic UdpOutDatagrams. node_netstat_Udp_RcvbufErrors unknown instance, ins, job, ip, cls Statistic UdpRcvbufErrors. node_netstat_Udp_SndbufErrors unknown instance, ins, job, ip, cls Statistic UdpSndbufErrors. node_network_address_assign_type gauge ip, device, ins, job, instance, cls Network device property: address_assign_type node_network_carrier gauge ip, device, ins, job, instance, cls Network device property: carrier node_network_carrier_changes_total counter ip, device, ins, job, instance, cls Network device property: carrier_changes_total node_network_carrier_down_changes_total counter ip, device, ins, job, instance, cls Network device property: carrier_down_changes_total node_network_carrier_up_changes_total counter ip, device, ins, job, instance, cls Network device property: carrier_up_changes_total node_network_device_id gauge ip, device, ins, job, instance, cls Network device property: device_id node_network_dormant gauge ip, device, ins, job, instance, cls Network device property: dormant node_network_flags gauge ip, device, ins, job, instance, cls Network device property: flags node_network_iface_id gauge ip, device, ins, job, instance, cls Network device property: iface_id node_network_iface_link gauge ip, device, ins, job, instance, cls Network device property: iface_link node_network_iface_link_mode gauge ip, device, ins, job, instance, cls Network device property: iface_link_mode node_network_info gauge broadcast, ip, device, operstate, ins, job, adminstate, duplex, address, instance, cls Non-numeric data from /sys/class/net/, value is always 1. node_network_mtu_bytes gauge ip, device, ins, job, instance, cls Network device property: mtu_bytes node_network_name_assign_type gauge ip, device, ins, job, instance, cls Network device property: name_assign_type node_network_net_dev_group gauge ip, device, ins, job, instance, cls Network device property: net_dev_group node_network_protocol_type gauge ip, device, ins, job, instance, cls Network device property: protocol_type node_network_receive_bytes_total counter ip, device, ins, job, instance, cls Network device statistic receive_bytes. node_network_receive_compressed_total counter ip, device, ins, job, instance, cls Network device statistic receive_compressed. node_network_receive_drop_total counter ip, device, ins, job, instance, cls Network device statistic receive_drop. node_network_receive_errs_total counter ip, device, ins, job, instance, cls Network device statistic receive_errs. node_network_receive_fifo_total counter ip, device, ins, job, instance, cls Network device statistic receive_fifo. node_network_receive_frame_total counter ip, device, ins, job, instance, cls Network device statistic receive_frame. node_network_receive_multicast_total counter ip, device, ins, job, instance, cls Network device statistic receive_multicast. node_network_receive_nohandler_total counter ip, device, ins, job, instance, cls Network device statistic receive_nohandler. node_network_receive_packets_total counter ip, device, ins, job, instance, cls Network device statistic receive_packets. node_network_speed_bytes gauge ip, device, ins, job, instance, cls Network device property: speed_bytes node_network_transmit_bytes_total counter ip, device, ins, job, instance, cls Network device statistic transmit_bytes. node_network_transmit_carrier_total counter ip, device, ins, job, instance, cls Network device statistic transmit_carrier. node_network_transmit_colls_total counter ip, device, ins, job, instance, cls Network device statistic transmit_colls. node_network_transmit_compressed_total counter ip, device, ins, job, instance, cls Network device statistic transmit_compressed. node_network_transmit_drop_total counter ip, device, ins, job, instance, cls Network device statistic transmit_drop. node_network_transmit_errs_total counter ip, device, ins, job, instance, cls Network device statistic transmit_errs. node_network_transmit_fifo_total counter ip, device, ins, job, instance, cls Network device statistic transmit_fifo. node_network_transmit_packets_total counter ip, device, ins, job, instance, cls Network device statistic transmit_packets. node_network_transmit_queue_length gauge ip, device, ins, job, instance, cls Network device property: transmit_queue_length node_network_up gauge ip, device, ins, job, instance, cls Value is 1 if operstate is ‘up’, 0 otherwise. node_nf_conntrack_entries gauge instance, ins, job, ip, cls Number of currently allocated flow entries for connection tracking. node_nf_conntrack_entries_limit gauge instance, ins, job, ip, cls Maximum size of connection tracking table. node_nf_conntrack_stat_drop gauge instance, ins, job, ip, cls Number of packets dropped due to conntrack failure. node_nf_conntrack_stat_early_drop gauge instance, ins, job, ip, cls Number of dropped conntrack entries to make room for new ones, if maximum table size was reached. node_nf_conntrack_stat_found gauge instance, ins, job, ip, cls Number of searched entries which were successful. node_nf_conntrack_stat_ignore gauge instance, ins, job, ip, cls Number of packets seen which are already connected to a conntrack entry. node_nf_conntrack_stat_insert gauge instance, ins, job, ip, cls Number of entries inserted into the list. node_nf_conntrack_stat_insert_failed gauge instance, ins, job, ip, cls Number of entries for which list insertion was attempted but failed. node_nf_conntrack_stat_invalid gauge instance, ins, job, ip, cls Number of packets seen which can not be tracked. node_nf_conntrack_stat_search_restart gauge instance, ins, job, ip, cls Number of conntrack table lookups which had to be restarted due to hashtable resizes. node_os_info gauge id, ip, version, version_id, ins, instance, job, pretty_name, id_like, cls A metric with a constant ‘1’ value labeled by build_id, id, id_like, image_id, image_version, name, pretty_name, variant, variant_id, version, version_codename, version_id. node_os_version gauge id, ip, ins, instance, job, id_like, cls Metric containing the major.minor part of the OS version. node_processes_max_processes gauge instance, ins, job, ip, cls Number of max PIDs limit node_processes_max_threads gauge instance, ins, job, ip, cls Limit of threads in the system node_processes_pids gauge instance, ins, job, ip, cls Number of PIDs node_processes_state gauge state, instance, ins, job, ip, cls Number of processes in each state. node_processes_threads gauge instance, ins, job, ip, cls Allocated threads in system node_processes_threads_state gauge instance, ins, job, thread_state, ip, cls Number of threads in each state. node_procs_blocked gauge instance, ins, job, ip, cls Number of processes blocked waiting for I/O to complete. node_procs_running gauge instance, ins, job, ip, cls Number of processes in runnable state. node_schedstat_running_seconds_total counter ip, ins, job, cpu, instance, cls Number of seconds CPU spent running a process. node_schedstat_timeslices_total counter ip, ins, job, cpu, instance, cls Number of timeslices executed by CPU. node_schedstat_waiting_seconds_total counter ip, ins, job, cpu, instance, cls Number of seconds spent by processing waiting for this CPU. node_scrape_collector_duration_seconds gauge ip, collector, ins, job, instance, cls node_exporter: Duration of a collector scrape. node_scrape_collector_success gauge ip, collector, ins, job, instance, cls node_exporter: Whether a collector succeeded. node_selinux_enabled gauge instance, ins, job, ip, cls SELinux is enabled, 1 is true, 0 is false node_sockstat_FRAG6_inuse gauge instance, ins, job, ip, cls Number of FRAG6 sockets in state inuse. node_sockstat_FRAG6_memory gauge instance, ins, job, ip, cls Number of FRAG6 sockets in state memory. node_sockstat_FRAG_inuse gauge instance, ins, job, ip, cls Number of FRAG sockets in state inuse. node_sockstat_FRAG_memory gauge instance, ins, job, ip, cls Number of FRAG sockets in state memory. node_sockstat_RAW6_inuse gauge instance, ins, job, ip, cls Number of RAW6 sockets in state inuse. node_sockstat_RAW_inuse gauge instance, ins, job, ip, cls Number of RAW sockets in state inuse. node_sockstat_TCP6_inuse gauge instance, ins, job, ip, cls Number of TCP6 sockets in state inuse. node_sockstat_TCP_alloc gauge instance, ins, job, ip, cls Number of TCP sockets in state alloc. node_sockstat_TCP_inuse gauge instance, ins, job, ip, cls Number of TCP sockets in state inuse. node_sockstat_TCP_mem gauge instance, ins, job, ip, cls Number of TCP sockets in state mem. node_sockstat_TCP_mem_bytes gauge instance, ins, job, ip, cls Number of TCP sockets in state mem_bytes. node_sockstat_TCP_orphan gauge instance, ins, job, ip, cls Number of TCP sockets in state orphan. node_sockstat_TCP_tw gauge instance, ins, job, ip, cls Number of TCP sockets in state tw. node_sockstat_UDP6_inuse gauge instance, ins, job, ip, cls Number of UDP6 sockets in state inuse. node_sockstat_UDPLITE6_inuse gauge instance, ins, job, ip, cls Number of UDPLITE6 sockets in state inuse. node_sockstat_UDPLITE_inuse gauge instance, ins, job, ip, cls Number of UDPLITE sockets in state inuse. node_sockstat_UDP_inuse gauge instance, ins, job, ip, cls Number of UDP sockets in state inuse. node_sockstat_UDP_mem gauge instance, ins, job, ip, cls Number of UDP sockets in state mem. node_sockstat_UDP_mem_bytes gauge instance, ins, job, ip, cls Number of UDP sockets in state mem_bytes. node_sockstat_sockets_used gauge instance, ins, job, ip, cls Number of IPv4 sockets in use. node_tcp_connection_states gauge state, instance, ins, job, ip, cls Number of connection states. node_textfile_scrape_error gauge instance, ins, job, ip, cls 1 if there was an error opening or reading a file, 0 otherwise node_time_clocksource_available_info gauge ip, device, ins, clocksource, job, instance, cls Available clocksources read from ‘/sys/devices/system/clocksource’. node_time_clocksource_current_info gauge ip, device, ins, clocksource, job, instance, cls Current clocksource read from ‘/sys/devices/system/clocksource’. node_time_seconds gauge instance, ins, job, ip, cls System time in seconds since epoch (1970). node_time_zone_offset_seconds gauge instance, ins, job, time_zone, ip, cls System time zone offset in seconds. node_timex_estimated_error_seconds gauge instance, ins, job, ip, cls Estimated error in seconds. node_timex_frequency_adjustment_ratio gauge instance, ins, job, ip, cls Local clock frequency adjustment. node_timex_loop_time_constant gauge instance, ins, job, ip, cls Phase-locked loop time constant. node_timex_maxerror_seconds gauge instance, ins, job, ip, cls Maximum error in seconds. node_timex_offset_seconds gauge instance, ins, job, ip, cls Time offset in between local system and reference clock. node_timex_pps_calibration_total counter instance, ins, job, ip, cls Pulse per second count of calibration intervals. node_timex_pps_error_total counter instance, ins, job, ip, cls Pulse per second count of calibration errors. node_timex_pps_frequency_hertz gauge instance, ins, job, ip, cls Pulse per second frequency. node_timex_pps_jitter_seconds gauge instance, ins, job, ip, cls Pulse per second jitter. node_timex_pps_jitter_total counter instance, ins, job, ip, cls Pulse per second count of jitter limit exceeded events. node_timex_pps_shift_seconds gauge instance, ins, job, ip, cls Pulse per second interval duration. node_timex_pps_stability_exceeded_total counter instance, ins, job, ip, cls Pulse per second count of stability limit exceeded events. node_timex_pps_stability_hertz gauge instance, ins, job, ip, cls Pulse per second stability, average of recent frequency changes. node_timex_status gauge instance, ins, job, ip, cls Value of the status array bits. node_timex_sync_status gauge instance, ins, job, ip, cls Is clock synchronized to a reliable server (1 = yes, 0 = no). node_timex_tai_offset_seconds gauge instance, ins, job, ip, cls International Atomic Time (TAI) offset. node_timex_tick_seconds gauge instance, ins, job, ip, cls Seconds between clock ticks. node_udp_queues gauge ip, queue, ins, job, exported_ip, instance, cls Number of allocated memory in the kernel for UDP datagrams in bytes. node_uname_info gauge ip, sysname, version, domainname, release, ins, job, nodename, instance, cls, machine Labeled system information as provided by the uname system call. node_up Unknown instance, ins, job, ip, cls N/A node_vmstat_oom_kill unknown instance, ins, job, ip, cls /proc/vmstat information field oom_kill. node_vmstat_pgfault unknown instance, ins, job, ip, cls /proc/vmstat information field pgfault. node_vmstat_pgmajfault unknown instance, ins, job, ip, cls /proc/vmstat information field pgmajfault. node_vmstat_pgpgin unknown instance, ins, job, ip, cls /proc/vmstat information field pgpgin. node_vmstat_pgpgout unknown instance, ins, job, ip, cls /proc/vmstat information field pgpgout. node_vmstat_pswpin unknown instance, ins, job, ip, cls /proc/vmstat information field pswpin. node_vmstat_pswpout unknown instance, ins, job, ip, cls /proc/vmstat information field pswpout. process_cpu_seconds_total counter instance, ins, job, ip, cls Total user and system CPU time spent in seconds. process_max_fds gauge instance, ins, job, ip, cls Maximum number of open file descriptors. process_open_fds gauge instance, ins, job, ip, cls Number of open file descriptors. process_resident_memory_bytes gauge instance, ins, job, ip, cls Resident memory size in bytes. process_start_time_seconds gauge instance, ins, job, ip, cls Start time of the process since unix epoch in seconds. process_virtual_memory_bytes gauge instance, ins, job, ip, cls Virtual memory size in bytes. process_virtual_memory_max_bytes gauge instance, ins, job, ip, cls Maximum amount of virtual memory available in bytes. prometheus_remote_storage_exemplars_in_total counter instance, ins, job, ip, cls Exemplars in to remote storage, compare to exemplars out for queue managers. prometheus_remote_storage_histograms_in_total counter instance, ins, job, ip, cls HistogramSamples in to remote storage, compare to histograms out for queue managers. prometheus_remote_storage_samples_in_total counter instance, ins, job, ip, cls Samples in to remote storage, compare to samples out for queue managers. prometheus_remote_storage_string_interner_zero_reference_releases_total counter instance, ins, job, ip, cls The number of times release has been called for strings that are not interned. prometheus_sd_azure_failures_total counter instance, ins, job, ip, cls Number of Azure service discovery refresh failures. prometheus_sd_consul_rpc_duration_seconds summary ip, call, quantile, ins, job, instance, cls, endpoint The duration of a Consul RPC call in seconds. prometheus_sd_consul_rpc_duration_seconds_count Unknown ip, call, ins, job, instance, cls, endpoint N/A prometheus_sd_consul_rpc_duration_seconds_sum Unknown ip, call, ins, job, instance, cls, endpoint N/A prometheus_sd_consul_rpc_failures_total counter instance, ins, job, ip, cls The number of Consul RPC call failures. prometheus_sd_consulagent_rpc_duration_seconds summary ip, call, quantile, ins, job, instance, cls, endpoint The duration of a Consul Agent RPC call in seconds. prometheus_sd_consulagent_rpc_duration_seconds_count Unknown ip, call, ins, job, instance, cls, endpoint N/A prometheus_sd_consulagent_rpc_duration_seconds_sum Unknown ip, call, ins, job, instance, cls, endpoint N/A prometheus_sd_consulagent_rpc_failures_total Unknown instance, ins, job, ip, cls N/A prometheus_sd_dns_lookup_failures_total counter instance, ins, job, ip, cls The number of DNS-SD lookup failures. prometheus_sd_dns_lookups_total counter instance, ins, job, ip, cls The number of DNS-SD lookups. prometheus_sd_file_read_errors_total counter instance, ins, job, ip, cls The number of File-SD read errors. prometheus_sd_file_scan_duration_seconds summary quantile, instance, ins, job, ip, cls The duration of the File-SD scan in seconds. prometheus_sd_file_scan_duration_seconds_count Unknown instance, ins, job, ip, cls N/A prometheus_sd_file_scan_duration_seconds_sum Unknown instance, ins, job, ip, cls N/A prometheus_sd_file_watcher_errors_total counter instance, ins, job, ip, cls The number of File-SD errors caused by filesystem watch failures. prometheus_sd_kubernetes_events_total counter ip, event, ins, job, role, instance, cls The number of Kubernetes events handled. prometheus_target_scrape_pool_exceeded_label_limits_total counter instance, ins, job, ip, cls Total number of times scrape pools hit the label limits, during sync or config reload. prometheus_target_scrape_pool_exceeded_target_limit_total counter instance, ins, job, ip, cls Total number of times scrape pools hit the target limit, during sync or config reload. prometheus_target_scrape_pool_reloads_failed_total counter instance, ins, job, ip, cls Total number of failed scrape pool reloads. prometheus_target_scrape_pool_reloads_total counter instance, ins, job, ip, cls Total number of scrape pool reloads. prometheus_target_scrape_pools_failed_total counter instance, ins, job, ip, cls Total number of scrape pool creations that failed. prometheus_target_scrape_pools_total counter instance, ins, job, ip, cls Total number of scrape pool creation attempts. prometheus_target_scrapes_cache_flush_forced_total counter instance, ins, job, ip, cls How many times a scrape cache was flushed due to getting big while scrapes are failing. prometheus_target_scrapes_exceeded_body_size_limit_total counter instance, ins, job, ip, cls Total number of scrapes that hit the body size limit prometheus_target_scrapes_exceeded_sample_limit_total counter instance, ins, job, ip, cls Total number of scrapes that hit the sample limit and were rejected. prometheus_target_scrapes_exemplar_out_of_order_total counter instance, ins, job, ip, cls Total number of exemplar rejected due to not being out of the expected order. prometheus_target_scrapes_sample_duplicate_timestamp_total counter instance, ins, job, ip, cls Total number of samples rejected due to duplicate timestamps but different values. prometheus_target_scrapes_sample_out_of_bounds_total counter instance, ins, job, ip, cls Total number of samples rejected due to timestamp falling outside of the time bounds. prometheus_target_scrapes_sample_out_of_order_total counter instance, ins, job, ip, cls Total number of samples rejected due to not being out of the expected order. prometheus_template_text_expansion_failures_total counter instance, ins, job, ip, cls The total number of template text expansion failures. prometheus_template_text_expansions_total counter instance, ins, job, ip, cls The total number of template text expansions. prometheus_treecache_watcher_goroutines gauge instance, ins, job, ip, cls The current number of watcher goroutines. prometheus_treecache_zookeeper_failures_total counter instance, ins, job, ip, cls The total number of ZooKeeper failures. promhttp_metric_handler_errors_total counter ip, cause, ins, job, instance, cls Total number of internal errors encountered by the promhttp metric handler. promhttp_metric_handler_requests_in_flight gauge instance, ins, job, ip, cls Current number of scrapes being served. promhttp_metric_handler_requests_total counter ip, ins, code, job, instance, cls Total number of scrapes by HTTP status code. promtail_batch_retries_total Unknown host, ip, ins, job, instance, cls N/A promtail_build_info gauge ip, version, revision, goversion, branch, ins, goarch, job, tags, instance, cls, goos A metric with a constant ‘1’ value labeled by version, revision, branch, goversion from which promtail was built, and the goos and goarch for the build. promtail_config_reload_fail_total Unknown instance, ins, job, ip, cls N/A promtail_config_reload_success_total Unknown instance, ins, job, ip, cls N/A promtail_dropped_bytes_total Unknown host, ip, ins, job, reason, instance, cls N/A promtail_dropped_entries_total Unknown host, ip, ins, job, reason, instance, cls N/A promtail_encoded_bytes_total Unknown host, ip, ins, job, instance, cls N/A promtail_file_bytes_total gauge path, instance, ins, job, ip, cls Number of bytes total. promtail_files_active_total gauge instance, ins, job, ip, cls Number of active files. promtail_mutated_bytes_total Unknown host, ip, ins, job, reason, instance, cls N/A promtail_mutated_entries_total Unknown host, ip, ins, job, reason, instance, cls N/A promtail_read_bytes_total gauge path, instance, ins, job, ip, cls Number of bytes read. promtail_read_lines_total Unknown path, instance, ins, job, ip, cls N/A promtail_request_duration_seconds_bucket Unknown host, ip, ins, job, status_code, le, instance, cls N/A promtail_request_duration_seconds_count Unknown host, ip, ins, job, status_code, instance, cls N/A promtail_request_duration_seconds_sum Unknown host, ip, ins, job, status_code, instance, cls N/A promtail_sent_bytes_total Unknown host, ip, ins, job, instance, cls N/A promtail_sent_entries_total Unknown host, ip, ins, job, instance, cls N/A promtail_targets_active_total gauge instance, ins, job, ip, cls Number of active total. promtail_up Unknown instance, ins, job, ip, cls N/A request_duration_seconds_bucket Unknown instance, ins, job, status_code, route, ws, le, ip, cls, method N/A request_duration_seconds_count Unknown instance, ins, job, status_code, route, ws, ip, cls, method N/A request_duration_seconds_sum Unknown instance, ins, job, status_code, route, ws, ip, cls, method N/A request_message_bytes_bucket Unknown instance, ins, job, route, le, ip, cls, method N/A request_message_bytes_count Unknown instance, ins, job, route, ip, cls, method N/A request_message_bytes_sum Unknown instance, ins, job, route, ip, cls, method N/A response_message_bytes_bucket Unknown instance, ins, job, route, le, ip, cls, method N/A response_message_bytes_count Unknown instance, ins, job, route, ip, cls, method N/A response_message_bytes_sum Unknown instance, ins, job, route, ip, cls, method N/A scrape_duration_seconds Unknown instance, ins, job, ip, cls N/A scrape_samples_post_metric_relabeling Unknown instance, ins, job, ip, cls N/A scrape_samples_scraped Unknown instance, ins, job, ip, cls N/A scrape_series_added Unknown instance, ins, job, ip, cls N/A tcp_connections gauge instance, ins, job, protocol, ip, cls Current number of accepted TCP connections. tcp_connections_limit gauge instance, ins, job, protocol, ip, cls The max number of TCP connections that can be accepted (0 means no limit). up Unknown instance, ins, job, ip, cls N/A ","categories":["Reference"],"description":"Complete list of monitoring metrics provided by Pigsty NODE module","excerpt":"Complete list of monitoring metrics provided by Pigsty NODE module","ref":"/docs/node/metric/","tags":"","title":"Metrics"},{"body":" How to configure NTP service? NTP is critical for various production services. If NTP is not configured, you can use public NTP services or the Chronyd on the admin node as the time standard.\nIf your nodes already have NTP configured, you can preserve the existing configuration without making any changes by setting node_ntp_enabled to false.\nOtherwise, if you have Internet access, you can use public NTP services such as pool.ntp.org.\nIf you don’t have Internet access, you can use the following approach to ensure all nodes in the environment are synchronized with the admin node, or use another internal NTP time service.\nnode_ntp_servers: # NTP servers in /etc/chrony.conf - pool cn.pool.ntp.org iburst - pool ${admin_ip} iburst # assume non-admin nodes do not have internet access, at least sync with admin node How to force sync time on nodes? Use chronyc to sync time. You must configure the NTP service first.\nansible all -b -a 'chronyc -a makestep' # sync time You can replace all with any group or host IP address to limit the execution scope.\nRemote nodes are not accessible via SSH? If the target machine is hidden behind an SSH jump host, or some customizations prevent direct access using ssh ip, you can use Ansible connection parameters to specify various SSH connection options, such as:\npg-test: vars: { pg_cluster: pg-test } hosts: 10.10.10.11: {pg_seq: 1, pg_role: primary, ansible_host: node-1 } 10.10.10.12: {pg_seq: 2, pg_role: replica, ansible_port: 22223, ansible_user: admin } 10.10.10.13: {pg_seq: 3, pg_role: offline, ansible_port: 22224 } Password required for remote node SSH and SUDO? When performing deployments and changes, the admin user used must have ssh and sudo privileges for all nodes. Passwordless login is not required.\nYou can pass ssh and sudo passwords via the -k|-K parameters when executing playbooks, or even use another user to run playbooks via -eansible_host=\u003canother_user\u003e.\nHowever, Pigsty strongly recommends configuring SSH passwordless login with passwordless sudo for the admin user.\nHow to create a dedicated admin user with an existing admin user? Use the following command to create a new standard admin user defined by node_admin_username using an existing admin user on that node.\n./node.yml -k -K -e ansible_user=\u003canother_admin\u003e -t node_admin How to expose services using HAProxy on nodes? You can use haproxy_services in the configuration to expose services, and use node.yml -t haproxy_config,haproxy_reload to update the configuration.\nHere’s an example of exposing a MinIO service: Expose MinIO Service\nWhy are all my /etc/yum.repos.d/* files gone? Pigsty builds a local software repository on infra nodes that includes all dependencies. All regular nodes will reference and use the local software repository on Infra nodes according to the default configuration of node_repo_modules as local.\nThis design avoids Internet access and enhances installation stability and reliability. All original repo definition files are moved to the /etc/yum.repos.d/backup directory; you can copy them back as needed.\nIf you want to preserve the original repo definition files during regular node installation, set node_repo_remove to false.\nIf you want to preserve the original repo definition files during Infra node local repo construction, set repo_remove to false.\nWhy did my command line prompt change? How to restore it? The shell command line prompt used by Pigsty is specified by the environment variable PS1, defined in the /etc/profile.d/node.sh file.\nIf you don’t like it and want to modify or restore it, you can remove this file and log in again.\nWhy did my hostname change? Pigsty will modify your node hostname in two situations:\nnodename value is explicitly defined (default is empty) The PGSQL module is declared on the node and the node_id_from_pg parameter is enabled (default is true) If you don’t want the hostname to be modified, you can set nodename_overwrite to false at the global/cluster/instance level (default is true).\nFor details, see the NODE_ID section.\nWhat compatibility issues exist with Tencent OpenCloudOS? The softdog kernel module is not available on OpenCloudOS and needs to be removed from node_kernel_modules. Add the following configuration item to the global variables in the config file to override:\nnode_kernel_modules: [ ip_vs, ip_vs_rr, ip_vs_wrr, ip_vs_sh ] What common issues exist on Debian systems? When using Pigsty on Debian/Ubuntu systems, you may encounter the following issues:\nMissing locale\nIf the system reports locale-related errors, you can fix them with the following command:\nlocaledef -i en_US -f UTF-8 en_US.UTF-8 Missing rsync tool\nPigsty relies on rsync for file synchronization. If the system doesn’t have it installed, you can install it with:\napt-get install rsync ","categories":["Reference"],"description":"Frequently asked questions about Pigsty NODE module","excerpt":"Frequently asked questions about Pigsty NODE module","ref":"/docs/node/faq/","tags":"","title":"FAQ"},{"body":"ETCD is a distributed, reliable key-value store for critical system config data.\nPigsty uses etcd as DCS (Distributed Config Store), critical for PostgreSQL HA and automatic failover.\nThe ETCD module depends on NODE module and is required by PGSQL module. Install NODE module to manage nodes before installing ETCD.\nDeploy ETCD cluster before any PGSQL cluster—patroni and vip-manager for PG HA rely on etcd for HA and L2 VIP binding to primary.\nflowchart LR subgraph PGSQL [PGSQL] patroni[Patroni] vip[VIP Manager] end subgraph ETCD [ETCD] etcd[DCS Service] end subgraph NODE [NODE] node[Software Repo] end PGSQL --\u003e|depends| ETCD --\u003e|depends| NODE style PGSQL fill:#3E668F,stroke:#2d4a66,color:#fff style ETCD fill:#5B9CD5,stroke:#4178a8,color:#fff style NODE fill:#FCDB72,stroke:#d4b85e,color:#333 style patroni fill:#2d4a66,stroke:#1e3347,color:#fff style vip fill:#2d4a66,stroke:#1e3347,color:#fff style etcd fill:#4178a8,stroke:#2d5a7a,color:#fff style node fill:#d4b85e,stroke:#b89a4a,color:#333 One etcd cluster per Pigsty deployment serves multiple PG clusters.\nPigsty enables RBAC by default. Each PG cluster uses independent credentials for multi-tenant isolation. Admins use etcd root user with full permissions over all PG clusters.\n","categories":["Reference"],"description":"Pigsty deploys etcd as DCS for reliable distributed config storage, supporting PostgreSQL HA.","excerpt":"Pigsty deploys etcd as DCS for reliable distributed config storage, …","ref":"/docs/etcd/","tags":"","title":"Module: ETCD"},{"body":"Before deployment, define etcd cluster in config inventory. Typical choices:\nOne Node: No HA, suitable for dev, test, demo, or standalone deployments using external S3 backup for PITR Three Nodes: Basic HA, tolerates 1 node failure, suitable for small-medium prod Five Nodes: Better HA, tolerates 2 node failures, suitable for large prod Even-numbered clusters don’t make sense; 5+ node clusters uncommon. Typical configs: single, 3-node, 5-node.\nCluster Size Quorum Fault Tolerance Use Case 1 node 1 0 Dev, test, demo 3 nodes 2 1 Small-medium prod 5 nodes 3 2 Large prod 7 nodes 4 3 Special HA requirements One Node Define singleton etcd instance in Pigsty—single line of config:\netcd: { hosts: { 10.10.10.10: { etcd_seq: 1 } }, vars: { etcd_cluster: etcd } } All single-node config templates include this line. Placeholder IP 10.10.10.10 replaced with current admin node’s IP.\nOnly required params: etcd_seq and etcd_cluster—uniquely identify each etcd instance.\nThree Nodes Most common config: 3-node etcd cluster tolerates 1 node failure, suitable for small-medium prod.\nExample: Pigsty’s 3-node templates trio and safe use 3-node etcd:\netcd: hosts: 10.10.10.10: { etcd_seq: 1 } # etcd_seq (instance number) required 10.10.10.11: { etcd_seq: 2 } # positive integers, sequential from 0 or 1 10.10.10.12: { etcd_seq: 3 } # immutable for life, never recycled vars: # cluster-level params etcd_cluster: etcd # default cluster name: 'etcd', don't change unless deploying multiple etcd clusters etcd_safeguard: false # enable safeguard? Enable after prod init to prevent accidental deletion etcd_clean: true # force remove existing during init? Enable for testing for true idempotency Five Nodes 5-node cluster tolerates 2 node failures, suitable for large prod.\nExample: Pigsty’s prod sim template prod uses 5-node etcd:\netcd: hosts: 10.10.10.21 : { etcd_seq: 1 } 10.10.10.22 : { etcd_seq: 2 } 10.10.10.23 : { etcd_seq: 3 } 10.10.10.24 : { etcd_seq: 4 } 10.10.10.25 : { etcd_seq: 5 } vars: { etcd_cluster: etcd } Services Using etcd Services using etcd in Pigsty:\nService Purpose Config File Patroni PG HA, stores cluster state and config /pg/bin/patroni.yml VIP-Manager Binds L2 VIP on PG clusters /etc/default/vip-manager When etcd cluster membership changes permanently, reload related service configs to ensure correct access.\nUpdate Patroni’s etcd endpoint ref:\n./pgsql.yml -t pg_conf # regenerate patroni config ansible all -f 1 -b -a 'systemctl reload patroni' # reload patroni config Update VIP-Manager’s etcd endpoint ref (only for PGSQL L2 VIP):\n./pgsql.yml -t pg_vip_config # regenerate vip-manager config ansible all -f 1 -b -a 'systemctl restart vip-manager' # restart vip-manager RBAC Authentication Config v4.0 enables etcd RBAC auth by default. Related params:\nParameter Description Default etcd_root_password etcd root password Etcd.Root pg_etcd_password Patroni’s password for etcd Empty (uses cluster name) Prod recommendations:\nall: vars: etcd_root_password: 'YourSecureEtcdPassword' # change default etcd: hosts: 10.10.10.10: { etcd_seq: 1 } 10.10.10.11: { etcd_seq: 2 } 10.10.10.12: { etcd_seq: 3 } vars: etcd_cluster: etcd etcd_safeguard: true # enable safeguard for production Filesystem Layout Module creates these directories/files on target hosts:\nPath Purpose Permissions /etc/etcd/ Config dir 0750, etcd:etcd /etc/etcd/etcd.conf Main config file 0644, etcd:etcd /etc/etcd/etcd.pass Root password file 0640, root:etcd /etc/etcd/ca.crt CA cert 0644, etcd:etcd /etc/etcd/server.crt Server cert 0644, etcd:etcd /etc/etcd/server.key Server private key 0600, etcd:etcd /var/lib/etcd/ Backup data dir 0770, etcd:etcd /data/etcd/ Main data dir (configurable) 0700, etcd:etcd /etc/profile.d/etcdctl.sh Client env vars 0755, root:root /etc/systemd/system/etcd.service Systemd service 0644, root:root ","categories":["Reference"],"description":"Choose etcd cluster size based on requirements, provide reliable access.","excerpt":"Choose etcd cluster size based on requirements, provide reliable …","ref":"/docs/etcd/config/","tags":"","title":"Configuration"},{"body":"The ETCD module has 13 parameters, divided into two sections:\nETCD: 10 parameters for etcd cluster deployment and configuration ETCD_REMOVE: 3 parameters for controlling etcd cluster removal Architecture Change: Pigsty v3.6+ Since Pigsty v3.6, the etcd.yml playbook no longer includes removal functionality—removal parameters have been migrated to a standalone etcd_remove role. Starting from v4.0, RBAC authentication is enabled by default, with a new etcd_root_password parameter.\nParameter Overview The ETCD parameter group is used for etcd cluster deployment and configuration, including instance identification, cluster name, data directory, ports, and authentication password.\nParameter Type Level Description etcd_seq int I etcd instance identifier, REQUIRED etcd_cluster string C etcd cluster name, fixed to etcd by default etcd_learner bool I/A initialize etcd instance as learner? etcd_data path C etcd data directory, /data/etcd by default etcd_port port C etcd client port, 2379 by default etcd_peer_port port C etcd peer port, 2380 by default etcd_init enum C etcd initial cluster state, new or existing etcd_election_timeout int C etcd election timeout, 1000ms by default etcd_heartbeat_interval int C etcd heartbeat interval, 100ms by default etcd_root_password password G etcd root user password for RBAC authentication The ETCD_REMOVE parameter group controls etcd cluster removal behavior, including safeguard protection, data cleanup, and package uninstallation.\nParameter Type Level Description etcd_safeguard bool G/C/A safeguard to prevent purging running etcd instances? etcd_rm_data bool G/C/A remove etcd data during removal? default is true etcd_rm_pkg bool G/C/A uninstall etcd packages during removal? default is false ETCD This section contains parameters for the etcd role, which are used by the etcd.yml playbook.\nParameters are defined in roles/etcd/defaults/main.yml\n#etcd_seq: 1 # etcd instance identifier, explicitly required etcd_cluster: etcd # etcd cluster \u0026 group name, etcd by default etcd_learner: false # run etcd instance as learner? default is false etcd_data: /data/etcd # etcd data directory, /data/etcd by default etcd_port: 2379 # etcd client port, 2379 by default etcd_peer_port: 2380 # etcd peer port, 2380 by default etcd_init: new # etcd initial cluster state, new or existing etcd_election_timeout: 1000 # etcd election timeout, 1000ms by default etcd_heartbeat_interval: 100 # etcd heartbeat interval, 100ms by default etcd_root_password: Etcd.Root # etcd root user password for RBAC authentication (please change!) etcd_seq Parameter: etcd_seq, Type: int, Level: I\netcd instance identifier. This is a required parameter—you must assign a unique identifier to each etcd instance.\nHere is an example of a 3-node etcd cluster with identifiers 1 through 3:\netcd: # dcs service for postgres/patroni ha consensus hosts: # 1 node for testing, 3 or 5 for production 10.10.10.10: { etcd_seq: 1 } # etcd_seq required 10.10.10.11: { etcd_seq: 2 } # assign from 1 ~ n 10.10.10.12: { etcd_seq: 3 } # use odd numbers vars: # cluster level parameter override roles/etcd etcd_cluster: etcd # mark etcd cluster name etcd etcd_safeguard: false # safeguard against purging etcd_cluster Parameter: etcd_cluster, Type: string, Level: C\netcd cluster \u0026 group name, default value is the hard-coded etcd.\nYou can modify this parameter when you want to deploy an additional etcd cluster for backup purposes.\netcd_learner Parameter: etcd_learner, Type: bool, Level: I/A\nInitialize etcd instance as learner? Default value is false.\nWhen set to true, the etcd instance will be initialized as a learner, meaning it cannot participate in voting elections within the etcd cluster.\nUse Cases:\nCluster Expansion: When adding new members to an existing cluster, using learner mode prevents affecting cluster quorum before data synchronization completes Safe Migration: In rolling upgrade or migration scenarios, join as a learner first, then promote after confirming data synchronization Workflow:\nSet etcd_learner: true to initialize the new member as a learner Wait for data synchronization to complete (check with etcdctl endpoint status) Use etcdctl member promote \u003cmember_id\u003e to promote it to a full member Note Learner instances do not count toward cluster quorum. For example, in a 3-node cluster with 1 learner, the actual voting members are 2, which cannot tolerate any node failure.\netcd_data Parameter: etcd_data, Type: path, Level: C\netcd data directory, default is /data/etcd.\netcd_port Parameter: etcd_port, Type: port, Level: C\netcd client port, default is 2379.\netcd_peer_port Parameter: etcd_peer_port, Type: port, Level: C\netcd peer port, default is 2380.\netcd_init Parameter: etcd_init, Type: enum, Level: C\netcd initial cluster state, can be new or existing, default value: new.\nOption Values:\nValue Description Use Case new Create a new etcd cluster Initial deployment, cluster rebuild existing Join an existing etcd cluster Cluster expansion, adding new members Important Notes:\nMust use existing when expanding When adding new members to an existing etcd cluster, you must set etcd_init=existing. Otherwise, the new instance will attempt to create an independent new cluster, causing split-brain or initialization failure.\nUsage Examples:\n# Create new cluster (default behavior) ./etcd.yml # Add new member to existing cluster ./etcd.yml -l \u003cnew_ip\u003e -e etcd_init=existing # Or use the convenience script (automatically sets etcd_init=existing) bin/etcd-add \u003cnew_ip\u003e etcd_election_timeout Parameter: etcd_election_timeout, Type: int, Level: C\netcd election timeout, default is 1000 (milliseconds), i.e., 1 second.\netcd_heartbeat_interval Parameter: etcd_heartbeat_interval, Type: int, Level: C\netcd heartbeat interval, default is 100 (milliseconds).\netcd_root_password Parameter: etcd_root_password, Type: password, Level: G\netcd root user password for RBAC authentication, default value is Etcd.Root.\nPigsty v4.0 enables etcd RBAC (Role-Based Access Control) authentication by default. During cluster initialization, the etcd_auth task automatically creates the root user and enables authentication.\nPassword Storage Location:\nPassword is stored in /etc/etcd/etcd.pass file File permissions are 0640 (owned by root, readable by etcd group) The etcdctl environment script /etc/profile.d/etcdctl.sh automatically reads this file Integration with Other Components:\nPatroni uses the pg_etcd_password parameter to configure the password for connecting to etcd If pg_etcd_password is empty, Patroni will use the cluster name as password (not recommended) VIP-Manager also requires the same authentication credentials to connect to etcd Security Recommendations:\nProduction Security In production environments, it is strongly recommended to change the default password Etcd.Root. Set it in global or cluster configuration:\netcd_root_password: 'YourSecurePassword' Using configure -g will automatically generate and replace etcd_root_password\nETCD_REMOVE This section contains parameters for the etcd_remove role, which are action flags used by the etcd-rm.yml playbook.\nParameters are defined in roles/etcd_remove/defaults/main.yml\netcd_safeguard: false # prevent purging running etcd instances? etcd_rm_data: true # remove etcd data and config files during removal? etcd_rm_pkg: false # uninstall etcd packages during removal? etcd_safeguard Parameter: etcd_safeguard, Type: bool, Level: G/C/A\nSafeguard to prevent purging running etcd instances? Default value is false.\nWhen enabled, the etcd-rm.yml playbook will abort when detecting running etcd instances, preventing accidental deletion of active etcd clusters.\nRecommended Settings:\nEnvironment Recommended Description Dev/Test false Convenient for rapid rebuilding and testing Production true Prevents service interruption from accidental operations In emergencies, you can override the configuration with command-line parameters:\n./etcd-rm.yml -e etcd_safeguard=false etcd_rm_data Parameter: etcd_rm_data, Type: bool, Level: G/C/A\nRemove etcd data and configuration files during removal? Default value is true.\nWhen enabled, the etcd-rm.yml playbook will delete the following contents when removing a cluster or member:\n/etc/etcd/ - Configuration directory (including certificates and password files) /var/lib/etcd/ - Alternate data directory {{ etcd_data }} - Primary data directory (default /data/etcd) {{ systemd_dir }}/etcd.service - Systemd service unit file /etc/profile.d/etcdctl.sh - Client environment script /etc/vector/etcd.yaml - Vector log collection config Use Cases:\nScenario Recommended Description Complete removal true (default) Full cleanup, free disk space Stop service only false Preserve data for troubleshooting or recovery # Stop service only, preserve data ./etcd-rm.yml -e etcd_rm_data=false etcd_rm_pkg Parameter: etcd_rm_pkg, Type: bool, Level: G/C/A\nUninstall etcd packages during removal? Default value is false.\nWhen enabled, the etcd-rm.yml playbook will uninstall etcd packages when removing a cluster or member.\nUse Cases:\nScenario Recommended Description Normal removal false (default) Keep packages for quick redeployment Complete cleanup true Full uninstall, save disk space # Uninstall packages during removal ./etcd-rm.yml -e etcd_rm_pkg=true Tip Usually there’s no need to uninstall etcd packages. Keeping the packages speeds up subsequent redeployments since no re-download or installation is required.\n","categories":["Reference"],"description":"ETCD module provides 13 configuration parameters for fine-grained control over cluster behavior.","excerpt":"ETCD module provides 13 configuration parameters for fine-grained …","ref":"/docs/etcd/param/","tags":"","title":"Parameters"},{"body":"Common etcd admin SOPs:\nCreate Cluster: Initialize an etcd cluster Destroy Cluster: Destroy an etcd cluster CLI Environment: Configure etcd client to access server cluster RBAC Authentication: Use etcd RBAC auth Reload Config: Update etcd server member list for clients Add Member: Add new member to existing etcd cluster Remove Member: Remove member from etcd cluster Utility Scripts: Simplify ops with bin/etcd-add and bin/etcd-rm For more, refer to FAQ: ETCD.\nCreate Cluster Define etcd cluster in config inventory:\netcd: hosts: 10.10.10.10: { etcd_seq: 1 } 10.10.10.11: { etcd_seq: 2 } 10.10.10.12: { etcd_seq: 3 } vars: { etcd_cluster: etcd } Run etcd.yml playbook:\n./etcd.yml # initialize etcd cluster Architecture Change: Pigsty v3.6+ Since v3.6, etcd.yml focuses on cluster install and member addition—no longer includes removal. Use dedicated etcd-rm.yml for all removals.\nFor prod etcd clusters, enable safeguard etcd_safeguard to prevent accidental deletion.\nDestroy Cluster Use dedicated etcd-rm.yml playbook to destroy etcd cluster. Use caution!\n./etcd-rm.yml # remove entire etcd cluster ./etcd-rm.yml -e etcd_safeguard=false # override safeguard Or use utility script:\nbin/etcd-rm # remove entire etcd cluster Removal playbook respects etcd_safeguard. If true, playbook aborts to prevent accidental deletion.\nWarning Before removing etcd cluster, ensure no PG clusters use it as DCS. PG HA will break otherwise.\nCLI Environment Uses etcd v3 API by default (v2 removed in v3.6+). Pigsty auto-configures env script /etc/profile.d/etcdctl.sh on etcd nodes, loaded on login.\nExample client env config:\nalias e=\"etcdctl\" alias em=\"etcdctl member\" export ETCDCTL_ENDPOINTS=https://10.10.10.10:2379 export ETCDCTL_CACERT=/etc/etcd/ca.crt export ETCDCTL_CERT=/etc/etcd/server.crt export ETCDCTL_KEY=/etc/etcd/server.key v4.0 enables RBAC auth by default—user auth required:\nexport ETCDCTL_USER=\"root:$(cat /etc/etcd/etcd.pass)\" After configuring client env, run etcd CRUD ops:\ne put a 10 ; e get a; e del a # basic KV ops e member list # list cluster members e endpoint health # check endpoint health e endpoint status # view endpoint status RBAC Authentication v4.0 enables etcd RBAC auth by default. During cluster init, etcd_auth task auto-creates root user and enables auth.\nRoot user password set by etcd_root_password, default: Etcd.Root. Stored in /etc/etcd/etcd.pass with 0640 perms (root-owned, etcd-group readable).\nStrongly recommended to change default password in prod:\netcd: hosts: 10.10.10.10: { etcd_seq: 1 } 10.10.10.11: { etcd_seq: 2 } 10.10.10.12: { etcd_seq: 3 } vars: etcd_cluster: etcd etcd_root_password: 'YourSecurePassword' # change default Client auth methods:\n# Method 1: env vars (recommended, auto-configured in /etc/profile.d/etcdctl.sh) export ETCDCTL_USER=\"root:$(cat /etc/etcd/etcd.pass)\" # Method 2: command line etcdctl --user root:YourSecurePassword member list Patroni and etcd auth:\nPatroni uses pg_etcd_password to configure etcd connection password. If empty, Patroni uses cluster name as password (not recommended). Configure separate etcd password per PG cluster in prod.\nReload Config If etcd cluster membership changes (add/remove members), refresh etcd service endpoint references. These etcd refs in Pigsty need updates:\nConfig Location Config File Update Method etcd member config /etc/etcd/etcd.conf ./etcd.yml -t etcd_conf etcdctl env vars /etc/profile.d/etcdctl.sh ./etcd.yml -t etcd_config Patroni DCS config /pg/bin/patroni.yml ./pgsql.yml -t pg_conf VIP-Manager config /etc/default/vip-manager ./pgsql.yml -t pg_vip_config Refresh etcd member config:\n./etcd.yml -t etcd_conf # refresh /etc/etcd/etcd.conf ansible etcd -f 1 -b -a 'systemctl restart etcd' # optional: restart etcd instances Refresh etcdctl client env:\n./etcd.yml -t etcd_config # refresh /etc/profile.d/etcdctl.sh Update Patroni DCS endpoint config:\n./pgsql.yml -t pg_conf # regenerate patroni config ansible all -f 1 -b -a 'systemctl reload patroni' # reload patroni config Update VIP-Manager endpoint config (only for PGSQL L2 VIP):\n./pgsql.yml -t pg_vip_config # regenerate vip-manager config ansible all -f 1 -b -a 'systemctl restart vip-manager' # restart vip-manager Tip Using bin/etcd-add / bin/etcd-rm utility scripts? Scripts prompt config refresh commands after completion.\nAdd Member ETCD Reference: Add a member\nRecommended: Utility Script Use bin/etcd-add script to add new members to existing etcd cluster:\n# First add new member definition to config inventory, then: bin/etcd-add \u003cip\u003e # add single new member bin/etcd-add \u003cip1\u003e \u003cip2\u003e ... # add multiple new members Script auto-performs:\nValidates IP address validity Executes etcd.yml playbook (auto-sets etcd_init=existing) Provides safety warnings and countdown Prompts config refresh commands after completion Manual: Step-by-Step Add new member to existing etcd cluster:\nUpdate config inventory: Add new instance to etcd group Notify cluster: Run etcdctl member add (optional, playbook auto-does this) Initialize new member: Run playbook with etcd_init=existing parameter Promote member: Promote learner to full member (optional, required when using etcd_learner=true) Reload config: Update etcd endpoint references for all clients # After config inventory update, initialize new member ./etcd.yml -l \u003cnew_ins_ip\u003e -e etcd_init=existing # If using learner mode, manually promote etcdctl member promote \u003cnew_ins_server_id\u003e Important When adding new members, must use etcd_init=existing parameter. New instance will create new cluster instead of joining existing one otherwise.\nDetailed: Add member to etcd cluster Detailed steps. Start from single-instance etcd cluster:\netcd: hosts: 10.10.10.10: { etcd_seq: 1 } # \u003c--- only existing instance in cluster 10.10.10.11: { etcd_seq: 2 } # \u003c--- add this new member to inventory vars: { etcd_cluster: etcd } Add new member using utility script (recommended):\n$ bin/etcd-add 10.10.10.11 Or manual. First use etcdctl member add to announce new learner instance etcd-2 to existing etcd cluster:\n$ etcdctl member add etcd-2 --learner=true --peer-urls=https://10.10.10.11:2380 Member 33631ba6ced84cf8 added to cluster 6646fbcf5debc68f ETCD_NAME=\"etcd-2\" ETCD_INITIAL_CLUSTER=\"etcd-2=https://10.10.10.11:2380,etcd-1=https://10.10.10.10:2380\" ETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://10.10.10.11:2380\" ETCD_INITIAL_CLUSTER_STATE=\"existing\" Check member list with etcdctl member list (or em list), see unstarted new member:\n33631ba6ced84cf8, unstarted, , https://10.10.10.11:2380, , true # unstarted new member here 429ee12c7fbab5c1, started, etcd-1, https://10.10.10.10:2380, https://10.10.10.10:2379, false Next, use etcd.yml playbook to initialize new etcd instance etcd-2. After completion, new member has started:\n$ ./etcd.yml -l 10.10.10.11 -e etcd_init=existing # must add existing parameter ... 33631ba6ced84cf8, started, etcd-2, https://10.10.10.11:2380, https://10.10.10.11:2379, true 429ee12c7fbab5c1, started, etcd-1, https://10.10.10.10:2380, https://10.10.10.10:2379, false After new member initialized and running stably, promote from learner to follower:\n$ etcdctl member promote 33631ba6ced84cf8 # promote learner to follower Member 33631ba6ced84cf8 promoted in cluster 6646fbcf5debc68f $ em list # check again, new member promoted to full member 33631ba6ced84cf8, started, etcd-2, https://10.10.10.11:2380, https://10.10.10.11:2379, false 429ee12c7fbab5c1, started, etcd-1, https://10.10.10.10:2380, https://10.10.10.10:2379, false New member added. Don’t forget to reload config so all clients know new member.\nRepeat steps to add more members. Prod environments need at least 3 members.\nRemove Member Recommended: Utility Script Use bin/etcd-rm script to remove members from etcd cluster:\nbin/etcd-rm \u003cip\u003e # remove specified member bin/etcd-rm \u003cip1\u003e \u003cip2\u003e ... # remove multiple members bin/etcd-rm # remove entire etcd cluster Script auto-performs:\nGracefully removes members from cluster Stops and disables etcd service Cleans up data and config files Deregisters from monitoring system Manual: Step-by-Step Remove member instance from etcd cluster:\nRemove from config inventory: Comment out or delete instance, and reload config Kick from cluster: Use etcdctl member remove command Clean up instance: Use etcd-rm.yml playbook to clean up # Use dedicated removal playbook (recommended) ./etcd-rm.yml -l \u003cip\u003e # Or manual etcdctl member remove \u003cserver_id\u003e # kick from cluster ./etcd-rm.yml -l \u003cip\u003e # clean up instance Detailed: Remove member from etcd cluster Example: 3-node etcd cluster, remove instance 3.\nMethod 1: Utility script (recommended)\n$ bin/etcd-rm 10.10.10.12 Script auto-completes all operations: remove from cluster, stop service, clean up data.\nMethod 2: Manual\nFirst, refresh config by commenting out member to delete, then reload config so all clients stop using this instance.\netcd: hosts: 10.10.10.10: { etcd_seq: 1 } 10.10.10.11: { etcd_seq: 2 } # 10.10.10.12: { etcd_seq: 3 } # \u003c---- comment out this member vars: { etcd_cluster: etcd } Then use removal playbook:\n$ ./etcd-rm.yml -l 10.10.10.12 Playbook auto-executes:\nGet member list, find corresponding member ID Execute etcdctl member remove to kick from cluster Stop etcd service Clean up data and config files If manual:\n$ etcdctl member list 429ee12c7fbab5c1, started, etcd-1, https://10.10.10.10:2380, https://10.10.10.10:2379, false 33631ba6ced84cf8, started, etcd-2, https://10.10.10.11:2380, https://10.10.10.11:2379, false 93fcf23b220473fb, started, etcd-3, https://10.10.10.12:2380, https://10.10.10.12:2379, false # \u003c--- remove this $ etcdctl member remove 93fcf23b220473fb # kick from cluster Member 93fcf23b220473fb removed from cluster 6646fbcf5debc68f After execution, permanently remove from config inventory. Member removal complete.\nRepeat to remove more members. Combined with Add Member, perform rolling upgrades and migrations of etcd cluster.\nUtility Scripts v3.6+ provides utility scripts to simplify etcd cluster scaling:\nbin/etcd-add Add new members to existing etcd cluster:\nbin/etcd-add \u003cip\u003e # add single new member bin/etcd-add \u003cip1\u003e \u003cip2\u003e ... # add multiple new members Script features:\nValidates IP addresses in config inventory Auto-sets etcd_init=existing parameter Executes etcd.yml playbook to complete member addition Prompts config refresh commands after completion bin/etcd-rm Remove members or entire cluster from etcd:\nbin/etcd-rm \u003cip\u003e # remove specified member bin/etcd-rm \u003cip1\u003e \u003cip2\u003e ... # remove multiple members bin/etcd-rm # remove entire etcd cluster Script features:\nProvides safety warnings and confirmation countdown Auto-executes etcd-rm.yml playbook Gracefully removes members from cluster Cleans up data and config files ","categories":["Task"],"description":"etcd cluster management SOP: create, destroy, scale, config, and RBAC.","excerpt":"etcd cluster management SOP: create, destroy, scale, config, and RBAC.","ref":"/docs/etcd/admin/","tags":"","title":"Administration"},{"body":"The ETCD module provides two core playbooks: etcd.yml for installing and configuring etcd clusters, and etcd-rm.yml for removing etcd clusters or members.\nArchitecture Change: Pigsty v3.6+ Since Pigsty v3.6, the etcd.yml playbook focuses on cluster installation and member addition. All removal operations have been moved to the dedicated etcd-rm.yml playbook using the etcd_remove role.\netcd.yml Playbook source: etcd.yml\nThis playbook installs and configures an etcd cluster on the hardcoded etcd group, then launches the etcd service.\nThe following subtasks are available in etcd.yml:\netcd_assert : Validate etcd identity parameters (etcd_seq must be defined as a non-negative integer) etcd_install : Install etcd packages etcd_dir : Create etcd data and configuration directories etcd_config : Generate etcd configuration etcd_conf : Generate etcd main config file /etc/etcd/etcd.conf etcd_cert : Generate etcd TLS certificates (CA, server cert, private key) etcd_member : Add new member to existing cluster (only runs when etcd_init=existing) etcd_launch : Launch etcd service etcd_auth : Enable RBAC authentication (create root user and enable auth) etcd_register : Register etcd to VictoriaMetrics/Prometheus monitoring etcd-rm.yml Playbook source: etcd-rm.yml\nA dedicated playbook for removing etcd clusters or individual members. The following subtasks are available in etcd-rm.yml:\netcd_safeguard : Check safeguard and abort if enabled etcd_pause : Pause for 3 seconds, allowing user to abort with Ctrl-C etcd_deregister : Remove etcd registration from VictoriaMetrics monitoring targets etcd_leave : Try graceful leaving etcd cluster before purge etcd_svc : Stop and disable etcd service with systemd etcd_data : Remove etcd data (disable with etcd_rm_data=false) etcd_pkg : Uninstall etcd packages (enable with etcd_rm_pkg=true) The removal playbook uses the etcd_remove role with the following configurable parameters:\netcd_safeguard: Prevents accidental removal when set to true etcd_rm_data: Controls whether ETCD data is deleted (default: true) etcd_rm_pkg: Controls whether ETCD packages are uninstalled (default: false) Demo Cheatsheet Etcd Installation \u0026 Configuration:\n./etcd.yml # Initialize etcd cluster ./etcd.yml -t etcd_launch # Restart entire etcd cluster ./etcd.yml -t etcd_conf # Refresh /etc/etcd/etcd.conf with latest state ./etcd.yml -t etcd_cert # Regenerate etcd TLS certificates ./etcd.yml -l 10.10.10.12 -e etcd_init=existing # Scale out: add new member to existing cluster Etcd Removal \u0026 Cleanup:\n./etcd-rm.yml # Remove entire etcd cluster ./etcd-rm.yml -l 10.10.10.12 # Remove single etcd member ./etcd-rm.yml -e etcd_safeguard=false # Override safeguard to force removal ./etcd-rm.yml -e etcd_rm_data=false # Stop service only, preserve data ./etcd-rm.yml -e etcd_rm_pkg=true # Also uninstall etcd packages Convenience Scripts:\nbin/etcd-add \u003cip\u003e # Add new member to existing cluster (recommended) bin/etcd-rm \u003cip\u003e # Remove specific member from cluster (recommended) bin/etcd-rm # Remove entire etcd cluster Safeguard To prevent accidental deletion, Pigsty’s ETCD module provides a safeguard mechanism controlled by the etcd_safeguard parameter, which defaults to false (safeguard disabled).\nFor production etcd clusters that have been initialized, it’s recommended to enable the safeguard to prevent accidental deletion of existing etcd instances:\netcd: hosts: 10.10.10.10: { etcd_seq: 1 } 10.10.10.11: { etcd_seq: 2 } 10.10.10.12: { etcd_seq: 3 } vars: etcd_cluster: etcd etcd_safeguard: true # Enable safeguard protection When etcd_safeguard is set to true, the etcd-rm.yml playbook will detect running etcd instances and abort to prevent accidental deletion. You can override this behavior using command-line parameters:\n./etcd-rm.yml -e etcd_safeguard=false # Force override safeguard Unless you clearly understand what you’re doing, we do not recommend arbitrarily removing etcd clusters.\n","categories":["Task"],"description":"Manage etcd clusters with Ansible playbooks and quick command reference.","excerpt":"Manage etcd clusters with Ansible playbooks and quick command …","ref":"/docs/etcd/playbook/","tags":"","title":"Playbook"},{"body":" Dashboards ETCD module provides one monitoring dashboard: Etcd Overview.\nETCD Overview Dashboard ETCD Overview: Overview of ETCD cluster\nDashboard provides key ETCD status info. Notable: ETCD Aliveness—shows overall etcd cluster service status.\nRed bands = instance downtime; blue-gray below = cluster unavailable.\nAlert Rules Pigsty provides 5 preset alert rules for etcd, defined in files/prometheus/rules/etcd.yml:\nEtcdServerDown: etcd node down, CRIT alert EtcdNoLeader: etcd cluster no leader, CRIT alert EtcdQuotaFull: etcd quota \u003e 90%, WARN alert EtcdNetworkPeerRTSlow: etcd network latency slow, INFO alert EtcdWalFsyncSlow: etcd disk fsync slow, INFO alert #==============================================================# # Aliveness # #==============================================================# # etcd server instance down - alert: EtcdServerDown expr: etcd_up \u003c 1 for: 1m labels: { level: 0, severity: CRIT, category: etcd } annotations: summary: \"CRIT EtcdServerDown {{ $labels.ins }}@{{ $labels.instance }}\" description: | etcd_up[ins={{ $labels.ins }}, instance={{ $labels.instance }}] = {{ $value }} \u003c 1 https://demo.pigsty.io/d/etcd-overview #==============================================================# # Error # #==============================================================# # Etcd no Leader triggers P0 alert immediately # if dcs_failsafe mode not enabled, may cause global outage - alert: EtcdNoLeader expr: min(etcd_server_has_leader) by (cls) \u003c 1 for: 15s labels: { level: 0, severity: CRIT, category: etcd } annotations: summary: \"CRIT EtcdNoLeader: {{ $labels.cls }} {{ $value }}\" description: | etcd_server_has_leader[cls={{ $labels.cls }}] = {{ $value }} \u003c 1 https://demo.pigsty.io/d/etcd-overview?from=now-5m\u0026to=now\u0026var-cls={{$labels.cls}} #==============================================================# # Saturation # #==============================================================# - alert: EtcdQuotaFull expr: etcd:cls:quota_usage \u003e 0.90 for: 1m labels: { level: 1, severity: WARN, category: etcd } annotations: summary: \"WARN EtcdQuotaFull: {{ $labels.cls }}\" description: | etcd:cls:quota_usage[cls={{ $labels.cls }}] = {{ $value | printf \"%.3f\" }} \u003e 90% https://demo.pigsty.io/d/etcd-overview #==============================================================# # Latency # #==============================================================# # etcd network peer rt p95 \u003e 200ms for 1m - alert: EtcdNetworkPeerRTSlow expr: etcd:ins:network_peer_rt_p95_5m \u003e 0.200 for: 1m labels: { level: 2, severity: INFO, category: etcd } annotations: summary: \"INFO EtcdNetworkPeerRTSlow: {{ $labels.cls }} {{ $labels.ins }}\" description: | etcd:ins:network_peer_rt_p95_5m[cls={{ $labels.cls }}, ins={{ $labels.ins }}] = {{ $value }} \u003e 200ms https://demo.pigsty.io/d/etcd-instance?from=now-10m\u0026to=now\u0026var-cls={{ $labels.cls }} # Etcd wal fsync rt p95 \u003e 50ms - alert: EtcdWalFsyncSlow expr: etcd:ins:wal_fsync_rt_p95_5m \u003e 0.050 for: 1m labels: { level: 2, severity: INFO, category: etcd } annotations: summary: \"INFO EtcdWalFsyncSlow: {{ $labels.cls }} {{ $labels.ins }}\" description: | etcd:ins:wal_fsync_rt_p95_5m[cls={{ $labels.cls }}, ins={{ $labels.ins }}] = {{ $value }} \u003e 50ms https://demo.pigsty.io/d/etcd-instance?from=now-10m\u0026to=now\u0026var-cls={{ $labels.cls }} ","categories":["Reference"],"description":"etcd monitoring dashboards, metrics, and alert rules.","excerpt":"etcd monitoring dashboards, metrics, and alert rules.","ref":"/docs/etcd/monitor/","tags":"","title":"Monitoring"},{"body":"The ETCD module has 177 available metrics.\nMetric Name Type Labels Description etcd:ins:backend_commit_rt_p99_5m Unknown cls, ins, instance, job, ip N/A etcd:ins:disk_fsync_rt_p99_5m Unknown cls, ins, instance, job, ip N/A etcd:ins:network_peer_rt_p99_1m Unknown cls, To, ins, instance, job, ip N/A etcd_cluster_version gauge cls, cluster_version, ins, instance, job, ip Running version. 1 = ‘cluster_version’ label with current version etcd_debugging_auth_revision gauge cls, ins, instance, job, ip Current auth store revision. etcd_debugging_disk_backend_commit_rebalance_duration_seconds_bucket Unknown cls, ins, instance, job, le, ip N/A etcd_debugging_disk_backend_commit_rebalance_duration_seconds_count Unknown cls, ins, instance, job, ip N/A etcd_debugging_disk_backend_commit_rebalance_duration_seconds_sum Unknown cls, ins, instance, job, ip N/A etcd_debugging_disk_backend_commit_spill_duration_seconds_bucket Unknown cls, ins, instance, job, le, ip N/A etcd_debugging_disk_backend_commit_spill_duration_seconds_count Unknown cls, ins, instance, job, ip N/A etcd_debugging_disk_backend_commit_spill_duration_seconds_sum Unknown cls, ins, instance, job, ip N/A etcd_debugging_disk_backend_commit_write_duration_seconds_bucket Unknown cls, ins, instance, job, le, ip N/A etcd_debugging_disk_backend_commit_write_duration_seconds_count Unknown cls, ins, instance, job, ip N/A etcd_debugging_disk_backend_commit_write_duration_seconds_sum Unknown cls, ins, instance, job, ip N/A etcd_debugging_lease_granted_total counter cls, ins, instance, job, ip Total granted leases. etcd_debugging_lease_renewed_total counter cls, ins, instance, job, ip Renewed leases seen by leader. etcd_debugging_lease_revoked_total counter cls, ins, instance, job, ip Revoked leases. etcd_debugging_lease_ttl_total_bucket Unknown cls, ins, instance, job, le, ip N/A etcd_debugging_lease_ttl_total_count Unknown cls, ins, instance, job, ip N/A etcd_debugging_lease_ttl_total_sum Unknown cls, ins, instance, job, ip N/A etcd_debugging_mvcc_compact_revision gauge cls, ins, instance, job, ip Last compaction revision in store. etcd_debugging_mvcc_current_revision gauge cls, ins, instance, job, ip Current store revision. etcd_debugging_mvcc_db_compaction_keys_total counter cls, ins, instance, job, ip DB keys compacted. etcd_debugging_mvcc_db_compaction_last gauge cls, ins, instance, job, ip Last db compaction unix time. Resets to 0 on start. etcd_debugging_mvcc_db_compaction_pause_duration_milliseconds_bucket Unknown cls, ins, instance, job, le, ip N/A etcd_debugging_mvcc_db_compaction_pause_duration_milliseconds_count Unknown cls, ins, instance, job, ip N/A etcd_debugging_mvcc_db_compaction_pause_duration_milliseconds_sum Unknown cls, ins, instance, job, ip N/A etcd_debugging_mvcc_db_compaction_total_duration_milliseconds_bucket Unknown cls, ins, instance, job, le, ip N/A etcd_debugging_mvcc_db_compaction_total_duration_milliseconds_count Unknown cls, ins, instance, job, ip N/A etcd_debugging_mvcc_db_compaction_total_duration_milliseconds_sum Unknown cls, ins, instance, job, ip N/A etcd_debugging_mvcc_events_total counter cls, ins, instance, job, ip Events sent by this member. etcd_debugging_mvcc_index_compaction_pause_duration_milliseconds_bucket Unknown cls, ins, instance, job, le, ip N/A etcd_debugging_mvcc_index_compaction_pause_duration_milliseconds_count Unknown cls, ins, instance, job, ip N/A etcd_debugging_mvcc_index_compaction_pause_duration_milliseconds_sum Unknown cls, ins, instance, job, ip N/A etcd_debugging_mvcc_keys_total gauge cls, ins, instance, job, ip Total keys. etcd_debugging_mvcc_pending_events_total gauge cls, ins, instance, job, ip Pending events to send. etcd_debugging_mvcc_range_total counter cls, ins, instance, job, ip Ranges seen by this member. etcd_debugging_mvcc_slow_watcher_total gauge cls, ins, instance, job, ip Unsynced slow watchers. etcd_debugging_mvcc_total_put_size_in_bytes gauge cls, ins, instance, job, ip Total put kv size seen by this member. etcd_debugging_mvcc_watch_stream_total gauge cls, ins, instance, job, ip Watch streams. etcd_debugging_mvcc_watcher_total gauge cls, ins, instance, job, ip Watchers. etcd_debugging_server_lease_expired_total counter cls, ins, instance, job, ip Expired leases. etcd_debugging_snap_save_marshalling_duration_seconds_bucket Unknown cls, ins, instance, job, le, ip N/A etcd_debugging_snap_save_marshalling_duration_seconds_count Unknown cls, ins, instance, job, ip N/A etcd_debugging_snap_save_marshalling_duration_seconds_sum Unknown cls, ins, instance, job, ip N/A etcd_debugging_snap_save_total_duration_seconds_bucket Unknown cls, ins, instance, job, le, ip N/A etcd_debugging_snap_save_total_duration_seconds_count Unknown cls, ins, instance, job, ip N/A etcd_debugging_snap_save_total_duration_seconds_sum Unknown cls, ins, instance, job, ip N/A etcd_debugging_store_expires_total counter cls, ins, instance, job, ip Expired keys. etcd_debugging_store_reads_total counter cls, action, ins, instance, job, ip Reads (get/getRecursive) to this member. etcd_debugging_store_watch_requests_total counter cls, ins, instance, job, ip Incoming watch requests (new/reestablished). etcd_debugging_store_watchers gauge cls, ins, instance, job, ip Active watchers. etcd_debugging_store_writes_total counter cls, action, ins, instance, job, ip Writes (set/compareAndDelete) to this member. etcd_disk_backend_commit_duration_seconds_bucket Unknown cls, ins, instance, job, le, ip N/A etcd_disk_backend_commit_duration_seconds_count Unknown cls, ins, instance, job, ip N/A etcd_disk_backend_commit_duration_seconds_sum Unknown cls, ins, instance, job, ip N/A etcd_disk_backend_defrag_duration_seconds_bucket Unknown cls, ins, instance, job, le, ip N/A etcd_disk_backend_defrag_duration_seconds_count Unknown cls, ins, instance, job, ip N/A etcd_disk_backend_defrag_duration_seconds_sum Unknown cls, ins, instance, job, ip N/A etcd_disk_backend_snapshot_duration_seconds_bucket Unknown cls, ins, instance, job, le, ip N/A etcd_disk_backend_snapshot_duration_seconds_count Unknown cls, ins, instance, job, ip N/A etcd_disk_backend_snapshot_duration_seconds_sum Unknown cls, ins, instance, job, ip N/A etcd_disk_defrag_inflight gauge cls, ins, instance, job, ip Defrag active. 1 = active, 0 = not. etcd_disk_wal_fsync_duration_seconds_bucket Unknown cls, ins, instance, job, le, ip N/A etcd_disk_wal_fsync_duration_seconds_count Unknown cls, ins, instance, job, ip N/A etcd_disk_wal_fsync_duration_seconds_sum Unknown cls, ins, instance, job, ip N/A etcd_disk_wal_write_bytes_total gauge cls, ins, instance, job, ip WAL bytes written. etcd_grpc_proxy_cache_hits_total gauge cls, ins, instance, job, ip Cache hits. etcd_grpc_proxy_cache_keys_total gauge cls, ins, instance, job, ip Keys/ranges cached. etcd_grpc_proxy_cache_misses_total gauge cls, ins, instance, job, ip Cache misses. etcd_grpc_proxy_events_coalescing_total counter cls, ins, instance, job, ip Events coalescing. etcd_grpc_proxy_watchers_coalescing_total gauge cls, ins, instance, job, ip Current watchers coalescing. etcd_mvcc_db_open_read_transactions gauge cls, ins, instance, job, ip Open read transactions. etcd_mvcc_db_total_size_in_bytes gauge cls, ins, instance, job, ip DB physical bytes allocated. etcd_mvcc_db_total_size_in_use_in_bytes gauge cls, ins, instance, job, ip DB logical bytes in use. etcd_mvcc_delete_total counter cls, ins, instance, job, ip Deletes seen by this member. etcd_mvcc_hash_duration_seconds_bucket Unknown cls, ins, instance, job, le, ip N/A etcd_mvcc_hash_duration_seconds_count Unknown cls, ins, instance, job, ip N/A etcd_mvcc_hash_duration_seconds_sum Unknown cls, ins, instance, job, ip N/A etcd_mvcc_hash_rev_duration_seconds_bucket Unknown cls, ins, instance, job, le, ip N/A etcd_mvcc_hash_rev_duration_seconds_count Unknown cls, ins, instance, job, ip N/A etcd_mvcc_hash_rev_duration_seconds_sum Unknown cls, ins, instance, job, ip N/A etcd_mvcc_put_total counter cls, ins, instance, job, ip Puts seen by this member. etcd_mvcc_range_total counter cls, ins, instance, job, ip Ranges seen by this member. etcd_mvcc_txn_total counter cls, ins, instance, job, ip Txns seen by this member. etcd_network_active_peers gauge cls, ins, Local, instance, job, ip, Remote Active peer connections. etcd_network_client_grpc_received_bytes_total counter cls, ins, instance, job, ip gRPC client bytes received. etcd_network_client_grpc_sent_bytes_total counter cls, ins, instance, job, ip gRPC client bytes sent. etcd_network_peer_received_bytes_total counter cls, ins, instance, job, ip, From Peer bytes received. etcd_network_peer_round_trip_time_seconds_bucket Unknown cls, To, ins, instance, job, le, ip N/A etcd_network_peer_round_trip_time_seconds_count Unknown cls, To, ins, instance, job, ip N/A etcd_network_peer_round_trip_time_seconds_sum Unknown cls, To, ins, instance, job, ip N/A etcd_network_peer_sent_bytes_total counter cls, To, ins, instance, job, ip Peer bytes sent. etcd_server_apply_duration_seconds_bucket Unknown cls, version, ins, instance, job, le, success, ip, op N/A etcd_server_apply_duration_seconds_count Unknown cls, version, ins, instance, job, success, ip, op N/A etcd_server_apply_duration_seconds_sum Unknown cls, version, ins, instance, job, success, ip, op N/A etcd_server_client_requests_total counter client_api_version, cls, ins, instance, type, job, ip Client requests per version. etcd_server_go_version gauge cls, ins, instance, job, server_go_version, ip Go version running. 1 = ‘server_go_version’ label with current version. etcd_server_has_leader gauge cls, ins, instance, job, ip Leader exists. 1 = exists, 0 = not. etcd_server_health_failures counter cls, ins, instance, job, ip Failed health checks. etcd_server_health_success counter cls, ins, instance, job, ip Successful health checks. etcd_server_heartbeat_send_failures_total counter cls, ins, instance, job, ip Leader heartbeat send failures (likely overloaded from slow disk). etcd_server_id gauge cls, ins, instance, job, server_id, ip Server/member ID (hex). 1 = ‘server_id’ label with current ID. etcd_server_is_leader gauge cls, ins, instance, job, ip Member is leader. 1 if is, 0 otherwise. etcd_server_is_learner gauge cls, ins, instance, job, ip Member is learner. 1 if is, 0 otherwise. etcd_server_leader_changes_seen_total counter cls, ins, instance, job, ip Leader changes seen. etcd_server_learner_promote_successes counter cls, ins, instance, job, ip Successful learner promotions while this member is leader. etcd_server_proposals_applied_total gauge cls, ins, instance, job, ip Consensus proposals applied. etcd_server_proposals_committed_total gauge cls, ins, instance, job, ip Consensus proposals committed. etcd_server_proposals_failed_total counter cls, ins, instance, job, ip Failed proposals seen. etcd_server_proposals_pending gauge cls, ins, instance, job, ip Pending proposals to commit. etcd_server_quota_backend_bytes gauge cls, ins, instance, job, ip Backend storage quota bytes. etcd_server_read_indexes_failed_total counter cls, ins, instance, job, ip Failed read indexes seen. etcd_server_slow_apply_total counter cls, ins, instance, job, ip Slow apply requests (likely overloaded from slow disk). etcd_server_slow_read_indexes_total counter cls, ins, instance, job, ip Pending read indexes not in sync with leader or timed out read index requests. etcd_server_snapshot_apply_in_progress_total gauge cls, ins, instance, job, ip 1 if server applying incoming snapshot. 0 if none. etcd_server_version gauge cls, server_version, ins, instance, job, ip Version running. 1 = ‘server_version’ label with current version. etcd_snap_db_fsync_duration_seconds_bucket Unknown cls, ins, instance, job, le, ip N/A etcd_snap_db_fsync_duration_seconds_count Unknown cls, ins, instance, job, ip N/A etcd_snap_db_fsync_duration_seconds_sum Unknown cls, ins, instance, job, ip N/A etcd_snap_db_save_total_duration_seconds_bucket Unknown cls, ins, instance, job, le, ip N/A etcd_snap_db_save_total_duration_seconds_count Unknown cls, ins, instance, job, ip N/A etcd_snap_db_save_total_duration_seconds_sum Unknown cls, ins, instance, job, ip N/A etcd_snap_fsync_duration_seconds_bucket Unknown cls, ins, instance, job, le, ip N/A etcd_snap_fsync_duration_seconds_count Unknown cls, ins, instance, job, ip N/A etcd_snap_fsync_duration_seconds_sum Unknown cls, ins, instance, job, ip N/A etcd_up Unknown cls, ins, instance, job, ip N/A go_gc_duration_seconds summary cls, ins, instance, job, quantile, ip GC pause duration summary. go_gc_duration_seconds_count Unknown cls, ins, instance, job, ip N/A go_gc_duration_seconds_sum Unknown cls, ins, instance, job, ip N/A go_goroutines gauge cls, ins, instance, job, ip Goroutines. go_info gauge cls, version, ins, instance, job, ip Go environment info. go_memstats_alloc_bytes gauge cls, ins, instance, job, ip Bytes allocated and in use. go_memstats_alloc_bytes_total counter cls, ins, instance, job, ip Bytes allocated, even if freed. go_memstats_buck_hash_sys_bytes gauge cls, ins, instance, job, ip Bytes used by profiling bucket hash table. go_memstats_frees_total counter cls, ins, instance, job, ip Frees. go_memstats_gc_cpu_fraction gauge cls, ins, instance, job, ip GC CPU fraction since program started. go_memstats_gc_sys_bytes gauge cls, ins, instance, job, ip Bytes used for GC system metadata. go_memstats_heap_alloc_bytes gauge cls, ins, instance, job, ip Heap bytes allocated and in use. go_memstats_heap_idle_bytes gauge cls, ins, instance, job, ip Heap bytes waiting to be used. go_memstats_heap_inuse_bytes gauge cls, ins, instance, job, ip Heap bytes in use. go_memstats_heap_objects gauge cls, ins, instance, job, ip Allocated objects. go_memstats_heap_released_bytes gauge cls, ins, instance, job, ip Heap bytes released to OS. go_memstats_heap_sys_bytes gauge cls, ins, instance, job, ip Heap bytes obtained from system. go_memstats_last_gc_time_seconds gauge cls, ins, instance, job, ip Seconds since 1970 of last GC. go_memstats_lookups_total counter cls, ins, instance, job, ip Pointer lookups. go_memstats_mallocs_total counter cls, ins, instance, job, ip Mallocs. go_memstats_mcache_inuse_bytes gauge cls, ins, instance, job, ip Bytes in use by mcache structures. go_memstats_mcache_sys_bytes gauge cls, ins, instance, job, ip Bytes used for mcache structures from system. go_memstats_mspan_inuse_bytes gauge cls, ins, instance, job, ip Bytes in use by mspan structures. go_memstats_mspan_sys_bytes gauge cls, ins, instance, job, ip Bytes used for mspan structures from system. go_memstats_next_gc_bytes gauge cls, ins, instance, job, ip Heap bytes when next GC will take place. go_memstats_other_sys_bytes gauge cls, ins, instance, job, ip Bytes used for other system allocations. go_memstats_stack_inuse_bytes gauge cls, ins, instance, job, ip Bytes in use by stack allocator. go_memstats_stack_sys_bytes gauge cls, ins, instance, job, ip Bytes obtained from system for stack allocator. go_memstats_sys_bytes gauge cls, ins, instance, job, ip Bytes obtained from system. go_threads gauge cls, ins, instance, job, ip OS threads created. grpc_server_handled_total counter cls, ins, instance, job, grpc_code, grpc_method, grpc_type, ip, grpc_service RPCs completed on server. grpc_server_msg_received_total counter cls, ins, instance, job, grpc_type, grpc_method, ip, grpc_service RPC stream messages received on server. grpc_server_msg_sent_total counter cls, ins, instance, job, grpc_type, grpc_method, ip, grpc_service gRPC stream messages sent on server. grpc_server_started_total counter cls, ins, instance, job, grpc_type, grpc_method, ip, grpc_service RPCs started on server. os_fd_limit gauge cls, ins, instance, job, ip FD limit. os_fd_used gauge cls, ins, instance, job, ip Used FDs. process_cpu_seconds_total counter cls, ins, instance, job, ip User + system CPU seconds. process_max_fds gauge cls, ins, instance, job, ip Max FDs. process_open_fds gauge cls, ins, instance, job, ip Open FDs. process_resident_memory_bytes gauge cls, ins, instance, job, ip Resident memory bytes. process_start_time_seconds gauge cls, ins, instance, job, ip Start time (unix epoch seconds). process_virtual_memory_bytes gauge cls, ins, instance, job, ip Virtual memory bytes. process_virtual_memory_max_bytes gauge cls, ins, instance, job, ip Max virtual memory bytes. promhttp_metric_handler_requests_in_flight gauge cls, ins, instance, job, ip Current scrapes. promhttp_metric_handler_requests_total counter cls, ins, instance, job, ip, code Scrapes by HTTP status code. scrape_duration_seconds Unknown cls, ins, instance, job, ip N/A scrape_samples_post_metric_relabeling Unknown cls, ins, instance, job, ip N/A scrape_samples_scraped Unknown cls, ins, instance, job, ip N/A scrape_series_added Unknown cls, ins, instance, job, ip N/A up Unknown cls, ins, instance, job, ip N/A ","categories":["Reference"],"description":"Complete monitoring metrics list provided by Pigsty ETCD module","excerpt":"Complete monitoring metrics list provided by Pigsty ETCD module","ref":"/docs/etcd/metric/","tags":"","title":"Metrics"},{"body":" What is etcd’s role in Pigsty? etcd is a distributed, reliable key-value store for critical system data. Pigsty uses etcd as DCS (Distributed Config Store) service for Patroni, storing PG HA status.\nPatroni uses etcd for: cluster failure detection, auto failover, primary-replica switchover, and cluster config management.\netcd is critical for PG HA. etcd’s availability and DR ensured through multiple distributed nodes.\nWhat’s the appropriate etcd cluster size? If more than half (including exactly half) of etcd instances unavailable, etcd cluster enters unavailable state—refuses service.\nExample: 3-node cluster allows max 1 node failure while 2 others continue; 5-node cluster tolerates 2 node failures.\nNote: Learner instances don’t count toward members—3-node cluster with 1 learner = 2 actual members, zero fault tolerance.\nIn prod, use odd number of instances. For prod, recommend 3-node or 5-node for reliability.\nImpact of etcd unavailability? If etcd cluster unavailable, affects PG control plane but not data plane—existing PG clusters continue running, but Patroni management ops fail.\nDuring etcd failure: PG HA can’t auto failover, can’t use patronictl for PG management (config changes, manual failover, etc.).\nAnsible playbooks unaffected by etcd failure: create DB, create user, refresh HBA/Service config. During etcd failure, operate PG clusters directly.\nNote: Behavior applies to Patroni \u003e=3.0 (Pigsty \u003e=2.0). With older Patroni (\u003c3.0, Pigsty 1.x), etcd/consul failure causes severe global impact:\nAll PG clusters demote: primaries → replicas, reject writes, etcd failure amplifies to global PG failure. Patroni 3.0 introduced DCS Failsafe—significantly improved.\nWhat data does etcd store? In Pigsty, etcd is PG HA only—no other config/state data.\nPG HA component Patroni auto-generates and manages etcd data. If lost in etcd, Patroni auto-rebuilds.\nThus, by default, etcd in Pigsty = “stateless service”—destroyable and rebuildable, simplifies maintenance.\nIf using etcd for other purposes (K8s metadata, custom storage), backup etcd data yourself and restore after cluster recovery.\nRecover from etcd failure? Since etcd in Pigsty = PG HA only = “stateless service”—disposable, rebuildable. Failures? “restart” or “reset” to stop bleeding.\nRestart etcd cluster:\n./etcd.yml -t etcd_launch Reset etcd cluster:\n./etcd.yml For custom etcd data: backup and restore after recovery.\nEtcd maintenance considerations? Simple answer: don’t fill up etcd.\nPigsty v2.6+ enables etcd auto-compaction and 16GB backend quota—usually fine.\netcd’s data model = each write generates new version.\nFrequent writes (even few keys) = growing etcd DB size. At capacity limit, etcd rejects writes → PG HA breaks.\nPigsty’s default etcd config includes optimizations:\nauto-compaction-mode: periodic # periodic auto compaction auto-compaction-retention: \"24h\" # retain 24 hours history quota-backend-bytes: 17179869184 # 16 GiB quota More details: etcd official maintenance guide.\nNote Before Pigsty v2.6? Manually enable etcd auto GC.\nEnable etcd auto garbage collection? Earlier Pigsty (v2.0 - v2.5)? Enable etcd auto-compaction in prod to avoid quota-based unavailability.\nEdit etcd config template: roles/etcd/templates/etcd.conf.j2:\nauto-compaction-mode: periodic auto-compaction-retention: \"24h\" quota-backend-bytes: 17179869184 Then set related PG clusters to maintenance mode and redeploy etcd with ./etcd.yml.\nThis increases default quota from 2 GiB → 16 GiB, retains last 24h writes—avoids infinite growth.\nWhere is PG HA data stored in etcd? By default, Patroni uses pg_namespace prefix (default: /pg) for all metadata keys, followed by PG cluster name.\nExample: PG cluster pg-meta stores metadata under /pg/pg-meta.\netcdctl get /pg/pg-meta --prefix Sample data:\n/pg/pg-meta/config {\"ttl\":30,\"loop_wait\":10,\"retry_timeout\":10,\"primary_start_timeout\":10,\"maximum_lag_on_failover\":1048576,\"maximum_lag_on_syncnode\":-1,\"primary_stop_timeout\":30,\"synchronous_mode\":false,\"synchronous_mode_strict\":false,\"failsafe_mode\":true,\"pg_version\":16,\"pg_cluster\":\"pg-meta\",\"pg_shard\":\"pg-meta\",\"pg_group\":0,\"postgresql\":{\"use_slots\":true,\"use_pg_rewind\":true,\"remove_data_directory_on_rewind_failure\":true,\"parameters\":{\"max_connections\":100,\"superuser_reserved_connections\":10,\"max_locks_per_transaction\":200,\"max_prepared_transactions\":0,\"track_commit_timestamp\":\"on\",\"wal_level\":\"logical\",\"wal_log_hints\":\"on\",\"max_worker_processes\":16,\"max_wal_senders\":50,\"max_replication_slots\":50,\"password_encryption\":\"scram-sha-256\",\"ssl\":\"on\",\"ssl_cert_file\":\"/pg/cert/server.crt\",\"ssl_key_file\":\"/pg/cert/server.key\",\"ssl_ca_file\":\"/pg/cert/ca.crt\",\"shared_buffers\":\"7969MB\",\"maintenance_work_mem\":\"1993MB\",\"work_mem\":\"79MB\",\"max_parallel_workers\":8,\"max_parallel_maintenance_workers\":2,\"max_parallel_workers_per_gather\":0,\"hash_mem_multiplier\":8.0,\"huge_pages\":\"try\",\"temp_file_limit\":\"7GB\",\"vacuum_cost_delay\":\"20ms\",\"vacuum_cost_limit\":2000,\"bgwriter_delay\":\"10ms\",\"bgwriter_lru_maxpages\":800,\"bgwriter_lru_multiplier\":5.0,\"min_wal_size\":\"7GB\",\"max_wal_size\":\"28GB\",\"max_slot_wal_keep_size\":\"42GB\",\"wal_buffers\":\"16MB\",\"wal_writer_delay\":\"20ms\",\"wal_writer_flush_after\":\"1MB\",\"commit_delay\":20,\"commit_siblings\":10,\"checkpoint_timeout\":\"15min\",\"checkpoint_completion_target\":0.8,\"archive_mode\":\"on\",\"archive_timeout\":300,\"archive_command\":\"pgbackrest --stanza=pg-meta archive-push %p\",\"max_standby_archive_delay\":\"10min\",\"max_standby_streaming_delay\":\"3min\",\"wal_receiver_status_interval\":\"1s\",\"hot_standby_feedback\":\"on\",\"wal_receiver_timeout\":\"60s\",\"max_logical_replication_workers\":8,\"max_sync_workers_per_subscription\":6,\"random_page_cost\":1.1,\"effective_io_concurrency\":1000,\"effective_cache_size\":\"23907MB\",\"default_statistics_target\":200,\"log_destination\":\"csvlog\",\"logging_collector\":\"on\",\"l... ode=prefer\"}} /pg/pg-meta/failsafe {\"pg-meta-2\":\"http://10.10.10.11:8008/patroni\",\"pg-meta-1\":\"http://10.10.10.10:8008/patroni\"} /pg/pg-meta/initialize 7418384210787662172 /pg/pg-meta/leader pg-meta-1 /pg/pg-meta/members/pg-meta-1 {\"conn_url\":\"postgres://10.10.10.10:5432/postgres\",\"api_url\":\"http://10.10.10.10:8008/patroni\",\"state\":\"running\",\"role\":\"primary\",\"version\":\"4.0.1\",\"tags\":{\"clonefrom\":true,\"version\":\"16\",\"spec\":\"8C.32G.125G\",\"conf\":\"tiny.yml\"},\"xlog_location\":184549376,\"timeline\":1} /pg/pg-meta/members/pg-meta-2 {\"conn_url\":\"postgres://10.10.10.11:5432/postgres\",\"api_url\":\"http://10.10.10.11:8008/patroni\",\"state\":\"running\",\"role\":\"replica\",\"version\":\"4.0.1\",\"tags\":{\"clonefrom\":true,\"version\":\"16\",\"spec\":\"8C.32G.125G\",\"conf\":\"tiny.yml\"},\"xlog_location\":184549376,\"replication_state\":\"streaming\",\"timeline\":1} /pg/pg-meta/status {\"optime\":184549376,\"slots\":{\"pg_meta_2\":184549376,\"pg_meta_1\":184549376},\"retain_slots\":[\"pg_meta_1\",\"pg_meta_2\"]} Use external existing etcd cluster? Config inventory hardcodes etcd group—members used as DCS servers for PGSQL. Initialize with etcd.yml or assume external cluster exists.\nTo use external etcd: define as usual. Skip etcd.yml execution since cluster exists—no deployment needed.\nRequirement: external etcd cluster certificate must use same CA as Pigsty—otherwise clients can’t use Pigsty’s self-signed certs.\nAdd new member to existing etcd cluster? For detailed process, refer to Add member to etcd cluster\nRecommended: Utility script\n# First add new member to config inventory, then: bin/etcd-add \u003cip\u003e # add single new member bin/etcd-add \u003cip1\u003e # add multiple new members Manual method:\netcdctl member add \u003cetcd-?\u003e --learner=true --peer-urls=https://\u003cnew_ins_ip\u003e:2380 # announce new member ./etcd.yml -l \u003cnew_ins_ip\u003e -e etcd_init=existing # initialize new member etcdctl member promote \u003cnew_ins_server_id\u003e # promote to full member Recommend: add one new member at a time.\nRemove member from existing etcd cluster? For detailed process, refer to Remove member from etcd cluster\nRecommended: Utility script\nbin/etcd-rm \u003cip\u003e # remove specified member bin/etcd-rm # remove entire etcd cluster Manual method:\n./etcd-rm.yml -l \u003cins_ip\u003e # use dedicated removal playbook etcdctl member remove \u003cetcd_server_id\u003e # kick from cluster ./etcd-rm.yml -l \u003cins_ip\u003e # clean up instance Configure etcd RBAC authentication? Pigsty v4.0 enables etcd RBAC auth by default. Root password set by etcd_root_password, default: Etcd.Root.\nProd recommendation: change default password\nall: vars: etcd_root_password: 'YourSecurePassword' Client auth:\n# On etcd nodes, env vars auto-configured source /etc/profile.d/etcdctl.sh etcdctl member list # Manual auth config export ETCDCTL_USER=\"root:YourSecurePassword\" export ETCDCTL_CACERT=/etc/etcd/ca.crt export ETCDCTL_CERT=/etc/etcd/server.crt export ETCDCTL_KEY=/etc/etcd/server.key More: RBAC Authentication.\n","categories":["Reference"],"description":"Frequently asked questions about Pigsty etcd module","excerpt":"Frequently asked questions about Pigsty etcd module","ref":"/docs/etcd/faq/","tags":"","title":"FAQ"},{"body":"MinIO is an S3-compatible multi-cloud object storage software, open-sourced under the AGPLv3 license.\nMinIO can be used to store documents, images, videos, and backups. Pigsty natively supports deploying various MinIO clusters with native multi-node multi-disk high availability support, easy to scale, secure, and ready to use out of the box. It has been used in production environments at 10PB+ scale.\nMinIO is an optional module in Pigsty. You can use MinIO as an optional storage repository for PostgreSQL backups, supplementing the default local POSIX filesystem repository. If using the MinIO backup repository, the MINIO module should be installed before any PGSQL modules. MinIO requires a trusted CA certificate to work, so it depends on the NODE module.\nQuick Start Here’s a simple example of MinIO single-node single-disk deployment:\n# Define MinIO cluster in the config inventory minio: { hosts: { 10.10.10.10: { minio_seq: 1 } }, vars: { minio_cluster: minio } } ./minio.yml -l minio # Deploy MinIO module on the minio group After deployment, you can access MinIO via:\nS3 API: https://sss.pigsty:9000 (requires DNS resolution for the domain) Web Console: https://\u003cminio-ip\u003e:9001 (default username/password: minioadmin / S3User.MinIO) Command Line: mcli ls sss/ (alias pre-configured on the admin node) Deployment Modes MinIO supports three major deployment modes:\nMode Description Use Cases Single-Node Single-Disk (SNSD) Single node, single data directory Development, testing, demo Single-Node Multi-Disk (SNMD) Single node, multiple disks Resource-constrained small-scale deployments Multi-Node Multi-Disk (MNMD) Multiple nodes, multiple disks per node Recommended for production Additionally, you can use multi-pool deployment to scale existing clusters, or deploy multiple clusters.\nKey Features S3 Compatible: Fully compatible with AWS S3 API, seamlessly integrates with various S3 clients and tools High Availability: Native support for multi-node multi-disk deployment, tolerates node and disk failures Secure: HTTPS encrypted transmission enabled by default, supports server-side encryption Monitoring: Out-of-the-box Grafana dashboards and Prometheus alerting rules Easy to Use: Pre-configured mcli client alias, one-click deployment and management ","categories":["Reference"],"description":"Pigsty has built-in MinIO support, an open-source S3-compatible object storage that can be used for PGSQL cold backup storage.","excerpt":"Pigsty has built-in MinIO support, an open-source S3-compatible object …","ref":"/docs/minio/","tags":"","title":"Module: MINIO"},{"body":"After you configure and deploy the MinIO cluster with the playbook, you can start using and accessing the MinIO cluster by following the instructions here.\nDeploy Cluster Deploying an out-of-the-box single-node single-disk MinIO instance in Pigsty is straightforward. First, define a MinIO cluster in the config inventory:\nminio: { hosts: { 10.10.10.10: { minio_seq: 1 } }, vars: { minio_cluster: minio } } Then, run the minio.yml playbook provided by Pigsty against the defined group (here minio):\n./minio.yml -l minio Note that in deploy.yml, pre-defined MinIO clusters will be automatically created, so you don’t need to manually run the minio.yml playbook again.\nIf you plan to deploy a production-grade large-scale multi-node MinIO cluster, we strongly recommend reading the Pigsty MinIO configuration documentation and the MinIO official documentation before proceeding.\nAccess Cluster Note: MinIO services must be accessed via domain name and HTTPS, so make sure the MinIO service domain (default sss.pigsty) correctly points to the MinIO server node.\nYou can add static resolution records in node_etc_hosts, or manually modify the /etc/hosts file You can add a record on the internal DNS server if you already have an existing DNS service If you have enabled the DNS server on Infra nodes, you can add records in dns_records For production environment access to MinIO, we recommend using the first method: static DNS resolution records, to avoid MinIO’s additional dependency on DNS.\nYou should point the MinIO service domain to the IP address and service port of the MinIO server node, or the IP address and service port of the load balancer. Pigsty uses the default MinIO service domain sss.pigsty, which defaults to localhost for single-node deployment, serving on port 9000.\nIn some examples, HAProxy instances are also deployed on the MinIO cluster to expose services. In this case, 9002 is the service port used in the templates.\nAdding Alias To access the MinIO server cluster using the mcli client, you need to first configure the server alias:\nmcli alias ls # list minio alias (default is sss) mcli alias set sss https://sss.pigsty:9000 minioadmin S3User.MinIO # root user mcli alias set sss https://sss.pigsty:9002 minioadmin S3User.MinIO # root user, using load balancer port 9002 mcli alias set pgbackrest https://sss.pigsty:9000 pgbackrest S3User.Backup # use backup user On the admin user of the admin node, a MinIO alias named sss is pre-configured and can be used directly.\nFor the full functionality reference of the MinIO client tool mcli, please refer to the documentation: MinIO Client.\nNote: Use Your Actual Password The password S3User.MinIO in the above examples is the Pigsty default. If you modified minio_secret_key during deployment, please use your actual configured password.\nUser Management You can manage business users in MinIO using mcli. For example, here we can create two business users using the command line:\nmcli admin user list sss # list all users on sss set +o history # hide password in history and create minio users mcli admin user add sss dba S3User.DBA mcli admin user add sss pgbackrest S3User.Backup set -o history Bucket Management You can perform CRUD operations on buckets in MinIO:\nmcli ls sss/ # list all buckets on alias 'sss' mcli mb --ignore-existing sss/hello # create a bucket named 'hello' mcli rb --force sss/hello # force delete the 'hello' bucket Object Management You can also perform CRUD operations on objects within buckets. For details, please refer to the official documentation: Object Management\nmcli cp /www/pigsty/* sss/infra/ # upload local repo content to MinIO infra bucket mcli cp sss/infra/plugins.tgz /tmp/ # download file from minio to local mcli ls sss/infra # list all files in the infra bucket mcli rm sss/infra/plugins.tgz # delete specific file in infra bucket mcli cat sss/infra/repo_complete # view file content in infra bucket Using rclone Pigsty repository provides rclone, a convenient multi-cloud object storage client that you can use to access MinIO services.\nyum install rclone; # EL-compatible systems apt install rclone; # Debian/Ubuntu systems mkdir -p ~/.config/rclone/; tee ~/.config/rclone/rclone.conf \u003e /dev/null \u003c\u003cEOF [sss] type = s3 access_key_id = minioadmin secret_access_key = S3User.MinIO endpoint = https://sss.pigsty:9000 EOF rclone ls sss:/ Note: HTTPS and Certificate Trust If MinIO uses HTTPS (default configuration), you need to ensure the client trusts Pigsty’s CA certificate (/etc/pki/ca.crt), or add no_check_certificate = true in the rclone configuration to skip certificate verification (not recommended for production).\nConfigure Backup Repository In Pigsty, the default use case for MinIO is as a backup storage repository for pgBackRest. When you modify pgbackrest_method to minio, the PGSQL module will automatically switch the backup repository to MinIO.\npgbackrest_method: local # pgbackrest repo method: local,minio,[user-defined...] pgbackrest_repo: # pgbackrest repo: https://pgbackrest.org/configuration.html#section-repository local: # default pgbackrest repo with local posix fs path: /pg/backup # local backup directory, `/pg/backup` by default retention_full_type: count # retention full backups by count retention_full: 2 # keep 2, at most 3 full backup when using local fs repo minio: # optional minio repo for pgbackrest type: s3 # minio is s3-compatible, so s3 is used s3_endpoint: sss.pigsty # minio endpoint domain name, `sss.pigsty` by default s3_region: us-east-1 # minio region, us-east-1 by default, useless for minio s3_bucket: pgsql # minio bucket name, `pgsql` by default s3_key: pgbackrest # minio user access key for pgbackrest s3_key_secret: S3User.Backup # minio user secret key for pgbackrest s3_uri_style: path # use path style uri for minio rather than host style path: /pgbackrest # minio backup path, default is `/pgbackrest` storage_port: 9000 # minio port, 9000 by default storage_ca_file: /pg/cert/ca.crt # minio ca file path, `/pg/cert/ca.crt` by default bundle: y # bundle small files into a single file cipher_type: aes-256-cbc # enable AES encryption for remote backup repo cipher_pass: pgBackRest # AES encryption password, default is 'pgBackRest' retention_full_type: time # retention full backup by time on minio repo retention_full: 14 # keep full backup for last 14 days Note that if you are using a multi-node MinIO cluster and exposing services through a load balancer, you need to modify the s3_endpoint and storage_port parameters accordingly.\n","categories":["Reference"],"description":"Getting started: how to use MinIO? How to reliably access MinIO? How to use mc / rclone client tools?","excerpt":"Getting started: how to use MinIO? How to reliably access MinIO? How …","ref":"/docs/minio/usage/","tags":"","title":"Usage"},{"body":"Before deploying MinIO, you need to define a MinIO cluster in the config inventory. MinIO has three classic deployment modes:\nSingle-Node Single-Disk: SNSD: Single-node single-disk mode, can use any directory as a data disk, for development, testing, and demo only. Single-Node Multi-Disk: SNMD: Compromise mode, using multiple disks (\u003e=2) on a single server, only when resources are extremely limited. Multi-Node Multi-Disk: MNMD: Multi-node multi-disk mode, standard production deployment with the best reliability, but requires multiple servers. We recommend using SNSD and MNMD modes - the former for development and testing, the latter for production deployment. SNMD should only be used when resources are limited (only one server).\nAdditionally, you can use multi-pool deployment to scale existing MinIO clusters, or directly deploy multiple clusters.\nWhen using a multi-node MinIO cluster, you can access the service from any node, so the best practice is to use load balancing with high availability service access in front of the MinIO cluster.\nCore Parameters In MinIO deployment, MINIO_VOLUMES is a core configuration parameter that specifies the MinIO deployment mode. Pigsty provides convenient parameters to automatically generate MINIO_VOLUMES and other configuration values based on the config inventory, but you can also specify them directly.\nSingle-Node Single-Disk: MINIO_VOLUMES points to a regular directory on the local machine, specified by minio_data, defaulting to /data/minio. Single-Node Multi-Disk: MINIO_VOLUMES points to a series of mount points on the local machine, also specified by minio_data, but requires special syntax to explicitly specify real mount points, e.g., /data{1...4}. Multi-Node Multi-Disk: MINIO_VOLUMES points to mount points across multiple servers, automatically generated from two parts: First, use minio_data to specify the disk mount point sequence for each cluster member /data{1...4} Also use minio_node to specify the node naming pattern ${minio_cluster}-${minio_seq}.pigsty Multi-Pool: You need to explicitly specify the minio_volumes parameter to allocate nodes for each storage pool Single-Node Single-Disk SNSD mode, deployment reference: MinIO Single-Node Single-Drive\nIn Pigsty, defining a singleton MinIO instance is straightforward:\n# 1 Node 1 Driver (DEFAULT) minio: { hosts: { 10.10.10.10: { minio_seq: 1 } }, vars: { minio_cluster: minio } } In single-node mode, the only required parameters are minio_seq and minio_cluster, which uniquely identify each MinIO instance.\nSingle-node single-disk mode is for development purposes only, so you can use a regular directory as the data directory, specified by minio_data, defaulting to /data/minio.\nWhen using MinIO, we strongly recommend accessing it via a statically resolved domain name. For example, if minio_domain uses the default sss.pigsty, you can add a static resolution on all nodes to facilitate access to this service.\nnode_etc_hosts: [\"10.10.10.10 sss.pigsty\"] # domain name to access minio from all nodes (required) SNSD is for Development Only Single-node single-disk mode should only be used for development, testing, and demo purposes, as it cannot tolerate any hardware failure and does not benefit from multi-disk performance improvements. For production, use Multi-Node Multi-Disk mode.\nSingle-Node Multi-Disk SNMD mode, deployment reference: MinIO Single-Node Multi-Drive\nTo use multiple disks on a single node, the operation is similar to Single-Node Single-Disk, but you need to specify minio_data in the format {{ prefix }}{x...y}, which defines a series of disk mount points.\nminio: hosts: { 10.10.10.10: { minio_seq: 1 } } vars: minio_cluster: minio # minio cluster name, minio by default minio_data: '/data{1...4}' # minio data dir(s), use {x...y} to specify multi drivers Use Real Disk Mount Points Note that SNMD mode does not support using regular directories as data directories. If you start MinIO in SNMD mode but the data directory is not a valid disk mount point, MinIO will refuse to start. Ensure you use real disks formatted with XFS.\nFor example, the Vagrant MinIO sandbox defines a single-node MinIO cluster with 4 disks: /data1, /data2, /data3, and /data4. Before starting MinIO, you need to mount them properly (be sure to format disks with xfs):\nmkfs.xfs /dev/vdb; mkdir /data1; mount -t xfs /dev/sdb /data1; # mount disk 1... mkfs.xfs /dev/vdc; mkdir /data2; mount -t xfs /dev/sdb /data2; # mount disk 2... mkfs.xfs /dev/vdd; mkdir /data3; mount -t xfs /dev/sdb /data3; # mount disk 3... mkfs.xfs /dev/vde; mkdir /data4; mount -t xfs /dev/sdb /data4; # mount disk 4... Disk mounting is part of server provisioning and beyond Pigsty’s scope. Mounted disks should be written to /etc/fstab for auto-mounting after server restart.\n/dev/vdb /data1 xfs defaults,noatime,nodiratime 0 0 /dev/vdc /data2 xfs defaults,noatime,nodiratime 0 0 /dev/vdd /data3 xfs defaults,noatime,nodiratime 0 0 /dev/vde /data4 xfs defaults,noatime,nodiratime 0 0 SNMD mode can utilize multiple disks on a single machine to provide higher performance and capacity, and tolerate partial disk failures. However, single-node mode cannot tolerate entire node failure, and you cannot add new nodes at runtime, so we do not recommend using SNMD mode in production unless you have special reasons.\nMulti-Node Multi-Disk MNMD mode, deployment reference: MinIO Multi-Node Multi-Drive\nIn addition to minio_data for specifying disk drives as in Single-Node Multi-Disk mode, multi-node MinIO deployment requires an additional minio_node parameter.\nFor example, the following configuration defines a MinIO cluster with four nodes, each with four disks:\nminio: hosts: 10.10.10.10: { minio_seq: 1 } # actual nodename: minio-1.pigsty 10.10.10.11: { minio_seq: 2 } # actual nodename: minio-2.pigsty 10.10.10.12: { minio_seq: 3 } # actual nodename: minio-3.pigsty 10.10.10.13: { minio_seq: 4 } # actual nodename: minio-4.pigsty vars: minio_cluster: minio minio_data: '/data{1...4}' # 4-disk per node minio_node: '${minio_cluster}-${minio_seq}.pigsty' # minio node name pattern The minio_node parameter specifies the MinIO node name pattern, used to generate a unique name for each node. By default, the node name is ${minio_cluster}-${minio_seq}.pigsty, where ${minio_cluster} is the cluster name and ${minio_seq} is the node sequence number. The MinIO instance name is crucial and will be automatically written to /etc/hosts on MinIO nodes for static resolution. MinIO relies on these names to identify and access other nodes in the cluster.\nIn this case, MINIO_VOLUMES will be set to https://minio-{1...4}.pigsty/data{1...4} to identify the four disks on four nodes. You can directly specify the minio_volumes parameter in the MinIO cluster to override the automatically generated value. However, this is usually not necessary as Pigsty will automatically generate it based on the config inventory.\nMulti-Pool MinIO’s architecture allows scaling by adding new storage pools. In Pigsty, you can achieve cluster scaling by explicitly specifying the minio_volumes parameter to allocate nodes for each storage pool.\nFor example, suppose you have already created the MinIO cluster defined in the Multi-Node Multi-Disk example, and now you want to add a new storage pool with four more nodes.\nYou need to directly override the minio_volumes parameter:\nminio: hosts: 10.10.10.10: { minio_seq: 1 } 10.10.10.11: { minio_seq: 2 } 10.10.10.12: { minio_seq: 3 } 10.10.10.13: { minio_seq: 4 } 10.10.10.14: { minio_seq: 5 } 10.10.10.15: { minio_seq: 6 } 10.10.10.16: { minio_seq: 7 } 10.10.10.17: { minio_seq: 8 } vars: minio_cluster: minio minio_data: \"/data{1...4}\" minio_node: '${minio_cluster}-${minio_seq}.pigsty' # minio node name pattern minio_volumes: 'https://minio-{1...4}.pigsty:9000/data{1...4} https://minio-{5...8}.pigsty:9000/data{1...4}' Here, the two space-separated parameters represent two storage pools, each with four nodes and four disks per node. For more information on storage pools, refer to Administration: MinIO Cluster Expansion\nMultiple Clusters You can deploy new MinIO nodes as a completely new MinIO cluster by defining a new group with a different cluster name. The following configuration declares two independent MinIO clusters:\nminio1: hosts: 10.10.10.10: { minio_seq: 1 } 10.10.10.11: { minio_seq: 2 } 10.10.10.12: { minio_seq: 3 } 10.10.10.13: { minio_seq: 4 } vars: minio_cluster: minio2 minio_data: \"/data{1...4}\" minio2: hosts: 10.10.10.14: { minio_seq: 5 } 10.10.10.15: { minio_seq: 6 } 10.10.10.16: { minio_seq: 7 } 10.10.10.17: { minio_seq: 8 } vars: minio_cluster: minio2 minio_data: \"/data{1...4}\" minio_alias: sss2 minio_domain: sss2.pigsty minio_endpoint: sss2.pigsty:9000 Note that Pigsty defaults to having only one MinIO cluster per deployment. If you need to deploy multiple MinIO clusters, some parameters with default values must be explicitly set and cannot be omitted, otherwise naming conflicts will occur, as shown above.\nExpose Service MinIO serves on port 9000 by default. A multi-node MinIO cluster can be accessed by connecting to any one of its nodes.\nService access falls under the scope of the NODE module, and we’ll provide only a basic introduction here.\nHigh-availability access to a multi-node MinIO cluster can be achieved using L2 VIP or HAProxy. For example, you can use keepalived to bind an L2 VIP to the MinIO cluster, or use the haproxy component provided by the NODE module to expose MinIO services through a load balancer.\n# minio cluster with 4 nodes and 4 drivers per node minio: hosts: 10.10.10.10: { minio_seq: 1 , nodename: minio-1 } 10.10.10.11: { minio_seq: 2 , nodename: minio-2 } 10.10.10.12: { minio_seq: 3 , nodename: minio-3 } 10.10.10.13: { minio_seq: 4 , nodename: minio-4 } vars: minio_cluster: minio minio_data: '/data{1...4}' minio_buckets: [ { name: pgsql }, { name: infra }, { name: redis } ] minio_users: - { access_key: dba , secret_key: S3User.DBA, policy: consoleAdmin } - { access_key: pgbackrest , secret_key: S3User.SomeNewPassWord , policy: readwrite } # bind a node l2 vip (10.10.10.9) to minio cluster (optional) node_cluster: minio vip_enabled: true vip_vrid: 128 vip_address: 10.10.10.9 vip_interface: eth1 # expose minio service with haproxy on all nodes haproxy_services: - name: minio # [REQUIRED] service name, unique port: 9002 # [REQUIRED] service port, unique balance: leastconn # [OPTIONAL] load balancer algorithm options: # [OPTIONAL] minio health check - option httpchk - option http-keep-alive - http-check send meth OPTIONS uri /minio/health/live - http-check expect status 200 servers: - { name: minio-1 ,ip: 10.10.10.10 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-2 ,ip: 10.10.10.11 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-3 ,ip: 10.10.10.12 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-4 ,ip: 10.10.10.13 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } For example, the configuration above enables HAProxy on all nodes of the MinIO cluster, exposing MinIO services on port 9002, and binds a Layer 2 VIP to the cluster. When in use, users should point the sss.pigsty domain name to the VIP address 10.10.10.9 and access MinIO services using port 9002. This ensures high availability, as the VIP will automatically switch to another node if any node fails.\nIn this scenario, you may also need to globally modify the domain name resolution destination and the minio_endpoint parameter to change the endpoint address for the MinIO alias on the admin node:\nminio_endpoint: https://sss.pigsty:9002 # Override the default: https://sss.pigsty:9000 node_etc_hosts: [\"10.10.10.9 sss.pigsty\"] # Other nodes will use sss.pigsty domain to access MinIO Dedicated Load Balancer Pigsty allows using a dedicated load balancer server group instead of the cluster itself to run VIP and HAProxy. For example, the prod template uses this approach.\nproxy: hosts: 10.10.10.18 : { nodename: proxy1 ,node_cluster: proxy ,vip_interface: eth1 ,vip_role: master } 10.10.10.19 : { nodename: proxy2 ,node_cluster: proxy ,vip_interface: eth1 ,vip_role: backup } vars: vip_enabled: true vip_address: 10.10.10.20 vip_vrid: 20 haproxy_services: # expose minio service : sss.pigsty:9000 - name: minio # [REQUIRED] service name, unique port: 9000 # [REQUIRED] service port, unique balance: leastconn # Use leastconn algorithm and minio health check options: [ \"option httpchk\", \"option http-keep-alive\", \"http-check send meth OPTIONS uri /minio/health/live\", \"http-check expect status 200\" ] servers: # reload service with ./node.yml -t haproxy_config,haproxy_reload - { name: minio-1 ,ip: 10.10.10.21 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-2 ,ip: 10.10.10.22 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-3 ,ip: 10.10.10.23 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-4 ,ip: 10.10.10.24 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-5 ,ip: 10.10.10.25 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } In this case, you typically need to globally modify the MinIO domain resolution to point sss.pigsty to the load balancer address, and modify the minio_endpoint parameter to change the endpoint address for the MinIO alias on the admin node:\nminio_endpoint: https://sss.pigsty:9002 # overwrite the defaults: https://sss.pigsty:9000 node_etc_hosts: [\"10.10.10.20 sss.pigsty\"] # domain name to access minio from all nodes (required) Access Service To access MinIO exposed via HAProxy, taking PGSQL backup configuration as an example, you can modify the configuration in pgbackrest_repo to add a new backup repository definition:\n# This is the newly added HA MinIO Repo definition, USE THIS INSTEAD! minio_ha: type: s3 s3_endpoint: minio-1.pigsty # s3_endpoint can be any load balancer: 10.10.10.1{0,1,2}, or domain names pointing to any of the nodes s3_region: us-east-1 # you can use external domain name: sss.pigsty, which resolves to any member (`minio_domain`) s3_bucket: pgsql # instance \u0026 nodename can be used: minio-1.pigsty minio-1.pigsty minio-1.pigsty minio-1 minio-2 minio-3 s3_key: pgbackrest # Better using a dedicated password for MinIO pgbackrest user s3_key_secret: S3User.SomeNewPassWord s3_uri_style: path path: /pgbackrest storage_port: 9002 # Use load balancer port 9002 instead of default 9000 (direct access) storage_ca_file: /etc/pki/ca.crt bundle: y cipher_type: aes-256-cbc # Better using a new cipher password for your production environment cipher_pass: pgBackRest.With.Some.Extra.PassWord.And.Salt.${pg_cluster} retention_full_type: time retention_full: 14 Expose Console MinIO provides a Web console interface on port 9001 by default (specified by the minio_admin_port parameter).\nExposing the admin interface to external networks may pose security risks. If you want to do this, add MinIO to infra_portal and refresh the Nginx configuration.\n# ./infra.yml -t nginx infra_portal: home : { domain: h.pigsty } # MinIO console requires HTTPS / Websocket to work minio : { domain: m.pigsty ,endpoint: \"10.10.10.10:9001\" ,scheme: https ,websocket: true } minio10 : { domain: m10.pigsty ,endpoint: \"10.10.10.10:9001\" ,scheme: https ,websocket: true } minio11 : { domain: m11.pigsty ,endpoint: \"10.10.10.11:9001\" ,scheme: https ,websocket: true } minio12 : { domain: m12.pigsty ,endpoint: \"10.10.10.12:9001\" ,scheme: https ,websocket: true } minio13 : { domain: m13.pigsty ,endpoint: \"10.10.10.13:9001\" ,scheme: https ,websocket: true } Note that the MinIO console requires HTTPS. Please DO NOT expose an unencrypted MinIO console in production.\nThis means you typically need to add a resolution record for m.pigsty in your DNS server or local /etc/hosts file to access the MinIO console.\nMeanwhile, if you are using Pigsty’s self-signed CA rather than a proper public CA, you usually need to manually trust the CA or certificate to skip the “insecure” warning in the browser.\n","categories":["Reference"],"description":"Choose the appropriate MinIO deployment type based on your requirements and provide reliable access.","excerpt":"Choose the appropriate MinIO deployment type based on your …","ref":"/docs/minio/config/","tags":"","title":"Configuration"},{"body":"The MinIO module parameter list contains 21 parameters in two groups:\nMINIO: 18 parameters for MinIO cluster deployment and configuration MINIO_REMOVE: 3 parameters for MinIO cluster removal Architecture Change: Pigsty v3.6+ Since Pigsty v3.6, the minio.yml playbook no longer includes removal functionality. Removal-related parameters have been migrated to the dedicated minio_remove role and minio-rm.yml playbook.\nParameter Overview The MINIO parameter group is used for MinIO cluster deployment and configuration, including identity, storage paths, ports, authentication credentials, and provisioning of buckets and users.\nParameter Type Level Description minio_seq int I minio instance identifier, REQUIRED minio_cluster string C minio cluster name, minio by default minio_user username C minio os user, minio by default minio_https bool G/C enable HTTPS for MinIO? true by default minio_node string C minio node name pattern minio_data path C minio data dir, use {x...y} for multiple disks minio_volumes string C minio core parameter for nodes and disks, auto-gen minio_domain string G minio external domain, sss.pigsty by default minio_port port C minio service port, 9000 by default minio_admin_port port C minio console port, 9001 by default minio_access_key username C root access key, minioadmin by default minio_secret_key password C root secret key, S3User.MinIO by default minio_extra_vars string C extra environment variables for minio server minio_provision bool G/C run minio provisioning tasks? true by default minio_alias string G minio client alias for the deployment minio_endpoint string C endpoint for the minio client alias minio_buckets bucket[] C list of minio buckets to be created minio_users user[] C list of minio users to be created The MINIO_REMOVE parameter group controls MinIO cluster removal behavior, including safeguard protection, data cleanup, and package uninstallation.\nParameter Type Level Description minio_safeguard bool G/C/A prevent accidental removal? false by default minio_rm_data bool G/C/A remove minio data during removal? true by default minio_rm_pkg bool G/C/A uninstall minio packages during removal? false by default The minio_volumes and minio_endpoint are auto-generated parameters, but you can explicitly override them.\nDefaults MINIO: 18 parameters, defined in roles/minio/defaults/main.yml\n#----------------------------------------------------------------- # MINIO #----------------------------------------------------------------- #minio_seq: 1 # minio instance identifier, REQUIRED minio_cluster: minio # minio cluster name, minio by default minio_user: minio # minio os user, `minio` by default minio_https: true # enable HTTPS for MinIO? true by default minio_node: '${minio_cluster}-${minio_seq}.pigsty' # minio node name pattern minio_data: '/data/minio' # minio data dir, use `{x...y}` for multiple disks #minio_volumes: # minio core parameter, auto-generated if not specified minio_domain: sss.pigsty # minio external domain, `sss.pigsty` by default minio_port: 9000 # minio service port, 9000 by default minio_admin_port: 9001 # minio console port, 9001 by default minio_access_key: minioadmin # root access key, `minioadmin` by default minio_secret_key: S3User.MinIO # root secret key, `S3User.MinIO` by default minio_extra_vars: '' # extra environment variables for minio server minio_provision: true # run minio provisioning tasks? minio_alias: sss # minio client alias for the deployment #minio_endpoint: https://sss.pigsty:9000 # endpoint for alias, auto-generated if not specified minio_buckets: # list of minio buckets to be created - { name: pgsql } - { name: meta ,versioning: true } - { name: data } minio_users: # list of minio users to be created - { access_key: pgbackrest ,secret_key: S3User.Backup ,policy: pgsql } - { access_key: s3user_meta ,secret_key: S3User.Meta ,policy: meta } - { access_key: s3user_data ,secret_key: S3User.Data ,policy: data } MINIO_REMOVE: 3 parameters, defined in roles/minio_remove/defaults/main.yml\n#----------------------------------------------------------------- # MINIO_REMOVE #----------------------------------------------------------------- minio_safeguard: false # prevent accidental removal? false by default minio_rm_data: true # remove minio data during removal? true by default minio_rm_pkg: false # uninstall minio packages during removal? false by default MINIO This section contains parameters for the minio role, used by the minio.yml playbook.\nminio_seq Parameter: minio_seq, Type: int, Level: I\nMinIO instance identifier, a required identity parameter. No default value—you must assign it manually.\nBest practice is to start from 1, increment by 1, and never reuse previously assigned sequence numbers. The sequence number, together with the cluster name minio_cluster, uniquely identifies each MinIO instance (e.g., minio-1).\nIn multi-node deployments, sequence numbers are also used to generate node names, which are written to the /etc/hosts file for static resolution.\nminio_cluster Parameter: minio_cluster, Type: string, Level: C\nMinIO cluster name, default is minio. This is useful when deploying multiple MinIO clusters.\nThe cluster name, together with the sequence number minio_seq, uniquely identifies each MinIO instance. For example, with cluster name minio and sequence 1, the instance name is minio-1.\nNote that Pigsty defaults to a single MinIO cluster per deployment. If you need multiple MinIO clusters, you must explicitly set minio_alias, minio_domain, minio_endpoint, and other parameters to avoid naming conflicts.\nminio_user Parameter: minio_user, Type: username, Level: C\nMinIO operating system user, default is minio.\nThe MinIO service runs under this user. SSL certificates used by MinIO are stored in this user’s home directory (default /home/minio), under the ~/.minio/certs/ directory.\nminio_https Parameter: minio_https, Type: bool, Level: G/C\nEnable HTTPS for MinIO service? Default is true.\nNote that pgBackREST requires MinIO to use HTTPS to work properly. If you don’t use MinIO for PostgreSQL backups and don’t need HTTPS, you can set this to false.\nWhen HTTPS is enabled, Pigsty automatically issues SSL certificates for the MinIO server, containing the domain specified in minio_domain and the IP addresses of each node.\nminio_node Parameter: minio_node, Type: string, Level: C\nMinIO node name pattern, used for multi-node deployments.\nDefault value: ${minio_cluster}-${minio_seq}.pigsty, which uses the instance name plus .pigsty suffix as the default node name.\nThe domain pattern specified here is used to generate node names, which are written to the /etc/hosts file on all MinIO nodes.\nminio_data Parameter: minio_data, Type: path, Level: C\nMinIO data directory(s), default value: /data/minio, a common directory for single-node deployments.\nFor multi-node-multi-drive and single-node-multi-drive deployments, use the {x...y} notation to specify multiple disks.\nminio_volumes Parameter: minio_volumes, Type: string, Level: C\nMinIO core parameter. By default, this is not specified and is auto-generated using the following rule:\nminio_volumes: \"{% if minio_cluster_size|int \u003e 1 %}https://{{ minio_node|replace('${minio_cluster}', minio_cluster)|replace('${minio_seq}',minio_seq_range) }}:{{ minio_port|default(9000) }}{% endif %}{{ minio_data }}\" In single-node deployment (single or multi-drive), minio_volumes directly uses the minio_data value. In multi-node deployment, minio_volumes uses minio_node, minio_port, and minio_data to generate multi-node addresses. In multi-pool deployment, you typically need to explicitly specify and override minio_volumes to define multiple node pool addresses. When specifying this parameter, ensure the values are consistent with minio_node, minio_port, and minio_data.\nminio_domain Parameter: minio_domain, Type: string, Level: G\nMinIO service domain name, default is sss.pigsty.\nClients can access the MinIO S3 service via this domain name. This name is registered in local DNSMASQ and included in SSL certificates’ SAN (Subject Alternative Name) field.\nIt’s recommended to add a static DNS record in node_etc_hosts pointing this domain to the MinIO server node’s IP (single-node deployment) or load balancer VIP (multi-node deployment).\nminio_port Parameter: minio_port, Type: port, Level: C\nMinIO service port, default is 9000.\nThis is the MinIO S3 API listening port. Clients access the object storage service through this port. In multi-node deployments, this port is also used for inter-node communication.\nminio_admin_port Parameter: minio_admin_port, Type: port, Level: C\nMinIO console port, default is 9001.\nThis is the listening port for MinIO’s built-in web management console. You can access MinIO’s graphical management interface at https://\u003cminio-ip\u003e:9001.\nTo expose the MinIO console through Nginx, add it to infra_portal. Note that the MinIO console requires HTTPS and WebSocket support.\nminio_access_key Parameter: minio_access_key, Type: username, Level: C\nRoot access key (username), default is minioadmin.\nThis is the MinIO super administrator username with full access to all buckets and objects. It’s recommended to change this default value in production environments.\nminio_secret_key Parameter: minio_secret_key, Type: password, Level: C\nRoot secret key (password), default is S3User.MinIO.\nThis is the MinIO super administrator’s password, used together with minio_access_key.\nSecurity Warning: Change the default password! Using default passwords is a high-risk behavior! Make sure to change this password in your production deployment.\nTip: Running ./configure or ./configure -g will automatically replace these default passwords in the configuration template.\nminio_extra_vars Parameter: minio_extra_vars, Type: string, Level: C\nExtra environment variables for MinIO server. See the MinIO Server documentation for the complete list.\nDefault is an empty string. You can use multiline strings to pass multiple environment variables:\nminio_extra_vars: | MINIO_BROWSER_REDIRECT_URL=https://minio.example.com MINIO_SERVER_URL=https://s3.example.com minio_provision Parameter: minio_provision, Type: bool, Level: G/C\nRun MinIO provisioning tasks? Default is true.\nWhen enabled, Pigsty automatically creates the buckets and users defined in minio_buckets and minio_users. Set this to false if you don’t need automatic provisioning of these resources.\nminio_alias Parameter: minio_alias, Type: string, Level: G\nMinIO client alias for the local MinIO cluster, default value: sss.\nThis alias is written to the MinIO client configuration file (~/.mcli/config.json) for the admin user on the admin node, allowing you to directly use mcli \u003calias\u003e commands to access the MinIO cluster, e.g., mcli ls sss/.\nIf deploying multiple MinIO clusters, specify different aliases for each cluster to avoid conflicts.\nminio_endpoint Parameter: minio_endpoint, Type: string, Level: C\nEndpoint for the client alias. If specified, this minio_endpoint (e.g., https://sss.pigsty:9002) will replace the default value as the target endpoint for the MinIO alias written on the admin node.\nmcli alias set {{ minio_alias }} {% if minio_endpoint is defined and minio_endpoint != '' %}{{ minio_endpoint }}{% else %}https://{{ minio_domain }}:{{ minio_port }}{% endif %} {{ minio_access_key }} {{ minio_secret_key }} This MinIO alias is configured on the admin node as the default admin user.\nminio_buckets Parameter: minio_buckets, Type: bucket[], Level: C\nList of MinIO buckets to create by default:\nminio_buckets: - { name: pgsql } - { name: meta ,versioning: true } - { name: data } Three default buckets are created with different purposes and policies:\npgsql bucket: Used by default for PostgreSQL pgBackREST backup storage. meta bucket: Open bucket with versioning enabled, suitable for storing important metadata requiring version management. data bucket: Open bucket for other purposes, e.g., Supabase templates may use this bucket for business data. Each bucket has a corresponding access policy with the same name. For example, the pgsql policy has full access to the pgsql bucket, and so on.\nYou can also add a lock flag to bucket definitions to enable object locking, preventing accidental deletion of objects in the bucket.\nminio_users Parameter: minio_users, Type: user[], Level: C\nList of MinIO users to create, default value:\nminio_users: - { access_key: pgbackrest ,secret_key: S3User.Backup ,policy: pgsql } - { access_key: s3user_meta ,secret_key: S3User.Meta ,policy: meta } - { access_key: s3user_data ,secret_key: S3User.Data ,policy: data } The default configuration creates three users corresponding to three default buckets:\npgbackrest: For PostgreSQL pgBackREST backups, with access to the pgsql bucket. s3user_meta: For accessing the meta bucket. s3user_data: For accessing the data bucket. Using default passwords is dangerous! Make sure to change these credentials in your deployment! Tip: ./configure -g will automatically replace these passwords in the configuration template if they appear as defaults.\nMINIO_REMOVE This section contains parameters for the minio_remove role, used by the minio-rm.yml playbook.\nminio_safeguard Parameter: minio_safeguard, Type: bool, Level: G/C/A\nSafeguard switch to prevent accidental deletion, default value is false.\nWhen enabled, the minio-rm.yml playbook will abort and refuse to remove the MinIO cluster, providing protection against accidental deletions.\nIt’s recommended to enable this safeguard in production environments to prevent data loss from accidental operations:\nminio_safeguard: true # When enabled, minio-rm.yml will refuse to execute minio_rm_data Parameter: minio_rm_data, Type: bool, Level: G/C/A\nRemove MinIO data during removal? Default value is true.\nWhen enabled, the minio-rm.yml playbook will delete MinIO data directories and configuration files during cluster removal.\nminio_rm_pkg Parameter: minio_rm_pkg, Type: bool, Level: G/C/A\nUninstall MinIO packages during removal? Default value is false.\nWhen enabled, the minio-rm.yml playbook will uninstall MinIO packages during cluster removal. This is disabled by default to preserve the MinIO installation for potential future use.\n","categories":["Reference"],"description":"MinIO module provides 21 configuration parameters for customizing your MinIO cluster.","excerpt":"MinIO module provides 21 configuration parameters for customizing your …","ref":"/docs/minio/param/","tags":"","title":"Parameters"},{"body":"The MinIO module provides two built-in playbooks for cluster management:\nminio.yml: Install MinIO cluster minio-rm.yml: Remove MinIO cluster minio.yml Playbook minio.yml installs the MinIO module on nodes.\nminio-id : Generate/validate minio identity parameters minio_install : Install minio minio_os_user : Create OS user minio minio_pkg : Install minio/mcli packages minio_dir : Create minio directories minio_config : Generate minio configuration minio_conf : Minio main config file minio_cert : Minio SSL certificate issuance minio_dns : Minio DNS record insertion minio_launch : Launch minio service minio_register : Register minio to monitoring minio_provision : Create minio aliases/buckets/users minio_alias : Create minio client alias (on admin node) minio_bucket : Create minio buckets minio_user : Create minio business users Before running the playbook, complete the MinIO cluster configuration in the config inventory.\nExecution Condition The playbook automatically skips hosts without minio_seq defined. This means you can safely execute the playbook on mixed host groups - only actual MinIO nodes will be processed.\nArchitecture Change: Pigsty v3.6+ Since Pigsty v3.6, the minio.yml playbook focuses on cluster installation. All removal operations have been moved to the dedicated minio-rm.yml playbook using the minio_remove role.\nminio-rm.yml Playbook minio-rm.yml removes the MinIO cluster.\nminio-id : Generate minio identity parameters for removal (with any_errors_fatal - stops immediately on identity validation failure) minio_safeguard : Safety check, prevent accidental deletion (default: false) minio_pause : Pause 3 seconds, allow user to abort (Ctrl+C to cancel) minio_deregister : Remove targets from Victoria/Prometheus monitoring, clean up DNS records minio_svc : Stop and disable minio systemd service minio_data : Remove minio data directory (disable with minio_rm_data=false) minio_pkg : Uninstall minio packages (enable with minio_rm_pkg=true) Execution Condition \u0026 Safety Mechanisms The playbook automatically skips hosts without minio_seq defined, preventing accidental operations on non-MinIO nodes Identity validation uses any_errors_fatal - the playbook stops immediately upon detecting invalid MinIO identity A 3-second pause before removal gives users a chance to abort the operation The removal playbook uses the minio_remove role with the following configurable parameters:\nminio_safeguard: Prevents accidental deletion when set to true minio_rm_data: Controls whether MinIO data is deleted (default: true) minio_rm_pkg: Controls whether MinIO packages are uninstalled (default: false) Cheatsheet Common MINIO playbook commands:\n./minio.yml -l \u003ccls\u003e # Install MINIO module on group \u003ccls\u003e ./minio.yml -l minio -t minio_install # Install MinIO service, prepare data dirs, without configure \u0026 launch ./minio.yml -l minio -t minio_config # Reconfigure MinIO cluster ./minio.yml -l minio -t minio_launch # Restart MinIO cluster ./minio.yml -l minio -t minio_provision # Re-run provisioning (create buckets and users) ./minio-rm.yml -l minio # Remove MinIO cluster (using dedicated removal playbook) ./minio-rm.yml -l minio -e minio_rm_data=false # Remove cluster but preserve data ./minio-rm.yml -l minio -e minio_rm_pkg=true # Remove cluster and uninstall packages Safeguard To prevent accidental deletion, Pigsty’s MINIO module provides a safeguard mechanism controlled by the minio_safeguard parameter.\nBy default, minio_safeguard is false, allowing removal operations. If you want to protect the MinIO cluster from accidental deletion, enable this safeguard in the config inventory:\nminio_safeguard: true # When enabled, minio-rm.yml will refuse to execute If you need to remove a protected cluster, override with command-line parameters:\n./minio-rm.yml -l minio -e minio_safeguard=false Demo ","categories":["Task"],"description":"Manage MinIO clusters with Ansible playbooks and quick command reference.","excerpt":"Manage MinIO clusters with Ansible playbooks and quick command …","ref":"/docs/minio/playbook/","tags":"","title":"Playbook"},{"body":" Create Cluster To create a cluster, define it in the config inventory and run the minio.yml playbook.\nminio: { hosts: { 10.10.10.10: { minio_seq: 1 } }, vars: { minio_cluster: minio } } For example, the above configuration defines an SNSD Single-Node Single-Disk MinIO cluster. Use the following command to create this MinIO cluster:\n./minio.yml -l minio # Install MinIO module on the minio group Remove Cluster To destroy a cluster, run the dedicated minio-rm.yml playbook:\n./minio-rm.yml -l minio # Remove MinIO cluster ./minio-rm.yml -l minio -e minio_rm_data=false # Remove cluster but keep data ./minio-rm.yml -l minio -e minio_rm_pkg=true # Remove cluster and uninstall packages Architecture Change: Pigsty v3.6+ Starting from Pigsty v3.6, cluster removal has been migrated from minio.yml playbook to the dedicated minio-rm.yml playbook. The old minio_clean task has been deprecated.\nThe removal playbook automatically performs the following:\nDeregisters MinIO targets from Victoria/Prometheus monitoring Removes records from the DNS service on INFRA nodes Stops and disables MinIO systemd service Deletes MinIO data directory and configuration files (optional) Uninstalls MinIO packages (optional) Expand Cluster Expand Cluster Tutorial MinIO cannot scale at the node/disk level, but can scale at the storage pool (multiple nodes) level.\nAssume you have a four-node MinIO cluster and want to double the capacity by adding a new four-node storage pool.\nminio: hosts: 10.10.10.10: { minio_seq: 1 , nodename: minio-1 } 10.10.10.11: { minio_seq: 2 , nodename: minio-2 } 10.10.10.12: { minio_seq: 3 , nodename: minio-3 } 10.10.10.13: { minio_seq: 4 , nodename: minio-4 } vars: minio_cluster: minio minio_data: '/data{1...4}' minio_buckets: [ { name: pgsql }, { name: infra }, { name: redis } ] minio_users: - { access_key: dba , secret_key: S3User.DBA, policy: consoleAdmin } - { access_key: pgbackrest , secret_key: S3User.SomeNewPassWord , policy: readwrite } # bind a node l2 vip (10.10.10.9) to minio cluster (optional) node_cluster: minio vip_enabled: true vip_vrid: 128 vip_address: 10.10.10.9 vip_interface: eth1 # expose minio service with haproxy on all nodes haproxy_services: - name: minio # [REQUIRED] service name, unique port: 9002 # [REQUIRED] service port, unique balance: leastconn # [OPTIONAL] load balancer algorithm options: # [OPTIONAL] minio health check - option httpchk - option http-keep-alive - http-check send meth OPTIONS uri /minio/health/live - http-check expect status 200 servers: - { name: minio-1 ,ip: 10.10.10.10 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-2 ,ip: 10.10.10.11 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-3 ,ip: 10.10.10.12 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-4 ,ip: 10.10.10.13 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } First, modify the MinIO cluster definition to add four new nodes, assigning sequence numbers 5 to 8. The key step is to modify the minio_volumes parameter to designate the new four nodes as a new storage pool.\nminio: hosts: 10.10.10.10: { minio_seq: 1 , nodename: minio-1 } 10.10.10.11: { minio_seq: 2 , nodename: minio-2 } 10.10.10.12: { minio_seq: 3 , nodename: minio-3 } 10.10.10.13: { minio_seq: 4 , nodename: minio-4 } # new nodes 10.10.10.14: { minio_seq: 5 , nodename: minio-5 } 10.10.10.15: { minio_seq: 6 , nodename: minio-6 } 10.10.10.16: { minio_seq: 7 , nodename: minio-7 } 10.10.10.17: { minio_seq: 8 , nodename: minio-8 } vars: minio_cluster: minio minio_data: '/data{1...4}' minio_volumes: 'https://minio-{1...4}.pigsty:9000/data{1...4} https://minio-{5...8}.pigsty:9000/data{1...4}' # new cluster config # ... other configs omitted Step 2: Add these nodes to Pigsty:\n./node.yml -l 10.10.10.14,10.10.10.15,10.10.10.16,10.10.10.17 Step 3: On the new nodes, use the Ansible playbook to install and prepare MinIO software:\n./minio.yml -l 10.10.10.14,10.10.10.15,10.10.10.16,10.10.10.17 -t minio_install Step 4: On the entire cluster, use the Ansible playbook to reconfigure the MinIO cluster:\n./minio.yml -l minio -t minio_config This step updates the MINIO_VOLUMES configuration on the existing four nodes\nStep 5: Restart the entire MinIO cluster at once (be careful, do not rolling restart!):\n./minio.yml -l minio -t minio_launch -f 10 # 8 parallel, ensure simultaneous restart Step 6 (optional): If you are using a load balancer, make sure the load balancer configuration is updated. For example, add the new four nodes to the load balancer configuration:\n# expose minio service with haproxy on all nodes haproxy_services: - name: minio # [REQUIRED] service name, unique port: 9002 # [REQUIRED] service port, unique balance: leastconn # [OPTIONAL] load balancer algorithm options: # [OPTIONAL] minio health check - option httpchk - option http-keep-alive - http-check send meth OPTIONS uri /minio/health/live - http-check expect status 200 servers: - { name: minio-1 ,ip: 10.10.10.10 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-2 ,ip: 10.10.10.11 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-3 ,ip: 10.10.10.12 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-4 ,ip: 10.10.10.13 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-5 ,ip: 10.10.10.14 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-6 ,ip: 10.10.10.15 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-7 ,ip: 10.10.10.16 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-8 ,ip: 10.10.10.17 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } Then, run the haproxy subtask of the node.yml playbook to update the load balancer configuration:\n./node.yml -l minio -t haproxy_config,haproxy_reload # Update and reload load balancer config If you use L2 VIP for reliable load balancer access, you also need to add new nodes (if any) to the existing NODE VIP group:\n./node.yml -l minio -t node_vip # Refresh cluster L2 VIP configuration Shrink Cluster MinIO cannot shrink at the node/disk level, but can retire at the storage pool (multiple nodes) level — add a new storage pool, drain the old storage pool to the new one, then retire the old storage pool.\nShrink Cluster Tutorial Upgrade Cluster Upgrade Cluster Tutorial First, download the new version of MinIO packages to the local software repository on the INFRA node, then rebuild the repository index:\nminio: amd64: https://dl.min.io/server/minio/release/linux-amd64/ arm64: https://dl.min.io/server/minio/release/linux-arm64/ mcli: amd64: https://dl.min.io/client/mc/release/linux-amd64/ arm64: https://dl.min.io/client/mc/release/linux-arm64/ ./infra.yml -t repo_create Next, use Ansible to batch upgrade MinIO packages:\nansible minio -m package -b -a 'name=minio state=latest' # Upgrade MinIO server ansible minio -m package -b -a 'name=mcli state=latest' # Upgrade MinIO client Finally, use the mc command line tool to instruct the MinIO cluster to restart:\nmc admin service restart sss Node Failure Recovery Node Failure Tutorial # 1. Remove the failed node from the cluster bin/node-rm \u003cyour_old_node_ip\u003e # 2. Replace the failed node with the same node name (if IP changes, modify the MinIO cluster definition) bin/node-add \u003cyour_new_node_ip\u003e # 3. Install and configure MinIO on the new node ./minio.yml -l \u003cyour_new_node_ip\u003e # 4. Instruct MinIO to perform heal action mc admin heal Disk Failure Recovery Disk Failure Tutorial # 1. Unmount the failed disk from the cluster umount /dev/\u003cyour_disk_device\u003e # 2. Replace the failed disk, format with xfs mkfs.xfs /dev/sdb -L DRIVE1 # 3. Don't forget to setup fstab for auto-mount vi /etc/fstab # LABEL=DRIVE1 /mnt/drive1 xfs defaults,noatime 0 2 # 4. Remount mount -a # 5. Instruct MinIO to perform heal action mc admin heal ","categories":["Task"],"description":"MinIO cluster management SOP: create, destroy, expand, shrink, and handle node and disk failures.","excerpt":"MinIO cluster management SOP: create, destroy, expand, shrink, and …","ref":"/docs/minio/admin/","tags":"","title":"Administration"},{"body":" Built-in Console MinIO has a built-in management console. By default, you can access this interface via HTTPS through the admin port (minio_admin_port, default 9001) of any MinIO instance.\nIn most configuration templates that provide MinIO services, MinIO is exposed as a custom service at m.pigsty. After configuring domain name resolution, you can access the MinIO console at https://m.pigsty.\nLog in with the admin credentials configured by minio_access_key and minio_secret_key (default minioadmin / S3User.MinIO).\nNote: HTTPS and Certificate Trust The MinIO console requires HTTPS access. If you use Pigsty’s self-signed CA, you need to trust the CA certificate in your browser, or manually accept the security warning.\nPigsty Monitoring Pigsty provides two monitoring dashboards related to the MINIO module:\nMinIO Overview: Displays overall monitoring metrics for the MinIO cluster, including cluster status, storage usage, request rates, etc. MinIO Instance: Displays monitoring metrics details for a single MinIO instance, including CPU, memory, network, disk, etc. MinIO monitoring metrics are collected through MinIO’s native Prometheus endpoint (/minio/v2/metrics/cluster), and by default are scraped and stored by Victoria Metrics.\nPigsty Alerting Pigsty provides the following three alerting rules for MinIO:\nMinIO Server Down MinIO Node Offline MinIO Disk Offline #==============================================================# # Aliveness # #==============================================================# # MinIO server instance down - alert: MinioServerDown expr: minio_up \u003c 1 for: 1m labels: { level: 0, severity: CRIT, category: minio } annotations: summary: \"CRIT MinioServerDown {{ $labels.ins }}@{{ $labels.instance }}\" description: | minio_up[ins={{ $labels.ins }}, instance={{ $labels.instance }}] = {{ $value }} \u003c 1 http://g.pigsty/d/minio-overview #==============================================================# # Error # #==============================================================# # MinIO node offline triggers a p1 alert - alert: MinioNodeOffline expr: avg_over_time(minio_cluster_nodes_offline_total{job=\"minio\"}[5m]) \u003e 0 for: 3m labels: { level: 1, severity: WARN, category: minio } annotations: summary: \"WARN MinioNodeOffline: {{ $labels.cls }} {{ $value }}\" description: | minio_cluster_nodes_offline_total[cls={{ $labels.cls }}] = {{ $value }} \u003e 0 http://g.pigsty/d/minio-overview?from=now-5m\u0026to=now\u0026var-cls={{$labels.cls}} # MinIO disk offline triggers a p1 alert - alert: MinioDiskOffline expr: avg_over_time(minio_cluster_disk_offline_total{job=\"minio\"}[5m]) \u003e 0 for: 3m labels: { level: 1, severity: WARN, category: minio } annotations: summary: \"WARN MinioDiskOffline: {{ $labels.cls }} {{ $value }}\" description: | minio_cluster_disk_offline_total[cls={{ $labels.cls }}] = {{ $value }} \u003e 0 http://g.pigsty/d/minio-overview?from=now-5m\u0026to=now\u0026var-cls={{$labels.cls}} ","categories":["Reference"],"description":"How to monitor MinIO in Pigsty? How to use MinIO's built-in console? What alerting rules are worth noting?","excerpt":"How to monitor MinIO in Pigsty? How to use MinIO's built-in console? …","ref":"/docs/minio/monitor/","tags":"","title":"Monitoring"},{"body":"The MINIO module contains 79 available monitoring metrics.\nMetric Name Type Labels Description minio_audit_failed_messages counter ip, job, target_id, cls, instance, server, ins Total number of messages that failed to send since start minio_audit_target_queue_length gauge ip, job, target_id, cls, instance, server, ins Number of unsent messages in queue for target minio_audit_total_messages counter ip, job, target_id, cls, instance, server, ins Total number of messages sent since start minio_cluster_bucket_total gauge ip, job, cls, instance, server, ins Total number of buckets in the cluster minio_cluster_capacity_raw_free_bytes gauge ip, job, cls, instance, server, ins Total free capacity online in the cluster minio_cluster_capacity_raw_total_bytes gauge ip, job, cls, instance, server, ins Total capacity online in the cluster minio_cluster_capacity_usable_free_bytes gauge ip, job, cls, instance, server, ins Total free usable capacity online in the cluster minio_cluster_capacity_usable_total_bytes gauge ip, job, cls, instance, server, ins Total usable capacity online in the cluster minio_cluster_drive_offline_total gauge ip, job, cls, instance, server, ins Total drives offline in this cluster minio_cluster_drive_online_total gauge ip, job, cls, instance, server, ins Total drives online in this cluster minio_cluster_drive_total gauge ip, job, cls, instance, server, ins Total drives in this cluster minio_cluster_health_erasure_set_healing_drives gauge pool, ip, job, cls, set, instance, server, ins Get the count of healing drives of this erasure set minio_cluster_health_erasure_set_online_drives gauge pool, ip, job, cls, set, instance, server, ins Get the count of the online drives in this erasure set minio_cluster_health_erasure_set_read_quorum gauge pool, ip, job, cls, set, instance, server, ins Get the read quorum for this erasure set minio_cluster_health_erasure_set_status gauge pool, ip, job, cls, set, instance, server, ins Get current health status for this erasure set minio_cluster_health_erasure_set_write_quorum gauge pool, ip, job, cls, set, instance, server, ins Get the write quorum for this erasure set minio_cluster_health_status gauge ip, job, cls, instance, server, ins Get current cluster health status minio_cluster_nodes_offline_total gauge ip, job, cls, instance, server, ins Total number of MinIO nodes offline minio_cluster_nodes_online_total gauge ip, job, cls, instance, server, ins Total number of MinIO nodes online minio_cluster_objects_size_distribution gauge ip, range, job, cls, instance, server, ins Distribution of object sizes across a cluster minio_cluster_objects_version_distribution gauge ip, range, job, cls, instance, server, ins Distribution of object versions across a cluster minio_cluster_usage_deletemarker_total gauge ip, job, cls, instance, server, ins Total number of delete markers in a cluster minio_cluster_usage_object_total gauge ip, job, cls, instance, server, ins Total number of objects in a cluster minio_cluster_usage_total_bytes gauge ip, job, cls, instance, server, ins Total cluster usage in bytes minio_cluster_usage_version_total gauge ip, job, cls, instance, server, ins Total number of versions (includes delete marker) in a cluster minio_cluster_webhook_failed_messages counter ip, job, cls, instance, server, ins Number of messages that failed to send minio_cluster_webhook_online gauge ip, job, cls, instance, server, ins Is the webhook online? minio_cluster_webhook_queue_length counter ip, job, cls, instance, server, ins Webhook queue length minio_cluster_webhook_total_messages counter ip, job, cls, instance, server, ins Total number of messages sent to this target minio_cluster_write_quorum gauge ip, job, cls, instance, server, ins Maximum write quorum across all pools and sets minio_node_file_descriptor_limit_total gauge ip, job, cls, instance, server, ins Limit on total number of open file descriptors for the MinIO Server process minio_node_file_descriptor_open_total gauge ip, job, cls, instance, server, ins Total number of open file descriptors by the MinIO Server process minio_node_go_routine_total gauge ip, job, cls, instance, server, ins Total number of go routines running minio_node_ilm_expiry_pending_tasks gauge ip, job, cls, instance, server, ins Number of pending ILM expiry tasks in the queue minio_node_ilm_transition_active_tasks gauge ip, job, cls, instance, server, ins Number of active ILM transition tasks minio_node_ilm_transition_missed_immediate_tasks gauge ip, job, cls, instance, server, ins Number of missed immediate ILM transition tasks minio_node_ilm_transition_pending_tasks gauge ip, job, cls, instance, server, ins Number of pending ILM transition tasks in the queue minio_node_ilm_versions_scanned counter ip, job, cls, instance, server, ins Total number of object versions checked for ilm actions since server start minio_node_io_rchar_bytes counter ip, job, cls, instance, server, ins Total bytes read by the process from the underlying storage system including cache, /proc/[pid]/io rchar minio_node_io_read_bytes counter ip, job, cls, instance, server, ins Total bytes read by the process from the underlying storage system, /proc/[pid]/io read_bytes minio_node_io_wchar_bytes counter ip, job, cls, instance, server, ins Total bytes written by the process to the underlying storage system including page cache, /proc/[pid]/io wchar minio_node_io_write_bytes counter ip, job, cls, instance, server, ins Total bytes written by the process to the underlying storage system, /proc/[pid]/io write_bytes minio_node_process_cpu_total_seconds counter ip, job, cls, instance, server, ins Total user and system CPU time spent in seconds minio_node_process_resident_memory_bytes gauge ip, job, cls, instance, server, ins Resident memory size in bytes minio_node_process_starttime_seconds gauge ip, job, cls, instance, server, ins Start time for MinIO process per node, time in seconds since Unix epoc minio_node_process_uptime_seconds gauge ip, job, cls, instance, server, ins Uptime for MinIO process per node in seconds minio_node_scanner_bucket_scans_finished counter ip, job, cls, instance, server, ins Total number of bucket scans finished since server start minio_node_scanner_bucket_scans_started counter ip, job, cls, instance, server, ins Total number of bucket scans started since server start minio_node_scanner_directories_scanned counter ip, job, cls, instance, server, ins Total number of directories scanned since server start minio_node_scanner_objects_scanned counter ip, job, cls, instance, server, ins Total number of unique objects scanned since server start minio_node_scanner_versions_scanned counter ip, job, cls, instance, server, ins Total number of object versions scanned since server start minio_node_syscall_read_total counter ip, job, cls, instance, server, ins Total read SysCalls to the kernel. /proc/[pid]/io syscr minio_node_syscall_write_total counter ip, job, cls, instance, server, ins Total write SysCalls to the kernel. /proc/[pid]/io syscw minio_notify_current_send_in_progress gauge ip, job, cls, instance, server, ins Number of concurrent async Send calls active to all targets (deprecated, please use ‘minio_notify_target_current_send_in_progress’ instead) minio_notify_events_errors_total counter ip, job, cls, instance, server, ins Events that were failed to be sent to the targets (deprecated, please use ‘minio_notify_target_failed_events’ instead) minio_notify_events_sent_total counter ip, job, cls, instance, server, ins Total number of events sent to the targets (deprecated, please use ‘minio_notify_target_total_events’ instead) minio_notify_events_skipped_total counter ip, job, cls, instance, server, ins Events that were skipped to be sent to the targets due to the in-memory queue being full minio_s3_requests_4xx_errors_total counter ip, job, cls, instance, server, ins, api Total number of S3 requests with (4xx) errors minio_s3_requests_errors_total counter ip, job, cls, instance, server, ins, api Total number of S3 requests with (4xx and 5xx) errors minio_s3_requests_incoming_total gauge ip, job, cls, instance, server, ins Total number of incoming S3 requests minio_s3_requests_inflight_total gauge ip, job, cls, instance, server, ins, api Total number of S3 requests currently in flight minio_s3_requests_rejected_auth_total counter ip, job, cls, instance, server, ins Total number of S3 requests rejected for auth failure minio_s3_requests_rejected_header_total counter ip, job, cls, instance, server, ins Total number of S3 requests rejected for invalid header minio_s3_requests_rejected_invalid_total counter ip, job, cls, instance, server, ins Total number of invalid S3 requests minio_s3_requests_rejected_timestamp_total counter ip, job, cls, instance, server, ins Total number of S3 requests rejected for invalid timestamp minio_s3_requests_total counter ip, job, cls, instance, server, ins, api Total number of S3 requests minio_s3_requests_ttfb_seconds_distribution gauge ip, job, cls, le, instance, server, ins, api Distribution of time to first byte across API calls minio_s3_requests_waiting_total gauge ip, job, cls, instance, server, ins Total number of S3 requests in the waiting queue minio_s3_traffic_received_bytes counter ip, job, cls, instance, server, ins Total number of s3 bytes received minio_s3_traffic_sent_bytes counter ip, job, cls, instance, server, ins Total number of s3 bytes sent minio_software_commit_info gauge ip, job, cls, instance, commit, server, ins Git commit hash for the MinIO release minio_software_version_info gauge ip, job, cls, instance, version, server, ins MinIO Release tag for the server minio_up Unknown ip, job, cls, instance, ins N/A minio_usage_last_activity_nano_seconds gauge ip, job, cls, instance, server, ins Time elapsed (in nano seconds) since last scan activity. scrape_duration_seconds Unknown ip, job, cls, instance, ins N/A scrape_samples_post_metric_relabeling Unknown ip, job, cls, instance, ins N/A scrape_samples_scraped Unknown ip, job, cls, instance, ins N/A scrape_series_added Unknown ip, job, cls, instance, ins N/A up Unknown ip, job, cls, instance, ins N/A ","categories":["Reference"],"description":"Complete list of monitoring metrics provided by the Pigsty MINIO module with explanations","excerpt":"Complete list of monitoring metrics provided by the Pigsty MINIO …","ref":"/docs/minio/metric/","tags":"","title":"Metrics"},{"body":" What version of MinIO does Pigsty use? MinIO announced entering maintenance mode on 2025-12-03, no longer releasing new feature versions, only security patches and maintenance versions, and stopped releasing binary RPM/DEB on 2025-10-15. So Pigsty forked its own MinIO and used minio/pkger to create the latest 2025-12-03 version.\nThis version fixes the MinIO CVE-2025-62506 security vulnerability, ensuring Pigsty users’ MinIO deployments are safe and reliable. You can find the RPM/DEB packages and build scripts in the Pigsty Infra repository.\nWhy does MinIO require HTTPS? When pgBackRest uses object storage as a backup repository, HTTPS is mandatory to ensure data transmission security. If your MinIO is not used for pgBackRest backup, you can still choose to use HTTP protocol. You can disable HTTPS by modifying the parameter minio_https.\nGetting invalid certificate error when accessing MinIO from containers? Unless you use certificates issued by a real enterprise CA, MinIO uses self-signed certificates by default, which causes client tools inside containers (such as mc / rclone / awscli, etc.) to be unable to verify the identity of the MinIO server, resulting in invalid certificate errors.\nFor example, for Node.js applications, you can mount the MinIO server’s CA certificate into the container and specify the CA certificate path via the environment variable NODE_EXTRA_CA_CERTS:\nenvironment: NODE_EXTRA_CA_CERTS: /etc/pki/ca.crt volumes: - /etc/pki/ca.crt:/etc/pki/ca.crt:ro Of course, if your MinIO is not used as a pgBackRest backup repository, you can also choose to disable MinIO’s HTTPS support and use HTTP protocol instead.\nWhat if multi-node/multi-disk MinIO cluster fails to start? In Single-Node Multi-Disk or Multi-Node Multi-Disk mode, if the data directory is not a valid disk mount point, MinIO will refuse to start. Please use mounted disks as MinIO’s data directory instead of regular directories. You can only use regular directories as MinIO’s data directory in Single-Node Single-Disk mode, which is only suitable for development testing or non-critical scenarios.\nHow to add new members to an existing MinIO cluster? Before deployment, you should plan MinIO cluster capacity, as adding new members requires a global restart.\nYou can scale MinIO by adding new server nodes to the existing cluster to create a new storage pool.\nNote that once MinIO is deployed, you cannot modify the number of nodes and disks in the existing cluster! You can only scale by adding new storage pools.\nFor detailed steps, please refer to the Pigsty documentation: Expand Cluster, and the MinIO official documentation: Expand MinIO Deployment\nHow to remove a MinIO cluster? Starting from Pigsty v3.6, removing a MinIO cluster requires using the dedicated minio-rm.yml playbook:\n./minio-rm.yml -l minio # Remove MinIO cluster ./minio-rm.yml -l minio -e minio_rm_data=false # Remove cluster but keep data If you have enabled minio_safeguard protection, you need to explicitly override it to perform removal:\n./minio-rm.yml -l minio -e minio_safeguard=false What’s the difference between mcli and mc commands? mcli is a renamed version of the official MinIO client mc. In Pigsty, we use mcli instead of mc to avoid conflicts with Midnight Commander (a common file manager that also uses the mc command).\nBoth have identical functionality, just with different command names. You can find the complete command reference in the MinIO Client documentation.\nHow to monitor MinIO cluster status? Pigsty provides out-of-the-box monitoring capabilities for MinIO:\nGrafana Dashboards: MinIO Overview and MinIO Instance Alerting Rules: Including MinIO down, node offline, disk offline alerts MinIO Built-in Console: Access via https://\u003cminio-ip\u003e:9001 For details, please refer to the Monitoring documentation\n","categories":["Reference"],"description":"Frequently asked questions about the Pigsty MINIO object storage module","excerpt":"Frequently asked questions about the Pigsty MINIO object storage …","ref":"/docs/minio/faq/","tags":"","title":"FAQ"},{"body":"Redis is a widely popular open-source high-performance in-memory data structure server, and a great companion to PostgreSQL. Redis in Pigsty is a production-ready complete solution supporting master-slave replication, sentinel high availability, and native cluster mode, with integrated monitoring and logging capabilities, along with automated installation, configuration, and operation playbooks.\n","categories":["Reference"],"description":"Pigsty has built-in Redis support, a high-performance in-memory data structure server. Deploy Redis in standalone, cluster, or sentinel mode as a companion to PostgreSQL.","excerpt":"Pigsty has built-in Redis support, a high-performance in-memory data …","ref":"/docs/redis/","tags":"","title":"Module: REDIS"},{"body":" Concept The entity model of Redis is almost the same as that of PostgreSQL, which also includes the concepts of Cluster and Instance. Note that the Cluster here does not refer to the native Redis Cluster mode.\nThe core difference between the REDIS module and the PGSQL module is that Redis uses a single-node multi-instance deployment rather than the 1:1 deployment: multiple Redis instances are typically deployed on a physical/virtual machine node to utilize multi-core CPUs fully. Therefore, the ways to configure and administer Redis instances are slightly different from PGSQL.\nIn Redis managed by Pigsty, nodes are entirely subordinate to the cluster, which means that currently, it is not allowed to deploy Redis instances of two different clusters on one node. However, this does not affect deploying multiple independent Redis primary-replica instances on one node. Of course, there are some limitations; for example, in this case, you cannot specify different passwords for different instances on the same node.\nIdentity Parameters Redis identity parameters are required parameters when defining a Redis cluster.\nName Attribute Description Example redis_cluster REQUIRED, cluster level Cluster name redis-test redis_node REQUIRED, node level Node sequence number 1,2 redis_instances REQUIRED, node level Instance definition { 6001 : {} ,6002 : {}} redis_cluster: Redis cluster name, serves as the top-level namespace for cluster resources. redis_node: Redis node number, an integer unique within the cluster to distinguish different nodes. redis_instances: JSON object where keys are instance port numbers and values are JSON objects containing other instance configurations. Redis Mode There are three different working modes for Redis, specified by the redis_mode parameter:\nstandalone: Default standalone master-slave mode cluster: Redis native distributed cluster mode sentinel: Sentinel mode, providing high availability for standalone master-slave Redis Here are three examples of Redis cluster definitions:\nA 1-node, one master \u0026 one slave Redis Standalone cluster: redis-ms A 1-node, 3-instance Redis Sentinel cluster: redis-sentinel A 2-node, 6-instance Redis Cluster: redis-cluster redis-ms: # redis classic primary \u0026 replica hosts: { 10.10.10.10: { redis_node: 1 , redis_instances: { 6379: { }, 6380: { replica_of: '10.10.10.10 6379' } } } } vars: { redis_cluster: redis-ms ,redis_password: 'redis.ms' ,redis_max_memory: 64MB } redis-meta: # redis sentinel x 3 hosts: { 10.10.10.11: { redis_node: 1 , redis_instances: { 26379: { } ,26380: { } ,26381: { } } } } vars: redis_cluster: redis-meta redis_password: 'redis.meta' redis_mode: sentinel redis_max_memory: 16MB redis_sentinel_monitor: # primary list for redis sentinel, use cls as name, primary ip:port - { name: redis-ms, host: 10.10.10.10, port: 6379 ,password: redis.ms, quorum: 2 } redis-test: # redis native cluster: 3m x 3s hosts: 10.10.10.12: { redis_node: 1 ,redis_instances: { 6379: { } ,6380: { } ,6381: { } } } 10.10.10.13: { redis_node: 2 ,redis_instances: { 6379: { } ,6380: { } ,6381: { } } } vars: { redis_cluster: redis-test ,redis_password: 'redis.test' ,redis_mode: cluster, redis_max_memory: 32MB } Limitations A Redis node can only belong to one Redis cluster, which means you cannot assign a node to two different Redis clusters simultaneously. On each Redis node, you need to assign a unique port number to each Redis instance to avoid port conflicts. Typically, the same Redis cluster will use the same password, but multiple Redis instances on a Redis node cannot have different passwords (because redis_exporter only allows one password). Redis Cluster has built-in HA, while standalone master-slave HA requires additional manual configuration in Sentinel since we don’t know if you have deployed Sentinel. Fortunately, configuring HA for standalone Redis is straightforward through Sentinel. For details, see Administration - Configure HA with Sentinel. Typical Configuration Examples Here are some common Redis configuration examples for different scenarios:\nCache Cluster (Pure In-Memory) For pure caching scenarios with no data persistence requirements:\nredis-cache: hosts: 10.10.10.10: { redis_node: 1 , redis_instances: { 6379: { }, 6380: { } } } 10.10.10.11: { redis_node: 2 , redis_instances: { 6379: { }, 6380: { } } } vars: redis_cluster: redis-cache redis_password: 'cache.password' redis_max_memory: 2GB redis_mem_policy: allkeys-lru # evict LRU keys when memory is full redis_rdb_save: [] # disable RDB persistence redis_aof_enabled: false # disable AOF persistence Session Store Cluster For web application session storage with some persistence needs:\nredis-session: hosts: 10.10.10.10: { redis_node: 1 , redis_instances: { 6379: { }, 6380: { replica_of: '10.10.10.10 6379' } } } vars: redis_cluster: redis-session redis_password: 'session.password' redis_max_memory: 1GB redis_mem_policy: volatile-lru # only evict keys with expire set redis_rdb_save: ['300 1'] # save every 5 minutes if at least 1 change redis_aof_enabled: false Message Queue Cluster For simple message queue scenarios requiring higher data reliability:\nredis-queue: hosts: 10.10.10.10: { redis_node: 1 , redis_instances: { 6379: { }, 6380: { replica_of: '10.10.10.10 6379' } } } vars: redis_cluster: redis-queue redis_password: 'queue.password' redis_max_memory: 4GB redis_mem_policy: noeviction # reject writes when memory full, don't evict redis_rdb_save: ['60 1'] # save every minute if at least 1 change redis_aof_enabled: true # enable AOF for better persistence High Availability Master-Slave Cluster Master-slave cluster with Sentinel automatic failover:\n# Master-slave cluster redis-ha: hosts: 10.10.10.10: { redis_node: 1 , redis_instances: { 6379: { } } } # primary 10.10.10.11: { redis_node: 2 , redis_instances: { 6379: { replica_of: '10.10.10.10 6379' } } } # replica 1 10.10.10.12: { redis_node: 3 , redis_instances: { 6379: { replica_of: '10.10.10.10 6379' } } } # replica 2 vars: redis_cluster: redis-ha redis_password: 'ha.password' redis_max_memory: 8GB # Sentinel cluster (manages the above master-slave cluster) redis-sentinel: hosts: 10.10.10.10: { redis_node: 1 , redis_instances: { 26379: { } } } 10.10.10.11: { redis_node: 2 , redis_instances: { 26379: { } } } 10.10.10.12: { redis_node: 3 , redis_instances: { 26379: { } } } vars: redis_cluster: redis-sentinel redis_password: 'sentinel.password' redis_mode: sentinel redis_max_memory: 64MB redis_sentinel_monitor: - { name: redis-ha, host: 10.10.10.10, port: 6379, password: 'ha.password', quorum: 2 } Large-Scale Native Cluster For high-volume, high-throughput scenarios using native distributed cluster:\nredis-cluster: hosts: 10.10.10.10: { redis_node: 1 , redis_instances: { 6379: { }, 6380: { }, 6381: { } } } 10.10.10.11: { redis_node: 2 , redis_instances: { 6379: { }, 6380: { }, 6381: { } } } 10.10.10.12: { redis_node: 3 , redis_instances: { 6379: { }, 6380: { }, 6381: { } } } 10.10.10.13: { redis_node: 4 , redis_instances: { 6379: { }, 6380: { }, 6381: { } } } vars: redis_cluster: redis-cluster redis_password: 'cluster.password' redis_mode: cluster redis_cluster_replicas: 1 # 1 replica per primary shard redis_max_memory: 16GB # max memory per instance redis_rdb_save: ['900 1'] redis_aof_enabled: false # This creates a 6-primary, 6-replica native cluster # Total capacity ~96GB (6 * 16GB) Security Hardening Configuration Recommended security configuration for production environments:\nredis-secure: hosts: 10.10.10.10: { redis_node: 1 , redis_instances: { 6379: { } } } vars: redis_cluster: redis-secure redis_password: 'StrongP@ssw0rd!' # use strong password redis_bind_address: '' # bind to internal IP instead of 0.0.0.0 redis_max_memory: 4GB redis_rename_commands: # rename dangerous commands FLUSHDB: 'DANGEROUS_FLUSHDB' FLUSHALL: 'DANGEROUS_FLUSHALL' DEBUG: '' # disable command CONFIG: 'ADMIN_CONFIG' ","categories":["Reference"],"description":"Choose the appropriate Redis mode for your use case and express your requirements through the inventory","excerpt":"Choose the appropriate Redis mode for your use case and express your …","ref":"/docs/redis/config/","tags":"","title":"Configuration"},{"body":"Parameter list for the REDIS module.\nParameter Overview The REDIS parameter group is used for Redis cluster deployment and configuration, including identity, instance definitions, operating mode, memory configuration, persistence, and monitoring.\nParameter Type Level Description redis_cluster string C Redis cluster name, required identity parameter redis_instances dict I Redis instance definitions on this node redis_node int I Redis node number, unique positive integer in cluster redis_fs_main path C Redis main data directory, /data by default redis_exporter_enabled bool C Enable Redis Exporter? redis_exporter_port port C Redis Exporter listen port redis_exporter_options string C/I Redis Exporter CLI arguments redis_mode enum C Redis mode: standalone, cluster, sentinel redis_conf string C Redis config template, except sentinel redis_bind_address ip C Redis bind address, empty uses host IP redis_max_memory size C/I Max memory for each Redis instance redis_mem_policy enum C Redis memory eviction policy redis_password password C Redis password, empty disables password redis_rdb_save string[] C Redis RDB save directives, empty list disables RDB redis_aof_enabled bool C Enable Redis AOF? redis_rename_commands dict C Rename dangerous Redis commands redis_cluster_replicas int C Replicas per master in Redis native cluster redis_sentinel_monitor master[] C Master list for Redis Sentinel to monitor The REDIS_REMOVE parameter group controls Redis instance removal behavior.\nParameter Type Level Description redis_safeguard bool G/C/A Prevent removing running Redis instances? redis_rm_data bool G/C/A Remove Redis data directory when removing? redis_rm_pkg bool G/C/A Uninstall Redis packages when removing? The Redis module contains 18 deployment parameters and 3 removal parameters.\n#redis_cluster: \u003cCLUSTER\u003e # Redis cluster name, required identity parameter #redis_node: 1 \u003cNODE\u003e # Redis node number, unique in cluster #redis_instances: {} \u003cNODE\u003e # Redis instance definitions on this node redis_fs_main: /data # Redis main data directory, `/data` by default redis_exporter_enabled: true # Enable Redis Exporter? redis_exporter_port: 9121 # Redis Exporter listen port redis_exporter_options: '' # Redis Exporter CLI arguments redis_mode: standalone # Redis mode: standalone, cluster, sentinel redis_conf: redis.conf # Redis config template, except sentinel redis_bind_address: '0.0.0.0' # Redis bind address, empty uses host IP redis_max_memory: 1GB # Max memory for each Redis instance redis_mem_policy: allkeys-lru # Redis memory eviction policy redis_password: '' # Redis password, empty disables password redis_rdb_save: ['1200 1'] # Redis RDB save directives, empty disables RDB redis_aof_enabled: false # Enable Redis AOF? redis_rename_commands: {} # Rename dangerous Redis commands redis_cluster_replicas: 1 # Replicas per master in Redis native cluster redis_sentinel_monitor: [] # Master list for Sentinel, sentinel mode only # REDIS_REMOVE redis_safeguard: false # Prevent removing running Redis instances? redis_rm_data: true # Remove Redis data directory when removing? redis_rm_pkg: false # Uninstall Redis packages when removing? redis_cluster Parameter: redis_cluster, Type: string, Level: C\nRedis cluster name, a required identity parameter that must be explicitly configured at the cluster level. It serves as the namespace for resources within the cluster.\nMust follow the naming pattern [a-z][a-z0-9-]* to comply with various identity constraints. Using redis- as a cluster name prefix is recommended.\nredis_node Parameter: redis_node, Type: int, Level: I\nRedis node sequence number, a required identity parameter that must be explicitly configured at the node (Host) level.\nA positive integer that should be unique within the cluster, used to distinguish and identify different nodes. Assign starting from 0 or 1.\nredis_instances Parameter: redis_instances, Type: dict, Level: I\nRedis instance definitions on the current node, a required parameter that must be explicitly configured at the node (Host) level.\nFormat is a JSON key-value object where keys are numeric port numbers and values are instance-specific JSON configuration items.\nredis-test: # redis native cluster: 3m x 3s hosts: 10.10.10.12: { redis_node: 1 ,redis_instances: { 6379: { } ,6380: { } ,6381: { } } } 10.10.10.13: { redis_node: 2 ,redis_instances: { 6379: { } ,6380: { } ,6381: { } } } vars: { redis_cluster: redis-test ,redis_password: 'redis.test' ,redis_mode: cluster, redis_max_memory: 32MB } Each Redis instance listens on a unique port on its node. The replica_of field in instance configuration sets the upstream master address to establish replication:\nredis_instances: 6379: {} 6380: { replica_of: '10.10.10.13 6379' } 6381: { replica_of: '10.10.10.13 6379' } redis_fs_main Parameter: redis_fs_main, Type: path, Level: C\nMain data disk mount point for Redis, default is /data. Pigsty creates a redis directory under this path to store Redis data.\nThe actual data storage directory is /data/redis, owned by the redis OS user. See FHS: Redis for internal structure details.\nredis_exporter_enabled Parameter: redis_exporter_enabled, Type: bool, Level: C\nEnable Redis Exporter monitoring component?\nEnabled by default, deploying one exporter per Redis node, listening on redis_exporter_port 9121 by default. It scrapes metrics from all Redis instances on the node.\nWhen set to false, roles/redis/tasks/exporter.yml still renders config files but skips starting the redis_exporter systemd service (the redis_exporter_launch task has when: redis_exporter_enabled|bool), allowing manually configured exporters to remain.\nredis_exporter_port Parameter: redis_exporter_port, Type: port, Level: C\nRedis Exporter listen port, default value: 9121\nredis_exporter_options Parameter: redis_exporter_options, Type: string, Level: C/I\nExtra CLI arguments for Redis Exporter, rendered to /etc/default/redis_exporter (see roles/redis/tasks/exporter.yml), default is empty string. REDIS_EXPORTER_OPTS is appended to the systemd service’s ExecStart=/bin/redis_exporter $REDIS_EXPORTER_OPTS, useful for configuring extra scrape targets or filtering behavior.\nredis_mode Parameter: redis_mode, Type: enum, Level: C\nRedis cluster operating mode, three options: standalone, cluster, sentinel. Default: standalone\nstandalone: Default, independent Redis master-slave mode cluster: Redis native cluster mode sentinel: Redis high availability component: Sentinel When using standalone mode, Pigsty sets up Redis replication based on the replica_of parameter.\nWhen using cluster mode, Pigsty creates a native Redis cluster using all defined instances based on the redis_cluster_replicas parameter.\nWhen redis_mode=sentinel, redis.yml executes the redis-ha phase (lines 80-130 of redis.yml) to distribute targets from redis_sentinel_monitor to all sentinels. When redis_mode=cluster, it also executes the redis-join phase (lines 134-180) calling redis-cli --cluster create --cluster-yes ... --cluster-replicas {{ redis_cluster_replicas }}. Both phases are automatically triggered in normal ./redis.yml -l \u003ccluster\u003e runs, or can be run separately with -t redis-ha or -t redis-join.\nredis_conf Parameter: redis_conf, Type: string, Level: C\nRedis config template path, except for Sentinel.\nDefault: redis.conf, a template file at roles/redis/templates/redis.conf.\nTo use your own Redis config template, place it in the templates/ directory and set this parameter to the template filename.\nNote: Redis Sentinel uses a different template file: roles/redis/templates/redis-sentinel.conf.\nredis_bind_address Parameter: redis_bind_address, Type: ip, Level: C\nIP address Redis server binds to. Empty string uses the hostname defined in the inventory.\nDefault: 0.0.0.0, binding to all available IPv4 addresses on the host.\nFor security in production environments, bind only to internal IPs by setting this to empty string ''.\nWhen empty, the template roles/redis/templates/redis.conf uses inventory_hostname to render bind \u003cip\u003e, binding to the management address declared in the inventory.\nredis_max_memory Parameter: redis_max_memory, Type: size, Level: C/I\nMaximum memory for each Redis instance, default: 1GB.\nredis_mem_policy Parameter: redis_mem_policy, Type: enum, Level: C\nRedis memory eviction policy, default: allkeys-lru\nnoeviction: Don’t save new values when memory limit is reached; only applies to primary when using replication allkeys-lru: Keep most recently used keys; remove least recently used (LRU) keys allkeys-lfu: Keep frequently used keys; remove least frequently used (LFU) keys volatile-lru: Remove least recently used keys with expire field set volatile-lfu: Remove least frequently used keys with expire field set allkeys-random: Randomly remove keys to make space for new data volatile-random: Randomly remove keys with expire field set volatile-ttl: Remove keys with expire field set and shortest remaining TTL See Redis Eviction Policy for details.\nredis_password Parameter: redis_password, Type: password, Level: C/N\nRedis password. Empty string disables password, which is the default behavior.\nNote that due to redis_exporter implementation limitations, you can only set one redis_password per node. This is usually not a problem since Pigsty doesn’t allow deploying two different Redis clusters on the same node.\nPigsty automatically writes this password to /etc/default/redis_exporter (REDIS_PASSWORD=...) and uses it in the redis-ha phase with redis-cli -a \u003cpassword\u003e, so no need to separately configure exporter or Sentinel authentication.\nUse a strong password in production environments\nredis_rdb_save Parameter: redis_rdb_save, Type: string[], Level: C\nRedis RDB save directives. Use empty list to disable RDB.\nDefault is [\"1200 1\"]: dump dataset to disk every 20 minutes if at least 1 key changed.\nSee Redis Persistence for details.\nredis_aof_enabled Parameter: redis_aof_enabled, Type: bool, Level: C\nEnable Redis AOF? Default is false, meaning AOF is not used.\nredis_rename_commands Parameter: redis_rename_commands, Type: dict, Level: C\nRename dangerous Redis commands. A k:v dictionary where old is the command to rename and new is the new name.\nDefault: {}. You can hide dangerous commands like FLUSHDB and FLUSHALL. Example:\n{ \"keys\": \"op_keys\", \"flushdb\": \"op_flushdb\", \"flushall\": \"op_flushall\", \"config\": \"op_config\" } redis_cluster_replicas Parameter: redis_cluster_replicas, Type: int, Level: C\nNumber of replicas per master/primary in Redis native cluster. Default: 1, meaning one replica per master.\nredis_sentinel_monitor Parameter: redis_sentinel_monitor, Type: master[], Level: C\nList of masters for Redis Sentinel to monitor, used only on sentinel clusters. Each managed master is defined as:\nredis_sentinel_monitor: # primary list for redis sentinel, use cls as name, primary ip:port - { name: redis-src, host: 10.10.10.45, port: 6379 ,password: redis.src, quorum: 1 } - { name: redis-dst, host: 10.10.10.48, port: 6379 ,password: redis.dst, quorum: 1 } name and host are required; port, password, and quorum are optional. quorum sets the number of sentinels needed to agree on master failure, typically more than half of sentinel instances (default is 1).\nStarting from Pigsty 4.0, you can add remove: true to an entry, causing the redis-ha phase to only execute SENTINEL REMOVE \u003cname\u003e, useful for cleaning up targets no longer needed.\nREDIS_REMOVE The following parameters are used by the redis_remove role, invoked by the redis-rm.yml playbook, controlling Redis instance removal behavior.\nredis_safeguard Parameter: redis_safeguard, Type: bool, Level: G/C/A\nRedis safety guard to prevent accidental removal: when enabled, the redis-rm.yml playbook cannot remove running Redis instances.\nDefault is false. When set to true, the redis-rm.yml playbook refuses to execute, preventing accidental deletion of running Redis instances.\nOverride with CLI argument -e redis_safeguard=false to force removal.\nredis_rm_data Parameter: redis_rm_data, Type: bool, Level: G/C/A\nRemove Redis data directory when removing Redis instances? Default is true.\nThe data directory (/data/redis/) contains Redis RDB and AOF files. If not removed, newly deployed Redis instances will load data from these backup files.\nSet to false to preserve data directories for later recovery.\nredis_rm_pkg Parameter: redis_rm_pkg, Type: bool, Level: G/C/A\nUninstall Redis and redis_exporter packages when removing Redis instances? Default is false.\nTypically not needed to uninstall packages; only enable when completely cleaning up a node.\n","categories":["Reference"],"description":"REDIS module provides 18 deployment parameters + 3 removal parameters","excerpt":"REDIS module provides 18 deployment parameters + 3 removal parameters","ref":"/docs/redis/param/","tags":"","title":"Parameters"},{"body":"The REDIS module provides two playbooks for deploying/removing Redis clusters/nodes/instances:\nredis.yml: Deploy Redis cluster/node/instance redis-rm.yml: Remove Redis cluster/node/instance redis.yml The redis.yml playbook for deploying Redis contains the following subtasks:\nredis_node : Init redis node - redis_install : Install redis \u0026 redis_exporter - redis_user : Create OS user redis - redis_dir : Configure redis FHS directory structure redis_exporter : Configure redis_exporter monitoring - redis_exporter_config : Generate redis_exporter config - redis_exporter_launch : Launch redis_exporter redis_instance : Init and restart redis cluster/node/instance - redis_config : Generate redis instance config - redis_launch : Launch redis instance redis_register : Register redis to infrastructure redis_ha : Configure redis sentinel (sentinel mode only) redis_join : Join redis native cluster (cluster mode only) Operation Levels redis.yml supports three operation levels, controlled by -l to limit target scope and -e redis_port=\u003cport\u003e to specify a single instance:\nLevel Parameters Description Cluster -l \u003ccluster\u003e Deploy all nodes and instances of the entire Redis cluster Node -l \u003cip\u003e Deploy all Redis instances on the specified node Instance -l \u003cip\u003e -e redis_port=\u003cport\u003e Deploy only a single instance on the specified node Cluster-Level Operations Deploy an entire Redis cluster, including all instances on all nodes:\n./redis.yml -l redis-ms # deploy the entire redis-ms cluster ./redis.yml -l redis-test # deploy the entire redis-test cluster ./redis.yml -l redis-sentinel # deploy sentinel cluster Cluster-level operations will:\nInstall Redis packages on all nodes Create redis user and directory structure on all nodes Start redis_exporter on all nodes Deploy and start all defined Redis instances Register all instances to the monitoring system If sentinel mode, configure sentinel monitoring targets If cluster mode, form the native cluster Node-Level Operations Deploy only all Redis instances on the specified node:\n./redis.yml -l 10.10.10.10 # deploy all instances on this node ./redis.yml -l 10.10.10.11 # deploy another node Node-level operations are useful for:\nScaling up by adding new nodes to an existing cluster Redeploying all instances on a specific node Reinitializing after node failure recovery Note: Node-level operations do not execute the redis-ha and redis-join stages. If you need to add a new node to a native cluster, you must manually run redis-cli --cluster add-node\nInstance-Level Operations Use the -e redis_port=\u003cport\u003e parameter to operate on a single instance:\n# Deploy only the 6379 port instance on 10.10.10.10 ./redis.yml -l 10.10.10.10 -e redis_port=6379 # Deploy only the 6380 port instance on 10.10.10.11 ./redis.yml -l 10.10.10.11 -e redis_port=6380 Instance-level operations are useful for:\nAdding new instances to an existing node Redeploying a single failed instance Updating a single instance’s configuration When redis_port is specified:\nOnly renders the config file for that port Only starts/restarts the systemd service for that port Only registers that instance to the monitoring system Does not affect other instances on the same node Common Tags Use the -t \u003ctag\u003e parameter to selectively execute certain tasks:\n# Install packages only, don't start services ./redis.yml -l redis-ms -t redis_node # Update config and restart instances only ./redis.yml -l redis-ms -t redis_config,redis_launch # Update monitoring registration only ./redis.yml -l redis-ms -t redis_register # Configure sentinel monitoring targets only (sentinel mode) ./redis.yml -l redis-sentinel -t redis-ha # Form native cluster only (cluster mode, auto-runs after first deployment) ./redis.yml -l redis-cluster -t redis-join Idempotency redis.yml is idempotent and safe to run repeatedly:\nRepeated execution overwrites existing config files Repeated execution restarts Redis instances Does not check if instances already exist; directly renders config and restarts Suitable for batch updates after configuration changes Tip: If you only want to update configs without restarting all instances, use -t redis_config to render configs only, then manually restart the instances you need.\nredis-rm.yml The redis-rm.yml playbook for removing Redis contains the following subtasks:\nredis_safeguard : Safety check, abort if redis_safeguard=true redis_deregister : Remove registration from monitoring system - rm_metrics : Delete /infra/targets/redis/*.yml - rm_logs : Revoke /etc/vector/redis.yaml redis_exporter : Stop and disable redis_exporter redis : Stop and disable redis instances redis_data : Delete data directories (when redis_rm_data=true) redis_pkg : Uninstall packages (when redis_rm_pkg=true) Operation Levels redis-rm.yml also supports three operation levels:\nLevel Parameters Description Cluster -l \u003ccluster\u003e Remove all nodes and instances of the entire Redis cluster Node -l \u003cip\u003e Remove all Redis instances on the specified node Instance -l \u003cip\u003e -e redis_port=\u003cport\u003e Remove only a single instance on the specified node Cluster-Level Removal Remove an entire Redis cluster:\n./redis-rm.yml -l redis-ms # remove entire redis-ms cluster ./redis-rm.yml -l redis-test # remove entire redis-test cluster Cluster-level removal will:\nDeregister all instances on all nodes from the monitoring system Stop redis_exporter on all nodes Stop and disable all Redis instances Delete all data directories (if redis_rm_data=true) Uninstall packages (if redis_rm_pkg=true) Node-Level Removal Remove only all Redis instances on the specified node:\n./redis-rm.yml -l 10.10.10.10 # remove all instances on this node ./redis-rm.yml -l 10.10.10.11 # remove another node Node-level removal is useful for:\nScaling down by removing an entire node Cleanup before node decommission Preparation before node migration Node-level removal will:\nDeregister all instances on that node from the monitoring system Stop redis_exporter on that node Stop all Redis instances on that node Delete all data directories on that node Delete Vector logging config on that node Instance-Level Removal Use the -e redis_port=\u003cport\u003e parameter to remove a single instance:\n# Remove only the 6379 port instance on 10.10.10.10 ./redis-rm.yml -l 10.10.10.10 -e redis_port=6379 # Remove only the 6380 port instance on 10.10.10.11 ./redis-rm.yml -l 10.10.10.11 -e redis_port=6380 Instance-level removal is useful for:\nRemoving a single replica from a node Removing instances no longer needed Removing the original primary after failover Behavioral differences when redis_port is specified:\nComponent Node-Level (no redis_port) Instance-Level (with redis_port) Monitoring registration Delete entire node’s registration file Only remove that instance from registration file redis_exporter Stop and disable No operation (other instances still need it) Redis instances Stop all instances Only stop the specified port’s instance Data directory Delete entire /data/redis/ directory Only delete /data/redis/\u003ccluster\u003e-\u003cnode\u003e-\u003cport\u003e/ Vector config Delete /etc/vector/redis.yaml No operation (other instances still need it) Packages Optionally uninstall No operation Control Parameters redis-rm.yml provides the following control parameters:\nParameter Default Description redis_safeguard false Safety guard; when true, refuses to execute removal redis_rm_data true Whether to delete data directories (RDB/AOF files) redis_rm_pkg false Whether to uninstall Redis packages Usage examples:\n# Remove cluster but keep data directories ./redis-rm.yml -l redis-ms -e redis_rm_data=false # Remove cluster and uninstall packages ./redis-rm.yml -l redis-ms -e redis_rm_pkg=true # Bypass safeguard to force removal ./redis-rm.yml -l redis-ms -e redis_safeguard=false Safeguard Mechanism When a cluster has redis_safeguard: true configured, redis-rm.yml will refuse to execute:\nredis-production: vars: redis_safeguard: true # enable protection for production $ ./redis-rm.yml -l redis-production TASK [ABORT due to redis_safeguard enabled] *** fatal: [10.10.10.10]: FAILED! =\u003e {\"msg\": \"Abort due to redis_safeguard...\"} Explicit override is required to execute:\n./redis-rm.yml -l redis-production -e redis_safeguard=false Quick Reference Deployment Quick Reference # Deploy entire cluster ./redis.yml -l \u003ccluster\u003e # Scale up: deploy new node ./redis.yml -l \u003cnew-node-ip\u003e # Scale up: add new instance to existing node (add definition to config first) ./redis.yml -l \u003cip\u003e -e redis_port=\u003cnew-port\u003e # Update config and restart ./redis.yml -l \u003ccluster\u003e -t redis_config,redis_launch # Update single instance config only ./redis.yml -l \u003cip\u003e -e redis_port=\u003cport\u003e -t redis_config,redis_launch Removal Quick Reference # Remove entire cluster ./redis-rm.yml -l \u003ccluster\u003e # Scale down: remove entire node ./redis-rm.yml -l \u003cip\u003e # Scale down: remove single instance ./redis-rm.yml -l \u003cip\u003e -e redis_port=\u003cport\u003e # Remove but keep data ./redis-rm.yml -l \u003ccluster\u003e -e redis_rm_data=false # Complete cleanup (including packages) ./redis-rm.yml -l \u003ccluster\u003e -e redis_rm_pkg=true Wrapper Scripts Pigsty provides convenient wrapper scripts:\n# Deploy bin/redis-add \u003ccluster\u003e # deploy cluster bin/redis-add \u003cip\u003e # deploy node bin/redis-add \u003cip\u003e \u003cport\u003e # deploy instance # Remove bin/redis-rm \u003ccluster\u003e # remove cluster bin/redis-rm \u003cip\u003e # remove node bin/redis-rm \u003cip\u003e \u003cport\u003e # remove instance Demo Initialize Redis cluster with Redis playbook:\n","categories":["Task"],"description":"Manage Redis clusters with Ansible playbooks and quick command reference.","excerpt":"Manage Redis clusters with Ansible playbooks and quick command …","ref":"/docs/redis/playbook/","tags":"","title":"Playbook"},{"body":"Here are some common Redis administration task SOPs (Standard Operating Procedures):\nBasic Operations\nInitialize Redis Remove Redis Reconfigure Redis Using Redis Client High Availability\nConfigure Redis Replica Configure HA with Sentinel Initialize Redis Native Cluster Scaling \u0026 Migration\nScale Up Redis Nodes Scale Down Redis Nodes Backup and Restore Troubleshooting\nCommon Issue Diagnosis Performance Tuning For more questions, please refer to FAQ: REDIS.\nInitialize Redis You can use the redis.yml playbook to initialize Redis clusters, nodes, or instances:\n# Initialize all Redis instances in the cluster ./redis.yml -l \u003ccluster\u003e # init redis cluster # Initialize all Redis instances on a specific node ./redis.yml -l 10.10.10.10 # init redis node # Initialize a specific Redis instance: 10.10.10.11:6379 ./redis.yml -l 10.10.10.11 -e redis_port=6379 -t redis You can also use wrapper scripts to initialize:\nbin/redis-add redis-ms # create redis cluster 'redis-ms' bin/redis-add 10.10.10.10 # create redis node '10.10.10.10' bin/redis-add 10.10.10.10 6379 # create redis instance '10.10.10.10:6379' Remove Redis You can use the redis-rm.yml playbook to remove Redis clusters, nodes, or instances:\n# Remove Redis cluster `redis-test` ./redis-rm.yml -l redis-test # Remove Redis cluster `redis-test` and uninstall Redis packages ./redis-rm.yml -l redis-test -e redis_rm_pkg=true # Remove all instances on Redis node 10.10.10.13 ./redis-rm.yml -l 10.10.10.13 # Remove a specific Redis instance 10.10.10.13:6379 ./redis-rm.yml -l 10.10.10.13 -e redis_port=6379 You can also use wrapper scripts to remove Redis clusters/nodes/instances:\nbin/redis-rm redis-ms # remove redis cluster 'redis-ms' bin/redis-rm 10.10.10.10 # remove redis node '10.10.10.10' bin/redis-rm 10.10.10.10 6379 # remove redis instance '10.10.10.10:6379' Reconfigure Redis You can partially run the redis.yml playbook to reconfigure Redis clusters, nodes, or instances:\n./redis.yml -l \u003ccluster\u003e -t redis_config,redis_launch Note that Redis cannot reload configuration online. You must restart Redis using the launch task to make configuration changes take effect.\nUsing Redis Client Access Redis instances with redis-cli:\n$ redis-cli -h 10.10.10.10 -p 6379 # \u003c--- connect with host and port 10.10.10.10:6379\u003e auth redis.ms # \u003c--- authenticate with password OK 10.10.10.10:6379\u003e set a 10 # \u003c--- set a key OK 10.10.10.10:6379\u003e get a # \u003c--- get the key value \"10\" Redis provides the redis-benchmark tool, which can be used for Redis performance evaluation or to generate load for testing.\nredis-benchmark -h 10.10.10.13 -p 6379 Configure Redis Replica https://redis.io/commands/replicaof/\n# Promote a Redis instance to primary \u003e REPLICAOF NO ONE \"OK\" # Make a Redis instance a replica of another instance \u003e REPLICAOF 127.0.0.1 6799 \"OK\" Configure HA with Sentinel Redis standalone master-slave clusters can be configured for automatic high availability through Redis Sentinel. For detailed information, please refer to the Sentinel official documentation.\nUsing the four-node sandbox environment as an example, a Redis Sentinel cluster redis-meta can be used to manage multiple standalone Redis master-slave clusters.\nTaking the one-master-one-slave Redis standalone cluster redis-ms as an example, you need to add the target on each Sentinel instance using SENTINEL MONITOR and provide the password using SENTINEL SET, and the high availability is configured.\n# For each sentinel, add the redis master to sentinel management: (26379,26380,26381) $ redis-cli -h 10.10.10.11 -p 26379 -a redis.meta 10.10.10.11:26379\u003e SENTINEL MONITOR redis-ms 10.10.10.10 6379 1 10.10.10.11:26379\u003e SENTINEL SET redis-ms auth-pass redis.ms # if auth enabled, password needs to be configured If you want to remove a Redis master-slave cluster managed by Sentinel, use SENTINEL REMOVE \u003cname\u003e.\nYou can use the redis_sentinel_monitor parameter defined on the Sentinel cluster to automatically configure the list of masters managed by Sentinel.\nredis_sentinel_monitor: # list of masters to be monitored, port, password, quorum (should be more than 1/2 of sentinels) are optional - { name: redis-src, host: 10.10.10.45, port: 6379 ,password: redis.src, quorum: 1 } - { name: redis-dst, host: 10.10.10.48, port: 6379 ,password: redis.dst, quorum: 1 } The redis-ha stage in redis.yml will render /tmp/\u003ccluster\u003e.monitor on each sentinel instance based on this list and execute SENTINEL REMOVE and SENTINEL MONITOR commands sequentially, ensuring the sentinel management state remains consistent with the inventory. If you only want to remove a target without re-adding it, set remove: true on the monitor object, and the playbook will skip re-registration after SENTINEL REMOVE.\nUse the following command to refresh the managed master list on the Redis Sentinel cluster:\n./redis.yml -l redis-meta -t redis-ha # replace redis-meta if your Sentinel cluster has a different name Initialize Redis Native Cluster When redis_mode is set to cluster, redis.yml will additionally execute the redis-join stage: it uses redis-cli --cluster create --cluster-yes ... --cluster-replicas {{ redis_cluster_replicas }} in /tmp/\u003ccluster\u003e-join.sh to join all instances into a native cluster.\nThis step runs automatically during the first deployment. Subsequently re-running ./redis.yml -l \u003ccluster\u003e -t redis-join will regenerate and execute the same command. Since --cluster create is not idempotent, you should only trigger this stage separately when you are sure you need to rebuild the entire native cluster.\nScale Up Redis Nodes Scale Up Standalone Cluster When adding new nodes/instances to an existing Redis master-slave cluster, first add the new definition in the inventory:\nredis-ms: hosts: 10.10.10.10: { redis_node: 1 , redis_instances: { 6379: { }, 6380: { replica_of: '10.10.10.10 6379' } } } 10.10.10.11: { redis_node: 2 , redis_instances: { 6379: { replica_of: '10.10.10.10 6379' } } } # new node vars: { redis_cluster: redis-ms ,redis_password: 'redis.ms' ,redis_max_memory: 64MB } Then deploy only the new node:\n./redis.yml -l 10.10.10.11 # deploy only the new node Scale Up Native Cluster Adding new nodes to a Redis native cluster requires additional steps:\n# 1. Add the new node definition in the inventory # 2. Deploy the new node ./redis.yml -l 10.10.10.14 # 3. Add the new node to the cluster (manual execution) redis-cli --cluster add-node 10.10.10.14:6379 10.10.10.12:6379 # 4. Reshard slots if needed redis-cli --cluster reshard 10.10.10.12:6379 Scale Up Sentinel Cluster To add new instances to a Sentinel cluster:\n# Add new sentinel instances in the inventory, then execute: ./redis.yml -l \u003csentinel-cluster\u003e -t redis_instance Scale Down Redis Nodes Scale Down Standalone Cluster # 1. If removing a replica, just remove it directly ./redis-rm.yml -l 10.10.10.11 -e redis_port=6379 # 2. If removing the primary, first perform a failover redis-cli -h 10.10.10.10 -p 6380 REPLICAOF NO ONE # promote replica redis-cli -h 10.10.10.10 -p 6379 REPLICAOF 10.10.10.10 6380 # demote original primary # 3. Then remove the original primary ./redis-rm.yml -l 10.10.10.10 -e redis_port=6379 # 4. Update the inventory to remove the definition Scale Down Native Cluster # 1. First migrate data slots redis-cli --cluster reshard 10.10.10.12:6379 \\ --cluster-from \u003cnode-id\u003e --cluster-to \u003ctarget-node-id\u003e --cluster-slots \u003ccount\u003e # 2. Remove node from cluster redis-cli --cluster del-node 10.10.10.12:6379 \u003cnode-id\u003e # 3. Remove the instance ./redis-rm.yml -l 10.10.10.14 # 4. Update the inventory Backup and Restore Manual Backup # Trigger RDB snapshot redis-cli -h 10.10.10.10 -p 6379 -a \u003cpassword\u003e BGSAVE # Check snapshot status redis-cli -h 10.10.10.10 -p 6379 -a \u003cpassword\u003e LASTSAVE # Copy RDB file (default location) cp /data/redis/redis-ms-1-6379/dump.rdb /backup/redis-ms-$(date +%Y%m%d).rdb Data Restore # 1. Stop Redis instance sudo systemctl stop redis-ms-1-6379 # 2. Replace RDB file cp /backup/redis-ms-20241231.rdb /data/redis/redis-ms-1-6379/dump.rdb chown redis:redis /data/redis/redis-ms-1-6379/dump.rdb # 3. Start Redis instance sudo systemctl start redis-ms-1-6379 Using AOF Persistence If you need higher data safety, enable AOF:\nredis-ms: vars: redis_aof_enabled: true redis_rdb_save: ['900 1', '300 10', '60 10000'] # keep RDB as well Redeploy to apply AOF configuration:\n./redis.yml -l redis-ms -t redis_config,redis_launch Common Issue Diagnosis Connection Troubleshooting # Check Redis service status systemctl status redis-ms-1-6379 # Check port listening ss -tlnp | grep 6379 # Check firewall sudo iptables -L -n | grep 6379 # Test connection redis-cli -h 10.10.10.10 -p 6379 PING Memory Troubleshooting # Check memory usage redis-cli -h 10.10.10.10 -p 6379 INFO memory # Find big keys redis-cli -h 10.10.10.10 -p 6379 --bigkeys # Memory analysis report redis-cli -h 10.10.10.10 -p 6379 MEMORY DOCTOR Performance Troubleshooting # Check slow query log redis-cli -h 10.10.10.10 -p 6379 SLOWLOG GET 10 # Real-time command monitoring redis-cli -h 10.10.10.10 -p 6379 MONITOR # Check client connections redis-cli -h 10.10.10.10 -p 6379 CLIENT LIST Replication Troubleshooting # Check replication status redis-cli -h 10.10.10.10 -p 6379 INFO replication # Check replication lag redis-cli -h 10.10.10.10 -p 6380 INFO replication | grep lag Performance Tuning Memory Optimization redis-cache: vars: redis_max_memory: 4GB # set based on available memory redis_mem_policy: allkeys-lru # LRU recommended for cache scenarios redis_conf: redis.conf Persistence Optimization # Pure cache scenario: disable persistence redis-cache: vars: redis_rdb_save: [] # disable RDB redis_aof_enabled: false # disable AOF # Data safety scenario: enable both RDB and AOF redis-data: vars: redis_rdb_save: ['900 1', '300 10', '60 10000'] redis_aof_enabled: true Connection Pool Recommendations When connecting to Redis from client applications:\nUse connection pooling to avoid frequent connection creation Set reasonable timeout values (recommended 1-3 seconds) Enable TCP keepalive For high-concurrency scenarios, consider using Pipeline for batch operations Key Monitoring Metrics Monitor these metrics through Grafana dashboards:\nMemory usage: Pay attention when redis:ins:mem_usage \u003e 80% CPU usage: Pay attention when redis:ins:cpu_usage \u003e 70% QPS: Watch for spikes and abnormal fluctuations Response time: Investigate when redis:ins:rt \u003e 1ms Connection count: Monitor connection growth trends Replication lag: Important for master-slave replication scenarios ","categories":["Task"],"description":"Redis cluster management SOPs for creating, destroying, scaling, and configuring high availability","excerpt":"Redis cluster management SOPs for creating, destroying, scaling, and …","ref":"/docs/redis/admin/","tags":"","title":"Administration"},{"body":" Dashboards The REDIS module provides 3 monitoring dashboards:\nRedis Overview: Overview of all Redis clusters Redis Cluster: Details of a single Redis cluster Redis Instance: Details of a single Redis instance Monitoring Pigsty provides three monitoring dashboards for the REDIS module:\nRedis Overview Redis Overview: Overview of all Redis clusters/instances\nRedis Cluster Redis Cluster: Details of a single Redis cluster\nRedis Cluster Dashboard Redis Instance Redis Instance: Details of a single Redis instance\nRedis Instance Dashboard Alert Rules Pigsty provides the following six predefined alert rules for Redis, defined in files/victoria/rules/redis.yml:\nRedisDown: Redis instance is down RedisRejectConn: Redis instance rejecting connections RedisRTHigh: Redis instance response time is too high RedisCPUHigh: Redis instance CPU usage is too high RedisMemHigh: Redis instance memory usage is too high RedisQPSHigh: Redis instance QPS is too high #==============================================================# # Error # #==============================================================# # redis down triggers a P0 alert - alert: RedisDown expr: redis_up \u003c 1 for: 1m labels: { level: 0, severity: CRIT, category: redis } annotations: summary: \"CRIT RedisDown: {{ $labels.ins }} {{ $labels.instance }} {{ $value }}\" description: | redis_up[ins={{ $labels.ins }}, instance={{ $labels.instance }}] = {{ $value }} == 0 http://g.pigsty/d/redis-instance?from=now-5m\u0026to=now\u0026var-ins={{$labels.ins}} # redis reject connection in last 5m - alert: RedisRejectConn expr: redis:ins:conn_reject \u003e 0 labels: { level: 0, severity: CRIT, category: redis } annotations: summary: \"CRIT RedisRejectConn: {{ $labels.ins }} {{ $labels.instance }} {{ $value }}\" description: | redis:ins:conn_reject[cls={{ $labels.cls }}, ins={{ $labels.ins }}][5m] = {{ $value }} \u003e 0 http://g.pigsty/d/redis-instance?from=now-10m\u0026to=now\u0026viewPanel=88\u0026fullscreen\u0026var-ins={{ $labels.ins }} #==============================================================# # Latency # #==============================================================# # redis avg query response time \u003e 160 µs - alert: RedisRTHigh expr: redis:ins:rt \u003e 0.00016 for: 1m labels: { level: 1, severity: WARN, category: redis } annotations: summary: \"WARN RedisRTHigh: {{ $labels.cls }} {{ $labels.ins }}\" description: | pg:ins:query_rt[cls={{ $labels.cls }}, ins={{ $labels.ins }}] = {{ $value }} \u003e 160µs http://g.pigsty/d/redis-instance?from=now-10m\u0026to=now\u0026viewPanel=97\u0026fullscreen\u0026var-ins={{ $labels.ins }} #==============================================================# # Saturation # #==============================================================# # redis cpu usage more than 70% for 1m - alert: RedisCPUHigh expr: redis:ins:cpu_usage \u003e 0.70 for: 1m labels: { level: 1, severity: WARN, category: redis } annotations: summary: \"WARN RedisCPUHigh: {{ $labels.cls }} {{ $labels.ins }}\" description: | redis:ins:cpu_all[cls={{ $labels.cls }}, ins={{ $labels.ins }}] = {{ $value }} \u003e 60% http://g.pigsty/d/redis-instance?from=now-10m\u0026to=now\u0026viewPanel=43\u0026fullscreen\u0026var-ins={{ $labels.ins }} # redis mem usage more than 70% for 1m - alert: RedisMemHigh expr: redis:ins:mem_usage \u003e 0.70 for: 1m labels: { level: 1, severity: WARN, category: redis } annotations: summary: \"WARN RedisMemHigh: {{ $labels.cls }} {{ $labels.ins }}\" description: | redis:ins:mem_usage[cls={{ $labels.cls }}, ins={{ $labels.ins }}] = {{ $value }} \u003e 80% http://g.pigsty/d/redis-instance?from=now-10m\u0026to=now\u0026viewPanel=7\u0026fullscreen\u0026var-ins={{ $labels.ins }} #==============================================================# # Traffic # #==============================================================# # redis qps more than 32000 for 5m - alert: RedisQPSHigh expr: redis:ins:qps \u003e 32000 for: 5m labels: { level: 2, severity: INFO, category: redis } annotations: summary: \"INFO RedisQPSHigh: {{ $labels.cls }} {{ $labels.ins }}\" description: | redis:ins:qps[cls={{ $labels.cls }}, ins={{ $labels.ins }}] = {{ $value }} \u003e 16000 http://g.pigsty/d/redis-instance?from=now-10m\u0026to=now\u0026viewPanel=96\u0026fullscreen\u0026var-ins={{ $labels.ins }} ","categories":["Reference"],"description":"How to monitor Redis? What alert rules are worth paying attention to?","excerpt":"How to monitor Redis? What alert rules are worth paying attention to?","ref":"/docs/redis/monitor/","tags":"","title":"Monitoring"},{"body":"The REDIS module contains 275 available monitoring metrics.\nMetric Name Type Labels Description ALERTS Unknown cls, ip, level, severity, instance, category, ins, alertname, job, alertstate N/A ALERTS_FOR_STATE Unknown cls, ip, level, severity, instance, category, ins, alertname, job N/A redis:cls:aof_rewrite_time Unknown cls, job N/A redis:cls:blocked_clients Unknown cls, job N/A redis:cls:clients Unknown cls, job N/A redis:cls:cmd_qps Unknown cls, cmd, job N/A redis:cls:cmd_rt Unknown cls, cmd, job N/A redis:cls:cmd_time Unknown cls, cmd, job N/A redis:cls:conn_rate Unknown cls, job N/A redis:cls:conn_reject Unknown cls, job N/A redis:cls:cpu_sys Unknown cls, job N/A redis:cls:cpu_sys_child Unknown cls, job N/A redis:cls:cpu_usage Unknown cls, job N/A redis:cls:cpu_usage_child Unknown cls, job N/A redis:cls:cpu_user Unknown cls, job N/A redis:cls:cpu_user_child Unknown cls, job N/A redis:cls:fork_time Unknown cls, job N/A redis:cls:key_evict Unknown cls, job N/A redis:cls:key_expire Unknown cls, job N/A redis:cls:key_hit Unknown cls, job N/A redis:cls:key_hit_rate Unknown cls, job N/A redis:cls:key_miss Unknown cls, job N/A redis:cls:mem_max Unknown cls, job N/A redis:cls:mem_usage Unknown cls, job N/A redis:cls:mem_usage_max Unknown cls, job N/A redis:cls:mem_used Unknown cls, job N/A redis:cls:net_traffic Unknown cls, job N/A redis:cls:qps Unknown cls, job N/A redis:cls:qps_mu Unknown cls, job N/A redis:cls:qps_realtime Unknown cls, job N/A redis:cls:qps_sigma Unknown cls, job N/A redis:cls:rt Unknown cls, job N/A redis:cls:rt_mu Unknown cls, job N/A redis:cls:rt_sigma Unknown cls, job N/A redis:cls:rx Unknown cls, job N/A redis:cls:size Unknown cls, job N/A redis:cls:tx Unknown cls, job N/A redis:env:blocked_clients Unknown job N/A redis:env:clients Unknown job N/A redis:env:cmd_qps Unknown cmd, job N/A redis:env:cmd_rt Unknown cmd, job N/A redis:env:cmd_time Unknown cmd, job N/A redis:env:conn_rate Unknown job N/A redis:env:conn_reject Unknown job N/A redis:env:cpu_usage Unknown job N/A redis:env:cpu_usage_child Unknown job N/A redis:env:key_evict Unknown job N/A redis:env:key_expire Unknown job N/A redis:env:key_hit Unknown job N/A redis:env:key_hit_rate Unknown job N/A redis:env:key_miss Unknown job N/A redis:env:mem_usage Unknown job N/A redis:env:net_traffic Unknown job N/A redis:env:qps Unknown job N/A redis:env:qps_mu Unknown job N/A redis:env:qps_realtime Unknown job N/A redis:env:qps_sigma Unknown job N/A redis:env:rt Unknown job N/A redis:env:rt_mu Unknown job N/A redis:env:rt_sigma Unknown job N/A redis:env:rx Unknown job N/A redis:env:tx Unknown job N/A redis:ins Unknown cls, id, instance, ins, job N/A redis:ins:blocked_clients Unknown cls, ip, instance, ins, job N/A redis:ins:clients Unknown cls, ip, instance, ins, job N/A redis:ins:cmd_qps Unknown cls, cmd, ip, instance, ins, job N/A redis:ins:cmd_rt Unknown cls, cmd, ip, instance, ins, job N/A redis:ins:cmd_time Unknown cls, cmd, ip, instance, ins, job N/A redis:ins:conn_rate Unknown cls, ip, instance, ins, job N/A redis:ins:conn_reject Unknown cls, ip, instance, ins, job N/A redis:ins:cpu_sys Unknown cls, ip, instance, ins, job N/A redis:ins:cpu_sys_child Unknown cls, ip, instance, ins, job N/A redis:ins:cpu_usage Unknown cls, ip, instance, ins, job N/A redis:ins:cpu_usage_child Unknown cls, ip, instance, ins, job N/A redis:ins:cpu_user Unknown cls, ip, instance, ins, job N/A redis:ins:cpu_user_child Unknown cls, ip, instance, ins, job N/A redis:ins:key_evict Unknown cls, ip, instance, ins, job N/A redis:ins:key_expire Unknown cls, ip, instance, ins, job N/A redis:ins:key_hit Unknown cls, ip, instance, ins, job N/A redis:ins:key_hit_rate Unknown cls, ip, instance, ins, job N/A redis:ins:key_miss Unknown cls, ip, instance, ins, job N/A redis:ins:lsn_rate Unknown cls, ip, instance, ins, job N/A redis:ins:mem_usage Unknown cls, ip, instance, ins, job N/A redis:ins:net_traffic Unknown cls, ip, instance, ins, job N/A redis:ins:qps Unknown cls, ip, instance, ins, job N/A redis:ins:qps_mu Unknown cls, ip, instance, ins, job N/A redis:ins:qps_realtime Unknown cls, ip, instance, ins, job N/A redis:ins:qps_sigma Unknown cls, ip, instance, ins, job N/A redis:ins:rt Unknown cls, ip, instance, ins, job N/A redis:ins:rt_mu Unknown cls, ip, instance, ins, job N/A redis:ins:rt_sigma Unknown cls, ip, instance, ins, job N/A redis:ins:rx Unknown cls, ip, instance, ins, job N/A redis:ins:tx Unknown cls, ip, instance, ins, job N/A redis:node:ip Unknown cls, ip, instance, ins, job N/A redis:node:mem_alloc Unknown cls, ip, job N/A redis:node:mem_total Unknown cls, ip, job N/A redis:node:mem_used Unknown cls, ip, job N/A redis:node:qps Unknown cls, ip, job N/A redis_active_defrag_running gauge cls, ip, instance, ins, job active_defrag_running metric redis_allocator_active_bytes gauge cls, ip, instance, ins, job allocator_active_bytes metric redis_allocator_allocated_bytes gauge cls, ip, instance, ins, job allocator_allocated_bytes metric redis_allocator_frag_bytes gauge cls, ip, instance, ins, job allocator_frag_bytes metric redis_allocator_frag_ratio gauge cls, ip, instance, ins, job allocator_frag_ratio metric redis_allocator_resident_bytes gauge cls, ip, instance, ins, job allocator_resident_bytes metric redis_allocator_rss_bytes gauge cls, ip, instance, ins, job allocator_rss_bytes metric redis_allocator_rss_ratio gauge cls, ip, instance, ins, job allocator_rss_ratio metric redis_aof_current_rewrite_duration_sec gauge cls, ip, instance, ins, job aof_current_rewrite_duration_sec metric redis_aof_enabled gauge cls, ip, instance, ins, job aof_enabled metric redis_aof_last_bgrewrite_status gauge cls, ip, instance, ins, job aof_last_bgrewrite_status metric redis_aof_last_cow_size_bytes gauge cls, ip, instance, ins, job aof_last_cow_size_bytes metric redis_aof_last_rewrite_duration_sec gauge cls, ip, instance, ins, job aof_last_rewrite_duration_sec metric redis_aof_last_write_status gauge cls, ip, instance, ins, job aof_last_write_status metric redis_aof_rewrite_in_progress gauge cls, ip, instance, ins, job aof_rewrite_in_progress metric redis_aof_rewrite_scheduled gauge cls, ip, instance, ins, job aof_rewrite_scheduled metric redis_blocked_clients gauge cls, ip, instance, ins, job blocked_clients metric redis_client_recent_max_input_buffer_bytes gauge cls, ip, instance, ins, job client_recent_max_input_buffer_bytes metric redis_client_recent_max_output_buffer_bytes gauge cls, ip, instance, ins, job client_recent_max_output_buffer_bytes metric redis_clients_in_timeout_table gauge cls, ip, instance, ins, job clients_in_timeout_table metric redis_cluster_connections gauge cls, ip, instance, ins, job cluster_connections metric redis_cluster_current_epoch gauge cls, ip, instance, ins, job cluster_current_epoch metric redis_cluster_enabled gauge cls, ip, instance, ins, job cluster_enabled metric redis_cluster_known_nodes gauge cls, ip, instance, ins, job cluster_known_nodes metric redis_cluster_messages_received_total gauge cls, ip, instance, ins, job cluster_messages_received_total metric redis_cluster_messages_sent_total gauge cls, ip, instance, ins, job cluster_messages_sent_total metric redis_cluster_my_epoch gauge cls, ip, instance, ins, job cluster_my_epoch metric redis_cluster_size gauge cls, ip, instance, ins, job cluster_size metric redis_cluster_slots_assigned gauge cls, ip, instance, ins, job cluster_slots_assigned metric redis_cluster_slots_fail gauge cls, ip, instance, ins, job cluster_slots_fail metric redis_cluster_slots_ok gauge cls, ip, instance, ins, job cluster_slots_ok metric redis_cluster_slots_pfail gauge cls, ip, instance, ins, job cluster_slots_pfail metric redis_cluster_state gauge cls, ip, instance, ins, job cluster_state metric redis_cluster_stats_messages_meet_received gauge cls, ip, instance, ins, job cluster_stats_messages_meet_received metric redis_cluster_stats_messages_meet_sent gauge cls, ip, instance, ins, job cluster_stats_messages_meet_sent metric redis_cluster_stats_messages_ping_received gauge cls, ip, instance, ins, job cluster_stats_messages_ping_received metric redis_cluster_stats_messages_ping_sent gauge cls, ip, instance, ins, job cluster_stats_messages_ping_sent metric redis_cluster_stats_messages_pong_received gauge cls, ip, instance, ins, job cluster_stats_messages_pong_received metric redis_cluster_stats_messages_pong_sent gauge cls, ip, instance, ins, job cluster_stats_messages_pong_sent metric redis_commands_duration_seconds_total counter cls, cmd, ip, instance, ins, job Total amount of time in seconds spent per command redis_commands_failed_calls_total counter cls, cmd, ip, instance, ins, job Total number of errors prior command execution per command redis_commands_latencies_usec_bucket Unknown cls, cmd, ip, le, instance, ins, job N/A redis_commands_latencies_usec_count Unknown cls, cmd, ip, instance, ins, job N/A redis_commands_latencies_usec_sum Unknown cls, cmd, ip, instance, ins, job N/A redis_commands_processed_total counter cls, ip, instance, ins, job commands_processed_total metric redis_commands_rejected_calls_total counter cls, cmd, ip, instance, ins, job Total number of errors within command execution per command redis_commands_total counter cls, cmd, ip, instance, ins, job Total number of calls per command redis_config_io_threads gauge cls, ip, instance, ins, job config_io_threads metric redis_config_maxclients gauge cls, ip, instance, ins, job config_maxclients metric redis_config_maxmemory gauge cls, ip, instance, ins, job config_maxmemory metric redis_connected_clients gauge cls, ip, instance, ins, job connected_clients metric redis_connected_slave_lag_seconds gauge cls, ip, slave_ip, instance, slave_state, ins, slave_port, job Lag of connected slave redis_connected_slave_offset_bytes gauge cls, ip, slave_ip, instance, slave_state, ins, slave_port, job Offset of connected slave redis_connected_slaves gauge cls, ip, instance, ins, job connected_slaves metric redis_connections_received_total counter cls, ip, instance, ins, job connections_received_total metric redis_cpu_sys_children_seconds_total counter cls, ip, instance, ins, job cpu_sys_children_seconds_total metric redis_cpu_sys_main_thread_seconds_total counter cls, ip, instance, ins, job cpu_sys_main_thread_seconds_total metric redis_cpu_sys_seconds_total counter cls, ip, instance, ins, job cpu_sys_seconds_total metric redis_cpu_user_children_seconds_total counter cls, ip, instance, ins, job cpu_user_children_seconds_total metric redis_cpu_user_main_thread_seconds_total counter cls, ip, instance, ins, job cpu_user_main_thread_seconds_total metric redis_cpu_user_seconds_total counter cls, ip, instance, ins, job cpu_user_seconds_total metric redis_db_keys gauge cls, ip, instance, ins, db, job Total number of keys by DB redis_db_keys_expiring gauge cls, ip, instance, ins, db, job Total number of expiring keys by DB redis_defrag_hits gauge cls, ip, instance, ins, job defrag_hits metric redis_defrag_key_hits gauge cls, ip, instance, ins, job defrag_key_hits metric redis_defrag_key_misses gauge cls, ip, instance, ins, job defrag_key_misses metric redis_defrag_misses gauge cls, ip, instance, ins, job defrag_misses metric redis_dump_payload_sanitizations counter cls, ip, instance, ins, job dump_payload_sanitizations metric redis_errors_total counter cls, ip, err, instance, ins, job Total number of errors per error type redis_evicted_keys_total counter cls, ip, instance, ins, job evicted_keys_total metric redis_expired_keys_total counter cls, ip, instance, ins, job expired_keys_total metric redis_expired_stale_percentage gauge cls, ip, instance, ins, job expired_stale_percentage metric redis_expired_time_cap_reached_total gauge cls, ip, instance, ins, job expired_time_cap_reached_total metric redis_exporter_build_info gauge cls, golang_version, ip, commit_sha, instance, version, ins, job, build_date redis exporter build_info redis_exporter_last_scrape_connect_time_seconds gauge cls, ip, instance, ins, job exporter_last_scrape_connect_time_seconds metric redis_exporter_last_scrape_duration_seconds gauge cls, ip, instance, ins, job exporter_last_scrape_duration_seconds metric redis_exporter_last_scrape_error gauge cls, ip, instance, ins, job The last scrape error status. redis_exporter_scrape_duration_seconds_count Unknown cls, ip, instance, ins, job N/A redis_exporter_scrape_duration_seconds_sum Unknown cls, ip, instance, ins, job N/A redis_exporter_scrapes_total counter cls, ip, instance, ins, job Current total redis scrapes. redis_instance_info gauge cls, ip, os, role, instance, run_id, redis_version, tcp_port, process_id, ins, redis_mode, maxmemory_policy, redis_build_id, job Information about the Redis instance redis_io_threaded_reads_processed counter cls, ip, instance, ins, job io_threaded_reads_processed metric redis_io_threaded_writes_processed counter cls, ip, instance, ins, job io_threaded_writes_processed metric redis_io_threads_active gauge cls, ip, instance, ins, job io_threads_active metric redis_keyspace_hits_total counter cls, ip, instance, ins, job keyspace_hits_total metric redis_keyspace_misses_total counter cls, ip, instance, ins, job keyspace_misses_total metric redis_last_key_groups_scrape_duration_milliseconds gauge cls, ip, instance, ins, job Duration of the last key group metrics scrape in milliseconds redis_last_slow_execution_duration_seconds gauge cls, ip, instance, ins, job The amount of time needed for last slow execution, in seconds redis_latency_percentiles_usec summary cls, cmd, ip, instance, quantile, ins, job A summary of latency percentile distribution per command redis_latency_percentiles_usec_count Unknown cls, cmd, ip, instance, ins, job N/A redis_latency_percentiles_usec_sum Unknown cls, cmd, ip, instance, ins, job N/A redis_latest_fork_seconds gauge cls, ip, instance, ins, job latest_fork_seconds metric redis_lazyfree_pending_objects gauge cls, ip, instance, ins, job lazyfree_pending_objects metric redis_loading_dump_file gauge cls, ip, instance, ins, job loading_dump_file metric redis_master_last_io_seconds_ago gauge cls, ip, master_host, instance, ins, job, master_port Master last io seconds ago redis_master_link_up gauge cls, ip, master_host, instance, ins, job, master_port Master link status on Redis slave redis_master_repl_offset gauge cls, ip, instance, ins, job master_repl_offset metric redis_master_sync_in_progress gauge cls, ip, master_host, instance, ins, job, master_port Master sync in progress redis_mem_clients_normal gauge cls, ip, instance, ins, job mem_clients_normal metric redis_mem_clients_slaves gauge cls, ip, instance, ins, job mem_clients_slaves metric redis_mem_fragmentation_bytes gauge cls, ip, instance, ins, job mem_fragmentation_bytes metric redis_mem_fragmentation_ratio gauge cls, ip, instance, ins, job mem_fragmentation_ratio metric redis_mem_not_counted_for_eviction_bytes gauge cls, ip, instance, ins, job mem_not_counted_for_eviction_bytes metric redis_memory_max_bytes gauge cls, ip, instance, ins, job memory_max_bytes metric redis_memory_used_bytes gauge cls, ip, instance, ins, job memory_used_bytes metric redis_memory_used_dataset_bytes gauge cls, ip, instance, ins, job memory_used_dataset_bytes metric redis_memory_used_lua_bytes gauge cls, ip, instance, ins, job memory_used_lua_bytes metric redis_memory_used_overhead_bytes gauge cls, ip, instance, ins, job memory_used_overhead_bytes metric redis_memory_used_peak_bytes gauge cls, ip, instance, ins, job memory_used_peak_bytes metric redis_memory_used_rss_bytes gauge cls, ip, instance, ins, job memory_used_rss_bytes metric redis_memory_used_scripts_bytes gauge cls, ip, instance, ins, job memory_used_scripts_bytes metric redis_memory_used_startup_bytes gauge cls, ip, instance, ins, job memory_used_startup_bytes metric redis_migrate_cached_sockets_total gauge cls, ip, instance, ins, job migrate_cached_sockets_total metric redis_module_fork_in_progress gauge cls, ip, instance, ins, job module_fork_in_progress metric redis_module_fork_last_cow_size gauge cls, ip, instance, ins, job module_fork_last_cow_size metric redis_net_input_bytes_total counter cls, ip, instance, ins, job net_input_bytes_total metric redis_net_output_bytes_total counter cls, ip, instance, ins, job net_output_bytes_total metric redis_number_of_cached_scripts gauge cls, ip, instance, ins, job number_of_cached_scripts metric redis_process_id gauge cls, ip, instance, ins, job process_id metric redis_pubsub_channels gauge cls, ip, instance, ins, job pubsub_channels metric redis_pubsub_patterns gauge cls, ip, instance, ins, job pubsub_patterns metric redis_pubsubshard_channels gauge cls, ip, instance, ins, job pubsubshard_channels metric redis_rdb_bgsave_in_progress gauge cls, ip, instance, ins, job rdb_bgsave_in_progress metric redis_rdb_changes_since_last_save gauge cls, ip, instance, ins, job rdb_changes_since_last_save metric redis_rdb_current_bgsave_duration_sec gauge cls, ip, instance, ins, job rdb_current_bgsave_duration_sec metric redis_rdb_last_bgsave_duration_sec gauge cls, ip, instance, ins, job rdb_last_bgsave_duration_sec metric redis_rdb_last_bgsave_status gauge cls, ip, instance, ins, job rdb_last_bgsave_status metric redis_rdb_last_cow_size_bytes gauge cls, ip, instance, ins, job rdb_last_cow_size_bytes metric redis_rdb_last_save_timestamp_seconds gauge cls, ip, instance, ins, job rdb_last_save_timestamp_seconds metric redis_rejected_connections_total counter cls, ip, instance, ins, job rejected_connections_total metric redis_repl_backlog_first_byte_offset gauge cls, ip, instance, ins, job repl_backlog_first_byte_offset metric redis_repl_backlog_history_bytes gauge cls, ip, instance, ins, job repl_backlog_history_bytes metric redis_repl_backlog_is_active gauge cls, ip, instance, ins, job repl_backlog_is_active metric redis_replica_partial_resync_accepted gauge cls, ip, instance, ins, job replica_partial_resync_accepted metric redis_replica_partial_resync_denied gauge cls, ip, instance, ins, job replica_partial_resync_denied metric redis_replica_resyncs_full gauge cls, ip, instance, ins, job replica_resyncs_full metric redis_replication_backlog_bytes gauge cls, ip, instance, ins, job replication_backlog_bytes metric redis_second_repl_offset gauge cls, ip, instance, ins, job second_repl_offset metric redis_sentinel_master_ckquorum_status gauge cls, ip, message, instance, ins, master_name, job Master ckquorum status redis_sentinel_master_ok_sentinels gauge cls, ip, instance, ins, master_address, master_name, job The number of okay sentinels monitoring this master redis_sentinel_master_ok_slaves gauge cls, ip, instance, ins, master_address, master_name, job The number of okay slaves of the master redis_sentinel_master_sentinels gauge cls, ip, instance, ins, master_address, master_name, job The number of sentinels monitoring this master redis_sentinel_master_setting_ckquorum gauge cls, ip, instance, ins, master_address, master_name, job Show the current ckquorum config for each master redis_sentinel_master_setting_down_after_milliseconds gauge cls, ip, instance, ins, master_address, master_name, job Show the current down-after-milliseconds config for each master redis_sentinel_master_setting_failover_timeout gauge cls, ip, instance, ins, master_address, master_name, job Show the current failover-timeout config for each master redis_sentinel_master_setting_parallel_syncs gauge cls, ip, instance, ins, master_address, master_name, job Show the current parallel-syncs config for each master redis_sentinel_master_slaves gauge cls, ip, instance, ins, master_address, master_name, job The number of slaves of the master redis_sentinel_master_status gauge cls, ip, master_status, instance, ins, master_address, master_name, job Master status on Sentinel redis_sentinel_masters gauge cls, ip, instance, ins, job The number of masters this sentinel is watching redis_sentinel_running_scripts gauge cls, ip, instance, ins, job Number of scripts in execution right now redis_sentinel_scripts_queue_length gauge cls, ip, instance, ins, job Queue of user scripts to execute redis_sentinel_simulate_failure_flags gauge cls, ip, instance, ins, job Failures simulations redis_sentinel_tilt gauge cls, ip, instance, ins, job Sentinel is in TILT mode redis_slave_expires_tracked_keys gauge cls, ip, instance, ins, job slave_expires_tracked_keys metric redis_slave_info gauge cls, ip, master_host, instance, read_only, ins, job, master_port Information about the Redis slave redis_slave_priority gauge cls, ip, instance, ins, job slave_priority metric redis_slave_repl_offset gauge cls, ip, master_host, instance, ins, job, master_port Slave replication offset redis_slowlog_last_id gauge cls, ip, instance, ins, job Last id of slowlog redis_slowlog_length gauge cls, ip, instance, ins, job Total slowlog redis_start_time_seconds gauge cls, ip, instance, ins, job Start time of the Redis instance since unix epoch in seconds. redis_target_scrape_request_errors_total counter cls, ip, instance, ins, job Errors in requests to the exporter redis_total_error_replies counter cls, ip, instance, ins, job total_error_replies metric redis_total_reads_processed counter cls, ip, instance, ins, job total_reads_processed metric redis_total_system_memory_bytes gauge cls, ip, instance, ins, job total_system_memory_bytes metric redis_total_writes_processed counter cls, ip, instance, ins, job total_writes_processed metric redis_tracking_clients gauge cls, ip, instance, ins, job tracking_clients metric redis_tracking_total_items gauge cls, ip, instance, ins, job tracking_total_items metric redis_tracking_total_keys gauge cls, ip, instance, ins, job tracking_total_keys metric redis_tracking_total_prefixes gauge cls, ip, instance, ins, job tracking_total_prefixes metric redis_unexpected_error_replies counter cls, ip, instance, ins, job unexpected_error_replies metric redis_up gauge cls, ip, instance, ins, job Information about the Redis instance redis_uptime_in_seconds gauge cls, ip, instance, ins, job uptime_in_seconds metric scrape_duration_seconds Unknown cls, ip, instance, ins, job N/A scrape_samples_post_metric_relabeling Unknown cls, ip, instance, ins, job N/A scrape_samples_scraped Unknown cls, ip, instance, ins, job N/A scrape_series_added Unknown cls, ip, instance, ins, job N/A up Unknown cls, ip, instance, ins, job N/A ","categories":["Reference"],"description":"Complete list of monitoring metrics provided by the Pigsty REDIS module with explanations","excerpt":"Complete list of monitoring metrics provided by the Pigsty REDIS …","ref":"/docs/redis/metric/","tags":"","title":"Metrics"},{"body":" ABORT due to redis_safeguard enabled This means the Redis instance you are trying to remove has the safeguard enabled: this happens when attempting to remove a Redis instance with redis_safeguard set to true. The redis-rm.yml playbook refuses to execute to prevent accidental deletion of running Redis instances.\nYou can override this protection with the CLI argument -e redis_safeguard=false to force removal of the Redis instance. This is what redis_safeguard is designed for.\nHow to add a new Redis instance on a node? Use bin/redis-add \u003cip\u003e \u003cport\u003e to deploy a new Redis instance on the node.\nHow to remove a specific instance from a node? Use bin/redis-rm \u003cip\u003e \u003cport\u003e to remove a single Redis instance from the node.\nAre there plans to upgrade to Valkey or the latest version? Since Redis is not a core component of this project, there are currently no plans to update to the latest Redis RSAL / AGPLv3 version or Valkey. The Redis version in Pigsty is locked to 7.2.6, the last version using the BSD license.\nThis version has been validated in large-scale production environments, and Pigsty no longer has such scenarios to re-validate the stability and reliability of newer versions.\n","categories":["Reference"],"description":"Frequently asked questions about the Pigsty REDIS module","excerpt":"Frequently asked questions about the Pigsty REDIS module","ref":"/docs/redis/faq/","tags":"","title":"FAQ"},{"body":"You can use PostgreSQL as Grafana’s backend database.\nThis is a great opportunity to understand Pigsty’s deployment system. By completing this tutorial, you’ll learn:\nHow to create a new database cluster How to create new business users in an existing cluster How to create new business databases in an existing cluster How to access databases created by Pigsty How to manage Grafana dashboards How to manage PostgreSQL datasources in Grafana How to upgrade Grafana database in one step TL;DR vi pigsty.yml # Uncomment DB/User definitions: dbuser_grafana grafana bin/pgsql-user pg-meta dbuser_grafana bin/pgsql-db pg-meta grafana psql postgres://dbuser_grafana:DBUser.Grafana@meta:5436/grafana -c \\ 'CREATE TABLE t(); DROP TABLE t;' # Verify connection string works vi /etc/grafana/grafana.ini # Modify [database] type url systemctl restart grafana-server Create Database Cluster We can define a new database grafana on pg-meta, or create a dedicated Grafana database cluster pg-grafana on new nodes.\nDefine Cluster To create a new dedicated cluster pg-grafana on machines 10.10.10.11 and 10.10.10.12, use this config:\npg-grafana: hosts: 10.10.10.11: {pg_seq: 1, pg_role: primary} 10.10.10.12: {pg_seq: 2, pg_role: replica} vars: pg_cluster: pg-grafana pg_databases: - name: grafana owner: dbuser_grafana revokeconn: true comment: grafana primary database pg_users: - name: dbuser_grafana password: DBUser.Grafana pgbouncer: true roles: [dbrole_admin] comment: admin user for grafana database Create Cluster Use this command to create the pg-grafana cluster: pgsql.yml.\n./pgsql.yml -l pg-grafana # Initialize pg-grafana cluster This command is the Ansible Playbook pgsql.yml for creating database clusters.\nUsers and databases defined in pg_users and pg_databases are automatically created during cluster initialization. With this config, after cluster creation (without DNS), you can access the database using these connection strings (any one works):\npostgres://dbuser_grafana:DBUser.Grafana@10.10.10.11:5432/grafana # Direct primary connection postgres://dbuser_grafana:DBUser.Grafana@10.10.10.11:5436/grafana # Direct default service postgres://dbuser_grafana:DBUser.Grafana@10.10.10.11:5433/grafana # Primary read-write service postgres://dbuser_grafana:DBUser.Grafana@10.10.10.12:5432/grafana # Direct primary connection postgres://dbuser_grafana:DBUser.Grafana@10.10.10.12:5436/grafana # Direct default service postgres://dbuser_grafana:DBUser.Grafana@10.10.10.12:5433/grafana # Primary read-write service Since Pigsty is installed on a single meta node by default, the following steps will create Grafana’s user and database on the existing pg-meta cluster, not the pg-grafana cluster created here.\nCreate Grafana Business User The usual convention for business object management: create user first, then database. Because if the database has an owner configured, it depends on the corresponding user.\nDefine User To create user dbuser_grafana on the pg-meta cluster, first add this user definition to pg-meta’s cluster definition:\nLocation: all.children.pg-meta.vars.pg_users\n- name: dbuser_grafana password: DBUser.Grafana comment: admin user for grafana database pgbouncer: true roles: [ dbrole_admin ] If you define a different password here, replace the corresponding parameter in subsequent steps\nCreate User Use this command to create the dbuser_grafana user (either works):\nbin/pgsql-user pg-meta dbuser_grafana # Create `dbuser_grafana` user on pg-meta cluster This actually calls the Ansible Playbook pgsql-user.yml to create the user:\n./pgsql-user.yml -l pg-meta -e pg_user=dbuser_grafana # Ansible The dbrole_admin role has permission to execute DDL changes in the database, which is exactly what Grafana needs.\nCreate Grafana Business Database Define Database Creating a business database follows the same pattern as users. First add the new database grafana definition to pg-meta’s cluster definition.\nLocation: all.children.pg-meta.vars.pg_databases\n- { name: grafana, owner: dbuser_grafana, revokeconn: true } Create Database Use this command to create the grafana database (either works):\nbin/pgsql-db pg-meta grafana # Create `grafana` database on `pg-meta` cluster This actually calls the Ansible Playbook pgsql-db.yml to create the database:\n./pgsql-db.yml -l pg-meta -e pg_database=grafana # Actual Ansible playbook executed Use Grafana Business Database Verify Connection String Reachability You can access the database using different services or access methods, for example:\npostgres://dbuser_grafana:DBUser.Grafana@meta:5432/grafana # Direct connection postgres://dbuser_grafana:DBUser.Grafana@meta:5436/grafana # Default service postgres://dbuser_grafana:DBUser.Grafana@meta:5433/grafana # Primary service Here, we’ll use the Default service that directly accesses the primary through load balancer.\nFirst verify the connection string is reachable and has DDL execution permissions:\npsql postgres://dbuser_grafana:DBUser.Grafana@meta:5436/grafana -c \\ 'CREATE TABLE t(); DROP TABLE t;' Directly Modify Grafana Config To make Grafana use a Postgres datasource, edit /etc/grafana/grafana.ini and modify the config:\n[database] ;type = sqlite3 ;host = 127.0.0.1:3306 ;name = grafana ;user = root # If the password contains # or ; you have to wrap it with triple quotes. Ex \"\"\"#password;\"\"\" ;password = ;url = Change the default config to:\n[database] type = postgres url = postgres://dbuser_grafana:DBUser.Grafana@meta/grafana Then restart Grafana:\nsystemctl restart grafana-server When you see activity in the newly added grafana database from the monitoring system, Grafana is now using Postgres as its primary backend database. But there’s a new issue—the original Dashboards and Datasources in Grafana have disappeared! You need to re-import dashboards and Postgres datasources.\nManage Grafana Dashboards As admin user, navigate to the files/grafana directory under the Pigsty directory and run grafana.py init to reload Pigsty dashboards.\ncd ~/pigsty/files/grafana ./grafana.py init # Initialize Grafana dashboards using Dashboards in current directory Execution result:\nvagrant@meta:~/pigsty/files/grafana $ ./grafana.py init Grafana API: admin:pigsty @ http://10.10.10.10:3000 init dashboard : home.json init folder pgcat init dashboard: pgcat / pgcat-table.json init dashboard: pgcat / pgcat-bloat.json init dashboard: pgcat / pgcat-query.json init folder pgsql init dashboard: pgsql / pgsql-replication.json ... This script detects the current environment (defined in ~/pigsty during installation), gets Grafana access info, and replaces dashboard URL placeholder domains (*.pigsty) with actual domains used.\nexport GRAFANA_ENDPOINT=http://10.10.10.10:3000 export GRAFANA_USERNAME=admin export GRAFANA_PASSWORD=pigsty export NGINX_UPSTREAM_YUMREPO=yum.pigsty export NGINX_UPSTREAM_CONSUL=c.pigsty export NGINX_UPSTREAM_PROMETHEUS=p.pigsty export NGINX_UPSTREAM_ALERTMANAGER=a.pigsty export NGINX_UPSTREAM_GRAFANA=g.pigsty export NGINX_UPSTREAM_HAPROXY=h.pigsty As a side note, use grafana.py clean to clear target dashboards, and grafana.py load to load all dashboards from the current directory. When Pigsty dashboards change, use these two commands to upgrade all dashboards.\nManage Postgres Datasources When creating a new PostgreSQL cluster with pgsql.yml or a new business database with pgsql-db.yml, Pigsty registers new PostgreSQL datasources in Grafana. You can directly access target database instances through Grafana using the default monitoring user. Most pgcat application features depend on this.\nTo register Postgres databases, use the register_grafana task in pgsql.yml:\n./pgsql.yml -t register_grafana # Re-register all Postgres datasources in current environment ./pgsql.yml -t register_grafana -l pg-test # Re-register all databases in pg-test cluster One-Step Grafana Upgrade You can directly modify the Pigsty config file to change Grafana’s backend datasource, completing the database switch in one step. Edit the grafana_pgurl parameter in pigsty.yml:\ngrafana_pgurl: postgres://dbuser_grafana:DBUser.Grafana@meta:5436/grafana Then re-run the grafana task from infra.yml to complete the Grafana upgrade:\n./infra.yml -t grafana ","categories":["Task"],"description":"Use PostgreSQL instead of SQLite as Grafana's remote storage backend for better performance and availability.","excerpt":"Use PostgreSQL instead of SQLite as Grafana's remote storage backend …","ref":"/docs/infra/admin/grafana/","tags":"","title":"Grafana High Availability: Using PostgreSQL Backend"},{"body":"FERRET is an optional module in Pigsty for deploying FerretDB — a protocol translation middleware built on the PostgreSQL kernel and the DocumentDB extension. It enables applications using MongoDB drivers to connect and translates those requests into PostgreSQL operations.\nPigsty is a community partner of FerretDB. We have built binary packages for FerretDB and DocumentDB (FerretDB-specific fork), and provide a ready-to-use configuration template mongo.yml to help you easily deploy enterprise-grade FerretDB clusters.\n","categories":["Reference"],"description":"Add MongoDB-compatible protocol support to PostgreSQL using FerretDB","excerpt":"Add MongoDB-compatible protocol support to PostgreSQL using FerretDB","ref":"/docs/ferret/","tags":"","title":"Module: FERRET"},{"body":"This document describes how to install MongoDB client tools and connect to FerretDB.\nInstalling Client Tools You can use MongoDB’s command-line tool MongoSH to access FerretDB.\nUse the pig command to add the MongoDB repository, then install mongosh using yum or apt:\npig repo add mongo -u # Add the official MongoDB repository yum install mongodb-mongosh # RHEL/CentOS/Rocky/Alma apt install mongodb-mongosh # Debian/Ubuntu After installation, you can use the mongosh command to connect to FerretDB.\nConnecting to FerretDB You can access FerretDB using any language’s MongoDB driver via a MongoDB connection string. Here’s an example using the mongosh CLI tool:\n$ mongosh Current Mongosh Log ID:\t67ba8c1fe551f042bf51e943 Connecting to:\tmongodb://127.0.0.1:27017/?directConnection=true\u0026serverSelectionTimeoutMS=2000\u0026appName=mongosh+2.4.0 Using MongoDB:\t7.0.77 Using Mongosh:\t2.4.0 For mongosh info see: https://www.mongodb.com/docs/mongodb-shell/ test\u003e Using Connection Strings FerretDB authentication is entirely based on PostgreSQL. Since Pigsty-managed PostgreSQL clusters use scram-sha-256 authentication by default, you must specify the PLAIN authentication mechanism in the connection string:\nmongosh 'mongodb://dbuser_meta:DBUser.Meta@10.10.10.10:27017?authMechanism=PLAIN' Connection string format:\nmongodb://\u003cusername\u003e:\u003cpassword\u003e@\u003chost\u003e:\u003cport\u003e/\u003cdatabase\u003e?authMechanism=PLAIN Using Different Users You can connect to FerretDB using any user that has been created in PostgreSQL:\n# Using dbuser_dba user mongosh 'mongodb://dbuser_dba:DBUser.DBA@10.10.10.10:27017?authMechanism=PLAIN' # Using mongod superuser mongosh 'mongodb://mongod:DBUser.Mongo@10.10.10.10:27017?authMechanism=PLAIN' # Connecting to a specific database mongosh 'mongodb://test:test@10.10.10.11:27017/test?authMechanism=PLAIN' Basic Operations After connecting to FerretDB, you can operate it just like MongoDB. Here are some basic operation examples:\nDatabase Operations // Switch to / create database use mydb // Show all databases show dbs // Drop current database db.dropDatabase() Collection Operations // Create collection db.createCollection('users') // Show all collections show collections // Drop collection db.users.drop() Document Operations // Insert a single document db.users.insertOne({ name: 'Alice', age: 30, email: 'alice@example.com' }) // Insert multiple documents db.users.insertMany([ { name: 'Bob', age: 25 }, { name: 'Charlie', age: 35 } ]) // Query documents db.users.find() db.users.find({ age: { $gt: 25 } }) db.users.findOne({ name: 'Alice' }) // Update documents db.users.updateOne( { name: 'Alice' }, { $set: { age: 31 } } ) // Delete documents db.users.deleteOne({ name: 'Bob' }) db.users.deleteMany({ age: { $lt: 30 } }) Index Operations // Create indexes db.users.createIndex({ name: 1 }) db.users.createIndex({ age: -1 }) // View indexes db.users.getIndexes() // Drop index db.users.dropIndex('name_1') Differences from MongoDB FerretDB implements MongoDB’s wire protocol but uses PostgreSQL for underlying storage. This means:\nMongoDB commands are translated to SQL statements for execution Most basic operations are compatible with MongoDB Some advanced features may differ or not be supported You can consult the following resources for detailed information:\nFerretDB Supported Commands Differences from MongoDB FerretDB Authentication Programming Language Drivers In addition to the mongosh command-line tool, you can also connect to FerretDB using MongoDB drivers for various programming languages:\nPython from pymongo import MongoClient client = MongoClient('mongodb://dbuser_meta:DBUser.Meta@10.10.10.10:27017/?authMechanism=PLAIN') db = client.test collection = db.users collection.insert_one({'name': 'Alice', 'age': 30}) Node.js const { MongoClient } = require('mongodb'); const uri = 'mongodb://dbuser_meta:DBUser.Meta@10.10.10.10:27017/?authMechanism=PLAIN'; const client = new MongoClient(uri); async function run() { await client.connect(); const db = client.db('test'); const collection = db.collection('users'); await collection.insertOne({ name: 'Alice', age: 30 }); } Go import ( \"go.mongodb.org/mongo-driver/mongo\" \"go.mongodb.org/mongo-driver/mongo/options\" ) uri := \"mongodb://dbuser_meta:DBUser.Meta@10.10.10.10:27017/?authMechanism=PLAIN\" client, err := mongo.Connect(context.TODO(), options.Client().ApplyURI(uri)) Key point: All drivers require the authMechanism=PLAIN parameter in the connection string.\n","categories":["Reference"],"description":"Install client tools, connect to and use FerretDB","excerpt":"Install client tools, connect to and use FerretDB","ref":"/docs/ferret/usage/","tags":"","title":"Usage"},{"body":"Before deploying a FerretDB cluster, you need to define it in the configuration inventory using the relevant parameters.\nFerretDB Cluster The following example uses the default single-node pg-meta cluster’s meta database as FerretDB’s underlying storage:\nall: children: #----------------------------------# # ferretdb for mongodb on postgresql #----------------------------------# # ./mongo.yml -l ferret ferret: hosts: 10.10.10.10: { mongo_seq: 1 } vars: mongo_cluster: ferret mongo_pgurl: 'postgres://mongod:DBUser.Mongo@10.10.10.10:5432/meta' Here, mongo_cluster and mongo_seq are essential identity parameters. For FerretDB, mongo_pgurl is also required to specify the underlying PostgreSQL location.\nNote that the mongo_pgurl parameter requires a PostgreSQL superuser. In this example, a dedicated mongod superuser is defined for FerretDB.\nNote that FerretDB’s authentication is entirely based on PostgreSQL. You can create other regular users using either FerretDB or PostgreSQL.\nPostgreSQL Cluster FerretDB 2.0+ requires an extension: DocumentDB, which depends on several other extensions. Here’s a template for creating a PostgreSQL cluster for FerretDB:\nall: children: #----------------------------------# # pgsql (singleton on current node) #----------------------------------# # postgres cluster: pg-meta pg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta pg_users: - { name: mongod ,password: DBUser.Mongo ,pgbouncer: true ,roles: [dbrole_admin ] ,superuser: true ,comment: ferretdb super user } - { name: dbuser_meta ,password: DBUser.Meta ,pgbouncer: true ,roles: [dbrole_admin] ,comment: pigsty admin user } - { name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [dbrole_readonly] ,comment: read-only viewer for meta database } pg_databases: - {name: meta, owner: mongod ,baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [pigsty] ,extensions: [ documentdb, postgis, vector, pg_cron, rum ]} pg_hba_rules: - { user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes' } - { user: mongod , db: all ,addr: world ,auth: pwd ,title: 'mongodb password access from everywhere' } pg_extensions: - documentdb, citus, postgis, pgvector, pg_cron, rum pg_parameters: cron.database_name: meta pg_libs: 'pg_documentdb, pg_documentdb_core, pg_cron, pg_stat_statements, auto_explain' Key configuration points:\nUser configuration: You need to create a mongod user with superuser privileges for FerretDB to use Database configuration: The database needs to have the documentdb extension and its dependencies installed HBA rules: Allow the mongod user to connect from any address with password authentication Shared libraries: pg_documentdb and pg_documentdb_core need to be preloaded in pg_libs High Availability You can use Services to connect to a highly available PostgreSQL cluster, deploy multiple FerretDB instance replicas, and bind an L2 VIP for the FerretDB layer to achieve high availability.\nferret: hosts: 10.10.10.45: { mongo_seq: 1 } 10.10.10.46: { mongo_seq: 2 } 10.10.10.47: { mongo_seq: 3 } vars: mongo_cluster: ferret mongo_pgurl: 'postgres://mongod:DBUser.Mongo@10.10.10.3:5436/test' vip_enabled: true vip_vrid: 128 vip_address: 10.10.10.99 vip_interface: eth1 In this high-availability configuration:\nMulti-instance deployment: Deploy FerretDB instances on three nodes, with all instances connecting to the same PostgreSQL backend VIP configuration: Use Keepalived to bind the virtual IP 10.10.10.99, enabling failover at the FerretDB layer Service address: Use PostgreSQL’s service address (port 5436 is typically the primary service), ensuring connections go to the correct primary With this configuration, clients can connect to FerretDB through the VIP address. Even if one FerretDB instance fails, the VIP will automatically float to another available instance.\n","categories":["Reference"],"description":"Configure the FerretDB module and define cluster topology","excerpt":"Configure the FerretDB module and define cluster topology","ref":"/docs/ferret/config/","tags":"","title":"Configuration"},{"body":"Parameter Overview The FERRET parameter group is used for FerretDB deployment and configuration, including identity, underlying PostgreSQL connection, listen ports, and SSL settings.\nParameter Type Level Description mongo_seq int I mongo instance number, required identity param mongo_cluster string C mongo cluster name, required identity param mongo_pgurl pgurl C/I PostgreSQL URL for FerretDB backend mongo_ssl_enabled bool C Enable SSL? default is false mongo_listen ip C Listen address, empty listens on all addresses mongo_port port C Service port, default 27017 mongo_ssl_port port C TLS listen port, default 27018 mongo_exporter_port port C Exporter port, default 9216 mongo_extra_vars string C Extra environment variables, empty by default Defaults Default parameters are defined in roles/ferret/defaults/main.yml:\n# mongo_cluster: #CLUSTER # mongo cluster name, required identity param # mongo_seq: 0 #INSTANCE # mongo instance sequence, required identity param # mongo_pgurl: 'postgres:///' # mongo/ferretdb underlying postgresql url, required mongo_ssl_enabled: false # mongo/ferretdb ssl enabled, default false mongo_listen: '' # mongo/ferretdb listen address, '' for all mongo_port: 27017 # mongo/ferretdb listen port, default 27017 mongo_ssl_port: 27018 # mongo/ferretdb tls listen port, default 27018 mongo_exporter_port: 9216 # mongo/ferretdb exporter port, default 9216 mongo_extra_vars: '' # mongo/ferretdb extra environment variables mongo_cluster Parameter: mongo_cluster, Type: string, Level: C\nmongo cluster name, a required identity parameter.\nNo default value—you must explicitly define it for production environments.\nThe cluster name must comply with the regex [a-z][a-z0-9-]*. Using descriptive names is recommended.\nmongo_seq Parameter: mongo_seq, Type: int, Level: I\nmongo instance sequence number, a unique integer identifier within the cluster.\nYou must explicitly define the sequence number for each mongo instance. Integers start from 0 or 1.\nmongo_pgurl Parameter: mongo_pgurl, Type: pgurl, Level: C/I\nPostgreSQL URL for FerretDB backend connection, a required parameter.\nNo default value—you must explicitly define it. This is the PostgreSQL database connection string that FerretDB will use as its backend storage.\nFormat: postgres://username:password@host:port/database\nNotes:\nThe user needs to be a PostgreSQL superuser The target database needs the documentdb extension installed Using a dedicated mongod user is recommended mongo_ssl_enabled Parameter: mongo_ssl_enabled, Type: bool, Level: C\nEnable SSL/TLS encryption for FerretDB.\nDefault is false. Set to true to enable SSL/TLS encryption for mongo connections.\nWhen enabled, FerretDB will:\nGenerate and issue SSL certificates Listen for encrypted connections on mongo_ssl_port mongo_listen Parameter: mongo_listen, Type: ip, Level: C\nListen address for mongo binding.\nDefault is empty string '', meaning listen on all available addresses (0.0.0.0). You can specify a specific IP address to bind to.\nmongo_port Parameter: mongo_port, Type: port, Level: C\nService port for mongo client connections.\nDefault is 27017, which is the standard MongoDB port. Change this port if you need to avoid port conflicts or have security considerations.\nmongo_ssl_port Parameter: mongo_ssl_port, Type: port, Level: C\nTLS listen port for mongo encrypted connections.\nDefault is 27018. When SSL/TLS is enabled via mongo_ssl_enabled, FerretDB will accept encrypted connections on this port.\nmongo_exporter_port Parameter: mongo_exporter_port, Type: port, Level: C\nExporter port for mongo metrics collection.\nDefault is 9216. This port is used by FerretDB’s built-in metrics exporter to expose monitoring metrics to Prometheus.\nmongo_extra_vars Parameter: mongo_extra_vars, Type: string, Level: C\nExtra environment variables for FerretDB server.\nDefault is empty string ''. You can specify additional environment variables to pass to the FerretDB process in KEY=VALUE format, with multiple variables separated by spaces.\nExample:\nmongo_extra_vars: 'FERRETDB_LOG_LEVEL=debug FERRETDB_TELEMETRY=disable' ","categories":["Reference"],"description":"Customize FerretDB with 9 parameters","excerpt":"Customize FerretDB with 9 parameters","ref":"/docs/ferret/param/","tags":"","title":"Parameters"},{"body":"This document describes daily administration operations for FerretDB clusters.\nCreate FerretDB Cluster After defining a FerretDB cluster in the configuration inventory, you can install it with the following command:\n./mongo.yml -l ferret # Install FerretDB on the ferret group Since FerretDB uses PostgreSQL as its underlying storage, running this playbook multiple times is generally safe (idempotent).\nThe FerretDB service is configured to automatically restart on failure (Restart=on-failure), providing basic resilience for this stateless proxy layer.\nRemove FerretDB Cluster To remove a FerretDB cluster, run the mongo_purge subtask of the mongo.yml playbook with the mongo_purge parameter:\n./mongo.yml -l ferret -e mongo_purge=true -t mongo_purge Important: Always use the -l \u003ccluster\u003e parameter to limit the execution scope and avoid accidentally removing other clusters.\nThis command will:\nStop the FerretDB service Remove the systemd service file Clean up configuration files and certificates Deregister from Prometheus monitoring Connect to FerretDB You can access FerretDB using a MongoDB connection string with any language’s MongoDB driver. Here’s an example using the mongosh command-line tool:\nmongosh 'mongodb://dbuser_meta:DBUser.Meta@10.10.10.10:27017?authMechanism=PLAIN' mongosh 'mongodb://test:test@10.10.10.11:27017/test?authMechanism=PLAIN' Pigsty-managed PostgreSQL clusters use scram-sha-256 as the default authentication method, so you must use PLAIN authentication when connecting to FerretDB. See FerretDB: Authentication for details.\nYou can also use other PostgreSQL users to access FerretDB by specifying them in the connection string:\nmongosh 'mongodb://dbuser_dba:DBUser.DBA@10.10.10.10:27017?authMechanism=PLAIN' Quick Start After connecting to FerretDB, you can operate it just like MongoDB:\n$ mongosh 'mongodb://dbuser_meta:DBUser.Meta@10.10.10.10:27017?authMechanism=PLAIN' MongoDB commands are translated to SQL commands and executed in the underlying PostgreSQL:\nuse test // CREATE SCHEMA test; db.dropDatabase() // DROP SCHEMA test; db.createCollection('posts') // CREATE TABLE posts(_data JSONB,...) db.posts.insert({ // INSERT INTO posts VALUES(...); title: 'Post One', body: 'Body of post one', category: 'News', tags: ['news', 'events'], user: {name: 'John Doe', status: 'author'}, date: Date() }) db.posts.find().limit(2).pretty() // SELECT * FROM posts LIMIT 2; db.posts.createIndex({ title: 1 }) // CREATE INDEX ON posts(_data-\u003e\u003e'title'); If you’re not familiar with MongoDB, here’s a quick start tutorial that also applies to FerretDB: Perform CRUD Operations with MongoDB Shell\nBenchmark If you want to generate some sample load, you can use mongosh to execute the following simple test script:\ncat \u003e benchmark.js \u003c\u003c'EOF' const coll = \"testColl\"; const numDocs = 10000; for (let i = 0; i \u003c numDocs; i++) { // insert db.getCollection(coll).insert({ num: i, name: \"MongoDB Benchmark Test\" }); } for (let i = 0; i \u003c numDocs; i++) { // select db.getCollection(coll).find({ num: i }); } for (let i = 0; i \u003c numDocs; i++) { // update db.getCollection(coll).update({ num: i }, { $set: { name: \"Updated\" } }); } for (let i = 0; i \u003c numDocs; i++) { // delete db.getCollection(coll).deleteOne({ num: i }); } EOF mongosh 'mongodb://dbuser_meta:DBUser.Meta@10.10.10.10:27017?authMechanism=PLAIN' benchmark.js You can check the MongoDB commands supported by FerretDB, as well as some known differences. For basic usage, these differences usually aren’t a significant problem.\n","categories":["Task"],"description":"Create, remove, expand, shrink, and upgrade FerretDB clusters","excerpt":"Create, remove, expand, shrink, and upgrade FerretDB clusters","ref":"/docs/ferret/admin/","tags":"","title":"Administration"},{"body":"Pigsty provides a built-in playbook mongo.yml for installing FerretDB on nodes.\nImportant: This playbook only executes on hosts where mongo_seq is defined. Running the playbook against hosts without mongo_seq will skip all tasks safely, making it safe to run against mixed host groups.\nmongo.yml Playbook location: mongo.yml\nFunction: Install MongoDB/FerretDB on target hosts where mongo_seq is defined.\nThis playbook contains the following subtasks:\nSubtask Description mongo_check Check mongo identity parameters mongo_dbsu Create OS user mongod mongo_install Install ferretdb RPM/DEB packages mongo_purge Purge existing FerretDB (not by default) mongo_config Configure FerretDB service mongo_cert Issue FerretDB SSL certificates mongo_launch Launch FerretDB service mongo_register Register FerretDB to Prometheus Task Details mongo_check Check that required identity parameters are defined:\nmongo_cluster: Cluster name mongo_seq: Instance sequence number mongo_pgurl: PostgreSQL connection string If any parameter is missing, the playbook will exit with an error.\nmongo_dbsu Create OS user and group required for FerretDB:\nCreate mongod user group Create mongod user with home directory /var/lib/mongod mongo_install Install FerretDB packages:\nInstall ferretdb2 package on RPM-based distributions Install corresponding deb package on DEB-based distributions mongo_purge Purge existing FerretDB cluster. This task does not run by default and requires explicit specification:\n./mongo.yml -l \u003ccluster\u003e -e mongo_purge=true -t mongo_purge Important: Always use the -l \u003ccluster\u003e parameter to limit the execution scope.\nPurge operations include:\nStop and disable ferretdb service Remove systemd service file Remove configuration files and SSL certificates Deregister from Prometheus monitoring targets mongo_config Configure FerretDB service:\nRender environment variable config file /etc/default/ferretdb Create systemd service file mongo_cert When mongo_ssl_enabled is set to true, this task will:\nGenerate FerretDB SSL private key Create Certificate Signing Request (CSR) Issue SSL certificate using CA Deploy certificate files to /var/lib/mongod/ mongo_launch Launch FerretDB service:\nReload systemd configuration Start and enable ferretdb service Wait for service to be available on specified port (default 27017) The FerretDB service is configured with Restart=on-failure, so it will automatically restart if the process crashes unexpectedly. This provides basic resilience for this stateless proxy service.\nmongo_register Register FerretDB instance to Prometheus monitoring system:\nCreate monitoring target file on all infra nodes Target file path: /infra/targets/mongo/\u003ccluster\u003e-\u003cseq\u003e.yml Contains instance IP, labels, and metrics port information Usage Examples # Deploy FerretDB on ferret group ./mongo.yml -l ferret # Run config task only ./mongo.yml -l ferret -t mongo_config # Reissue SSL certificates ./mongo.yml -l ferret -t mongo_cert # Restart FerretDB service ./mongo.yml -l ferret -t mongo_launch # Purge FerretDB cluster ./mongo.yml -l ferret -e mongo_purge=true -t mongo_purge ","categories":["Task"],"description":"Ansible playbooks available for the FERRET module","excerpt":"Ansible playbooks available for the FERRET module","ref":"/docs/ferret/playbook/","tags":"","title":"Playbook"},{"body":"The FERRET module currently provides one monitoring dashboard.\nMongo Overview Mongo Overview: Mongo/FerretDB cluster overview\nThis dashboard provides basic monitoring metrics for FerretDB, including:\nInstance status: Running state of FerretDB instances Client connections: Client connection count and request statistics Resource usage: CPU, memory, goroutine count, etc. PostgreSQL connection pool: Backend PostgreSQL connection pool status Since FerretDB uses PostgreSQL as its underlying storage engine, for more monitoring metrics please refer to PostgreSQL Monitoring.\nMetrics FerretDB exposes Prometheus-format metrics through its built-in exporter on the mongo_exporter_port (default 9216) port.\nKey metric categories include:\nMetric Prefix Description ferretdb_* FerretDB core metrics ferretdb_client_* Client connection and request stats ferretdb_postgresql_* PostgreSQL backend status go_* Go runtime metrics process_* Process-level metrics For the complete list of metrics, see Metrics.\nAlerting Rules The FerretDB module currently uses basic instance liveness alerts:\n- alert: FerretDBDown expr: ferretdb_up == 0 for: 1m labels: severity: critical annotations: summary: \"FerretDB instance {{ $labels.ins }} is down\" description: \"FerretDB instance {{ $labels.ins }} on {{ $labels.ip }} has been down for more than 1 minute.\" Since FerretDB is a stateless proxy layer, the primary monitoring and alerting should focus on the underlying PostgreSQL cluster.\n","categories":["Reference"],"description":"Monitoring dashboards and alerting rules for the FerretDB module","excerpt":"Monitoring dashboards and alerting rules for the FerretDB module","ref":"/docs/ferret/monitor/","tags":"","title":"Monitoring"},{"body":"The MONGO module contains 54 available monitoring metrics.\nMetric Name Type Labels Description ferretdb_client_accepts_total Unknown error, cls, ip, ins, instance, job N/A ferretdb_client_duration_seconds_bucket Unknown error, le, cls, ip, ins, instance, job N/A ferretdb_client_duration_seconds_count Unknown error, cls, ip, ins, instance, job N/A ferretdb_client_duration_seconds_sum Unknown error, cls, ip, ins, instance, job N/A ferretdb_client_requests_total Unknown cls, ip, ins, opcode, instance, command, job N/A ferretdb_client_responses_total Unknown result, argument, cls, ip, ins, opcode, instance, command, job N/A ferretdb_postgresql_metadata_databases gauge cls, ip, ins, instance, job The current number of database in the registry. ferretdb_postgresql_pool_size gauge cls, ip, ins, instance, job The current number of pools. ferretdb_up gauge cls, version, commit, ip, ins, dirty, telemetry, package, update_available, uuid, instance, job, branch, debug FerretDB instance state. go_gc_duration_seconds summary cls, ip, ins, instance, quantile, job A summary of the pause duration of garbage collection cycles. go_gc_duration_seconds_count Unknown cls, ip, ins, instance, job N/A go_gc_duration_seconds_sum Unknown cls, ip, ins, instance, job N/A go_goroutines gauge cls, ip, ins, instance, job Number of goroutines that currently exist. go_info gauge cls, version, ip, ins, instance, job Information about the Go environment. go_memstats_alloc_bytes gauge cls, ip, ins, instance, job Number of bytes allocated and still in use. go_memstats_alloc_bytes_total counter cls, ip, ins, instance, job Total number of bytes allocated, even if freed. go_memstats_buck_hash_sys_bytes gauge cls, ip, ins, instance, job Number of bytes used by the profiling bucket hash table. go_memstats_frees_total counter cls, ip, ins, instance, job Total number of frees. go_memstats_gc_sys_bytes gauge cls, ip, ins, instance, job Number of bytes used for garbage collection system metadata. go_memstats_heap_alloc_bytes gauge cls, ip, ins, instance, job Number of heap bytes allocated and still in use. go_memstats_heap_idle_bytes gauge cls, ip, ins, instance, job Number of heap bytes waiting to be used. go_memstats_heap_inuse_bytes gauge cls, ip, ins, instance, job Number of heap bytes that are in use. go_memstats_heap_objects gauge cls, ip, ins, instance, job Number of allocated objects. go_memstats_heap_released_bytes gauge cls, ip, ins, instance, job Number of heap bytes released to OS. go_memstats_heap_sys_bytes gauge cls, ip, ins, instance, job Number of heap bytes obtained from system. go_memstats_last_gc_time_seconds gauge cls, ip, ins, instance, job Number of seconds since 1970 of last garbage collection. go_memstats_lookups_total counter cls, ip, ins, instance, job Total number of pointer lookups. go_memstats_mallocs_total counter cls, ip, ins, instance, job Total number of mallocs. go_memstats_mcache_inuse_bytes gauge cls, ip, ins, instance, job Number of bytes in use by mcache structures. go_memstats_mcache_sys_bytes gauge cls, ip, ins, instance, job Number of bytes used for mcache structures obtained from system. go_memstats_mspan_inuse_bytes gauge cls, ip, ins, instance, job Number of bytes in use by mspan structures. go_memstats_mspan_sys_bytes gauge cls, ip, ins, instance, job Number of bytes used for mspan structures obtained from system. go_memstats_next_gc_bytes gauge cls, ip, ins, instance, job Number of heap bytes when next garbage collection will take place. go_memstats_other_sys_bytes gauge cls, ip, ins, instance, job Number of bytes used for other system allocations. go_memstats_stack_inuse_bytes gauge cls, ip, ins, instance, job Number of bytes in use by the stack allocator. go_memstats_stack_sys_bytes gauge cls, ip, ins, instance, job Number of bytes obtained from system for stack allocator. go_memstats_sys_bytes gauge cls, ip, ins, instance, job Number of bytes obtained from system. go_threads gauge cls, ip, ins, instance, job Number of OS threads created. mongo_up Unknown cls, ip, ins, instance, job N/A process_cpu_seconds_total counter cls, ip, ins, instance, job Total user and system CPU time spent in seconds. process_max_fds gauge cls, ip, ins, instance, job Maximum number of open file descriptors. process_open_fds gauge cls, ip, ins, instance, job Number of open file descriptors. process_resident_memory_bytes gauge cls, ip, ins, instance, job Resident memory size in bytes. process_start_time_seconds gauge cls, ip, ins, instance, job Start time of the process since unix epoch in seconds. process_virtual_memory_bytes gauge cls, ip, ins, instance, job Virtual memory size in bytes. process_virtual_memory_max_bytes gauge cls, ip, ins, instance, job Maximum amount of virtual memory available in bytes. promhttp_metric_handler_errors_total counter job, cls, ip, ins, instance, cause Total number of internal errors encountered by the promhttp metric handler. promhttp_metric_handler_requests_in_flight gauge cls, ip, ins, instance, job Current number of scrapes being served. promhttp_metric_handler_requests_total counter job, cls, ip, ins, instance, code Total number of scrapes by HTTP status code. scrape_duration_seconds Unknown cls, ip, ins, instance, job N/A scrape_samples_post_metric_relabeling Unknown cls, ip, ins, instance, job N/A scrape_samples_scraped Unknown cls, ip, ins, instance, job N/A scrape_series_added Unknown cls, ip, ins, instance, job N/A up Unknown cls, ip, ins, instance, job N/A ","categories":["Reference"],"description":"Complete list of monitoring metrics provided by the FerretDB module","excerpt":"Complete list of monitoring metrics provided by the FerretDB module","ref":"/docs/ferret/metric/","tags":"","title":"Metrics"},{"body":" Why Use FerretDB? MongoDB was an amazing technology that allowed developers to escape the “schema constraints” of relational databases and rapidly build applications. However, over time, MongoDB abandoned its open-source roots and changed its license to SSPL, making it unusable for many open-source projects and early-stage commercial ventures. Most MongoDB users don’t actually need the advanced features MongoDB offers, but they do need an easy-to-use open-source document database solution. To fill this gap, FerretDB was born.\nPostgreSQL’s JSON support is already quite comprehensive: binary JSONB storage, GIN indexes for arbitrary fields, various JSON processing functions, JSON PATH and JSON Schema—it has long been a fully-featured, high-performance document database. But providing alternative functionality is not the same as direct emulation. FerretDB can provide a smooth migration path to PostgreSQL for applications using MongoDB drivers.\nPigsty’s FerretDB Support History Pigsty has provided Docker-based FerretDB templates since 1.x and added native deployment support in v2.3. As an optional component, it greatly enriches the PostgreSQL ecosystem. The Pigsty community has become a partner of the FerretDB community, and deeper collaboration and integration support will follow.\nFERRET is an optional module in Pigsty. Since v2.0, it requires the documentdb extension to work. Pigsty has packaged this extension and provides a mongo.yml template to help you easily deploy FerretDB clusters.\nInstalling MongoSH You can use MongoSH as a client tool to access FerretDB clusters.\nThe recommended approach is to use the pig command to add the MongoDB repository and install:\npig repo add mongo -u # Add the official MongoDB repository yum install mongodb-mongosh # RHEL/CentOS/Rocky/Alma apt install mongodb-mongosh # Debian/Ubuntu You can also manually add the MongoDB repository:\n# RHEL/CentOS family cat \u003e /etc/yum.repos.d/mongo.repo \u003c\u003cEOF [mongodb-org-7.0] name=MongoDB Repository baseurl=https://repo.mongodb.org/yum/redhat/\\$releasever/mongodb-org/7.0/\\$basearch/ gpgcheck=1 enabled=1 gpgkey=https://www.mongodb.org/static/pgp/server-7.0.asc EOF yum install -y mongodb-mongosh Authentication Method FerretDB authentication is entirely based on the underlying PostgreSQL. Since Pigsty-managed PostgreSQL clusters use scram-sha-256 authentication by default, you must specify the PLAIN authentication mechanism in the connection string:\nmongosh 'mongodb://user:password@host:27017?authMechanism=PLAIN' If you forget to add the authMechanism=PLAIN parameter, the connection will fail with an authentication error.\nCompatibility with MongoDB FerretDB implements MongoDB’s wire protocol but uses PostgreSQL for underlying storage. This means:\nMost basic CRUD operations are compatible with MongoDB Some advanced features may not be supported or may differ Aggregation pipeline support is limited For detailed compatibility information, see:\nFerretDB Supported Commands Differences from MongoDB Why Is a Superuser Required? FerretDB 2.0+ uses the documentdb extension, which requires superuser privileges to create and manage internal structures. Therefore, the user specified in mongo_pgurl must be a PostgreSQL superuser.\nIt’s recommended to create a dedicated mongod superuser for FerretDB to use, rather than using the default postgres user.\nHow to Achieve High Availability FerretDB itself is stateless—all data is stored in the underlying PostgreSQL. To achieve high availability:\nPostgreSQL layer: Use Pigsty’s PGSQL module to deploy a highly available PostgreSQL cluster FerretDB layer: Deploy multiple FerretDB instances with a VIP or load balancer For detailed configuration, see High Availability Configuration.\nPerformance Considerations FerretDB’s performance depends on the underlying PostgreSQL cluster. Since MongoDB commands need to be translated to SQL, there is some performance overhead. For most OLTP scenarios, the performance is acceptable.\nIf you need higher performance, you can:\nUse faster storage (NVMe SSD) Increase PostgreSQL resource allocation Optimize PostgreSQL parameters Use connection pooling to reduce connection overhead ","categories":["Reference"],"description":"Frequently asked questions about FerretDB and DocumentDB modules","excerpt":"Frequently asked questions about FerretDB and DocumentDB modules","ref":"/docs/ferret/faq/","tags":"","title":"FAQ"},{"body":"Docker is the most popular containerization platform, providing standardized software delivery capabilities.\nPigsty does not rely on Docker to deploy any of its components; instead, it provides the ability to deploy and install Docker — this is an optional module.\nPigsty offers a series of Docker software/tool/application templates for you to choose from as needed. This allows users to quickly spin up various containerized stateless software templates, adding extra functionality. You can use external, Pigsty-managed highly available database clusters while placing stateless applications inside containers.\nPigsty’s Docker module automatically configures accessible registry mirrors for users in mainland China to improve image pulling speed (and availability). You can easily configure Registry and Proxy settings to flexibly access different image sources.\n","categories":["Reference"],"description":"Docker daemon service that enables one-click deployment of containerized stateless software templates and additional functionality.","excerpt":"Docker daemon service that enables one-click deployment of …","ref":"/docs/docker/","tags":"","title":"Module: DOCKER"},{"body":"Pigsty has built-in Docker support, which you can use to quickly deploy containerized applications.\nGetting Started Docker is an optional module, and in most of Pigsty’s configuration templates, Docker is not enabled by default. Therefore, users need to explicitly download and configure it to use Docker in Pigsty.\nFor example, in the default meta template, Docker is not downloaded or installed by default. However, in the rich single-node template, Docker is downloaded and installed.\nThe key difference between these two configurations lies in these two parameters: repo_modules and repo_packages.\nrepo_modules: infra,node,pgsql,docker # \u003c--- Enable Docker repository repo_packages: - node-bootstrap, infra-package, infra-addons, node-package1, node-package2, pgsql-common, docker # \u003c--- Download Docker After Docker is downloaded, you need to set the docker_enabled: true flag on the nodes where you want to install Docker, and configure other parameters as needed.\ninfra: hosts: 10.10.10.10: { infra_seq: 1 ,nodename: infra-1 } 10.10.10.11: { infra_seq: 2 ,nodename: infra-2 } vars: docker_enabled: true # Install Docker on this group! Finally, use the docker.yml playbook to install it on the nodes:\n./docker.yml -l infra # Install Docker on the infra group Installation If you want to temporarily install Docker directly from the internet on certain nodes, you can use the following command:\n./node.yml -e '{\"node_repo_modules\":\"node,docker\",\"node_packages\":[\"docker-ce,docker-compose-plugin\"]}' -t node_repo,node_pkg -l \u003cselect_group_ip\u003e This command will first enable the upstream software sources for the node,docker modules on the target nodes, then install the docker-ce and docker-compose-plugin packages (same package names for EL/Debian).\nIf you want Docker-related packages to be automatically downloaded during Pigsty initialization, refer to the instructions below.\nRemoval Because it’s so simple, Pigsty doesn’t provide an uninstall playbook for the Docker module. You can directly remove Docker using an Ansible command:\nansible minio -m package -b -a 'name=docker-ce state=absent' # Remove docker This command will uninstall the docker-ce package using the OS package manager.\nDownload To download Docker during Pigsty installation, modify the repo_modules parameter in the configuration inventory to enable the Docker software repository, then specify Docker packages to download in the repo_packages or repo_extra_packages parameters.\nrepo_modules: infra,node,pgsql,docker # \u003c--- Enable Docker repository repo_packages: - node-bootstrap, infra-package, infra-addons, node-package1, node-package2, pgsql-common, docker # \u003c--- Download Docker repo_extra_packages: - pgsql-main docker # \u003c--- Can also be specified here The docker specified here (which actually corresponds to the docker-ce and docker-compose-plugin packages) will be automatically downloaded to the local repository during the default deploy.yml process. After downloading, the Docker packages will be available to all nodes via the local repository.\nIf you’ve already completed Pigsty installation and the local repository is initialized, you can run ./infra.yml -t repo_build after modifying the configuration to re-download and rebuild the offline repository.\nInstalling Docker requires the Docker YUM/APT repository, which is included by default in Pigsty but not enabled. You need to add docker to repo_modules to enable it before installation.\nRepository Downloading Docker requires upstream internet software repositories, which are defined in the default repo_upstream with module name docker:\n- { name: docker-ce ,description: 'Docker CE' ,module: docker ,releases: [7,8,9] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://download.docker.com/linux/centos/$releasever/$basearch/stable' ,china: 'https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/$basearch/stable' ,europe: 'https://mirrors.xtom.de/docker-ce/linux/centos/$releasever/$basearch/stable' }} - { name: docker-ce ,description: 'Docker CE' ,module: docker ,releases: [11,12,20,22,24] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://download.docker.com/linux/${distro_name} ${distro_codename} stable' ,china: 'https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux//${distro_name} ${distro_codename} stable' }} You can reference this repository using the docker module name in the repo_modules and node_repo_modules parameters.\nNote that Docker’s official software repository is blocked by default in mainland China. You need to use mirror sites in China to complete the download.\nIf you’re in mainland China and encounter Docker download failures, check whether region is set to default in your configuration inventory. The automatically configured region: china can resolve this issue.\nProxy If your network environment requires a proxy server to access the internet, you can configure the proxy_env parameter in Pigsty’s configuration inventory. This parameter will be written to the proxy related configuration in Docker’s configuration file.\nproxy_env: no_proxy: \"localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,*.pigsty,*.aliyun.com,mirrors.aliyuncs.com,mirrors.tuna.tsinghua.edu.cn,mirrors.zju.edu.cn\" #http_proxy: 'http://username:password@proxy.address.com' #https_proxy: 'http://username:password@proxy.address.com' #all_proxy: 'http://username:password@proxy.address.com' When running configure with the -x parameter, the proxy server configuration from your current environment will be automatically generated into Pigsty’s configuration file under proxy_env.\nIn addition to using a proxy server, you can also configure Docker Registry Mirrors to bypass blocks.\nRegistry Mirrors You can use the docker_registry_mirrors parameter to specify Docker Registry Mirrors:\nFor users outside the firewall, in addition to the official DockerHub site, you can also consider using the quay.io mirror site. If your internal network environment already has mature image infrastructure, you can use your internal Docker registry mirrors to avoid being affected by external mirror sites and improve download speeds.\nUsers of public cloud providers can consider using free internal Docker mirrors. For example, if you’re using Alibaba Cloud, you can use Alibaba Cloud’s internal Docker mirror site (requires login):\n[\"https://registry.cn-hangzhou.aliyuncs.com\"] # Alibaba Cloud mirror, requires explicit login If you’re using Tencent Cloud, you can use Tencent Cloud’s internal Docker mirror site (requires internal network):\n[\"https://ccr.ccs.tencentyun.com\"] # Tencent Cloud mirror, internal network only Additionally, you can use CF-Workers-docker.io to quickly set up your own Docker image proxy. You can also consider using free Docker proxy mirrors (use at your own risk!)\nPulling Images The docker_image and docker_image_cache parameters can be used to directly specify a list of images to pull during Docker installation.\nUsing this feature, Docker will come with the specified images after installation (provided they can be successfully pulled; this task will be automatically ignored and skipped on failure).\nFor example, you can specify images to pull in the configuration inventory:\ninfra: hosts: 10.10.10.10: { infra_seq: 1 } vars: docker_enabled: true # Install Docker on this group! docker_image: - redis:latest # Pull the latest Redis image Another way to preload images is to use locally saved tgz archives: if you’ve previously exported Docker images using docker save xxx | gzip -c \u003e /tmp/docker/xxx.tgz. These exported image files can be automatically loaded via the glob specified by the docker_image_cache parameter. The default location is: /tmp/docker/*.tgz.\nThis means you can place images in the /tmp/docker directory beforehand, and after running docker.yml to install Docker, these image packages will be automatically loaded.\nFor example, in the self-hosted Supabase tutorial, this technique is used. Before spinning up Supabase and installing Docker, the *.tgz image archives from the local /tmp/supabase directory are copied to the target node’s /tmp/docker directory.\n- name: copy local docker images copy: src=\"{{ item }}\" dest=\"/tmp/docker/\" with_fileglob: \"{{ supa_images }}\" vars: # you can override this with -e cli args supa_images: /tmp/supabase/*.tgz Applications Pigsty provides a series of ready-to-use, Docker Compose-based software templates, which you can use to spin up business software that uses external Pigsty-managed database clusters.\n","categories":["Reference"],"description":"Docker module quick start guide - installation, removal, download, repository, mirrors, proxy, and image pulling.","excerpt":"Docker module quick start guide - installation, removal, download, …","ref":"/docs/docker/usage/","tags":"","title":"Usage"},{"body":"The DOCKER module provides 8 configuration parameters.\nParameter Overview The DOCKER parameter group is used for Docker container engine deployment and configuration, including enable switch, data directory, storage driver, registry mirrors, and monitoring.\nParameter Type Level Description docker_enabled bool G/C/I Enable Docker on current node? disabled by default docker_data path G/C/I Docker data directory, /data/docker by default docker_storage_driver enum G/C/I Docker storage driver, overlay2 by default docker_cgroups_driver enum G/C/I Docker cgroup driver: cgroupfs or systemd docker_registry_mirrors string[] G/C/I Docker registry mirror list docker_exporter_port port G Docker metrics exporter port, 9323 by default docker_image string[] G/C/I Docker images to pull, empty list by default docker_image_cache path G/C/I Docker image cache tarball path, /tmp/docker/*.tgz You can use the docker.yml playbook to install and enable Docker on nodes.\nDefault parameters are defined in roles/docker/defaults/main.yml\ndocker_enabled: false # Enable Docker on current node? docker_data: /data/docker # Docker data directory, /data/docker by default docker_storage_driver: overlay2 # Docker storage driver, overlay2/zfs/btrfs... docker_cgroups_driver: systemd # Docker cgroup driver: cgroupfs or systemd docker_registry_mirrors: [] # Docker registry mirror list docker_exporter_port: 9323 # Docker metrics exporter port, 9323 by default docker_image: [] # Docker images to pull after startup docker_image_cache: /tmp/docker/*.tgz # Docker image cache tarball glob pattern docker_enabled Parameter: docker_enabled, Type: bool, Level: G/C/I\nEnable Docker on current node? Default: false, meaning Docker is not enabled.\ndocker_data Parameter: docker_data, Type: path, Level: G/C/I\nDocker data directory, default is /data/docker.\nThis directory stores Docker images, containers, volumes, and other data. If you have a dedicated data disk, it’s recommended to point this directory to that disk’s mount point.\ndocker_storage_driver Parameter: docker_storage_driver, Type: enum, Level: G/C/I\nDocker storage driver, default is overlay2.\nSee official documentation: https://docs.docker.com/engine/storage/drivers/select-storage-driver/\nAvailable storage drivers include:\noverlay2: Recommended default driver, suitable for most scenarios fuse-overlayfs: For rootless container scenarios btrfs: When using Btrfs filesystem zfs: When using ZFS filesystem vfs: For testing purposes, not recommended for production docker_cgroups_driver Parameter: docker_cgroups_driver, Type: enum, Level: G/C/I\nDocker cgroup filesystem driver, can be cgroupfs or systemd, default: systemd\ndocker_registry_mirrors Parameter: docker_registry_mirrors, Type: string[], Level: G/C/I\nDocker registry mirror list, default: [] empty array.\nYou can use Docker mirror sites to accelerate image pulls. Here are some examples:\n[\"https://docker.m.daocloud.io\"] # DaoCloud mirror [\"https://docker.1ms.run\"] # 1ms mirror [\"https://mirror.ccs.tencentyun.com\"] # Tencent Cloud internal mirror [\"https://registry.cn-hangzhou.aliyuncs.com\"] # Alibaba Cloud mirror (requires login) You can also consider using a Cloudflare Worker to set up a Docker Proxy for faster access.\nIf pull speeds are still too slow, consider using alternative registries: docker login quay.io\ndocker_exporter_port Parameter: docker_exporter_port, Type: port, Level: G\nDocker metrics exporter port, default is 9323.\nThe Docker daemon exposes Prometheus-format monitoring metrics on this port for collection by monitoring infrastructure.\ndocker_image Parameter: docker_image, Type: string[], Level: G/C/I\nList of Docker images to pull, default is empty list [].\nDocker image names specified here will be automatically pulled during the installation phase.\ndocker_image_cache Parameter: docker_image_cache, Type: path, Level: G/C/I\nLocal Docker image cache tarball glob pattern, default is /tmp/docker/*.tgz.\nYou can use docker save | gzip to package images and automatically import them during Docker installation via this parameter.\n.tgz tarball files matching this pattern will be imported into Docker one by one using:\ncat *.tgz | gzip -d -c - | docker load ","categories":["Reference"],"description":"DOCKER module provides 8 configuration parameters","excerpt":"DOCKER module provides 8 configuration parameters","ref":"/docs/docker/param/","tags":"","title":"Parameters"},{"body":"The Docker module provides a default playbook docker.yml for installing Docker Daemon and Docker Compose.\ndocker.yml Playbook source file: docker.yml\nRunning this playbook will install docker-ce and docker-compose-plugin on target nodes with the docker_enabled: true flag, and enable the dockerd service.\nThe following are the available task subsets in the docker.yml playbook:\ndocker_install : Install Docker and Docker Compose packages on the node docker_admin : Add specified users to the Docker admin user group docker_alias : Generate Docker command completion and alias scripts docker_dir : Create Docker related directories docker_config : Generate Docker daemon service configuration file docker_launch : Start the Docker daemon service docker_register : Register Docker daemon as a Prometheus monitoring target docker_image : Attempt to load pre-cached image tarballs from /tmp/docker/*.tgz (if they exist) The Docker module does not provide a dedicated uninstall playbook. If you need to uninstall Docker, you can manually stop Docker and then remove it:\nsystemctl stop docker # Stop Docker daemon service yum remove docker-ce docker-compose-plugin # Uninstall Docker on EL systems apt remove docker-ce docker-compose-plugin # Uninstall Docker on Debian systems ","categories":["Task"],"description":"How to use the built-in Ansible playbook to manage Docker and quick reference for common management commands.","excerpt":"How to use the built-in Ansible playbook to manage Docker and quick …","ref":"/docs/docker/playbook/","tags":"","title":"Playbooks"},{"body":"The DOCKER module contains 123 available monitoring metrics.\nMetric Name Type Labels Description builder_builds_failed_total counter ip, cls, reason, ins, job, instance Number of failed image builds builder_builds_triggered_total counter ip, cls, ins, job, instance Number of triggered image builds docker_up Unknown ip, cls, ins, job, instance N/A engine_daemon_container_actions_seconds_bucket Unknown ip, cls, ins, job, instance, le, action N/A engine_daemon_container_actions_seconds_count Unknown ip, cls, ins, job, instance, action N/A engine_daemon_container_actions_seconds_sum Unknown ip, cls, ins, job, instance, action N/A engine_daemon_container_states_containers gauge ip, cls, ins, job, instance, state The count of containers in various states engine_daemon_engine_cpus_cpus gauge ip, cls, ins, job, instance The number of cpus that the host system of the engine has engine_daemon_engine_info gauge ip, cls, architecture, ins, job, instance, os_version, kernel, version, graphdriver, os, daemon_id, commit, os_type The information related to the engine and the OS it is running on engine_daemon_engine_memory_bytes gauge ip, cls, ins, job, instance The number of bytes of memory that the host system of the engine has engine_daemon_events_subscribers_total gauge ip, cls, ins, job, instance The number of current subscribers to events engine_daemon_events_total counter ip, cls, ins, job, instance The number of events logged engine_daemon_health_checks_failed_total counter ip, cls, ins, job, instance The total number of failed health checks engine_daemon_health_check_start_duration_seconds_bucket Unknown ip, cls, ins, job, instance, le N/A engine_daemon_health_check_start_duration_seconds_count Unknown ip, cls, ins, job, instance N/A engine_daemon_health_check_start_duration_seconds_sum Unknown ip, cls, ins, job, instance N/A engine_daemon_health_checks_total counter ip, cls, ins, job, instance The total number of health checks engine_daemon_host_info_functions_seconds_bucket Unknown ip, cls, ins, job, instance, le, function N/A engine_daemon_host_info_functions_seconds_count Unknown ip, cls, ins, job, instance, function N/A engine_daemon_host_info_functions_seconds_sum Unknown ip, cls, ins, job, instance, function N/A engine_daemon_image_actions_seconds_bucket Unknown ip, cls, ins, job, instance, le, action N/A engine_daemon_image_actions_seconds_count Unknown ip, cls, ins, job, instance, action N/A engine_daemon_image_actions_seconds_sum Unknown ip, cls, ins, job, instance, action N/A engine_daemon_network_actions_seconds_bucket Unknown ip, cls, ins, job, instance, le, action N/A engine_daemon_network_actions_seconds_count Unknown ip, cls, ins, job, instance, action N/A engine_daemon_network_actions_seconds_sum Unknown ip, cls, ins, job, instance, action N/A etcd_debugging_snap_save_marshalling_duration_seconds_bucket Unknown ip, cls, ins, job, instance, le N/A etcd_debugging_snap_save_marshalling_duration_seconds_count Unknown ip, cls, ins, job, instance N/A etcd_debugging_snap_save_marshalling_duration_seconds_sum Unknown ip, cls, ins, job, instance N/A etcd_debugging_snap_save_total_duration_seconds_bucket Unknown ip, cls, ins, job, instance, le N/A etcd_debugging_snap_save_total_duration_seconds_count Unknown ip, cls, ins, job, instance N/A etcd_debugging_snap_save_total_duration_seconds_sum Unknown ip, cls, ins, job, instance N/A etcd_disk_wal_fsync_duration_seconds_bucket Unknown ip, cls, ins, job, instance, le N/A etcd_disk_wal_fsync_duration_seconds_count Unknown ip, cls, ins, job, instance N/A etcd_disk_wal_fsync_duration_seconds_sum Unknown ip, cls, ins, job, instance N/A etcd_disk_wal_write_bytes_total gauge ip, cls, ins, job, instance Total number of bytes written in WAL. etcd_snap_db_fsync_duration_seconds_bucket Unknown ip, cls, ins, job, instance, le N/A etcd_snap_db_fsync_duration_seconds_count Unknown ip, cls, ins, job, instance N/A etcd_snap_db_fsync_duration_seconds_sum Unknown ip, cls, ins, job, instance N/A etcd_snap_db_save_total_duration_seconds_bucket Unknown ip, cls, ins, job, instance, le N/A etcd_snap_db_save_total_duration_seconds_count Unknown ip, cls, ins, job, instance N/A etcd_snap_db_save_total_duration_seconds_sum Unknown ip, cls, ins, job, instance N/A etcd_snap_fsync_duration_seconds_bucket Unknown ip, cls, ins, job, instance, le N/A etcd_snap_fsync_duration_seconds_count Unknown ip, cls, ins, job, instance N/A etcd_snap_fsync_duration_seconds_sum Unknown ip, cls, ins, job, instance N/A go_gc_duration_seconds summary ip, cls, ins, job, instance, quantile A summary of the pause duration of garbage collection cycles. go_gc_duration_seconds_count Unknown ip, cls, ins, job, instance N/A go_gc_duration_seconds_sum Unknown ip, cls, ins, job, instance N/A go_goroutines gauge ip, cls, ins, job, instance Number of goroutines that currently exist. go_info gauge ip, cls, ins, job, version, instance Information about the Go environment. go_memstats_alloc_bytes counter ip, cls, ins, job, instance Total number of bytes allocated, even if freed. go_memstats_alloc_bytes_total counter ip, cls, ins, job, instance Total number of bytes allocated, even if freed. go_memstats_buck_hash_sys_bytes gauge ip, cls, ins, job, instance Number of bytes used by the profiling bucket hash table. go_memstats_frees_total counter ip, cls, ins, job, instance Total number of frees. go_memstats_gc_sys_bytes gauge ip, cls, ins, job, instance Number of bytes used for garbage collection system metadata. go_memstats_heap_alloc_bytes gauge ip, cls, ins, job, instance Number of heap bytes allocated and still in use. go_memstats_heap_idle_bytes gauge ip, cls, ins, job, instance Number of heap bytes waiting to be used. go_memstats_heap_inuse_bytes gauge ip, cls, ins, job, instance Number of heap bytes that are in use. go_memstats_heap_objects gauge ip, cls, ins, job, instance Number of allocated objects. go_memstats_heap_released_bytes gauge ip, cls, ins, job, instance Number of heap bytes released to OS. go_memstats_heap_sys_bytes gauge ip, cls, ins, job, instance Number of heap bytes obtained from system. go_memstats_last_gc_time_seconds gauge ip, cls, ins, job, instance Number of seconds since 1970 of last garbage collection. go_memstats_lookups_total counter ip, cls, ins, job, instance Total number of pointer lookups. go_memstats_mallocs_total counter ip, cls, ins, job, instance Total number of mallocs. go_memstats_mcache_inuse_bytes gauge ip, cls, ins, job, instance Number of bytes in use by mcache structures. go_memstats_mcache_sys_bytes gauge ip, cls, ins, job, instance Number of bytes used for mcache structures obtained from system. go_memstats_mspan_inuse_bytes gauge ip, cls, ins, job, instance Number of bytes in use by mspan structures. go_memstats_mspan_sys_bytes gauge ip, cls, ins, job, instance Number of bytes used for mspan structures obtained from system. go_memstats_next_gc_bytes gauge ip, cls, ins, job, instance Number of heap bytes when next garbage collection will take place. go_memstats_other_sys_bytes gauge ip, cls, ins, job, instance Number of bytes used for other system allocations. go_memstats_stack_inuse_bytes gauge ip, cls, ins, job, instance Number of bytes in use by the stack allocator. go_memstats_stack_sys_bytes gauge ip, cls, ins, job, instance Number of bytes obtained from system for stack allocator. go_memstats_sys_bytes gauge ip, cls, ins, job, instance Number of bytes obtained from system. go_threads gauge ip, cls, ins, job, instance Number of OS threads created. logger_log_entries_size_greater_than_buffer_total counter ip, cls, ins, job, instance Number of log entries which are larger than the log buffer logger_log_read_operations_failed_total counter ip, cls, ins, job, instance Number of log reads from container stdio that failed logger_log_write_operations_failed_total counter ip, cls, ins, job, instance Number of log write operations that failed process_cpu_seconds_total counter ip, cls, ins, job, instance Total user and system CPU time spent in seconds. process_max_fds gauge ip, cls, ins, job, instance Maximum number of open file descriptors. process_open_fds gauge ip, cls, ins, job, instance Number of open file descriptors. process_resident_memory_bytes gauge ip, cls, ins, job, instance Resident memory size in bytes. process_start_time_seconds gauge ip, cls, ins, job, instance Start time of the process since unix epoch in seconds. process_virtual_memory_bytes gauge ip, cls, ins, job, instance Virtual memory size in bytes. process_virtual_memory_max_bytes gauge ip, cls, ins, job, instance Maximum amount of virtual memory available in bytes. promhttp_metric_handler_requests_in_flight gauge ip, cls, ins, job, instance Current number of scrapes being served. promhttp_metric_handler_requests_total counter ip, cls, ins, job, instance, code Total number of scrapes by HTTP status code. scrape_duration_seconds Unknown ip, cls, ins, job, instance N/A scrape_samples_post_metric_relabeling Unknown ip, cls, ins, job, instance N/A scrape_samples_scraped Unknown ip, cls, ins, job, instance N/A scrape_series_added Unknown ip, cls, ins, job, instance N/A swarm_dispatcher_scheduling_delay_seconds_bucket Unknown ip, cls, ins, job, instance, le N/A swarm_dispatcher_scheduling_delay_seconds_count Unknown ip, cls, ins, job, instance N/A swarm_dispatcher_scheduling_delay_seconds_sum Unknown ip, cls, ins, job, instance N/A swarm_manager_configs_total gauge ip, cls, ins, job, instance The number of configs in the cluster object store swarm_manager_leader gauge ip, cls, ins, job, instance Indicates if this manager node is a leader swarm_manager_networks_total gauge ip, cls, ins, job, instance The number of networks in the cluster object store swarm_manager_nodes gauge ip, cls, ins, job, instance, state The number of nodes swarm_manager_secrets_total gauge ip, cls, ins, job, instance The number of secrets in the cluster object store swarm_manager_services_total gauge ip, cls, ins, job, instance The number of services in the cluster object store swarm_manager_tasks_total gauge ip, cls, ins, job, instance, state The number of tasks in the cluster object store swarm_node_manager gauge ip, cls, ins, job, instance Whether this node is a manager or not swarm_raft_snapshot_latency_seconds_bucket Unknown ip, cls, ins, job, instance, le N/A swarm_raft_snapshot_latency_seconds_count Unknown ip, cls, ins, job, instance N/A swarm_raft_snapshot_latency_seconds_sum Unknown ip, cls, ins, job, instance N/A swarm_raft_transaction_latency_seconds_bucket Unknown ip, cls, ins, job, instance, le N/A swarm_raft_transaction_latency_seconds_count Unknown ip, cls, ins, job, instance N/A swarm_raft_transaction_latency_seconds_sum Unknown ip, cls, ins, job, instance N/A swarm_store_batch_latency_seconds_bucket Unknown ip, cls, ins, job, instance, le N/A swarm_store_batch_latency_seconds_count Unknown ip, cls, ins, job, instance N/A swarm_store_batch_latency_seconds_sum Unknown ip, cls, ins, job, instance N/A swarm_store_lookup_latency_seconds_bucket Unknown ip, cls, ins, job, instance, le N/A swarm_store_lookup_latency_seconds_count Unknown ip, cls, ins, job, instance N/A swarm_store_lookup_latency_seconds_sum Unknown ip, cls, ins, job, instance N/A swarm_store_memory_store_lock_duration_seconds_bucket Unknown ip, cls, ins, job, instance, le N/A swarm_store_memory_store_lock_duration_seconds_count Unknown ip, cls, ins, job, instance N/A swarm_store_memory_store_lock_duration_seconds_sum Unknown ip, cls, ins, job, instance N/A swarm_store_read_tx_latency_seconds_bucket Unknown ip, cls, ins, job, instance, le N/A swarm_store_read_tx_latency_seconds_count Unknown ip, cls, ins, job, instance N/A swarm_store_read_tx_latency_seconds_sum Unknown ip, cls, ins, job, instance N/A swarm_store_write_tx_latency_seconds_bucket Unknown ip, cls, ins, job, instance, le N/A swarm_store_write_tx_latency_seconds_count Unknown ip, cls, ins, job, instance N/A swarm_store_write_tx_latency_seconds_sum Unknown ip, cls, ins, job, instance N/A up Unknown ip, cls, ins, job, instance N/A ","categories":["Reference"],"description":"Complete list of monitoring metrics provided by the Pigsty Docker module","excerpt":"Complete list of monitoring metrics provided by the Pigsty Docker …","ref":"/docs/docker/metric/","tags":"","title":"Metrics"},{"body":" Who Can Run Docker Commands? By default, Pigsty adds both the management user running the playbook on the remote node (i.e., the SSH login user on the target node) and the admin user specified in the node_admin_username parameter to the Docker operating system group. All users in this group (docker) can manage Docker using the docker CLI command.\nIf you want other users to be able to run Docker commands, add that OS user to the docker group:\nusermod -aG docker \u003cusername\u003e Working Through a Proxy During Docker installation, if the proxy_env parameter exists, the HTTP proxy server configuration will be written to the /etc/docker/daemon.json configuration file.\nDocker will use this proxy server when pulling images from upstream registries.\nTip: Running configure with the -x flag will write the proxy server configuration from your current environment into proxy_env.\nUsing Mirror Registries If you’re in mainland China and affected by the Great Firewall, you can consider using Docker mirror sites available within China, such as quay.io:\ndocker login quay.io # Enter username and password to log in Update (June 2024): All previously accessible Docker mirror sites in China have been blocked. Please use a proxy server to access and pull images.\nAdding Docker to Monitoring During Docker module installation, you can register Docker as a monitoring target by running the docker_register or register_prometheus subtask for specific nodes:\n./docker.yml -l \u003cyour-node-selector\u003e -t register_prometheus Using Software Templates Pigsty provides a collection of software templates that can be launched using Docker Compose, ready to use out of the box.\nBut you need to install the Docker module first.\n","categories":["Reference"],"description":"Frequently asked questions about the Pigsty Docker module","excerpt":"Frequently asked questions about the Pigsty Docker module","ref":"/docs/docker/faq/","tags":"","title":"FAQ"},{"body":"","categories":["Reference"],"description":"Extra modules in pilot development.","excerpt":"Extra modules in pilot development.","ref":"/docs/pilot/","tags":"","title":"Module: PILOT"},{"body":" MySQL used to be the “most popular open-source relational database in the world”.\nInstallation | Configuration | Administration | Playbook | Monitoring | Parameters\nOverview MySQL module is currently available in Pigsty Pro as a Beta Preview. Note that you should NOT use this MySQL deployment for production environments.\nInstallation You can install MySQL 8.0 from the official software source on EL systems directly on the nodes managed by Pigsty.\n# el 7,8,9 ./node.yml -t node_install -e '{\"node_repo_modules\":\"node,mysql\",\"node_packages\":[\"mysql-community-server,mysql-community-client\"]}' # debian / ubuntu ./node.yml -t node_install -e '{\"node_repo_modules\":\"node,mysql\",\"node_packages\":[\"mysql-server\"]}' You can also add the MySQL package to the local repo and use the playbook mysql.yml for production deployment.\nConfiguration This config snippet defines a single-node MySQL instance, along with its Databases and Users.\nmy-test: hosts: { 10.10.10.10: { mysql_seq: 1, mysql_role: primary } } vars: mysql_cluster: my-test mysql_databases: - { name: meta } mysql_users: - { name: dbuser_meta ,host: '%' ,password: 'dbuesr_meta' ,priv: { \"*.*\": \"SELECT, UPDATE, DELETE, INSERT\" } } - { name: dbuser_dba ,host: '%' ,password: 'DBUser.DBA' ,priv: { \"*.*\": \"ALL PRIVILEGES\" } } - { name: dbuser_monitor ,host: '%' ,password: 'DBUser.Monitor' ,priv: { \"*.*\": \"SELECT, PROCESS, REPLICATION CLIENT\" } ,connlimit: 3 } Administration Here are some basic MySQL cluster management operations:\nCreate MySQL cluster with mysql.yml:\n./mysql.yml -l my-test Playbook Pigsty has the following playbooks related to the MYSQL module:\nmysql.yml: Deploy MySQL according to the inventory mysql.yml The playbook mysql.yml contains the following subtasks:\nmysql-id : generate mysql instance identity mysql_clean : remove existing mysql instance (DANGEROUS) mysql_dbsu : create os user mysql mysql_install : install mysql rpm/deb packages mysql_dir : create mysql data \u0026 conf dir mysql_config : generate mysql config file mysql_boot : bootstrap mysql cluster mysql_launch : launch mysql service mysql_pass : write mysql password mysql_db : create mysql biz database mysql_user : create mysql biz user mysql_exporter : launch mysql exporter mysql_register : register mysql service to prometheus Monitoring Pigsty has two built-in MYSQL dashboards:\nMYSQL Overview: MySQL cluster overview\nMYSQL Instance: MySQL instance overview\nParameters MySQL’s available parameters:\n#----------------------------------------------------------------- # MYSQL_IDENTITY #----------------------------------------------------------------- # mysql_cluster: #CLUSTER # mysql cluster name, required identity parameter # mysql_role: replica #INSTANCE # mysql role, required, could be primary,replica # mysql_seq: 0 #INSTANCE # mysql instance seq number, required identity parameter #----------------------------------------------------------------- # MYSQL_BUSINESS #----------------------------------------------------------------- # mysql business object definition, overwrite in group vars mysql_users: [] # mysql business users mysql_databases: [] # mysql business databases mysql_services: [] # mysql business services # global credentials, overwrite in global vars mysql_root_username: root mysql_root_password: DBUser.Root mysql_replication_username: replicator mysql_replication_password: DBUser.Replicator mysql_admin_username: dbuser_dba mysql_admin_password: DBUser.DBA mysql_monitor_username: dbuser_monitor mysql_monitor_password: DBUser.Monitor #----------------------------------------------------------------- # MYSQL_INSTALL #----------------------------------------------------------------- # - install - # mysql_dbsu: mysql # os dbsu name, mysql by default, better not change it mysql_dbsu_uid: 27 # os dbsu uid and gid, 306 for default mysql users and groups mysql_dbsu_home: /var/lib/mysql # mysql home directory, `/var/lib/mysql` by default mysql_dbsu_ssh_exchange: true # exchange mysql dbsu ssh key among same mysql cluster mysql_packages: # mysql packages to be installed, `mysql-community*` by default - mysql-community* - mysqld_exporter # - bootstrap - # mysql_data: /data/mysql # mysql data directory, `/data/mysql` by default mysql_listen: '0.0.0.0' # mysql listen addresses, comma separated IP list mysql_port: 3306 # mysql listen port, 3306 by default mysql_sock: /var/lib/mysql/mysql.sock # mysql socket dir, `/var/lib/mysql/mysql.sock` by default mysql_pid: /var/run/mysqld/mysqld.pid # mysql pid file, `/var/run/mysqld/mysqld.pid` by default mysql_conf: /etc/my.cnf # mysql config file, `/etc/my.cnf` by default mysql_log_dir: /var/log # mysql log dir, `/var/log/mysql` by default mysql_exporter_port: 9104 # mysqld_exporter listen port, 9104 by default mysql_parameters: {} # extra parameters for mysqld mysql_default_parameters: # default parameters for mysqld ","categories":["Reference"],"description":"Deploy a MySQL 8.0 cluster with Pigsty for demonstration or benchmarking purposes.","excerpt":"Deploy a MySQL 8.0 cluster with Pigsty for demonstration or …","ref":"/docs/pilot/mysql/","tags":"","title":"Module: MySQL"},{"body":" Kafka is an open-source distributed event streaming platform: Installation | Configuration | Administration | Playbook | Monitoring | Parameters | Resources\nOverview Kafka module is currently available in Pigsty Pro as a Beta Preview.\nInstallation If you are using the open-source version of Pigsty, you can install Kafka and its Java dependencies on the specified node using the following command.\nPigsty provides Kafka 3.8.0 RPM and DEB packages in the official Infra repository, which can be downloaded and installed directly.\n./node.yml -t node_install -e '{\"node_repo_modules\":\"infra\",\"node_packages\":[\"kafka\"]}' Kafka requires a Java runtime environment, so you need to install an available JDK when installing Kafka (OpenJDK 17 is used by default, but other JDKs and versions, such as 8 and 11, can also be used).\n# EL7 (no JDK 17 support) ./node.yml -t node_install -e '{\"node_repo_modules\":\"node\",\"node_packages\":[\"java-11-openjdk-headless\"]}' # EL8 / EL9 (use OpenJDK 17) ./node.yml -t node_install -e '{\"node_repo_modules\":\"node\",\"node_packages\":[\"java-17-openjdk-headless\"]}' # Debian / Ubuntu (use OpenJDK 17) ./node.yml -t node_install -e '{\"node_repo_modules\":\"node\",\"node_packages\":[\"openjdk-17-jdk\"]}' Configuration Single node Kafka configuration example. Please note that in Pigsty single machine deployment mode, the 9093 port on the admin node is already occupied by AlertManager.\nIt is recommended to use other ports when installing Kafka on the admin node, such as (9095).\nkf-main: hosts: 10.10.10.10: { kafka_seq: 1, kafka_role: controller } vars: kafka_cluster: kf-main kafka_data: /data/kafka kafka_peer_port: 9095 # 9093 is already hold by alertmanager 3-node Kraft mode Kafka cluster configuration example:\nkf-test: hosts: 10.10.10.11: { kafka_seq: 1, kafka_role: controller } 10.10.10.12: { kafka_seq: 2, kafka_role: controller } 10.10.10.13: { kafka_seq: 3, kafka_role: controller } vars: kafka_cluster: kf-test Administration Here are some basic Kafka cluster management operations:\nCreate Kafka clusters with kafka.yml playbook:\n./kafka.yml -l kf-main ./kafka.yml -l kf-test Create a topic named test:\nkafka-topics.sh --create --topic test --partitions 1 --replication-factor 1 --bootstrap-server localhost:9092 Here the --replication-factor 1 means each data will be replicated once, and --partitions 1 means only one partition will be created.\nUse the following command to view the list of Topics in Kafka:\nkafka-topics.sh --bootstrap-server localhost:9092 --list Use the built-in Kafka producer to send messages to the test Topic:\nkafka-console-producer.sh --topic test --bootstrap-server localhost:9092 \u003ehaha \u003exixi \u003ehoho \u003ehello \u003eworld \u003e ^D Use the built-in Kafka consumer to read messages from the test Topic:\nkafka-console-consumer.sh --topic test --from-beginning --bootstrap-server localhost:9092 Playbook Pigsty provides 1 playbook related to the Kafka module for managing Kafka clusters.\nkafka.yml The kafka.yml playbook for deploying Kafka KRaft mode cluster contains the following subtasks:\nkafka-id : generate kafka instance identity kafka_clean : remove existing kafka instance (DANGEROUS) kafka_user : create os user kafka kafka_pkg : install kafka rpm/deb packages kafka_link : create symlink to /usr/kafka kafka_path : add kafka bin path to /etc/profile.d kafka_svc : install kafka systemd service kafka_dir : create kafka data \u0026 conf dir kafka_config : generate kafka config file kafka_boot : bootstrap kafka cluster kafka_launch : launch kafka service kafka_exporter : launch kafka exporter kafka_register : register kafka service to prometheus Monitoring Pigsty has provided two monitoring panels related to the KAFKA module:\nKAFKA Overview shows the overall monitoring metrics of the Kafka cluster.\nKAFKA Instance shows the monitoring metrics details of a single Kafka instance.\nParameters Available parameters for Kafka module:\n#kafka_cluster: #CLUSTER # kafka cluster name, required identity parameter #kafka_role: controller #INSTANCE # kafka role, controller, broker, or controller-only #kafka_seq: 0 #INSTANCE # kafka instance seq number, required identity parameter kafka_clean: false # cleanup kafka during init? false by default kafka_data: /data/kafka # kafka data directory, `/data/kafka` by default kafka_version: 3.8.0 # kafka version string scala_version: 2.13 # kafka binary scala version kafka_port: 9092 # kafka broker listen port kafka_peer_port: 9093 # kafka broker peer listen port, 9093 by default (conflict with alertmanager) kafka_exporter_port: 9308 # kafka exporter listen port, 9308 by default kafka_parameters: # kafka parameters to be added to server.properties num.network.threads: 3 num.io.threads: 8 socket.send.buffer.bytes: 102400 socket.receive.buffer.bytes: 102400 socket.request.max.bytes: 104857600 num.partitions: 1 num.recovery.threads.per.data.dir: 1 offsets.topic.replication.factor: 1 transaction.state.log.replication.factor: 1 transaction.state.log.min.isr: 1 log.retention.hours: 168 log.segment.bytes: 1073741824 log.retention.check.interval.ms: 300000 #log.retention.bytes: 1073741824 #log.flush.interval.ms: 1000 #log.flush.interval.messages: 10000 Resources Pigsty provides some Kafka-related extension plugins for PostgreSQL:\nkafka_fdw: A useful FDW that allows users to read and write Kafka Topic data directly from PostgreSQL wal2json: Used to logically decode WAL from PostgreSQL and generate JSON-formatted change data wal2mongo: Used to logically decode WAL from PostgreSQL and generate BSON-formatted change data decoder_raw: Used to logically decode WAL from PostgreSQL and generate SQL-formatted change data test_decoding: Used to logically decode WAL from PostgreSQL and generate RAW-formatted change data ","categories":["Reference"],"description":"Deploy Kafka KRaft cluster with Pigsty: open-source distributed event streaming platform","excerpt":"Deploy Kafka KRaft cluster with Pigsty: open-source distributed event …","ref":"/docs/pilot/kafka/","tags":"","title":"Module: Kafka"},{"body":" DuckDB is a fast in-process analytical database: Installation | Resources\nOverview DuckDB is an embedded database, so it does not require deployment or service management. You only need to install the DuckDB package on the node to use it.\nInstallation Pigsty already provides DuckDB software package (RPM / DEB) in the Infra software repository, you can install it with the following command:\n./node.yml -t node_install -e '{\"node_repo_modules\":\"infra\",\"node_packages\":[\"duckdb\"]}' Resources There are some DuckDB-related extension plugins provided by Pigsty for PostgreSQL:\npg_analytics: Add OLAP capabilities to PostgreSQL based on DuckDB pg_lakehouse: Data lakehouse plugin by ParadeDB, wrapping DuckDB. (Currently planned to be renamed back to pg_analytics) duckdb_fdw: Foreign data wrapper for DuckDB, read/write DuckDB data files from PG pg_duckdb: WIP extension plugin by DuckDB official MotherDuck and Hydra (only available on EL systems as a pilot) ","categories":["Reference"],"description":"Install DuckDB, a high-performance embedded analytical database component.","excerpt":"Install DuckDB, a high-performance embedded analytical database …","ref":"/docs/pilot/duckdb/","tags":"","title":"Module: DuckDB"},{"body":" TigerBeetle is a financial accounting transaction database offering extreme performance and reliability.\nOverview The TigerBeetle module is currently available for Beta preview only in the Pigsty Professional Edition.\nInstallation Pigsty Infra Repo has the RPM / DEB packages for TigerBeetle, use the following command to install:\n./node.yml -t node_install -e '{\"node_repo_modules\":\"infra\",\"node_packages\":[\"tigerbeetle\"]}' After installation, please refer to the official documentation for configuration: https://github.com/tigerbeetle/tigerbeetle\nTigerBeetle Requires Linux Kernel Version 5.5 or Higher! Please note that TigerBeetle supports only Linux kernel version 5.5 or higher, making it incompatible by default with EL7 (3.10) and EL8 (4.18) systems.\nTo install TigerBeetle, please use EL9 (5.14), Ubuntu 22.04 (5.15), Debian 12 (6.1), Debian 11 (5.10), or another supported system.\n","categories":["Reference"],"description":"Deploy TigerBeetle, the Financial Transactions Database that is 1000x faster.","excerpt":"Deploy TigerBeetle, the Financial Transactions Database that is 1000x …","ref":"/docs/pilot/tigerbeetle/","tags":"","title":"Module: TigerBeetle"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/_div_misc/","tags":"","title":"Miscellaneous"},{"body":"Kubernetes is a production-grade, open-source container orchestration platform. It helps you automate, deploy, scale, and manage containerized applications.\nPigsty has native support for ETCD clusters, which can be used by Kubernetes. Therefore, the pro version also provides the KUBE module for deploying production-grade Kubernetes clusters.\nThe KUBE module is currently in Beta status and only available for Pro edition customers.\nHowever, you can directly specify node repositories in Pigsty, install Kubernetes packages, and use Pigsty to adjust environment configurations and provision nodes for K8S deployment, solving the last mile delivery problem.\nSealOS SealOS is a lightweight, high-performance, and easy-to-use Kubernetes distribution. It is designed to simplify the deployment and management of Kubernetes clusters.\nPigsty provides SealOS 5.0 RPM and DEB packages in the Infra repository, which can be downloaded and installed directly, and use SealOS to manage clusters.\n./node.yml -t node_install -e '{\"node_repo_modules\":\"infra\",\"node_packages\":[\"sealos\"]}' Kubernetes If you prefer to deploy Kubernetes using the classic Kubeadm, please refer to the module reference below.\n./node.yml -t node_install -e '{\"node_repo_modules\":\"kube\",\"node_packages\":[\"kubeadm,kubelet,kubectl\"]}' Kubernetes supports multiple container runtimes. If you want to use Containerd as the container runtime, please make sure Containerd is installed on the node.\n./node.yml -t node_install -e '{\"node_repo_modules\":\"node,docker\",\"node_packages\":[\"containerd.io\"]}' If you want to use Docker as the container runtime, you need to install Docker and bridge with the cri-dockerd project (not available on EL9/D11/U20 yet):\n./node.yml -t node_install -e '{\"node_repo_modules\":\"node,infra,docker\",\"node_packages\":[\"docker-ce,docker-compose-plugin,cri-dockerd\"]}' Playbook kube.yml playbook (TBD)\nMonitoring TBD\nParameters Kubernetes module parameters:\n#kube_cluster: #IDENTITY# # define kubernetes cluster name kube_role: node # default kubernetes role (master|node) kube_version: 1.31.0 # kubernetes version kube_registry: registry.aliyuncs.com/google_containers # kubernetes version aliyun k8s miiror repository kube_pod_cidr: \"10.11.0.0/16\" # kubernetes pod network cidr kube_service_cidr: \"10.12.0.0/16\" # kubernetes service network cidr kube_dashboard_admin_user: dashboard-admin-sa # kubernetes dashboard admin user name ","categories":["Reference"],"description":"Deploy Kubernetes, the Production-Grade Container Orchestration Platform.","excerpt":"Deploy Kubernetes, the Production-Grade Container Orchestration …","ref":"/docs/pilot/kube/","tags":"","title":"Module: Kubernetes"},{"body":"Consul is a distributed DCS + KV + DNS + service registry/discovery component.\nIn the old version (1.x) of Pigsty, Consul was used as the default high-availability DCS. Now this support has been removed, but it will be provided as a separate module in the future.\nhttps://github.com/pgsty/pigsty/tree/v1.5.1/roles/consul Configuration To deploy Consul, you need to add the IP addresses and hostnames of all nodes to the consul group.\nAt least one node should be designated as the consul server with consul_role: server, while other nodes default to consul_role: node.\nconsul: hosts: 10.10.10.10: { nodename: meta , consul_role: server } 10.10.10.11: { nodename: node-1 } 10.10.10.12: { nodename: node-2 } 10.10.10.13: { nodename: node-3 } For production deployments, we recommend using an odd number of Consul Servers, preferably three.\nParameters #----------------------------------------------------------------- # CONSUL #----------------------------------------------------------------- consul_role: node # consul role, node or server, node by default consul_dc: pigsty # consul data center name, `pigsty` by default consul_data: /data/consul # consul data dir, `/data/consul` consul_clean: true # consul purge flag, if true, clean consul during init consul_ui: false # enable consul ui, the default value for consul server is true ","categories":["Reference"],"description":"Deploy Consul, the alternative to Etcd, with Pigsty.","excerpt":"Deploy Consul, the alternative to Etcd, with Pigsty.","ref":"/docs/pilot/consul/","tags":"","title":"Module: Consul"},{"body":" VictoriaMetrics is the in-place replacement for Prometheus, offering better performance and compression ratio.\nOverview Victoria is currently only available in the Pigsty Professional Edition Beta preview. It includes the deployment and management of VictoriaMetrics and VictoriaLogs components.\nInstallation Pigsty Infra Repo has the RPM / DEB packages for VictoriaMetrics, use the following command to install:\n./node.yml -t node_install -e '{\"node_repo_modules\":\"infra\",\"node_packages\":[\"victoria-metrics\"]}' ./node.yml -t node_install -e '{\"node_repo_modules\":\"infra\",\"node_packages\":[\"victoria-metrics-cluster\"]}' ./node.yml -t node_install -e '{\"node_repo_modules\":\"infra\",\"node_packages\":[\"victoria-metrics-utils\"]}' ./node.yml -t node_install -e '{\"node_repo_modules\":\"infra\",\"node_packages\":[\"victoria-logs\"]}' For common users, installing the standalone version of VictoriaMetrics is sufficient. If you need to deploy a cluster, you can install the victoria-metrics-cluster package.\n","categories":["Reference"],"description":"Deploy VictoriaMetrics \u0026 VictoriaLogs, the in-place replacement for Prometheus \u0026 Loki.","excerpt":"Deploy VictoriaMetrics \u0026 VictoriaLogs, the in-place replacement for …","ref":"/docs/pilot/victoria/","tags":"","title":"Module: Victoria"},{"body":"Run Jupyter notebook with Docker, you have to:\nChange the default password in .env: JUPYTER_TOKEN Create data dir with proper permission: make dir, owned by 1000:100 make up to pull up Jupyter with docker compose cd ~/pigsty/app/jupyter ; make dir up Visit http://lab.pigsty or http://10.10.10.10:8888, the default password is pigsty\nhttp://lab.pigsty?token=pigsty Prepare Create a data directory /data/jupyter, with the default uid \u0026 gid 1000:100:\nmake dir # mkdir -p /data/jupyter; chown -R 1000:100 /data/jupyter Connect to Postgres Use the Jupyter terminal to install psycopg2-binary \u0026 psycopg2 package.\npip install psycopg2-binary psycopg2 # install with a mirror pip install -i https://pypi.tuna.tsinghua.edu.cn/simple psycopg2-binary psycopg2 pip install --upgrade pip pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple Or installation with conda:\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ Then use the driver in your notebook:\nimport psycopg2 conn = psycopg2.connect('postgres://dbuser_dba:DBUser.DBA@10.10.10.10:5432/meta') cursor = conn.cursor() cursor.execute('SELECT * FROM pg_stat_activity') for i in cursor.fetchall(): print(i) Alias make up # pull up jupyter with docker compose make dir # create required /data/jupyter and set owner make run # launch jupyter with docker make view # print jupyter access point make log # tail -f jupyter logs make info # introspect jupyter with jq make stop # stop jupyter container make clean # remove jupyter container make pull # pull latest jupyter image make rmi # remove jupyter image make save # save jupyter image to /tmp/docker/jupyter.tgz make load # load jupyter image from /tmp/docker/jupyter.tgz ","categories":["Reference"],"description":"Launch Jupyter notebook server with Pigsty, a web-based interactive scientific notebook.","excerpt":"Launch Jupyter notebook server with Pigsty, a web-based interactive …","ref":"/docs/pilot/jupyter/","tags":"","title":"Module: Jupyter"},{"body":"— Postgres Install Genius, the missing extension package manager for the PostgreSQL ecosystem\nPIG is a command-line tool specifically designed for installing, managing, and building PostgreSQL and its extensions. Developed in Go, it’s ready to use out of the box, simple, and lightweight (4MB). PIG is not a reinvented wheel, but rather a PiggyBack — a high-level abstraction layer that leverages existing Linux distribution package managers (apt/dnf). It abstracts away the differences between operating systems, chip architectures, and PG major versions, allowing you to install and manage PG kernels and 431+ extensions with just a few simple commands.\nNote: For extension installation, pig is not a mandatory component — you can still use apt/dnf package managers to directly access the Pigsty PGSQL repository.\nIntroduction: Why do we need a dedicated PG package manager? Getting Started: Quick start guide and examples Installation: Download, install, and update pig Quick Start Use the following command to install PIG on your system:\nDefault Installation (Cloudflare CDN):\ncurl -fsSL https://repo.pigsty.io/pig | bash China Mirror:\ncurl -fsSL https://repo.pigsty.cc/pig | bash After installation, you can get started with just a few commands. For example, to install PG 18 and the pg_duckdb extension:\n$ pig repo set # One-time setup for Linux, Pigsty + PGDG repos (overwrites!) $ pig install pg18 # Install PostgreSQL 18 kernel (native PGDG packages) $ pig install pg_duckdb -v 18 # Install pg_duckdb extension (for PG 18) $ pig install -y postgis timescaledb # Install multiple extensions for current active PG version $ pig install -y vector # You can use extension name (vector) or package name (pgvector)! Command Reference Run pig help \u003ccommand\u003e to get detailed help for subcommands.\npig repo: Manage software repositories pig ext: Manage PG extensions pig build: Set up build environment pig sty: Manage Pigsty About The pig CLI tool is developed by Vonng (rh@vonng.com) and is open-sourced under the Apache 2.0 license.\nYou can also check out the PIGSTY project, which provides a complete PostgreSQL RDS DBaaS experience including extension delivery.\nPGEXT: Extension data and management tools PIG: PostgreSQL package manager PIGSTY: Batteries-included PostgreSQL distribution ","categories":"","description":"PostgreSQL Extension Ecosystem Package Manager","excerpt":"PostgreSQL Extension Ecosystem Package Manager","ref":"/docs/pig/","tags":"","title":"PIG the PGPM"},{"body":"Here’s a simple getting started tutorial to help you experience the core capabilities of the PIG package manager.\nShort Version curl -fsSL https://repo.pigsty.io/pig | bash # Install PIG from Cloudflare pig repo set # One-time setup for Linux, Pigsty + PGDG repos (overwrites!) pig install -v 18 -y pg18 pg_duckdb vector # Install PG 18 kernel, pg_duckdb, pgvector extensions... Installation You can install pig with the following command:\nDefault Installation (Cloudflare CDN):\ncurl -fsSL https://repo.pigsty.io/pig | bash China Mirror:\ncurl -fsSL https://repo.pigsty.cc/pig | bash The PIG binary is approximately 4 MB and will automatically use rpm or dpkg to install the latest available version on Linux:\n[INFO] kernel = Linux [INFO] machine = x86_64 [INFO] package = rpm [INFO] pkg_url = https://repo.pigsty.io/pkg/pig/v0.9.0/pig-0.9.0-1.x86_64.rpm [INFO] download = /tmp/pig-0.7.2-1.x86_64.rpm [INFO] downloading pig v0.7.2 curl -fSL https://repo.pigsty.io/pkg/pig/v0.7.2/pig-0.7.2-1.x86_64.rpm -o /tmp/pig-0.7.2-1.x86_64.rpm ######################################################################## 100.0% [INFO] md5sum = 85d75c16dfd3ce935d9d889fae345430 [INFO] installing: rpm -ivh /tmp/pig-0.7.2-1.x86_64.rpm Verifying... ################################# [100%] Preparing... ################################# [100%] Updating / installing... 1:pig-0.7.2-1 ################################# [100%] [INFO] pig v0.7.2 installed successfully check https://ext.pigsty.io for details Check Environment PIG is a Go-written binary program, installed by default at /usr/bin/pig. pig version prints version information:\n$ pig version pig version 0.7.2 linux/amd64 build: HEAD 9cdb57a 2025-11-10T11:14:17Z Use the pig status command to print the current environment status, OS code, PG installation status, and repository accessibility with latency.\n$ pig status # [Configuration] ================================ Pig Version : 0.7.2 Pig Config : /root/.pig/config.yml Log Level : info Log Path : stderr # [OS Environment] =============================== OS Distro Code : el10 OS OSArch : amd64 OS Package Type : rpm OS Vendor ID : rocky OS Version : 10 OS Version Full : 10.0 OS Version Code : el10 # [PG Environment] =============================== No PostgreSQL installation found No active PostgreSQL found in PATH: - /root/.local/bin - /root/bin - /usr/local/sbin - /usr/local/bin - /usr/sbin - /usr/bin # [Pigsty Environment] =========================== Inventory Path : Not Found Pigsty Home : Not Found # [Network Conditions] =========================== pigsty.cc ping ok: 612 ms pigsty.io ping ok: 1222 ms google.com request error Internet Access : true Pigsty Repo : pigsty.io Inferred Region : china Latest Pigsty Ver : v3.6.1 List Extensions Use the pig ext list command to print the built-in PG extension data catalog.\n[root@pg-meta ~]# pig ext list Name Version Cate Flags License RPM DEB PG Ver Description ---- ------- ---- ------ ------- ------ ------ ------ --------------------- timescaledb 2.23.0 TIME -dsl-- Timescale PIGSTY PIGSTY 15-18 Enables scalable inserts and complex queries for time-series dat... timescaledb_toolkit 1.22.0 TIME -ds-t- Timescale PIGSTY PIGSTY 15-18 Library of analytical hyperfunctions, time-series pipelining, an... timeseries 0.1.7 TIME -d---- PostgreSQL PIGSTY PIGSTY 13-18 Convenience API for time series stack periods 1.2.3 TIME -ds--- PostgreSQL PGDG PGDG 13-18 Provide Standard SQL functionality for PERIODs and SYSTEM VERSIO... temporal_tables 1.2.2 TIME -ds--r BSD 2-Clause PIGSTY PIGSTY 13-18 temporal tables ......... pg_fact_loader 2.0.1 ETL -ds--x MIT PGDG PGDG 13-18 build fact tables with Postgres pg_bulkload 3.1.22 ETL bds--- BSD 3-Clause PGDG PIGSTY 13-17 pg_bulkload is a high speed data loading utility for PostgreSQL test_decoding - ETL --s--x PostgreSQL CONTRIB CONTRIB 13-18 SQL-based test/example module for WAL logical decoding pgoutput - ETL --s--- PostgreSQL CONTRIB CONTRIB 13-18 Logical Replication output plugin (431 Rows) (Flags: b = HasBin, d = HasDDL, s = HasLib, l = NeedLoad, t = Trusted, r = Relocatable, x = Unknown) All extension metadata is defined in a data file named extension.csv. This file is updated with each pig version release. You can update it directly using the pig ext reload command. The updated file is placed in ~/.pig/extension.csv by default, which you can view and modify — you can also find the authoritative version of this data file in the project.\nAdd Repositories To install extensions, you first need to add upstream repositories. pig repo can be used to manage Linux APT/YUM/DNF software repository configuration.\nYou can use the straightforward pig repo set to overwrite existing repository configuration, ensuring only necessary repositories exist in the system:\npig repo set # One-time setup for all repos including Linux system, PGDG, PIGSTY (PGSQL+INFRA) Warning: pig repo set will backup and clear existing repository configuration, then add required repositories, implementing Overwrite semantics — please be aware!\nOr choose the gentler pig repo add to add needed repositories:\npig repo add pgdg pigsty # Add PGDG official repo and PIGSTY supplementary repo pig repo add pgsql # [Optional] You can also add PGDG and PIGSTY together as one \"pgsql\" module pig repo update # Update cache: apt update / yum makecache PIG will detect your network environment and choose to use Cloudflare global CDN or China cloud CDN, but you can force a specific region with the --region parameter.\npig repo set --region=china # Use China region mirror repos for faster downloads pig repo add pgdg --region=default --update # Force using PGDG upstream repo PIG itself doesn’t support offline installation. You can download RPM/DEB packages yourself and copy them to network-isolated production servers for installation. The related PIGSTY project provides local software repositories that can use pig to install already-downloaded extensions from local repos.\nInstall PG After adding repositories, you can use the pig ext add subcommand to install extensions (and related packages)\npig ext add -v 18 -y pgsql timescaledb postgis vector pg_duckdb pg_mooncake # Install PG 18 kernel and extensions, auto-confirm # This command automatically translates packages to: INFO[20:34:44] translate alias 'pgsql' to package: postgresql$v postgresql$v-server postgresql$v-libs postgresql$v-contrib postgresql$v-plperl postgresql$v-plpython3 postgresql$v-pltcl postgresql$v-llvmjit INFO[20:34:44] translate extension 'timescaledb' to package: timescaledb-tsl_18* INFO[20:34:44] translate extension 'postgis' to package: postgis36_18* INFO[20:34:44] translate extension 'vector' to package: pgvector_18* INFO[20:34:44] translate extension 'pg_duckdb' to package: pg_duckdb_18* INFO[20:34:44] translate extension 'pg_mooncake' to package: pg_mooncake_18* INFO[20:34:44] installing packages: dnf install -y postgresql18 postgresql18-server postgresql18-libs postgresql18-contrib postgresql18-plperl postgresql18-plpython3 postgresql18-pltcl postgresql18-llvmjit timescaledb-tsl_18* postgis36_18* pgvector_18* pg_duckdb_18* pg_mooncake_18* This uses an “alias translation” mechanism to translate clean PG kernel/extension logical package names into actual RPM/DEB lists. If you don’t need alias translation, you can use apt/dnf directly, or use the -n|--no-translation parameter with the variant pig install:\npig install vector # With translation, installs pgvector_18 or postgresql-18-pgvector for current PG 18 pig install vector -n # Without translation, installs the package literally named 'vector' (a log collector from pigsty-infra repo) Alias Translation PostgreSQL kernels and extensions correspond to a series of RPM/DEB packages. Remembering these packages is tedious, so pig provides many common aliases to simplify the installation process:\nFor example, on EL systems, the following aliases will be translated to the corresponding RPM package list on the right:\npgsql: \"postgresql$v postgresql$v-server postgresql$v-libs postgresql$v-contrib postgresql$v-plperl postgresql$v-plpython3 postgresql$v-pltcl postgresql$v-llvmjit\" pg18: \"postgresql18 postgresql18-server postgresql18-libs postgresql18-contrib postgresql18-plperl postgresql18-plpython3 postgresql18-pltcl postgresql18-llvmjit\" pg17-client: \"postgresql17\" pg17-server: \"postgresql17-server postgresql17-libs postgresql17-contrib\" pg17-devel: \"postgresql17-devel\" pg17-basic: \"pg_repack_17* wal2json_17* pgvector_17*\" pg16-mini: \"postgresql16 postgresql16-server postgresql16-libs postgresql16-contrib\" pg15-full: \"postgresql15 postgresql15-server postgresql15-libs postgresql15-contrib postgresql15-plperl postgresql15-plpython3 postgresql15-pltcl postgresql15-llvmjit postgresql15-test postgresql15-devel\" pg14-main: \"postgresql14 postgresql14-server postgresql14-libs postgresql14-contrib postgresql14-plperl postgresql14-plpython3 postgresql14-pltcl postgresql14-llvmjit pg_repack_14* wal2json_14* pgvector_14*\" pg13-core: \"postgresql13 postgresql13-server postgresql13-libs postgresql13-contrib postgresql13-plperl postgresql13-plpython3 postgresql13-pltcl postgresql13-llvmjit\" Note that the $v placeholder is replaced with the PG major version number, so when you use the pgsql alias, $v is actually replaced with 18, 17, etc. Therefore, when you install the pg17-server alias, on EL it actually installs postgresql17-server, postgresql17-libs, postgresql17-contrib, and on Debian/Ubuntu it installs postgresql-17 — pig handles all the details.\nCommon PostgreSQL Aliases EL alias translation list\n\"pgsql\": \"postgresql$v postgresql$v-server postgresql$v-libs postgresql$v-contrib postgresql$v-plperl postgresql$v-plpython3 postgresql$v-pltcl postgresql$v-llvmjit\", \"pgsql-mini\": \"postgresql$v postgresql$v-server postgresql$v-libs postgresql$v-contrib\", \"pgsql-core\": \"postgresql$v postgresql$v-server postgresql$v-libs postgresql$v-contrib postgresql$v-plperl postgresql$v-plpython3 postgresql$v-pltcl postgresql$v-llvmjit\", \"pgsql-full\": \"postgresql$v postgresql$v-server postgresql$v-libs postgresql$v-contrib postgresql$v-plperl postgresql$v-plpython3 postgresql$v-pltcl postgresql$v-llvmjit postgresql$v-test postgresql$v-devel\", \"pgsql-main\": \"postgresql$v postgresql$v-server postgresql$v-libs postgresql$v-contrib postgresql$v-plperl postgresql$v-plpython3 postgresql$v-pltcl postgresql$v-llvmjit pg_repack_$v* wal2json_$v* pgvector_$v*\", \"pgsql-client\": \"postgresql$v\", \"pgsql-server\": \"postgresql$v-server postgresql$v-libs postgresql$v-contrib\", \"pgsql-devel\": \"postgresql$v-devel\", \"pgsql-basic\": \"pg_repack_$v* wal2json_$v* pgvector_$v*\", Debian/Ubuntu alias translation\n\"pgsql\": \"postgresql-$v postgresql-client-$v postgresql-plpython3-$v postgresql-plperl-$v postgresql-pltcl-$v\", \"pgsql-mini\": \"postgresql-$v postgresql-client-$v\", \"pgsql-core\": \"postgresql-$v postgresql-client-$v postgresql-plpython3-$v postgresql-plperl-$v postgresql-pltcl-$v\", \"pgsql-full\": \"postgresql-$v postgresql-client-$v postgresql-plpython3-$v postgresql-plperl-$v postgresql-pltcl-$v postgresql-server-dev-$v\", \"pgsql-main\": \"postgresql-$v postgresql-client-$v postgresql-plpython3-$v postgresql-plperl-$v postgresql-pltcl-$v postgresql-$v-repack postgresql-$v-wal2json postgresql-$v-pgvector\", \"pgsql-client\": \"postgresql-client-$v\", \"pgsql-server\": \"postgresql-$v\", \"pgsql-devel\": \"postgresql-server-dev-$v\", \"pgsql-basic\": \"postgresql-$v-repack postgresql-$v-wal2json postgresql-$v-pgvector\", These aliases can be used directly and instantiated with major version numbers via parameters, or you can use alias variants with major version numbers: replacing pgsql with pg18, pg17, pgxx, etc. For example, for PostgreSQL 18, you can directly use these aliases:\npgsql pg18 pg17 pg16 pg15 pg14 pg13 pgsql pg18 pg17 pg16 pg15 pg14 pg13 pgsql-mini pg18-mini pg17-mini pg16-mini pg15-mini pg14-mini pg13-mini pgsql-core pg18-core pg17-core pg16-core pg15-core pg14-core pg13-core pgsql-full pg18-full pg17-full pg16-full pg15-full pg14-full pg13-full pgsql-main pg18-main pg17-main pg16-main pg15-main pg14-main pg13-main pgsql-client pg18-client pg17-client pg16-client pg15-client pg14-client pg13-client pgsql-server pg18-server pg17-server pg16-server pg15-server pg14-server pg13-server pgsql-devel pg18-devel pg17-devel pg16-devel pg15-devel pg14-devel pg13-devel pgsql-basic pg18-basic pg17-basic pg16-basic pg15-basic pg14-basic pg13-basic Install Extensions pig detects the PostgreSQL installation in the current system environment. If it detects an active PG installation (based on pg_config in PATH), pig will automatically install extensions for that PG major version without you explicitly specifying it.\npig install pg_smtp_client # Simpler pig install pg_smtp_client -v 18 # Explicitly specify major version, more stable and reliable pig install pg_smtp_client -p /usr/lib/postgresql/16/bin/pg_config # Another way to specify PG version dnf install pg_smtp_client_18 # Most direct... but not all extensions are this simple... Tip: To add a specific major version of PostgreSQL kernel binaries to PATH, use the pig ext link command:\npig ext link pg17 # Create /usr/pgsql symlink and write to /etc/profile.d/pgsql.sh . /etc/profile.d/pgsql.sh # Take effect immediately, update PATH environment variable If you want to install a specific version of software, you can use the name=ver syntax:\npig ext add -v 17 pgvector=0.7.2 # install pgvector 0.7.2 for PG 17 pig ext add pg16=16.5 # install PostgreSQL 16 with a specific minor version Warning: Note that currently only PGDG YUM repository provides historical extension versions. PIGSTY repository and PGDG APT repository only provide the latest version of extensions.\nShow Extensions The pig ext status command can be used to show currently installed extensions.\n$ pig ext status -v 18 Installed: - PostgreSQL 18.0 80 Extensions No active PostgreSQL found in PATH: - /root/.local/bin - /root/bin - /usr/local/sbin - /usr/local/bin - /usr/sbin - /usr/bin Extension Stat : 11 Installed (PIGSTY 3, PGDG 8) + 69 CONTRIB = 80 Total Name Version Cate Flags License Repo Package Description ---- ------- ---- ------ ------- ------ ------------ --------------------- timescaledb 2.23.0 TIME -dsl-- Timescale PIGSTY timescaledb-tsl_18* Enables scalable inserts and complex queries for time-series dat postgis 3.6.0 GIS -ds--- GPL-2.0 PGDG postgis36_18* PostGIS geometry and geography spatial types and functions postgis_topology 3.6.0 GIS -ds--- GPL-2.0 PGDG postgis36_18* PostGIS topology spatial types and functions postgis_raster 3.6.0 GIS -ds--- GPL-2.0 PGDG postgis36_18* PostGIS raster types and functions postgis_sfcgal 3.6.0 GIS -ds--r GPL-2.0 PGDG postgis36_18* PostGIS SFCGAL functions postgis_tiger_geocoder 3.6.0 GIS -ds-t- GPL-2.0 PGDG postgis36_18* PostGIS tiger geocoder and reverse geocoder address_standardizer 3.6.0 GIS -ds--r GPL-2.0 PGDG postgis36_18* Used to parse an address into constituent elements. Generally us address_standardizer_data_us 3.6.0 GIS -ds--r GPL-2.0 PGDG postgis36_18* Address Standardizer US dataset example vector 0.8.1 RAG -ds--r PostgreSQL PGDG pgvector_18* vector data type and ivfflat and hnsw access methods pg_duckdb 1.1.0 OLAP -dsl-- MIT PIGSTY pg_duckdb_18* DuckDB Embedded in Postgres pg_mooncake 0.2.0 OLAP -d---- MIT PIGSTY pg_mooncake_18* Columnstore Table in Postgres If PostgreSQL cannot be found in your current system path (based on pg_config in PATH), please make sure to specify the PG major version number or pg_config path via -v|-p.\nScan Extensions pig ext scan provides lower-level extension scanning functionality, scanning shared libraries in the specified PostgreSQL directory to discover installed extensions:\nroot@s37451:~# pig ext scan Installed: * PostgreSQL 17.6 (Debian 17.6-2.pgdg13+1) 70 Extensions - PostgreSQL 15.14 (Debian 15.14-1.pgdg13+1) 69 Extensions - PostgreSQL 14.19 (Debian 14.19-1.pgdg13+1) 66 Extensions - PostgreSQL 13.22 (Debian 13.22-1.pgdg13+1) 64 Extensions - PostgreSQL 18.0 (Debian 18.0-1.pgdg13+3) 70 Extensions - PostgreSQL 16.10 (Debian 16.10-1.pgdg13+1) 70 Extensions Active: PG Version : PostgreSQL 17.6 (Debian 17.6-2.pgdg13+1) Config Path : /usr/lib/postgresql/17/bin/pg_config Binary Path : /usr/lib/postgresql/17/bin Library Path : /usr/lib/postgresql/17/lib Extension Path : /usr/share/postgresql/17/extension Name Version SharedLibs Description Meta ---- ------- ---------- --------------------- ------ amcheck 1.4 functions for verifying relation integrity relocatable=true module_pathname=$libdir/amcheck lib=amcheck.so ... pg_duckdb 1.1.0 DuckDB Embedded in Postgres module_pathname=$libdir/pg_duckdb relocatable=false schema=public lib=libduckdb.so, pg_duckdb.so pg_mooncake 0.2.0 Real-time analytics on Postgres tables module_pathname=pg_mooncake relocatable=false requires=pg_duckdb superuser=true lib=pg_mooncake.so pg_prewarm 1.2 prewarm relation data module_pathname=$libdir/pg_prewarm relocatable=true lib=pg_prewarm.so pg_smtp_client 0.2.1 PostgreSQL extension to send email using SMTP relocatable=false superuser=false schema=smtp_client module_pathname=$libdir/pg_smtp_client lib=pg_smtp_client.so ... Encoding Libs: cyrillic_and_mic, euc2004_sjis2004, euc_cn_and_mic, euc_jp_and_sjis, euc_kr_and_mic, euc_tw_and_big5, latin2_and_win1250, latin_and_mic, utf8_and_big5, utf8_and_cyrillic, utf8_and_euc2004, utf8_and_euc_cn, utf8_and_euc_jp, utf8_and_euc_kr, utf8_and_euc_tw, utf8_and_gb18030, utf8_and_gbk, utf8_and_iso8859, utf8_and_iso8859_1, utf8_and_johab, utf8_and_sjis, utf8_and_sjis2004, utf8_and_uhc, utf8_and_win Built-in Libs: dict_snowball, libpqwalreceiver, llvmjit Container Practice You can create a fresh virtual machine, or use the following Docker container for testing. Create a d13 directory with a Dockerfile:\nFROM debian:13 USER root WORKDIR /root/ CMD [\"/bin/bash\"] RUN apt update \u0026\u0026 apt install -y ca-certificates curl \u0026\u0026 curl https://repo.pigsty.io/pig | bash docker build -t d13:latest . docker run -it d13:latest /bin/bash pig repo set --region=china # Add China region repositories pig install -y pg18 # Install PGDG 18 kernel packages pig install -y postgis timescaledb pgvector pg_duckdb ","categories":["Tutorial"],"description":"Quick start with pig, the PostgreSQL package manager","excerpt":"Quick start with pig, the PostgreSQL package manager","ref":"/docs/pig/start/","tags":"","title":"Getting Started"},{"body":"Have you ever struggled with installing or upgrading PostgreSQL extensions? Digging through outdated documentation, cryptic configuration scripts, or searching GitHub for forks and patches? Postgres’s rich extension ecosystem also means complex deployment processes — especially tricky across multiple distributions and architectures. PIG can solve these headaches for you.\nThis is exactly why Pig was created. Developed in Go, Pig is dedicated to one-stop management of Postgres and its 430+ extensions. Whether it’s TimescaleDB, Citus, PGVector, 30+ Rust extensions, or all the components needed to self-host Supabase — Pig’s unified CLI makes everything accessible. It completely eliminates source compilation and messy repositories, directly providing version-aligned RPM/DEB packages that perfectly support Debian, Ubuntu, RedHat, and other mainstream distributions on both x86 and Arm architectures — no guessing, no hassle.\nPig isn’t reinventing the wheel; it fully leverages native system package managers (APT, YUM, DNF) and strictly follows PGDG official packaging standards for seamless integration. You don’t need to choose between “the standard way” and “shortcuts”; Pig respects existing repositories, follows OS best practices, and coexists harmoniously with existing repositories and packages. If your Linux system and PostgreSQL major version aren’t in the supported list, you can use pig build to compile extensions for your specific combination.\nWant to supercharge your Postgres and escape the hassle? Visit the PIG official documentation for guides and check out the extensive extension list, turning your local Postgres database into an all-capable multi-modal data platform with one click. If Postgres’s future is unmatched extensibility, then Pig is the magic lamp that helps you unlock it. After all, no one ever complains about “too many extensions.”\nANNOUNCE pig: The Postgres Extension Wizard\nLinux Compatibility PIG and the Pigsty extension repository support the following Linux distribution and PostgreSQL version combinations:\nOS Code Vendor Major Minor Full Name PG Versions Notes el7.x86_64 EL 7 7.9 CentOS 7 x86 13-15 EOL el8.x86_64 EL 8 8.10 RockyLinux 8 x86 13-18 Near EOL el8.aarch64 EL 8 8.10 RockyLinux 8 ARM 13-18 Near EOL el9.x86_64 EL 9 9.6 RockyLinux 9 x86 13-18 ✅ el9.aarch64 EL 9 9.6 RockyLinux 9 ARM 13-18 ✅ el10.x86_64 EL 10 10.0 RockyLinux 10 x86 13-18 ✅ el10.aarch64 EL 10 10.0 RockyLinux 10 ARM 13-18 ✅ d11.x86_64 Debian 11 11.11 Debian 11 x86 13-18 EOL d11.aarch64 Debian 11 11.11 Debian 11 ARM 13-18 EOL d12.x86_64 Debian 12 12.12 Debian 12 x86 13-18 ✅ d12.aarch64 Debian 12 12.12 Debian 12 ARM 13-18 ✅ d13.x86_64 Debian 13 13.1 Debian 13 x86 13-18 ✅ d13.aarch64 Debian 13 13.1 Debian 13 ARM 13-18 ✅ u20.x86_64 Ubuntu 20 20.04.6 Ubuntu 20.04 x86 13-18 EOL u20.aarch64 Ubuntu 20 20.04.6 Ubuntu 20.04 ARM 13-18 EOL u22.x86_64 Ubuntu 22 22.04.5 Ubuntu 22.04 x86 13-18 ✅ u22.aarch64 Ubuntu 22 22.04.5 Ubuntu 22.04 ARM 13-18 ✅ u24.x86_64 Ubuntu 24 24.04.3 Ubuntu 24.04 x86 13-18 ✅ u24.aarch64 Ubuntu 24 24.04.3 Ubuntu 24.04 ARM 13-18 ✅ Notes:\nEL refers to RHEL-compatible distributions, including RHEL, CentOS, RockyLinux, AlmaLinux, OracleLinux, etc. EOL indicates the operating system has reached or is about to reach end of support; upgrading to a newer version is recommended ✅ indicates full support; recommended for use PG versions 13-18 means support for PostgreSQL 13, 14, 15, 16, 17, and 18 major versions ","categories":["Concept"],"description":"Why do we need yet another package manager? Especially for Postgres extensions?","excerpt":"Why do we need yet another package manager? Especially for Postgres …","ref":"/docs/pig/intro/","tags":"","title":"Introduction"},{"body":"Script Installation The simplest way to install pig is to run the following installation script:\nDefault Installation (Cloudflare CDN):\ncurl -fsSL https://repo.pigsty.io/pig | bash China Mirror:\ncurl -fsSL https://repo.pigsty.cc/pig | bash This script downloads the latest pig RPM/DEB package from the Pigsty software repository and installs it using rpm or dpkg.\nSpecify Version You can specify a particular version to install by passing the version number as an argument:\nDefault Installation (Cloudflare CDN):\ncurl -fsSL https://repo.pigsty.io/pig | bash -s 0.9.0 China Mirror:\ncurl -fsSL https://repo.pigsty.cc/pig | bash -s 0.9.0 Download from Release Page You can also download pig installation packages (RPM/DEB/tarball) directly from the Pigsty repository: GitHub Latest Release Page\nlatest └── v0.9.0 ├── pig_0.9.0-1_amd64.deb ├── pig_0.9.0-1_arm64.deb ├── pig-0.9.0-1.aarch64.rpm ├── pig-0.9.0-1.x86_64.rpm ├── pig-v0.9.0.linux-amd64.tar.gz ├── pig-v0.9.0.linux-arm64.tar.gz ├── pig-v0.9.0.darwin-amd64.tar.gz └── pig-v0.9.0.darwin-arm64.tar.gz After extracting, place the binary file in your system PATH.\nRepository Installation The pig software is located in the pigsty-infra repository. You can add this repository to your operating system and then install using the OS package manager:\nYUM For RHEL, RockyLinux, CentOS, Alma Linux, OracleLinux, and other EL distributions:\nsudo tee /etc/yum.repos.d/pigsty-infra.repo \u003e /dev/null \u003c\u003c-'EOF' [pigsty-infra] name=Pigsty Infra for $basearch baseurl=https://repo.pigsty.io/yum/infra/$basearch enabled = 1 gpgcheck = 0 module_hotfixes=1 EOF sudo yum makecache; sudo yum install -y pig APT For Debian, Ubuntu, and other DEB distributions:\nsudo tee /etc/apt/sources.list.d/pigsty-infra.list \u003e /dev/null \u003c\u003cEOF deb [trusted=yes] https://repo.pigsty.io/apt/infra generic main EOF sudo apt update; sudo apt install -y pig Update To upgrade an existing pig version to the latest available version, use the following command:\npig update # Upgrade pig itself to the latest version To update the extension data of an existing pig to the latest available version, use the following command:\npig ext reload # Update pig extension data to the latest version Uninstall apt remove -y pig # Debian / Ubuntu and other Debian-based systems yum remove -y pig # RHEL / CentOS / RockyLinux and other EL distributions rm -rf /usr/bin/pig # If installed directly from binary, just delete the binary file Build from Source You can also build pig yourself. pig is developed in Go and is very easy to build. The source code is hosted at github.com/pgsty/pig\ngit clone https://github.com/pgsty/pig.git; cd pig go get -u; go build All RPM/DEB packages are automatically built through GitHub CI/CD workflow using goreleaser.\n","categories":["Task"],"description":"How to download and install the pig package manager","excerpt":"How to download and install the pig package manager","ref":"/docs/pig/install/","tags":"","title":"Installation"},{"body":"The latest stable version is v0.9.0.\nVersion Date Summary GitHub v0.9.0 2025-12-28 Adjust pig sty command options, fix alias v0.9.0 v0.8.0 2025-12-26 440 extensions, remove sysupdate repo v0.8.0 v0.7.5 2025-12-12 Routine extension update, fixed aliyun mirror v0.7.5 v0.7.4 2025-12-01 Update ivory/pgtde kernel and pgdg extras v0.7.4 v0.7.3 2025-11-24 Fix repo for el10 \u0026 debian13 v0.7.3 v0.7.2 2025-11-20 437 extensions, fix pig build issue v0.7.2 v0.7.1 2025-11-10 New Website, improve in-docker experience v0.7.1 v0.7.0 2025-11-05 Build Enhancement and massive upgrade v0.7.0 v0.6.2 2025-10-03 PG 18 official Repo v0.6.2 v0.6.1 2025-08-14 CI/CD, el10 stub, PGDG CN Mirror v0.6.1 v0.6.0 2025-07-17 423 extension, percona pg_tde, mcp toolbox v0.6.0 v0.5.0 2025-06-30 422 extension, new extension catalog v0.5.0 v0.4.2 2025-05-27 421 extension, halo \u0026 oriole deb v0.4.2 v0.4.1 2025-05-07 414 extension, pg18 alias support v0.4.1 v0.4.0 2025-05-01 do \u0026 pt sub-cmd, halo \u0026 orioledb v0.4.0 v0.3.4 2025-04-05 routine update v0.3.4 v0.3.3 2025-03-25 alias, repo, deps v0.3.3 v0.3.2 2025-03-21 new extensions v0.3.2 v0.3.1 2025-03-19 minor bug fix v0.3.1 v0.3.0 2025-02-24 new home page and extension catalog v0.3.0 v0.2.2 2025-02-22 404 extensions v0.2.2 v0.2.0 2025-02-14 400 extensions v0.2.0 v0.1.4 2025-02-12 routine bugfix v0.1.4 v0.1.3 2025-01-23 390 extensions v0.1.3 v0.1.2 2025-01-12 the anon extension and 350 other ext v0.1.2 v0.1.1 2025-01-09 Update Extension List v0.1.1 v0.1.0 2024-12-29 repo, ext, sty, and self-update v0.1.0 v0.0.1 2024-12-23 Genesis Release v0.0.1 v0.9.0 Refactor command pig sty install to pig sty deploy Add new parameters for command pig sty conf, aligned with configure script Add llvmjit package to pgsql-full alias Checksums\nea0c098d0829720b6e364d2f2a91328876962c7f0ae94eee7bdcde0bd43313fa pig-0.9.0-1.aarch64.rpm 707f4e1fde76d3faa05165ac11e97969c22a8740c97ef84da52727d0328990cc pig-0.9.0-1.x86_64.rpm 56aeb61674ddfb64368e6f5535e06a38b76f62e3d6c9536a63be7df6babed93e pig-v0.9.0.darwin-amd64.tar.gz a213d16817d6124ffa83d93ad880a040598b6ed3fe23a74d43420c095ed43de4 pig-v0.9.0.darwin-arm64.tar.gz 6a1a1836217fa723ca42bc2276ecf1453cd2ee0acacddfc313164701b24a452f pig-v0.9.0.linux-amd64.tar.gz 5e5728aa5922138c61c900a731f97cdc1b9653c14d7fe804b6753fb6f222b8b0 pig-v0.9.0.linux-arm64.tar.gz e80d2cb3ceb5fd58fc0262ab4b39b44e8dcccb7712151c73a41ba50cb510353b pig_0.9.0-1_amd64.deb ecb504efffde8d696b765579332fc0b3304751fa8077c4c0394e7f3c44aa0fe2 pig_0.9.0-1_arm64.deb Release: https://github.com/pgsty/pig/releases/tag/v0.9.0\nv0.8.0 Extension Updates\nTotal extensions reached 440 New extension: pg_ai_query 0.1.1 New extension: pg_textsearch 0.1.0 New extension: pg_clickhouse 0.1.0 pg_biscuit upgraded from 1.0 to 2.0.1 (switched to new repo, renamed to biscuit) pg_search upgraded from 0.20.3 to 0.20.5 pg_duckdb upgraded to official release 1.1.1 vchord_bm25 upgraded from 0.2.2 to 0.3.0 pg_semver upgraded from 0.40.0 to 0.41.0 pg_timeseries upgraded from 0.1.7 to 0.1.8 Fixed debian/ubuntu pg18 extension issues: supautils, pg_summarize, pg_vectorize, pg_tiktoken, pg_tzf, pglite_fusion, pgsmcrypto, pgx_ulid, plprql Pigsty version synced to 4.0.0 Repository Updates\nRemoved pgdg yum sysupdate repo due to upstream changes Removed pgdg yum llvmjit package due to upstream changes Fixed patroni 3.0.4 duplicate package issue on el9.aarch64 Added priority for el repo definitions, docker repo skipped when unavailable Added epel 10 / pgdg 9/10 OS minor version hotfix Checksums\ne457832fb290e2f9975bf719966dc36e650bdcbf8505d319c9e0431f4c03bc9e pig-0.8.0-1.aarch64.rpm c97b1bfdd7541f0f464cab0ecc273e65535c8dd2603c38d5cf8dccbf7e95b523 pig-0.8.0-1.x86_64.rpm d892f06d3d3b440671529f40e6cc7949686e0167e2a4758adc666b8a3d75254d pig-v0.8.0.darwin-amd64.tar.gz 222413bafdf5a62dc682dac32ea1118cbc34ec3544e2a1b85076ec450b9cc7ae pig-v0.8.0.darwin-arm64.tar.gz d50aa9806bbab8fee5ad9228e104fc9e7ead48729228116b5bf889000791fedc pig-v0.8.0.linux-amd64.tar.gz d2f410f7b243a8323c8d479f462a0267ac72d217aa4a506c80b5a9927d12dff8 pig-v0.8.0.linux-arm64.tar.gz 4ccd330a995911d4f732e8c9d62aa0db479c21c9596f64c4bc129ec43f156abe pig_0.8.0-1_amd64.deb 5cb9eccce659110f3ba58e502575564bd6befffd51992a43d84df5a17f8eb8a0 pig_0.8.0-1_arm64.deb Release: https://github.com/pgsty/pig/releases/tag/v0.8.0\nv0.7.5 Extension Updates\ntimescaledb 2.23.1 -\u003e 2.24.0 pg_search 0.20.0 -\u003e 0.20.3 convert 0.0.4 -\u003e 0.0.5 pglinter 1.0.0 -\u003e 1.0.1 pgdd 0.6.0 -\u003e 0.6.1 pg_session_jwt 0.3.3 -\u003e 0.4.0 pg_anon 2.4.1 -\u003e 2.5.1 pg_enigma 0.4.0 -\u003e 0.5.0 wrappers 0.5.6 -\u003e 0.5.7 pg_vectorize 0.25.0 -\u003e 0.26.0 Repository Updates\nUse the fixed Aliyun PGDG mirror repository\nChecksums\n9de11ac1404fc4100074113f2a5d50e4ec42c353b6e122a0b29edc17e53feca6 pig-0.7.5-1.aarch64.rpm 071d655580f1cc63b33d41a8fb49368556b7b5a276318f4bd772a6ab50e22b34 pig-0.7.5-1.x86_64.rpm befe0a8f786e5243669ed7219acde8156d13d9adb0a5c2fb88ccf0f614a51f9b pig-v0.7.5.darwin-amd64.tar.gz 4766b4e9ba390a32a7115e9f2dd6b65cf158439e28f9c099bab5c7f2e588bae2 pig-v0.7.5.darwin-arm64.tar.gz dc45726c5e7fccd502cacaffc94c659570844151cdc279f2cac6500836071ade pig-v0.7.5.linux-amd64.tar.gz 1483cf967d4bc9c12d4c6724567644d6b88fcd2a93aaf1d317fc6ad4e1672c13 pig-v0.7.5.linux-arm64.tar.gz 0152b7bd254eccadd640e563845abd9fa62efa68f11c6b67a5f9f0eebfa2d92e pig_0.7.5-1_amd64.deb 7d22116d26ca09c5e2b8afbf086bb1acb1aea1148905efcc38944c18908fb105 pig_0.7.5-1_arm64.deb Release: https://github.com/pgsty/pig/releases/tag/v0.7.5\nv0.7.4 Update extension metadata: pg_search, pgmq, pg_stat_monitor Update pgdg repo URL, the extras now move to parent directory Bump ivorysql to 5.0 (compatible with PG 18.0) Bump Percona Postgres TDE Kernel to 18.1 Checksums\n5769b0051f04dcda22dd92b30b8effc8ddfa40097308bded76ce2b38d012ce57 pig-0.7.4-1.aarch64.rpm d15c829fa2e3ce8dcd1adc063c107607b8e70f2cf747646aaa2fa257cdbf979c pig-0.7.4-1.x86_64.rpm bb4c90e253a3d470e50316e633a41e90ed2d4a5c5a1fd3a8dbb68ee87d831d47 pig-v0.7.4.darwin-amd64.tar.gz faaf7ac7b08390f5048c081bb7a78100714387e35dc890e26d9746fc1caef415 pig-v0.7.4.darwin-arm64.tar.gz 037cacddd0dc1283f13dd2c9bace87ad7f2c74ffc245e629f1420be94bbf93df pig-v0.7.4.linux-amd64.tar.gz 2ce819b2c3686cfb9f86790fdf61acd30bf7798bd6cd3c4f589df22e273dc867 pig-v0.7.4.linux-arm64.tar.gz 97f62d62f1cca61ce6d335efed88e3855d94ea2cd4ed941f2755fbac73931fcd pig_0.7.4-1_amd64.deb d2b80af89ed42601716f6b41eda3f8bee16db34023527df9deef8a43aa25a498 pig_0.7.4-1_arm64.deb Release: https://github.com/pgsty/pig/releases/tag/v0.7.4\nv0.7.3 Add new command: pig repo reload to update repo metadata Fix EL PGDG sysupdate aarch64 repo issue (now aarch64 repo ready) Fix EL10.aarch64 PGDG repo renaming issue Update extension versions Bump Pigsty version to 3.7.0 Checksums\n786d72f6b685d6d6abf5f255f0a7de9204988a05630a26a53bfc7631823c0c6f pig-0.7.3-1.aarch64.rpm da59e24ef79d1164e348bacc43e3222e8e2778ec0e103e7ffc0c6df064758e8f pig-0.7.3-1.x86_64.rpm 73062a979749095e89abc07dd583d34d4f57908bb4ee935cf7640f129ca6a2cb pig-v0.7.3.darwin-amd64.tar.gz ca5f5576f6d0d9be1d10cad769821be9daa62220b2fb56b94d6e4c0cede6da61 pig-v0.7.3.darwin-arm64.tar.gz d193b4b87cf9a6e4775b1b07709802d30f0233ccb1b728843a09decb545168d3 pig-v0.7.3.linux-amd64.tar.gz e7f612df0e8e4d9fac6df3765862b9e491bb50aad651856abf7a6935986e6f99 pig-v0.7.3.linux-arm64.tar.gz 3d5306ce95dcf704dd498b05325d942637564b13115f1e5a5bb9ef6781df1ba6 pig_0.7.3-1_amd64.deb 32e695ba2d49a741d8cd92008f8f2dec29f10754d35b732035f48517b382c30d pig_0.7.3-1_arm64.deb Release: https://github.com/pgsty/pig/releases/tag/v0.7.3\nv0.7.2 Extension list update, + 6 new extensions, 437 total\nAdd PGDG EL10 Sysupdate repo\nAdd LLVM APT repo\nUse local extension.csv catalog in pig build sub command\nUpdated extensions: vchord pg_later pgvectorscale pglite_fusion pgx_ulid pg_search citus timescaledb pg_profile pg_stat_monitor documentdb\nNew extensions: pglinter pg_typeid pg_enigma pg_retry pg_biscuit pg_weighted_statistics\nChecksums\nf303c391fc28bc74832712e0aa58319abe0ebcae4f6c07fdf9a9e542b735d2ec pig-0.7.2-1.aarch64.rpm c096a61a4e3a49b1238659664bbe2cd7f29954c43fb6bb8e8e9fb271f95a612e pig-0.7.2-1.x86_64.rpm 5e037c891dff23b46856485108d6f64bede5216dfbd4f38a481f0d0672ee910b pig-v0.7.2.darwin-amd64.tar.gz 736b4b47999c543c3c886781f4d8dddbf4276f363c35c7bf50094b6f18d14600 pig-v0.7.2.darwin-arm64.tar.gz 20b13f059efed29dd76f6927b3e8d7b597c0c8d734f9e22ba3d0a2af6dbcd3bf pig-v0.7.2.linux-amd64.tar.gz 9548b530c05f2ffdc8d73b8f890718d47b74a51eb62852a99c08b1b52e47f014 pig-v0.7.2.linux-arm64.tar.gz b6faad9f92b926546a10f590274f2cb2afff21b9cea878094cfc5caf09e67d2c pig_0.7.2-1_amd64.deb 452f73f1fa035e5417ab49fc51d797925550179ffcc023e8f03d80144309212a pig_0.7.2-1_arm64.deb Release: https://github.com/pgsty/pig/releases/tag/v0.7.2\nv0.7.1 The brand-new website: https://pgext.cloud Remove unnecessary sudo usage, now can be used inside docker Allow using pg18, pg17 arg format in pig ext link command Add environment var PIG_NO_SUDO to force not using sudo RPM Changelog: Add PG 18 support to almost all extensions DEB Changelog: Add PG 18 support to almost all extensions Infra Changelog: Routine update to the latest version Checksums\na696c9ec784e2fc248e5f3d87cc8aae4116e890f78c5997957d30593f2c85ca6 pig-0.7.1-1.aarch64.rpm f669538a99cd1dc592d3005b949628fcceb9e78114fc78862d7726b340ee194d pig-0.7.1-1.x86_64.rpm e42bdaaf93b720c5b76b32b57362320e4b447109740c76089aefe030b7c8b836 pig-v0.7.1.darwin-amd64.tar.gz b4c240aadad34e785666ee0a755d9b7455724f790c2d088a1dd7c37ad3b2a457 pig-v0.7.1.darwin-arm64.tar.gz ffc687add0ca71ac90cba5749c8a7a6075cf7618cba85584072831cf3eb182f7 pig-v0.7.1.linux-amd64.tar.gz 7b0d1f158150d0a40c525692f02b6bce9f5b4ac523a4e59278d702c334e222e1 pig-v0.7.1.linux-arm64.tar.gz 43e91a3bea273d7cacb2d7a58c0a5745501dbd06348b5cb3af971171fae70268 pig_0.7.1-1_amd64.deb fc2a34aeb46e07cb0ae93611de47d6622c3bd46fe4c415ce4c9091840e0e08a2 pig_0.7.1-1_arm64.deb Release: https://github.com/pgsty/pig/releases/tag/v0.7.1\nv0.7.0 Add support for Debian 13 and EL 10 distributions Massive extension updates to the latest versions with PostgreSQL 18 support Almost all Rust extensions now support PG 18 via pgrx 0.16.1 pig build command overhaul pig build pkg \u003cpkg\u003e will now download source, prepare deps, and build in one go pig build pgrx is now separated from pig build rust pig build pgrx [-v pgrx_version] can now use existing PG installation directly pig build dep will now handle extension dependencies on both EL and Debian systems pig build ext now has more compact and elegant output, can build RPM on EL without build script pig build spec now supports downloading spec files directly from Pigsty repo pig build repo / pig repo add / pig repo set now use node,pgsql,infra as default repo modules instead of node,pgdg,pigsty Optimized error logging Brand new catalog website based on hugo and hextra Checksums\nad60f9abcde954769e46eb23de61965e pig_0.7.0-1_amd64.deb aa15d7088d561528e38b2778fe8f7cf9 pig_0.7.0-1_arm64.deb 05549fe01008e04f8d5a59d4f2a5f0b8 pig-0.7.0-1.aarch64.rpm 0cc9e46c7c72d43c127a6ad115873b67 pig-0.7.0-1.x86_64.rpm ddacfb052f3f3e5567a02e92fdb31cdd pig-v0.7.0.darwin-amd64.tar.gz 17d25b565308d3d35513e4b0d824946b pig-v0.7.0.darwin-arm64.tar.gz ee7e055ceff638039956765fb747f80b pig-v0.7.0.linux-amd64.tar.gz 284e674807b87447d4b33691fd7a420d pig-v0.7.0.linux-arm64.tar.gz Release: https://github.com/pgsty/pig/releases/tag/v0.7.0\nv0.6.2 Use official PG 18 repo instead of testing repo Add v prefix when specifying pigsty version string Improved network connectivity check Checksums\n01f5b7dc20644226c762dbb229768347 pig_0.6.2-1_amd64.deb ce4f00256adc12cbea91467b7f2241cd pig_0.6.2-1_arm64.deb cefc36ae8f348aede533b30836fba720 pig-0.6.2-1.aarch64.rpm d04a287c6eb92b11ecbf99542c2db602 pig-0.6.2-1.x86_64.rpm e637ca86a7f38866c67686b060223d9a pig-v0.6.2.darwin-amd64.tar.gz 79749bc69c683586bd8d761bdf6af98e pig-v0.6.2.darwin-arm64.tar.gz ad4f02993c7d7d8eec142f0224551bb4 pig-v0.6.2.linux-amd64.tar.gz 9793affa4a0cb60e9753e65b7cba3dca pig-v0.6.2.linux-arm64.tar.gz Release: https://github.com/pgsty/pig/releases/tag/v0.6.2\nv0.6.1 Add el10 and debian 13 trixie support stub Dedicated website: https://pgext.cloud/pig Rebuild with go 1.25 and CI/CD pipeline Use PIGSTY PGDG mirror in mainland China Remove unused pgdg-el10fix repo Use Pigsty WiltonDB mirror Add EL 10 dedicated epel repo pig version output with go build environment Release: https://github.com/pgsty/pig/releases/tag/v0.6.1\nv0.6.0 New extension catalog: https://ext.pgsty.com New subcommand: pig install to simplify pig ext install Add new kernel support: percona with pg_tde Add new package: Google GenAI MCP toolbox for databases Add new repo: percona repo and clickhouse repo Change extension summary info links to https://ext.pgsty.com Fix orioledb broken on the Debian/Ubuntu system Fix epel repo on EL distributions Bump golang to 1.24.5 Bump pigsty to v3.6.0 Checksums\n1804766d235b9267701a08f95903bc3b pig_0.6.0-1_amd64.deb 35f4efa35c1eaecdd12aa680d29eadcb pig_0.6.0-1_arm64.deb b523b54d9f2d7dcc5999bcc6bd046b1d pig-0.6.0-1.aarch64.rpm 9434d9dca7fd9725ea574c5fae1a7f52 pig-0.6.0-1.x86_64.rpm f635c12d9ad46a779aa7174552977d11 pig-v0.6.0.linux-amd64.tar.gz 165af4e63ec0031d303fe8b6c35c5732 pig-v0.6.0.linux-arm64.tar.gz Release: https://github.com/pgsty/pig/releases/tag/v0.6.0\nv0.5.0 Update the extension list to 422 New extension: pgactive from AWS Bump timescaledb to 2.20.3 Bump citus to 13.1.0 Bump vchord to 0.4.3 Bug fix pgvectorscale debian/ubuntu pg17 failure Bump kubernetes repo to 1.33 Bump default pigsty version to 3.5.0 Checksums\n9ec6f3caf3edbe867caab5de0e0ccb33 pig_0.5.0-1_amd64.deb 4fbb0a42cd8a88bce50b3c9d85745d77 pig_0.5.0-1_arm64.deb 9cf8208396b068cab438f72c90d39efe pig-0.5.0-1.aarch64.rpm d9a8d78c30f45e098b29c3d16471aa8d pig-0.5.0-1.x86_64.rpm 761df804ff7b83965c41492700717674 pig-v0.5.0.linux-amd64.tar.gz 5d1830069d98030728f08835f883ea39 pig-v0.5.0.linux-arm64.tar.gz Release: https://github.com/pgsty/pig/releases/tag/v0.5.0\nv0.4.2 Update the extension list to 421 Add openhalo/orioledb support for Debian / Ubuntu pgdd 0.6.0 (pgrx 0.14.1) convert 0.0.4 (pgrx 0.14.1) pg_idkit 0.3.0 (pgrx 0.14.1) pg_tokenizer.rs 0.1.0 (pgrx 0.13.1) pg_render 0.1.2 (pgrx 0.12.8) pgx_ulid 0.2.0 (pgrx 0.12.7) pg_ivm 1.11.0 for debian/ubuntu orioledb 1.4.0 beta11 Add el7 repo back Checksums\nbbf83fa3e3ec9a4dca82eeed921ae90a pig_0.4.2-1_amd64.deb e45753335faf80a70d4f2ef1d3100d72 pig_0.4.2-1_arm64.deb 966d60bbc2025ba9cc53393011605f9f pig-0.4.2-1.aarch64.rpm 1f31f54da144f10039fa026b7b6e75ad pig-0.4.2-1.x86_64.rpm 1eec26c4e69b40921e209bcaa4fe257a pig-v0.4.2.linux-amd64.tar.gz 768d43441917a3625c462ce9f2b9d4ef pig-v0.4.2.linux-arm64.tar.gz Release: https://github.com/pgsty/pig/releases/tag/v0.4.2\nv0.4.1 Update the extension list to 414 Add citus_wal2json and citus_pgoutput to pig ext scan mapping Add PG 18 beta repo Add PG 18 package alias Release: https://github.com/pgsty/pig/releases/tag/v0.4.1\nv0.4.0 Updated extension list, available extensions reached 407 Added pig do subcommand for executing Pigsty playbook tasks Added pig pt subcommand for wrapping Patroni command-line tools Added extension aliases: openhalo and orioledb Added gitlab-ce / gitlab-ee repository distinction Built with the latest Go 1.24.2 and upgraded dependency versions Fixed pig ext status panic issue under specific conditions Fixed pig ext scan unable to match several extensions Release: https://github.com/pgsty/pig/releases/tag/v0.4.0\nv0.3.4 curl https://repo.pigsty.io/pig | bash -s 0.3.4 Routine extension metadata update Use aliyun epel mirror instead of broken tsinghua tuna mirror Bump pigsty version string Add gitlab repo to the repo list Release: https://github.com/pgsty/pig/releases/tag/v0.3.4\nv0.3.3 Add pig build dep command to install extension build dependencies Update default repo list Use pigsty.io mirror for mssql module (wiltondb/babelfish) Merge docker module into infra Remove pg16/17 from el7 target Allow installing extensions in el7 Update package alias Release: https://github.com/pgsty/pig/releases/tag/v0.3.3\nv0.3.2 Enhancement\nNew extensions Use upx to reduce binary size Remove embedded pigsty to reduce binary size Release: https://github.com/pgsty/pig/releases/tag/v0.3.2\nv0.3.1 Routine bugfix\nFix repo format string Fix ext info links Update pg_mooncake metadata Release: https://github.com/pgsty/pig/releases/tag/v0.3.1\nv0.3.0 The pig project now has a new homepage, along with the PostgreSQL Extension Catalog.\nRelease: https://github.com/pgsty/pig/releases/tag/v0.3.0\nv0.2.2 404 Extensions Available in Pig v0.2.2\nRelease: https://github.com/pgsty/pig/releases/tag/v0.2.2\nv0.2.0 Release: https://github.com/pgsty/pig/releases/tag/v0.2.0\nv0.1.4 Release: https://github.com/pgsty/pig/releases/tag/v0.1.4\nv0.1.3 v0.1.3, routine update, with 390 extensions available now!\nRelease: https://github.com/pgsty/pig/releases/tag/v0.1.3\nv0.1.2 351 PostgreSQL Extensions, including the powerful postgresql-anonymizer 2.0\nRelease: https://github.com/pgsty/pig/releases/tag/v0.1.2\nv0.1.0 pig CLI v0.1 released\nRelease: https://github.com/pgsty/pig/releases/tag/v0.1.0\nv0.0.1 Genesis Release\nRelease: https://github.com/pgsty/pig/releases/tag/v0.0.1\n","categories":["Reference"],"description":"pig — PostgreSQL Package Manager Release Notes","excerpt":"pig — PostgreSQL Package Manager Release Notes","ref":"/docs/pig/release/","tags":"","title":"Release"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/pig/_div_cmd/","tags":"","title":"Command Reference"},{"body":"The pig CLI provides comprehensive tools for managing PostgreSQL installations, extensions, repositories, and building extensions from source. Check command documentation with pig help \u003ccommand\u003e.\npig repo: Manage software repositories pig ext: Manage PostgreSQL extensions pig build: Build extensions from source pig sty: Manage Pigsty installation Overview pig - the Linux Package Manager for PostgreSQL Usage: pig [command] Examples: pig repo add -ru # overwrite existing repo \u0026 update cache pig install pg17 # install postgresql 17 PGDG package pig install pg_duckdb # install certain postgresql extension pig install pgactive -v 18 # install extension for specifc pg major check https://pgext.cloud for details PostgreSQL Extension Manager build Build Postgres Extension ext Manage PostgreSQL Extensions (pgext) repo Manage Linux Software Repo (apt/dnf) install Install packages using native package manager Pigsty Management Commands do Run admin tasks sty Manage Pigsty installation Additional Commands: completion Generate the autocompletion script for the specified shell help Help about any command status Show Environment Status update Upgrade pig itself version Show pig version info Flags: --debug enable debug mode -h, --help help for pig -H, --home string pigsty home path -i, --inventory string config inventory path --log-level string log level: debug, info, warn, error, fatal, panic (default \"info\") --log-path string log file path, terminal by default Use \"pig [command] --help\" for more information about a command. pig repo Manage APT/YUM repositories for PostgreSQL packages. See pig repo for details.\npig repo list # List available repositories pig repo info pgdg # Show repository details pig repo status # Check current repo status pig repo add pgdg pigsty -u # Add repositories pig repo rm old-repo # Remove repositories pig repo update # Update package cache pig repo create /www/pigsty # Create local repository pig repo cache # Create offline package pig repo boot # Bootstrap from offline package pig ext Manage PostgreSQL extensions and kernel packages. See pig ext for details.\npig ext list duck # Search extensions pig ext info pg_duckdb # Extension details pig ext status # Show installed extensions pig ext add pg_duckdb -y # Install extension pig ext rm old_extension # Remove extension pig ext update # Update extensions pig ext scan # Scan installed extensions pig ext import pg_duckdb # Download for offline use pig ext link 17 # Link PG version to PATH pig ext reload # Refresh extension catalog pig build Build PostgreSQL extensions from source. See pig build for details.\n# Environment setup pig build spec # Initialize build specs pig build repo # Setup repositories pig build tool # Install build tools pig build rust -y # Install Rust (for Rust extensions) pig build pgrx # Install PGRX framework # Build extensions pig build pkg citus # Complete build pipeline = get + dep + ext pig build get citus # Download source pig build dep citus # Install dependencies pig build ext citus # Build package pig sty Install Pigsty distribution. See pig sty for details.\npig sty init # Install Pigsty to ~/pigsty pig sty boot # Install Ansible prerequisites pig sty conf # Generate configuration pig sty deploy # Run deployment playbook ","categories":["Admin"],"description":"pig CLI command reference overview","excerpt":"pig CLI command reference overview","ref":"/docs/pig/cmd/","tags":"","title":"CMD: pig"},{"body":"The pig ext command is a comprehensive tool for managing PostgreSQL extensions. It allows users to search, install, remove, update, and manage PostgreSQL extensions and even kernel packages.\nCommand Description Notes ext list Search extensions ext info Show extension details ext status Show installed extensions ext add Install extensions Requires sudo or root ext rm Remove extensions Requires sudo or root ext update Update extensions Requires sudo or root ext scan Scan installed extensions ext import Download for offline use Requires sudo or root ext link Link PG version to PATH Requires sudo or root ext reload Refresh extension catalog Quick Start pig ext list # List all extensions pig ext list duck # Search for \"duck\" extensions pig ext info pg_duckdb # Show pg_duckdb extension info pig install pg_duckdb # Install pg_duckdb extension pig install pg_duckdb -v 17 # Install pg_duckdb for PG 17 pig ext status # Show installed extensions ext list List or search extensions.\npig ext list # List all extensions pig ext list duck # Search for \"duck\" extensions pig ext list vector ai # Search multiple keywords pig ext list -c RAG # Filter by category pig ext list -v 17 # Filter by PG version Options:\n-c|--category: Filter by category (TIME, GIS, RAG, FTS, OLAP, FEAT, LANG, TYPE, UTIL, FUNC, ADMIN, STAT, SEC, FDW, SIM, ETL) -v|--version: Filter by PG version ext info Display detailed information about specific extensions.\npig ext info pg_duckdb # Show pg_duckdb info pig ext info vector postgis # Show info for multiple extensions ext status Display the status of installed extensions for the active PostgreSQL instance.\npig ext status # Show installed extensions pig ext status -v 17 # Show installed extensions for PG 17 ext add Install extensions. Also available via alias pig install.\npig ext add pg_duckdb # Install pg_duckdb pig ext add pg_duckdb -v 17 # Install for PG 17 pig ext add pg_duckdb -y # Auto-confirm installation pig ext add vector postgis # Install multiple extensions # Using alias pig install pg_duckdb pig install pg_duckdb -v 17 -y Options:\n-v|--version: Specify PG major version -y|--yes: Auto-confirm installation -n|--no-translation: Disable alias translation ext rm Remove extensions. Also available via alias pig remove.\npig ext rm pg_duckdb # Remove pg_duckdb pig ext rm pg_duckdb -v 17 # Remove for PG 17 pig remove pg_duckdb # Using alias ext update Update installed extensions.\npig ext update # Update all extensions pig ext update pg_duckdb # Update specific extension ext scan Scan installed PostgreSQL installations and their extensions.\npig ext scan # Scan all installed PG versions pig ext scan -v 17 # Scan PG 17 ext import Download extension packages for offline use.\npig ext import pg_duckdb # Download pg_duckdb pig ext import pg_duckdb -v 17 # Download for PG 17 ext link Link a specific PG version to the system PATH.\npig ext link 17 # Link PG 17 to PATH This command creates a /usr/pgsql symlink and writes to /etc/profile.d/pgsql.sh.\next reload Refresh extension metadata from GitHub.\npig ext reload # Refresh extension catalog The updated file is placed in ~/.pig/extension.csv.\nExamples To install PostgreSQL extensions, you’ll have to add the repo first:\npig repo add pgdg pigsty -u # gentle way to add pgdg and pigsty repo pig repo set -u # brute way to remove and add all required repos Then you can search and install PostgreSQL extensions:\npig ext install pg_duckdb pig ext install pg_partman pig ext install pg_cron pig ext install pg_repack pig ext install pg_stat_statements pig ext install pg_stat_kcache Check extension list for available extensions and their names.\nNotes:\nWhen no PostgreSQL version is specified, the tool will try to detect the active PostgreSQL installation from pg_config in your PATH PostgreSQL can be specified either by major version number (-v) or by pg_config path (-p). If -v is given, pig will use the well-known default path of PGDG kernel packages for the given version. On EL distros, it’s /usr/pgsql-$v/bin/pg_config for PG$v On DEB distros, it’s /usr/lib/postgresql/$v/bin/pg_config for PG$v If -p is given, pig will use the pg_config path to find the PostgreSQL installation The extension manager supports different package formats based on the underlying operating system: RPM packages for RHEL/CentOS/Rocky Linux/AlmaLinux DEB packages for Debian/Ubuntu Some extensions may have dependencies that will be automatically resolved during installation Use the -y flag with caution as it will automatically confirm all prompts Pigsty assumes you already have installed the official PGDG kernel packages. If not, you can install them with:\npig ext install pg17 # install PostgreSQL 17 kernels (all but devel) ","categories":["Admin"],"description":"Manage PostgreSQL extensions with pig ext subcommand","excerpt":"Manage PostgreSQL extensions with pig ext subcommand","ref":"/docs/pig/ext/","tags":"","title":"CMD: pig ext"},{"body":"The pig repo command is a comprehensive tool for managing package repositories on Linux systems. It provides functionality to add, remove, create, and manage software repositories for both RPM-based (RHEL/CentOS/Rocky/Alma) and Debian-based (Debian/Ubuntu) distributions.\nCommand Description Notes repo list Print available repo and module list repo info Get repo detailed information repo status Show current repo status repo add Add new repository Requires sudo or root repo set Wipe, overwrite, and update repository Requires sudo or root repo rm Remove repository Requires sudo or root repo update Update repo cache Requires sudo or root repo create Create local YUM/APT repository Requires sudo or root repo cache Create offline package from local repo Requires sudo or root repo boot Bootstrap repo from offline package Requires sudo or root Quick Start # Method 1: Clean existing repos, add all necessary repos and update cache (recommended) pig repo add all --remove --update # Remove old repos, add all essentials, update cache # Method 1 variant: One-step pig repo set # = pig repo add all --remove --update # Method 2: Gentle approach - only add required repos, keep existing config pig repo add pgsql # Add PGDG and Pigsty repos with cache update pig repo add pigsty --region=china # Add Pigsty repo, specify China region pig repo add pgdg --region=default # Add PGDG, specify default region pig repo add infra --region=europe # Add INFRA repo, specify Europe region # If no -u|--update option above, run this command additionally pig repo update # Update system package cache Modules In pig, APT/YUM repositories are organized into modules — groups of repositories serving a specific purpose.\nModule Description Repository List all All core modules needed to install PG node + infra + pgsql pgsql PGDG + Pigsty PG extensions pigsty-pgsql + pgdg pigsty Pigsty Infra + PGSQL repos pigsty-infra, pigsty-pgsql pgdg PGDG official repository pgdg-common, pgdg13-18 node Linux system repositories base, updates, extras, epel… infra Infrastructure component repos pigsty-infra, nginx, docker-ce repo add Add repository configuration files to the system. Requires root/sudo privileges.\npig repo add pgdg # Add PGDG repository pig repo add pgdg pigsty # Add multiple repositories pig repo add all # Add all essential repos (pgdg + pigsty + node) pig repo add pigsty -u # Add and update cache pig repo add all -r # Remove existing repos before adding pig repo add all -ru # Remove, add, and update (complete reset) pig repo add pgdg --region=china # Use China mirrors Options:\n-r|--remove: Remove existing repos before adding new ones -u|--update: Run package cache update after adding repos --region \u003cregion\u003e: Use regional mirror repositories (default / china / europe) repo set Equivalent to repo add --remove --update. Wipes existing repositories and sets up new ones, then updates cache.\npig repo set # Replace with default repos pig repo set pgdg pigsty # Replace with specific repos and update pig repo set all --region=china # Use China mirrors repo rm Remove repository configuration files and back them up.\npig repo rm # Remove all repos pig repo rm pgdg # Remove specific repo pig repo rm pgdg pigsty -u # Remove and update cache repo update Update package manager cache to reflect repository changes.\npig repo update # Update package cache Platform Equivalent Command EL dnf makecache Debian apt update repo create Create local package repository for offline installations.\npig repo create # Create at default location (/www/pigsty) pig repo create /srv/repo # Create at custom location repo cache Create compressed tarball of repository contents for offline distribution.\npig repo cache # Default: /www to /tmp/pkg.tgz pig repo cache -f # Force overwrite existing pig repo cache -d /srv # Custom source directory repo boot Extract and set up local repository from offline package.\npig repo boot # Default: /tmp/pkg.tgz to /www pig repo boot -p /mnt/pkg.tgz # Custom package path pig repo boot -d /srv # Custom target directory Common Scenarios Scenario 1: Fresh PostgreSQL Installation # Setup repositories sudo pig repo add -ru # Install PostgreSQL 17 sudo pig ext install pg17 # Install popular extensions sudo pig ext add pg_duckdb postgis timescaledb Scenario 2: Air-gapped Environment # On internet-connected machine: sudo pig repo add -ru sudo pig ext install pg17 sudo pig ext add pg_duckdb postgis sudo pig repo create sudo pig repo cache # Transfer /tmp/pkg.tgz to air-gapped machine # On air-gapped machine: sudo pig repo boot sudo pig repo add local sudo pig ext install pg17 sudo pig ext add pg_duckdb postgis Scenario 3: Using Regional Mirrors # For users in China sudo pig repo add all --region=china -u # Check mirror URLs pig repo info pgdg ","categories":["Admin"],"description":"Manage software repositories with pig repo subcommand","excerpt":"Manage software repositories with pig repo subcommand","ref":"/docs/pig/repo/","tags":"","title":"CMD: pig repo"},{"body":"The pig can also be used as a CLI tool for Pigsty — the battery-included free PostgreSQL RDS. Which brings HA, PITR, Monitoring, IaC, and all the extensions to your PostgreSQL cluster.\nCommand Description Notes sty init Install Pigsty sty boot Install Ansible prerequisites Requires sudo or root sty conf Generate configuration sty deploy Run deployment playbook Quick Start pig sty init # Install Pigsty to ~/pigsty pig sty boot # Install Ansible prerequisites pig sty conf # Generate configuration pig sty deploy # Run deployment playbook sty init Download and install Pigsty distribution to ~/pigsty directory.\npig sty init # Install latest Pigsty pig sty init -v 3.5.0 # Install specific version pig sty init -d /opt/pigsty # Install to specific directory Options:\n-v|--version: Specify Pigsty version -d|--dir: Specify installation directory -f|--force: Overwrite existing pigsty directory sty boot Install Ansible and its dependencies.\npig sty boot # Install Ansible pig sty boot -y # Auto-confirm pig sty boot -r china # Use China region mirrors Options:\n-r|--region: Upstream repo region (default, china, europe) -k|--keep: Keep existing upstream repo during bootstrap sty conf Generate Pigsty configuration file.\npig sty conf # Generate default configuration pig sty conf -c rich # Use conf/rich.yml template (more extensions) pig sty conf -c slim # Use conf/slim.yml template (minimal install) pig sty conf -c supabase # Use conf/supabase.yml template (self-hosting) pig sty conf -g # Generate with random passwords (recommended!) pig sty conf -v 17 # Use PostgreSQL 17 pig sty conf -r china # Use China region mirrors pig sty conf --ip 10.10.10.10 # Specify IP address Options:\n-c|--conf: Config template name -v|--version: PostgreSQL major version -r|--region: Upstream repo region --ip: Primary IP address -g|--generate: Generate random passwords -s|--skip: Skip IP address probing -o|--output: Output config file path sty deploy Run Pigsty deployment playbook.\npig sty deploy # Run full deployment This command runs the deploy.yml playbook from your Pigsty installation.\nWarning: This operation makes changes to your system. Use with caution!\nComplete Workflow Here’s the complete workflow to set up Pigsty:\n# 1. Download and install Pigsty pig sty init # 2. Install Ansible and dependencies cd ~/pigsty pig sty boot # 3. Generate configuration pig sty conf -g # Generate with random passwords # 4. Deploy Pigsty pig sty deploy For detailed setup instructions, check Get Started.\nConfiguration Templates Available configuration templates (-c option):\nTemplate Description meta Default single-node meta configuration rich Configuration with more extensions enabled slim Minimal installation full Full 4-node HA template supabase Self-hosting Supabase template Example:\npig sty conf -c rich -g -v 17 -r china This generates a configuration using the rich template with PostgreSQL 17, random passwords, and China region mirrors.\n","categories":["Admin"],"description":"Manage Pigsty installation with pig sty subcommand","excerpt":"Manage Pigsty installation with pig sty subcommand","ref":"/docs/pig/sty/","tags":"","title":"CMD: pig sty"},{"body":"The pig build command is a powerful tool that simplifies the entire workflow of building PostgreSQL extensions from source. It provides a complete build infrastructure setup, dependency management, and compilation environment for both standard and custom PostgreSQL extensions across different operating systems.\nSubcommands Command Description Notes build spec Initialize building spec repo build repo Initialize required repos Requires sudo or root build tool Initialize build tools Requires sudo or root build rust Install Rust toolchain Requires sudo or root build pgrx Install and initialize pgrx Requires sudo or root build pkg Complete build pipeline Requires sudo or root build get Download source code tarball build dep Install extension build dependencies Requires sudo or root build ext Build extension package Requires sudo or root Quick Start # Setup build environment pig build spec # Initialize build specs pig build repo # Setup repositories pig build tool # Install build tools # Install Rust (for Rust extensions) pig build rust -y # Install Rust pig build pgrx # Install PGRX framework # Build extensions pig build pkg citus # Complete build pipeline build spec Initialize build specification files.\npig build spec # Create default spec files pig build spec -f # Force overwrite existing files build repo Setup repositories required for building.\npig build repo # Setup repositories build tool Install build toolchain (gcc, make, cmake, etc.).\npig build tool # Install build tools pig build tool -y # Auto-confirm build rust Install Rust toolchain (for building Rust extensions).\npig build rust # Install Rust pig build rust -y # Auto-confirm build pgrx Install PGRX framework (for building PGRX extensions).\npig build pgrx # Install PGRX pig build pgrx -v 17 # Install for PG 17 build pkg Complete build pipeline: download source, install dependencies, build extension.\npig build pkg citus # Build citus pig build pkg citus -v 17 # Build for PG 17 pig build pkg citus -y # Auto-confirm build get Download extension source code.\npig build get citus # Download citus source build dep Install extension build dependencies.\npig build dep citus # Install citus dependencies pig build dep citus -y # Auto-confirm build ext Build extension package from source.\npig build ext citus # Build citus pig build ext citus -v 17 # Build for PG 17 Build Infrastructure Build Specifications The build system uses specification files that define how each extension should be built. These specs include:\nSource code location and version Build dependencies Compilation flags PostgreSQL version compatibility Platform-specific build instructions Directory Structure ~/ext/ # Default build spec directory ├── Makefile # Master build makefile ├── \u003cextension\u003e/ # Per-extension directory │ ├── Makefile # Extension-specific makefile │ ├── \u003cextension\u003e.spec # RPM spec file (EL) │ └── debian/ # Debian packaging files │ ├── control │ ├── rules │ └── ... Build output locations:\nEL Systems: ~/rpmbuild/RPMS/\u003carch\u003e/ Debian Systems: ~/ (deb files) Common Workflows Workflow 1: Building Standard Extension # 1. Setup build environment (once) pig build spec pig build repo pig build tool # 2. Build extension pig build pkg pg_partman # 3. Install built package sudo rpm -ivh ~/rpmbuild/RPMS/x86_64/pg_partman*.rpm # EL sudo dpkg -i ~/pg_partman*.deb # Debian Workflow 2: Building Rust Extension # 1. Setup Rust environment pig build spec pig build tool pig build rust -y pig build pgrx # 2. Build Rust extension pig build pkg pgmq # 3. Install sudo pig ext add pgmq Workflow 3: Building Multiple Versions # Build extension for multiple PostgreSQL versions pig build pkg citus --pg 15,16,17 # Results in packages for each version: # citus_15-*.rpm # citus_16-*.rpm # citus_17-*.rpm Troubleshooting Build Tools Not Found # Install build tools pig build tool # For specific compiler sudo dnf groupinstall \"Development Tools\" # EL sudo apt install build-essential # Debian Missing Dependencies # Install extension dependencies pig build dep \u003cextension\u003e # Check error messages for specific packages # Install manually if needed sudo dnf install \u003cpackage\u003e # EL sudo apt install \u003cpackage\u003e # Debian PostgreSQL Headers Not Found # Install PostgreSQL development package sudo pig ext install pg17-devel # Or specify pg_config path export PG_CONFIG=/usr/pgsql-17/bin/pg_config Rust/PGRX Issues # Reinstall Rust curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh # Update PGRX cargo install cargo-pgrx --force # Reinitialize PGRX cargo pgrx init ","categories":["Admin"],"description":"Build PostgreSQL extensions from source with pig build subcommand","excerpt":"Build PostgreSQL extensions from source with pig build subcommand","ref":"/docs/pig/build/","tags":"","title":"CMD: pig build"},{"body":"Pigsty has a repository that provides 340+ extra PostgreSQL extensions on mainstream Linux Distros. It is designed to work together with the official PostgreSQL Global Development Group (PGDG) repo. Together, they can provide up to 400+ PostgreSQL Extensions out-of-the-box.\nPGSQL Repo Description Link PGSQL Repo Pigsty Extension Repo, 340+ extra extensions pgsql.md INFRA Repo Pigsty Infrastructure Repo, monitoring/tools infra.md PGDG Repo PGDG Official Repo Mirror, PG Kernel pgdg.md GPG Key GPG Public Key, signature verification gpg.md Compatibility Overview OS / Arch OS x86_64 aarch64 EL8 el8 18 17 16 15 14 13 18 17 16 15 14 13 EL9 el9 18 17 16 15 14 13 18 17 16 15 14 13 EL10 el10 18 17 16 15 14 13 18 17 16 15 14 13 Debian 12 d12 18 17 16 15 14 13 18 17 16 15 14 13 Debian 13 d13 18 17 16 15 14 13 18 17 16 15 14 13 Ubuntu 22.04 u22 18 17 16 15 14 13 18 17 16 15 14 13 Ubuntu 24.04 u24 18 17 16 15 14 13 18 17 16 15 14 13 Get Started You can enable the pigsty infra \u0026 pgsql repo with the pig CLI tool:\nDefault Mirror curl https://repo.pigsty.io/pig | bash # download and install the pig CLI tool pig repo add all -u # add linux, pgdg, pigsty repo and update cache curl https://repo.pigsty.cc/pig | bash # download from mirror site pig repo add -u # add linux, pgdg, pigsty repo and update cache Manual Install You can also add these repos to your system manually with the default apt, dnf, yum approach.\nAPT YUM # Add Pigsty's GPG public key to your system keychain to verify package signatures curl -fsSL https://repo.pigsty.io/key | sudo gpg --dearmor -o /etc/apt/keyrings/pigsty.gpg # Get Debian distribution codename (distro_codename=jammy, focal, bullseye, bookworm), and write the corresponding upstream repository address to the APT List file distro_codename=$(lsb_release -cs) sudo tee /etc/apt/sources.list.d/pigsty-io.list \u003e /dev/null \u003c\u003cEOF deb [signed-by=/etc/apt/keyrings/pigsty.gpg] https://repo.pigsty.io/apt/infra generic main deb [signed-by=/etc/apt/keyrings/pigsty.gpg] https://repo.pigsty.io/apt/pgsql/${distro_codename} ${distro_codename} main EOF # Refresh APT repository cache sudo apt update # Add Pigsty's GPG public key to your system keychain to verify package signatures curl -fsSL https://repo.pigsty.io/key | sudo tee /etc/pki/rpm-gpg/RPM-GPG-KEY-pigsty \u003e/dev/null # Add Pigsty Repo definition files to /etc/yum.repos.d/ directory, including two repositories sudo tee /etc/yum.repos.d/pigsty-io.repo \u003e /dev/null \u003c\u003c-'EOF' [pigsty-infra] name=Pigsty Infra for $basearch baseurl=https://repo.pigsty.io/yum/infra/$basearch skip_if_unavailable = 1 enabled = 1 priority = 1 gpgcheck = 1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-pigsty module_hotfixes=1 [pigsty-pgsql] name=Pigsty PGSQL For el$releasever.$basearch baseurl=https://repo.pigsty.io/yum/pgsql/el$releasever.$basearch skip_if_unavailable = 1 enabled = 1 priority = 1 gpgcheck = 1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-pigsty module_hotfixes=1 EOF # Refresh YUM/DNF repository cache sudo yum makecache; All the RPM / DEB packages are signed with GPG Key fingerprint (B9BD8B20) in Pigsty repository.\nRepository Components Pigsty has two major repos: INFRA and PGSQL, providing DEB / RPM packages for x86_64 and aarch64 architecture.\nThe INFRA repo contains packages that are generic to any PostgreSQL version and Linux major version, including Prometheus \u0026 Grafana stack, admin tools for Postgres, and many utilities written in Go.\nLinux Package x86_64 aarch64 EL rpm ✓ ✓ Debian deb ✓ ✓ The PGSQL repo contains packages that are ad hoc to specific PostgreSQL Major Versions (often ad hoc to a specific Linux distro major version, too). Including extensions and some kernel forks.\nCompatibility Details OS Code Vendor Major Minor Fullname PG Major Version Comment el7.x86_64 EL 7 7.9 CentOS 7 x86 15 14 13 EOL el8.x86_64 EL 8 8.10 RockyLinux 8 x86 18 17 16 15 14 13 Near EOL el8.aarch64 EL 8 8.10 RockyLinux 8 ARM 18 17 16 15 14 13 Near EOL el9.x86_64 EL 9 9.6 RockyLinux 9 x86 18 17 16 15 14 13 OK el9.aarch64 EL 9 9.6 RockyLinux 9 ARM 18 17 16 15 14 13 OK el10.x86_64 EL 10 10.0 RockyLinux 10 x86 18 17 16 15 14 13 OK el10.aarch64 EL 10 10.0 RockyLinux 10 ARM 18 17 16 15 14 13 OK d11.x86_64 Debian 11 11.11 Debian 11 x86 17 16 15 14 13 EOL d11.aarch64 Debian 11 11.11 Debian 11 ARM 17 16 15 14 13 EOL d12.x86_64 Debian 12 12.12 Debian 12 x86 18 17 16 15 14 13 OK d12.aarch64 Debian 12 12.12 Debian 12 ARM 18 17 16 15 14 13 OK d13.x86_64 Debian 13 13.1 Debian 13 x86 18 17 16 15 14 13 OK d13.aarch64 Debian 13 13.1 Debian 13 ARM 18 17 16 15 14 13 OK u20.x86_64 Ubuntu 20 20.04.6 Ubuntu 20.04 x86 17 16 15 14 13 EOL u20.aarch64 Ubuntu 20 20.04.6 Ubuntu 20.04 ARM 17 16 15 14 13 EOL u22.x86_64 Ubuntu 22 22.04.5 Ubuntu 22.04 x86 18 17 16 15 14 13 OK u22.aarch64 Ubuntu 22 22.04.5 Ubuntu 22.04 ARM 18 17 16 15 14 13 OK u24.x86_64 Ubuntu 24 24.04.3 Ubuntu 24.04 x86 18 17 16 15 14 13 OK u24.aarch64 Ubuntu 24 24.04.3 Ubuntu 24.04 ARM 18 17 16 15 14 13 OK Source Building specs of these repos and packages are open-sourced on GitHub:\nhttps://github.com/pgsty/rpm https://github.com/pgsty/deb https://github.com/pgsty/infra-pkg ","categories":"","description":"The infrastructure to deliver PostgreSQL Extensions","excerpt":"The infrastructure to deliver PostgreSQL Extensions","ref":"/docs/repo/","tags":"","title":"Linux Repository"},{"body":"The Pigsty PGSQL Repo is designed to work together with the official PostgreSQL Global Development Group (PGDG) repo. Together, they can provide up to 400+ PostgreSQL Extensions out-of-the-box.\nMirror synced at 2025-12-29 12:00:00\nQuick Start You can install pig - the CLI tool, and add pgdg repo with it (recommended):\npig repo add pgdg # add pgdg repo file pig repo add pgdg -u # add pgdg repo and update cache pig repo add pgdg -u --region=default # add pgdg repo, enforce using the default repo (postgresql.org) pig repo add pgdg -u --region=china # add pgdg repo, always use the china mirror (repo.pigsty.cc) pig repo add pgsql -u # pgsql = pgdg + pigsty-pgsql (add pigsty + official PGDG) pig repo add -u # all = node + pgsql (pgdg + pigsty) + infra Mirror Since 2025-05, PGDG has closed the rsync/ftp sync channel, which makes almost all mirror sites out-of-sync.\nCurrently, Pigsty, Yandex, and Xtom are providing regular synced mirror service.\nThe Pigsty PGDG mirror is a subset of the official PGDG repo, covering EL 7-10, Debian 11-13, Ubuntu 20.04 - 24.04, with x86_64 \u0026 arm64 and PG 13 - 19alpha.\n2025-11 Update Notice: Aliyun/Tsinghua TUNA Resumed Currently, the Aliyun/Tsinghua TUNA mirror sites have resumed PGDG repository synchronization.\nCompatibility OS Code Vendor Major PG Major Version Comment el7.x86_64 EL 7 18 17 16 15 14 13 EOL el8.x86_64 EL 8 18 17 16 15 14 13 Near EOL el8.aarch64 EL 8 18 17 16 15 14 13 Near EOL el9.x86_64 EL 9 18 17 16 15 14 13 OK el9.aarch64 EL 9 18 17 16 15 14 13 OK el10.x86_64 EL 10 18 17 16 15 14 13 OK el10.aarch64 EL 10 18 17 16 15 14 13 OK d11.x86_64 Debian 11 18 17 16 15 14 13 EOL d11.aarch64 Debian 11 18 17 16 15 14 13 EOL d12.x86_64 Debian 12 18 17 16 15 14 13 OK d12.aarch64 Debian 12 18 17 16 15 14 13 OK d13.x86_64 Debian 13 18 17 16 15 14 13 OK d13.aarch64 Debian 13 18 17 16 15 14 13 OK u20.x86_64 Ubuntu 20 18 17 16 15 14 13 EOL u20.aarch64 Ubuntu 20 18 17 16 15 14 13 EOL u22.x86_64 Ubuntu 22 18 17 16 15 14 13 OK u22.aarch64 Ubuntu 22 18 17 16 15 14 13 OK u24.x86_64 Ubuntu 24 18 17 16 15 14 13 OK u24.aarch64 Ubuntu 24 18 17 16 15 14 13 OK Repo Configuration EL YUM/DNF Repo - { name: pgdg13 ,description: 'PostgreSQL 13' ,module: pgsql ,releases: [7,8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://download.postgresql.org/pub/repos/yum/13/redhat/rhel-$releasever-$basearch' ,china: 'https://repo.pigsty.cc/yum/pgdg/13/redhat/rhel-$releasever-$basearch' ,europe: 'https://mirrors.xtom.de/postgresql/repos/yum/13/redhat/rhel-$releasever-$basearch' }} - { name: pgdg14 ,description: 'PostgreSQL 14' ,module: pgsql ,releases: [7,8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://download.postgresql.org/pub/repos/yum/14/redhat/rhel-$releasever-$basearch' ,china: 'https://repo.pigsty.cc/yum/pgdg/14/redhat/rhel-$releasever-$basearch' ,europe: 'https://mirrors.xtom.de/postgresql/repos/yum/14/redhat/rhel-$releasever-$basearch' }} - { name: pgdg15 ,description: 'PostgreSQL 15' ,module: pgsql ,releases: [7,8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://download.postgresql.org/pub/repos/yum/15/redhat/rhel-$releasever-$basearch' ,china: 'https://repo.pigsty.cc/yum/pgdg/15/redhat/rhel-$releasever-$basearch' ,europe: 'https://mirrors.xtom.de/postgresql/repos/yum/15/redhat/rhel-$releasever-$basearch' }} - { name: pgdg16 ,description: 'PostgreSQL 16' ,module: pgsql ,releases: [ 8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://download.postgresql.org/pub/repos/yum/16/redhat/rhel-$releasever-$basearch' ,china: 'https://repo.pigsty.cc/yum/pgdg/16/redhat/rhel-$releasever-$basearch' ,europe: 'https://mirrors.xtom.de/postgresql/repos/yum/16/redhat/rhel-$releasever-$basearch' }} - { name: pgdg17 ,description: 'PostgreSQL 17' ,module: pgsql ,releases: [ 8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://download.postgresql.org/pub/repos/yum/17/redhat/rhel-$releasever-$basearch' ,china: 'https://repo.pigsty.cc/yum/pgdg/17/redhat/rhel-$releasever-$basearch' ,europe: 'https://mirrors.xtom.de/postgresql/repos/yum/17/redhat/rhel-$releasever-$basearch' }} - { name: pgdg18 ,description: 'PostgreSQL 18' ,module: pgsql ,releases: [ 8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://download.postgresql.org/pub/repos/yum/18/redhat/rhel-$releasever-$basearch' ,china: 'https://repo.pigsty.cc/yum/pgdg/18/redhat/rhel-$releasever-$basearch' ,europe: 'https://mirrors.xtom.de/postgresql/repos/yum/18/redhat/rhel-$releasever-$basearch' }} - { name: pgdg19-beta ,description: 'PostgreSQL 19 Beta' ,module: beta ,releases: [ 8,9,10] ,arch: [x86_64, aarch64] ,baseurl: { default: 'https://download.postgresql.org/pub/repos/yum/testing/19/redhat/rhel-$releasever-$basearch' ,china: 'https://repo.pigsty.cc/yum/pgdg/testing/19/redhat/rhel-$releasever-$basearch' ,europe: 'https://mirrors.xtom.de/postgresql/repos/yum/testing/19/redhat/rhel-$releasever-$basearch' }} Debian / Ubuntu APT Repo - { name: pgdg ,description: 'PGDG' ,module: pgsql ,releases: [11,12,13, 22,24] ,arch: [x86_64, aarch64] ,baseurl: { default: 'http://apt.postgresql.org/pub/repos/apt/ ${distro_codename}-pgdg main' ,china: 'https://repo.pigsty.cc/apt/pgdg/ ${distro_codename}-pgdg main' }} - { name: pgdg-beta ,description: 'PGDG Beta' ,module: beta ,releases: [11,12,13, 22,24] ,arch: [x86_64, aarch64] ,baseurl: { default: 'http://apt.postgresql.org/pub/repos/apt/ ${distro_codename}-pgdg-testing main 19' ,china: 'https://repo.pigsty.cc/apt/pgdg/ ${distro_codename}-pgdg-testing main 19' }} APT GPG Key PGDG APT repo is signed with the following GPG key: B97B0AFCAA1A47F044F244A07FCC7D46ACCC4CF8 (ACCC4CF8)\nMD5 checksum is f54c5c1aa1329dc26e33b29762faaec4, see https://www.postgresql.org/download/linux/debian/ for details.\nOfficial Mirror sudo curl -fsSL https://www.postgresql.org/media/keys/ACCC4CF8.asc -o /usr/share/postgresql-common/pgdg/apt.postgresql.org.asc . /etc/os-release sudo sh -c \"echo 'deb [signed-by=/usr/share/postgresql-common/pgdg/apt.postgresql.org.asc] https://apt.postgresql.org/pub/repos/apt $VERSION_CODENAME-pgdg main' \u003e /etc/apt/sources.list.d/pgdg.list\" sudo curl -fsSL https://repo.pigsty.cc/apt/pgdg/ACCC4CF8.key -o /usr/share/postgresql-common/pgdg/apt.postgresql.org.asc . /etc/os-release sudo sh -c \"echo 'deb [signed-by=/usr/share/postgresql-common/pgdg/apt.postgresql.org.asc] https://repo.pigsty.cc/apt/pgdg/ $VERSION_CODENAME-pgdg main' \u003e /etc/apt/sources.list.d/pgdg.list\" YUM GPG Key PGDG YUM repo is signed with a series of keys from https://ftp.postgresql.org/pub/repos/yum/keys/. Please choose and use as needed.\n","categories":"","description":"The official PostgreSQL APT/YUM repository","excerpt":"The official PostgreSQL APT/YUM repository","ref":"/docs/repo/pgdg/","tags":"","title":"PGDG Repo"},{"body":"You can verify the integrity of the packages you download from Pigsty repository by checking the GPG signature. This document describes how to import the GPG key used to sign the packages.\nSummary All the RPM / DEB packages are signed with GPG key fingerprint (B9BD8B20) in Pigsty repository.\nFull: 9592A7BC7A682E7333376E09E7935D8DB9BD8B20 Ruohang Feng (Pigsty) rh@vonng.com\npub rsa4096 2024-07-16 [SC] 9592A7BC7A682E7333376E09E7935D8DB9BD8B20 uid [ultimate] Ruohang Feng (Pigsty) \u003crh@vonng.com\u003e sub rsa4096 2024-07-16 [E] You can find the public GPG key at: https://repo.pigsty.io/key or https://repo.pigsty.cc/key\nImport On RHEL compatible Linux distributions, you can import this key with the following command:\nDefault Mirror curl -fsSL https://repo.pigsty.io/key | sudo tee /etc/pki/rpm-gpg/RPM-GPG-KEY-pigsty \u003e/dev/null curl -fsSL https://repo.pigsty.cc/key | sudo tee /etc/pki/rpm-gpg/RPM-GPG-KEY-pigsty \u003e/dev/null On Debian / Ubuntu compatible Linux distributions, you can import this key with the following command:\nDefault Mirror curl -fsSL https://repo.pigsty.io/key | sudo gpg --dearmor -o /etc/apt/keyrings/pigsty.gpg curl -fsSL https://repo.pigsty.cc/key | sudo gpg --dearmor -o /etc/apt/keyrings/pigsty.gpg Public Key The corresponding public key block is:\n-----BEGIN PGP PUBLIC KEY BLOCK----- mQINBGaV5PwBEACbErI+7yOrsXTT3mR83O6Fw9WyHJqozhyNPF3dA1gAtWpfWqd4 S9x6vBjVwUbIRn21jYgov0hDiaLABNQhRzifvVr0r1IjBW8lhA8zJGaO42Uz0aBW YIkajOklsXgYMX+gSmy5WXzM31sDQVMnzptHh9dwW067hMM5pJKDslu2pLMwSb9K QgIFcYsaR0taBkcDg4dNu1gncriD/GcdXIS0/V4R82DIYeIqj2S0lt0jDTACbUz3 C6esrTw2XerCeHKHb9c/V+KMhqvLJOOpy/aJWLrTGBoaH7xw6v0qg32OYiBxlUj9 VEzoQbDfbRkR+jlxiuYP3scUs/ziKrSh+0mshVbeuLRSNfuHLa7C4xTEnATcgD1J MZeMaJXIcDt+DN+1aHVQjY5YNvr5wA3ykxW51uReZf7/odgqVW3+1rhW5pd8NQKQ qoVUHOtIrC9KaiGfrczEtJTNUxcNZV9eBgcKHYDXB2hmR2pIf7WvydgXTs/qIsXg SIzfKjisi795Dd5GrvdLYXVnu9YzylWlkJ5rjod1wnSxkI/CcCJaoPLnXZA9KV7A cpMWWaUEXP/XBIwIU+vxDd1taBIaPIOv1KIdzvG7QqAQtf5Lphi5HfaGvBud/CVt mvWhRPJMr1J0ER2xAgU2iZR7dN0vSF6zDqc0W09RAoC0nDS3tupDX2BrOwARAQAB tCRSdW9oYW5nIEZlbmcgKFBpZ3N0eSkgPHJoQHZvbm5nLmNvbT6JAlEEEwEIADsW IQSVkqe8emguczM3bgnnk12Nub2LIAUCZpXk/AIbAwULCQgHAgIiAgYVCgkICwIE FgIDAQIeBwIXgAAKCRDnk12Nub2LIOMuEACBLVc09O4icFwc45R3KMvOMu14Egpn UkpmBKhErjup0TIunzI0zZH6HG8LGuf6XEdH4ItCJeLg5349UE00BUHNmxk2coo2 u4Wtu28LPqmxb6sqpuRAaefedU6vqfs7YN6WWp52pVF1KdOHkIOcgAQ9z3ZHdosM I/Y/UxO2t4pjdCAfJHOmGPrbgLcHSMpoLLxjuf3YIwS5NSfjNDd0Y8sKFUcMGLCF 5P0lv5feLLdZvh2Una34UmHKhZlXC5E3vlY9bf/LgsRzXRFQosD0RsCXbz3Tk+zF +j/eP3WhUvJshqIDuY6eJYCzMjiA8sM5gety+htVJuD0mewp+qAhjxE0d4bIr4qO BKQzBt9tT2ackCPdgW42VPS+IZymm1oMET0hgZfKiVpwsKO6qxeWn4RW2jJ0zkUJ MsrrxOPFdZQAtuFcLwa5PUAHHs6XQT2vzxDpeE9lInQ14lshofU5ZKIeb9sbvb/w P+xnDqvZ1pcotEIBvDK0S0jHbHHqtioIUdDFvdCBlBlYP1TQRNPlJ7TJDBBvhj8i fmjQsYSV1u36aHOJVGYNHv+SyJpVd3nHCZn97ADM9qHnDm7xljyHXPzIx4FMmBGJ UTiLH5yxa1xhWr42Iv3TykaQJVbpydmBuegFR8WbWitAvVqI3HvRG+FalLsjJruc 8YDAf7gHdj/937kCDQRmleT8ARAAmJxscC76NZzqFBiaeq2+aJxOt1HGPqKb4pbz jLKRX9sFkeXuzhfZaNDljnr2yrnQ75rit9Aah/loEhbSHanNUDCNmvOeSEISr9yA yfOnqlcVOtcwWQK57n6MvlCSM8Js3jdoSmCFHVtdFFwxejE5ok0dk1VFYDIg6DRk ZBMuxGO7ZJW7TzCxhK4AL+NNYA2wX6b+IVMn6CA9kwNwCNrrnGHR1sblSxZp7lPo +GsqzYY0LXGR2eEicgKd4lk38gaO8Q4d1mlpX95vgdhGKxR+CM26y9QU0qrO1hXP Fw6lX9HfIUkVNrqAa1mzgneYXivnLvcj8gc7bFAdweX4MyBHsmiPm32WqjUJFAmw kcKYaiyfDJ+1wusa/b+7RCnshWc8B9udYbXfvcpOGgphpUuvomKT8at3ToJfEWmR BzToYYTsgAAX8diY/X53BHCE/+MhLccglEUYNZyBRkTwDLrS9QgNkhrADaTwxsv1 8PwnVKve/ZxwOU0QGf4ZOhA2YQOE5hkRDR5uY2OHsOS5vHsd9Y6kNNnO8EBy99d1 QiBJOW3AP0nr4Cj1/NhdigAujsYRKiCAuPT7dgqART58VU4bZ3PgonMlziLe7+ht YYxV+wyP6LVqicDd0MLLvG7r/JOiWuABOUxsFFaRecehoPJjeAEQxnWJjedokXKL HVOFaEkAEQEAAYkCNgQYAQgAIBYhBJWSp7x6aC5zMzduCeeTXY25vYsgBQJmleT8 AhsMAAoJEOeTXY25vYsgG8sP/3UdsWuiwTsf/x4BTW82K+Uk9YwZDnUNH+4dUMED bKT1C6CbuSZ7Mnbi2rVsmGzOMs9MehIx6Ko8/iCR2OCeWi8Q+wM+iffAfWuT1GK6 7f/VIfoYBUWEa+kvDcPgEbd5Tu7ZdUO/jROVBSlXRSjzK9LpIj7GozBTJ8Vqy5x7 oqbWPPEYtGDVHime8o6f5/wfhNgL3mFnoq6srK7KhwACwfTXlNqAlGiXGa30Yj+b Cj6IvmxoII49E67/ovMEmzDCb3RXiaL6OATy25P+HQJvWvAam7Qq5Xn+bZg65Mup vXq3zoX0a7EKXc5vsJVNtTlXO1ATdYszKP5uNzkHrNAN52VRYaowq1vPy/MVMbSI rL/hTFKr7ZNhmC7jmS3OuJyCYQsfEerubtBUuc/W6JDc2oTI3xOG1S2Zj8f4PxLl H7vMG4E+p6eOrUGw6VQXjFsH9GtwhkPh/ZGMKENb2+JztJ02674Cok4s5c/lZFKz mmRUcNjX2bm2K0GfGG5/hAog/CHCeUZvwIh4hZLkdeJ1QsIYpN8xbvY7QP6yh4VB XrL18+2sontZ45MsGResrRibB35x7IrCrxZsVtRJZthHqshiORPatgy+AiWcAtEv UWEnnC1xBSasNebw4fSE8AJg9JMCRw+3GAetlotOeW9q7PN6yrXD9rGuV/QquQNd /c7w =4rRi -----END PGP PUBLIC KEY BLOCK----- Usage If you wish to distribute your own Repo with your own GPG key, here’s a tutorial:\nInstall GPG brew apt dnf brew install gnupg pinentry-mac sudo apt install gnupg2 pinentry-curses sudo dnf install gnupg2 pinentry-curses Generate GPG Key You can generate a GPG key with the following command:\ngpg --full-generate-key Import GPG Key If you have a GPG Private key, you can just import it with:\ngpg --import mykey.sec.as List GPG Key You can list GPG public keys and secret keys with the following commands:\n$ gpg --list-key [keyboxd] --------- pub rsa4096 2024-07-16 [SC] 9592A7BC7A682E7333376E09E7935D8DB9BD8B20 uid [ unknown] Ruohang Feng (Pigsty) \u003crh@vonng.com\u003e sub rsa4096 2024-07-16 [E] $ gpg --list-secret-key [keyboxd] --------- sec rsa4096 2024-07-16 [SC] 9592A7BC7A682E7333376E09E7935D8DB9BD8B20 uid [ unknown] Ruohang Feng (Pigsty) \u003crh@vonng.com\u003e ssb rsa4096 2024-07-16 [E] Sign RPM Packages If you wish to sign your RPM packages with a specific GPG key, you can specify the key in the ~/.rpmmacros file:\n%_signature gpg %_gpg_path ~/.gnupg %_gpg_name B9BD8B20 %_gpg_digest_algo sha256 rpm --addsign yourpackage.rpm Sign DEB Packages To sign your DEB packages, add the key id to reprepro configuration:\nOrigin: Pigsty Label: Pigsty INFRA Codename: generic Architectures: amd64 arm64 Components: main Description: pigsty apt repository for infra components SignWith: 9592A7BC7A682E7333376E09E7935D8DB9BD8B20 ","categories":"","description":"Import the GPG key for Pigsty repository","excerpt":"Import the GPG key for Pigsty repository","ref":"/docs/repo/gpg/","tags":"","title":"GPG Key"},{"body":"The pigsty-infra repo contains packages that are generic to any PostgreSQL version and Linux major version, including Prometheus \u0026 Grafana stack, admin tools for Postgres, and many utilities written in Go.\nThis repo is maintained by Ruohang Feng (Vonng) @ Pigsty, you can find all the build specs on https://github.com/pgsty/infra-pkg. Prebuilt RPM / DEB packages for RHEL / Debian / Ubuntu distros available for x86_64 and aarch64 arch. Hosted on Cloudflare CDN for free global access.\nLinux Package x86_64 aarch64 EL rpm ✓ ✓ Debian deb ✓ ✓ You can check the Release - Infra Changelog for the latest updates.\nQuick Start You can add the pigsty-infra repo with the pig CLI tool, it will automatically choose from apt/yum/dnf.\nDefault Mirror Hint curl https://repo.pigsty.io/pig | bash # download and install the pig CLI tool pig repo add infra # add pigsty-infra repo file to your system pig repo update # update local repo cache with apt / dnf # use when in mainland China or Cloudflare is down curl https://repo.pigsty.cc/pig | bash # install pig from China CDN mirror pig repo add infra # add pigsty-infra repo file to your system pig repo update # update local repo cache with apt / dnf # you can manage infra repo with these commands: pig repo add infra -u # add repo file, and update cache pig repo add infra -ru # remove all existing repo, add repo and make cache pig repo set infra # = pigsty repo add infra -ru pig repo add all # add infra, node, pgsql repo to your system pig repo set all # remove existing repo, add above repos and update cache Manual Setup You can also use this repo directly without the pig CLI tool, by adding them to your Linux OS repo list manually:\nAPT Repo On Debian / Ubuntu compatible Linux distros, you can add the GPG Key and APT repo file manually with:\nDefault Mirror NoKey # Add Pigsty's GPG public key to your system keychain to verify package signatures, or just trust curl -fsSL https://repo.pigsty.io/key | sudo gpg --dearmor -o /etc/apt/keyrings/pigsty.gpg # Get Debian distribution codename (distro_codename=jammy, focal, bullseye, bookworm) # and write the corresponding upstream repository address to the APT List file distro_codename=$(lsb_release -cs) sudo tee /etc/apt/sources.list.d/pigsty-infra.list \u003e /dev/null \u003c\u003cEOF deb [signed-by=/etc/apt/keyrings/pigsty.gpg] https://repo.pigsty.io/apt/infra generic main EOF # Refresh APT repository cache sudo apt update # use when in mainland China or Cloudflare is down # Add Pigsty's GPG public key to your system keychain to verify package signatures, or just trust curl -fsSL https://repo.pigsty.cc/key | sudo gpg --dearmor -o /etc/apt/keyrings/pigsty.gpg # Get Debian distribution codename (distro_codename=jammy, focal, bullseye, bookworm) # and write the corresponding upstream repository address to the APT List file distro_codename=$(lsb_release -cs) sudo tee /etc/apt/sources.list.d/pigsty-infra.list \u003e /dev/null \u003c\u003cEOF deb [signed-by=/etc/apt/keyrings/pigsty.gpg] https://repo.pigsty.cc/apt/infra generic main EOF # Refresh APT repository cache sudo apt update # If you don't want to trust any GPG key, just trust the repo directly distro_codename=$(lsb_release -cs) sudo tee /etc/apt/sources.list.d/pigsty-infra.list \u003e /dev/null \u003c\u003cEOF deb [trust=yes] https://repo.pigsty.io/apt/infra generic main EOF sudo apt update YUM Repo On RHEL compatible Linux distros, you can add the GPG Key and YUM repo file manually with:\nDefault Mirror NoKey # Add Pigsty's GPG public key to your system keychain to verify package signatures curl -fsSL https://repo.pigsty.io/key | sudo tee /etc/pki/rpm-gpg/RPM-GPG-KEY-pigsty \u003e/dev/null # Add Pigsty Repo definition files to /etc/yum.repos.d/ directory sudo tee /etc/yum.repos.d/pigsty-infra.repo \u003e /dev/null \u003c\u003c-'EOF' [pigsty-infra] name=Pigsty Infra for $basearch baseurl=https://repo.pigsty.io/yum/infra/$basearch skip_if_unavailable = 1 enabled = 1 priority = 1 gpgcheck = 1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-pigsty module_hotfixes=1 EOF # Refresh YUM/DNF repository cache sudo yum makecache; # use when in mainland China or Cloudflare is down # Add Pigsty's GPG public key to your system keychain to verify package signatures curl -fsSL https://repo.pigsty.cc/key | sudo tee /etc/pki/rpm-gpg/RPM-GPG-KEY-pigsty \u003e/dev/null # Add Pigsty Repo definition files to /etc/yum.repos.d/ directory sudo tee /etc/yum.repos.d/pigsty-infra.repo \u003e /dev/null \u003c\u003c-'EOF' [pigsty-infra] name=Pigsty Infra for $basearch baseurl=https://repo.pigsty.cc/yum/infra/$basearch skip_if_unavailable = 1 enabled = 1 priority = 1 gpgcheck = 1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-pigsty module_hotfixes=1 EOF # Refresh YUM/DNF repository cache sudo yum makecache; # If you don't want to trust any GPG key, just trust the repo directly sudo tee /etc/yum.repos.d/pigsty-infra.repo \u003e /dev/null \u003c\u003c-'EOF' [pigsty-infra] name=Pigsty Infra for $basearch baseurl=https://repo.pigsty.io/yum/infra/$basearch skip_if_unavailable = 1 enabled = 1 priority = 1 gpgcheck = 0 module_hotfixes=1 EOF sudo yum makecache; Content For a detailed list of all packages available in the Infra repository, see the Package List.\nFor the changelog and release history, see the Release Log.\nSource Building specs of this repo is open-sourced on GitHub:\nhttps://github.com/pgsty/infra-pkg If the platform is not supported, you can also build the packages from source code by yourself.\n","categories":"","description":"Packages that are generic to any PostgreSQL version and Linux major version.","excerpt":"Packages that are generic to any PostgreSQL version and Linux major …","ref":"/docs/repo/infra/","tags":"","title":"INFRA Repo"},{"body":" Grafana Stack Name Version License Comment grafana 12.3.1 AGPLv3 Observability and visualization platform loki 3.1.1 AGPLv3 Log aggregation system (deprecated) promtail 3.0.0 AGPLv3 Loki log collection agent (deprecated) grafana-infinity-ds 3.7.0 Apache-2.0 JSON/CSV/XML datasource support grafana-plugins 12.3.0 Apache-2.0 Extra panel plugins by Pigsty Victoria Stack Name Version License Comment victoria-metrics 1.133.0 Apache-2.0 High-performance TSDB, Prometheus alternative victoria-logs 1.43.1 Apache-2.0 High-performance log storage and query engine victoria-traces 0.5.1 Apache-2.0 Distributed tracing backend victoria-metrics-cluster 1.133.0 Apache-2.0 VictoriaMetrics distributed cluster vmutils 1.133.0 Apache-2.0 VictoriaMetrics CLI utilities vlogscli 1.43.1 Apache-2.0 VictoriaLogs interactive query client vlagent 1.43.1 Apache-2.0 VictoriaLogs log collection agent grafana-victorialogs-ds 0.23.3 Apache-2.0 VictoriaLogs Grafana datasource grafana-victoriametrics-ds 0.20.0 Apache-2.0 VictoriaMetrics Grafana datasource Note on Victoria Grafana Datasource Plugins Pigsty splits the Victoria datasource extensions into architecture-specific sub-packages. If you choose to install these plugins to your own Grafana instance, please configure the following parameter in /etc/grafana/grafana.ini to allow loading unsigned plugins.\nallow_loading_unsigned_plugins = victoriametrics-logs-datasource,victoriametrics-metrics-datasource Prometheus Stack Name Version License Comment prometheus 3.9.1 Apache-2.0 Cloud-native monitoring \u0026 TSDB pushgateway 1.11.2 Apache-2.0 Metrics push gateway for short-lived jobs alertmanager 0.30.0 Apache-2.0 Alert management \u0026 notification dispatch blackbox_exporter 0.27.0 Apache-2.0 Blackbox probing, endpoint availability Metric Exporters Name Version License Comment pg_exporter 1.1.2 Apache-2.0 Advanced Postgres metrics exporter pgbackrest_exporter 0.22.0 MIT Expose pgbackrest metrics node_exporter 1.10.2 Apache-2.0 Expose Linux node metrics keepalived_exporter 1.7.0 GPL-3.0 Expose keepalived/VIP metrics nginx_exporter 1.5.1 Apache-2.0 Expose nginx metrics zfs_exporter 3.8.1 MIT Expose zfs metrics mysqld_exporter 0.18.0 Apache-2.0 Expose mysql metrics redis_exporter 1.80.1 MIT Expose redis metrics kafka_exporter 1.9.0 Apache-2.0 Expose kafka metrics mongodb_exporter 0.47.2 Apache-2.0 Expose mongodb metrics mtail 3.0.8 Apache-2.0 Parse logs and generate metrics vector 0.52.0 MPL-2.0 Versatile log collector Object Storage Name Version License Comment minio 20251203120000 AGPLv3 FOSS S3 server, now built by pgsty mcli 20250813083541 AGPLv3 FOSS S3 client rustfs alpha.80 Apache-2.0 FOSS MinIO, Alpha garage 2.1.0 AGPL-3.0 Lightweight S3 seaweedfs 4.06 Apache-2.0 S3 for small files rclone 1.72.1 MIT S3 command line tool restic 0.18.1 BSD-2 Backup tool juicefs 1.3.1 Apache-2.0 Filesystem over S3 Databases PostgreSQL related tools, DBMS, and other utilities\nName Version License Comment etcd 3.6.7 Apache-2.0 Fault-tolerant distributed coordination kafka 4.1.1 Apache-2.0 Message queue duckdb 1.4.3 MIT Embedded OLAP ferretdb 2.7.0 Apache-2.0 MongoDB over PG tigerbeetle 0.16.68 Apache-2.0 Financial OLTP IvorySQL 5.1 Apache-2.0 Oracle compatible PG 18.1 Utilities Pig package manager, PostgreSQL tools, and other database related utilities\nName Version License Comment pig 0.9.1 Apache-2.0 PG package manager vip-manager 4.0.0 BSD-2 Bind L2 VIP to PG primary pgflo 0.0.15 Apache-2.0 Stream, transform, route PG data in real-time pgschema 1.4.2 Apache-2.0 Perform PG schema migration pg_timetable 6.2.0 PostgreSQL Advanced scheduling for PostgreSQL timescaledb-tools 0.18.1 Apache-2.0 Optimize timescaledb params timescaledb-event-streamer 0.20.0 Apache-2.0 CDC on timescaledb hypertable dblab 0.34.2 MIT Multi-database CLI tool sqlcmd 1.9.0 MIT MS SQL Server CLI client pev2 1.19.0 PostgreSQL PostgreSQL explain visualizer 2 sealos 5.0.1 Apache-2.0 Battery-included Kubernetes distribution vray 5.44.1 MIT Build proxies to bypass network restrictions asciinema 3.1.0 GPL-3.0 Terminal session recording and playback postgrest 14.3 MIT RESTful API for PostgreSQL npgsqlrest 3.4.3 MIT .NET REST API generator for PostgreSQL AI Coding AI Agent, MCP tools, python package manager, web IDE, etc…\nName Version License Comment claude 2.1.9 Proprietary Claude Code - Anthropic’s agentic coding tool opencode 1.1.23 MIT Terminal-based AI coding assistant code-server 4.108.0 MIT VS Code in the browser genai-toolbox 0.25.0 Apache-2.0 Google database MCP server uv 0.9.26 MIT Next-gen Python package manager golang 1.25.6 BSD-3 Go compiler nodejs 24.12.0 MIT/Mixed Run Javascript on serverside ","categories":"","description":"Available packages in the Infra repository","excerpt":"Available packages in the Infra repository","ref":"/docs/repo/infra/list/","tags":"","title":"Package List"},{"body":" 2026-01-16 Name Old Ver New Ver Note prometheus 3.8.1 3.9.1 victoria-metrics 1.132.0 1.133.0 tigerbeetle 0.16.65 0.16.68 kafka 4.0.0 4.1.1 grafana-victoriametrics-ds 0.19.7 0.20.0 grafana-victorialogs-ds 0.23.2 0.23.3 grafana-infinity-ds 3.6.0 3.7.0 uv 0.9.18 0.9.26 seaweedfs 4.01 4.06 rustfs alpha.71 alpha.80 v2ray 5.28.0 5.44.1 sqlcmd 1.8.0 1.9.0 opencode 1.0.223 1.1.23 claude 2.1.1 2.1.9 golang 1.25.5 1.25.6 asciinema 3.0.1 3.1.0 code-server 4.107.0 4.108.0 npgsqlrest 3.3.0 3.4.3 genai-toolbox 0.24.0 0.25.0 pg_exporter 1.1.1 1.1.2 pig 0.9.0 0.9.1 2026-01-08 Name Old Ver New Ver Note pg_exporter 1.1.0 1.1.1 new pg_timeline collector npgsqlrest - 3.3.3 new postgrest - 14.3 new opencode 1.0.223 new code-server 4.107.0 new claude 2.0.76 2.1.1 update genai-toolbox 0.23.0 0.24.0 remove broken oracle driver golang - 1.25.5 new nodejs - 24.12.0 new 2025-12-25 Name Old Ver New Ver Note pig 0.8.0 0.9.0 routine update etcd 3.6.6 3.6.7 routine update uv - 0.9.18 new python package manager ccm - 2.0.76 new claude code asciinema - 3.0.1 new terminal recorder ivorysql 5.0 5.1 grafana 12.3.0 12.3.1 vector 0.51.1 0.52.0 prometheus 3.8.0 3.8.1 alertmanager 0.29.0 0.30.0 victoria-logs 1.41.0 1.43.1 pgbackrest_exporter 0.21.0 0.22.0 grafana-victorialogs-ds 0.22.4 0.23.2 2025-12-16 Name Old Ver New Ver Note victoria-metrics 1.131.0 1.132.0 victoria-logs 1.40.0 1.41.0 blackbox_exporter 0.27.0 0.28.0 duckdb 1.4.2 1.4.3 rclone 1.72.0 1.72.1 pev2 1.17.0 1.19.0 pg_exporter 1.0.3 1.1.0 pig 0.7.4 0.8.0 genai-toolbox 0.22.0 0.23.0 minio 20250907161309 20251203120000 by pgsty 2025-12-04 Name Old Ver New Ver Note rustfs - 1.0.0-a71 new seaweedfs - 4.1.0 new garage - 2.1.0 new rclone 1.71.2 1.72.0 vector 0.51.0 0.51.1 prometheus 3.7.3 3.8.0 victoria-metrics 0.130.0 0.131.0 victoria-logs 0.38.0 0.40.0 victoria-traces - 0.5.1 new grafana-victorialogs-ds 0.22.1 0.22.4 redis_exporter 1.80.0 1.80.1 mongodb_exporter 0.47.1 0.47.2 genai-toolbox 0.21.0 0.22.0 2025-11-23 Name Old Ver New Ver Note pgschema - 1.4.2 new pgflo - 0.0.15 new vector 0.51.0 0.51.1 bug fix sealos 5.0.1 5.1.1 etcd 3.6.5 3.6.6 duckdb 1.4.1 1.4.2 pg_exporter 1.0.2 1.0.3 pig 0.7.1 0.7.2 grafana 12.1.0 12.3.0 pg_timetable 6.1.0 6.2.0 genai-toolbox 0.16.0 0.21.0 timescaledb-tools 0.18.0 0.18.1 moved from PGSQL to INFRA timescaledb-event-streamer 0.12.0 0.20.0 tigerbeetle 0.16.60 0.16.65 victoria-metrics 1.129.1 1.130.0 victoria-logs 1.37.2 1.38.0 grafana-victorialogs-ds 0.21.4 0.22.1 grafana-victoriametrics-ds 0.19.6 0.19.7 grafana-plugins 12.0.0 12.3.0 2025-11-11 Name Old Ver New Ver Note grafana 12.1.0 12.2.1 download url change prometheus 3.6.0 3.7.3 pushgateway 1.11.1 1.11.2 alertmanager 0.28.1 0.29.0 nginx_exporter 1.5.0 1.5.1 node_exporter 1.9.1 1.10.2 pgbackrest_exporter 0.20.0 0.21.0 redis_exporter 1.77.0 1.80.0 duckdb 1.4.0 1.4.1 dblab 0.33.0 0.34.2 pg_timetable 5.13.0 6.1.0 vector 0.50.0 0.51.0 rclone 1.71.1 1.71.2 victoria-metrics 1.126.0 1.129.1 victoria-logs 1.35.0 1.37.2 grafana-victorialogs-ds 0.21.0 0.21.4 grafana-victoriametrics-ds 0.19.4 0.19.6 grafana-infinity-ds 3.5.0 3.6.0 genai-toolbox 0.16.0 0.18.0 pev2 1.16.0 1.17.0 pig 0.6.2 0.7.1 2025-10-18 Name Old Ver New Ver Note prometheus 3.5.0 3.6.0 nginx_exporter 1.4.2 1.5.0 mysqld_exporter 0.17.2 0.18.0 redis_exporter 1.75.0 1.77.0 mongodb_exporter 0.47.0 0.47.1 victoria-metrics 1.121.0 1.126.0 victoria-logs 1.25.1 1.35.0 duckdb 1.3.2 1.4.0 etcd 3.6.4 3.6.5 restic 0.18.0 0.18.1 tigerbeetle 0.16.54 0.16.60 grafana-victorialogs-ds 0.19.3 0.21.0 grafana-victoriametrics-ds 0.18.3 0.19.4 grafana-infinity-ds 3.3.0 3.5.0 genai-toolbox 0.9.0 0.16.0 grafana 12.1.0 12.2.0 vector 0.49.0 0.50.0 rclone 1.70.3 1.71.1 minio 20250723155402 20250907161309 mcli 20250721052808 20250813083541 2025-08-15 Name Old Ver New Ver Note grafana 12.0.0 12.1.0 pg_exporter 1.0.1 1.0.2 pig 0.6.0 0.6.1 vector 0.48.0 0.49.0 redis_exporter 1.74.0 1.75.0 mongodb_exporter 0.46.0 0.47.0 victoria-metrics 1.121.0 1.123.0 victoria-logs 1.25.0 1.28.0 grafana-victoriametrics-ds 0.17.0 0.18.3 grafana-victorialogs-ds 0.18.3 0.19.3 grafana-infinity-ds 3.3.0 3.4.1 etcd 3.6.1 3.6.4 ferretdb 2.3.1 2.5.0 tigerbeetle 0.16.50 0.16.54 genai-toolbox 0.9.0 0.12.0 2025-07-24 Name Old Ver New Ver Note ferretdb - 2.4.0 pair with documentdb 1.105 etcd - 3.6.3 minio - 20250723155402 mcli - 20250721052808 ivorysql - 4.5-0ffca11-20250709 fix libxcrypt dep issue 2025-07-16 Name Old Ver New Ver Note genai-toolbox 0.8.0 0.9.0 MCP toolbox for various DBMS victoria-metrics 1.120.0 1.121.0 split into various packages victoria-logs 1.24.0 1.25.0 split into various packages prometheus 3.4.2 3.5.0 duckdb 1.3.1 1.3.2 etcd 3.6.1 3.6.2 tigerbeetle 0.16.48 0.16.50 grafana-victoriametrics-ds 0.16.0 0.17.0 rclone 1.69.3 1.70.3 pig 0.5.0 0.6.0 pev2 1.15.0 1.16.0 pg_exporter 1.0.0 1.0.1 2025-07-04 Name Old Ver New Ver Note prometheus 3.4.1 3.4.2 grafana 12.0.1 12.0.2 vector 0.47.0 0.48.0 rclone 1.69.0 1.70.2 vip-manager 3.0.0 4.0.0 blackbox_exporter 0.26.0 0.27.0 redis_exporter 1.72.1 1.74.0 duckdb 1.3.0 1.3.1 etcd 3.6.0 3.6.1 ferretdb 2.2.0 2.3.1 dblab 0.32.0 0.33.0 tigerbeetle 0.16.41 0.16.48 grafana-victorialogs-ds 0.16.3 0.18.1 grafana-victoriametrics-ds 0.15.1 0.16.0 grafana-infinity-ds 3.2.1 3.3.0 victoria-logs 1.22.2 1.24.0 victoria-metrics 1.117.1 1.120.0 2025-06-01 Name Old Ver New Ver Note grafana - 12.0.1 prometheus - 3.4.1 keepalived_exporter - 1.7.0 redis_exporter - 1.73.0 victoria-metrics - 1.118.0 victoria-logs - 1.23.1 tigerbeetle - 0.16.42 grafana-victorialogs-ds - 0.17.0 grafana-infinity-ds - 3.2.2 2025-05-22 Name Old Ver New Ver Note dblab - 0.32.0 prometheus - 3.4.0 duckdb - 1.3.0 etcd - 3.6.0 pg_exporter - 1.0.0 ferretdb - 2.2.0 rclone - 1.69.3 minio - 20250422221226 last version with admin GUI mcli - 20250416181326 nginx_exporter - 1.4.2 keepalived_exporter - 1.6.2 pgbackrest_exporter - 0.20.0 redis_exporter - 1.27.1 victoria-metrics - 1.117.1 victoria-logs - 1.22.2 pg_timetable - 5.13.0 tigerbeetle - 0.16.41 pev2 - 1.15.0 grafana - 12.0.0 grafana-victorialogs-ds - 0.16.3 grafana-victoriametrics-ds - 0.15.1 grafana-infinity-ds - 3.2.1 grafana-plugins - 12.0.0 2025-04-23 Name Old Ver New Ver Note mtail - 3.0.8 new pig - 0.4.0 pg_exporter - 0.9.0 prometheus - 3.3.0 pushgateway - 1.11.1 keepalived_exporter - 1.6.0 redis_exporter - 1.70.0 victoria-metrics - 1.115.0 victoria-logs - 1.20.0 duckdb - 1.2.2 pg_timetable - 5.12.0 vector - 0.46.1 minio - 20250422221226 mcli - 20250416181326 2025-04-05 Name Old Ver New Ver Note pig - 0.3.4 etcd - 3.5.21 restic - 0.18.0 ferretdb - 2.1.0 tigerbeetle - 0.16.34 pg_exporter - 0.8.1 node_exporter - 1.9.1 grafana - 11.6.0 zfs_exporter - 3.8.1 mongodb_exporter - 0.44.0 victoria-metrics - 1.114.0 minio - 20250403145628 mcli - 20250403170756 2025-03-23 Name Old Ver New Ver Note etcd - 3.5.20 pgbackrest_exporter - 0.19.0 rebuilt victoria-logs - 1.17.0 vlogscli - 1.17.0 2025-03-17 Name Old Ver New Ver Note kafka - 4.0.0 prometheus - 3.2.1 alertmanager - 0.28.1 blackbox_exporter - 0.26.0 node_exporter - 1.9.0 mysqld_exporter - 0.17.2 kafka_exporter - 1.9.0 redis_exporter - 1.69.0 duckdb - 1.2.1 etcd - 3.5.19 ferretdb - 2.0.0 tigerbeetle - 0.16.31 vector - 0.45.0 victoria-metrics - 1.114.0 victoria-logs - 1.16.0 rclone - 1.69.1 pev2 - 1.14.0 grafana-victorialogs-ds - 0.16.0 grafana-victoriametrics-ds - 0.14.0 grafana-infinity-ds - 3.0.0 timescaledb-event-streamer - 0.12.0 new restic - 0.17.3 new juicefs - 1.2.3 new 2025-02-12 Name Old Ver New Ver Note pushgateway 1.10.0 1.11.0 alertmanager 0.27.0 0.28.0 nginx_exporter 1.4.0 1.4.1 pgbackrest_exporter 0.18.0 0.19.0 redis_exporter 1.66.0 1.67.0 mongodb_exporter 0.43.0 0.43.1 victoria-metrics 1.107.0 1.111.0 victoria-logs 1.3.2 1.9.1 duckdb 1.1.3 1.2.0 etcd 3.5.17 3.5.18 pg_timetable 5.10.0 5.11.0 ferretdb 1.24.0 2.0.0 tigerbeetle 0.16.13 0.16.27 grafana 11.4.0 11.5.1 vector 0.43.1 0.44.0 minio 20241218131544 20250207232109 mcli 20241121172154 20250208191421 rclone 1.68.2 1.69.0 2024-11-19 Name Old Ver New Ver Note prometheus 2.54.0 3.0.0 victoria-metrics 1.102.1 1.106.1 victoria-logs 0.28.0 1.0.0 mysqld_exporter 0.15.1 0.16.0 redis_exporter 1.62.0 1.66.0 mongodb_exporter 0.41.2 0.42.0 keepalived_exporter 1.3.3 1.4.0 duckdb 1.1.2 1.1.3 etcd 3.5.16 3.5.17 tigerbeetle 16.8 0.16.13 grafana - 11.3.0 vector - 0.42.0 ","categories":["Reference"],"description":"pigsty-infra repository changelog and observability package release notes","excerpt":"pigsty-infra repository changelog and observability package release …","ref":"/docs/repo/infra/log/","tags":"","title":"Release Log"},{"body":"The pigsty-pgsql repo contains packages that are ad hoc to specific PostgreSQL Major Versions (often ad hoc to a specific Linux distro major version, too). Including extensions and some kernel forks.\nYou can check the Release - RPM Changelog / Release - DEB Changelog for the latest updates.\nCompatibility OS / Arch OS x86_64 aarch64 EL8 el8 18 17 16 15 14 13 18 17 16 15 14 13 EL9 el9 18 17 16 15 14 13 18 17 16 15 14 13 EL10 el10 18 17 16 15 14 13 18 17 16 15 14 13 Debian 12 d12 18 17 16 15 14 13 18 17 16 15 14 13 Debian 13 d13 18 17 16 15 14 13 18 17 16 15 14 13 Ubuntu 22.04 u22 18 17 16 15 14 13 18 17 16 15 14 13 Ubuntu 24.04 u24 18 17 16 15 14 13 18 17 16 15 14 13 Quick Start PIG You can install pig - the CLI tool, and add pgdg / pigsty repo with it (recommended):\npig repo add pigsty # add pigsty-pgsql repo pig repo add pigsty -u # add pigsty-pgsql repo, and update cache pig repo add pigsty -u --region=default # add pigsty-pgsql repo and enforce default region (pigsty.io) pig repo add pigsty -u --region=china # add pigsty-pgsql repo with china region (pigsty.cc) pig repo add pgsql -u # pgsql = pgdg + pigsty-pgsql (add pigsty + official PGDG) pig repo add -u # all = node + pgsql (pgdg + pigsty) + infra Hint: If you are in mainland China, consider using the China CDN mirror (replace pigsty.io with pigsty.cc)\nAPT You can also enable this repo with apt directly on Debian / Ubuntu:\nDefault Mirror # Add Pigsty's GPG public key to your system keychain to verify package signatures curl -fsSL https://repo.pigsty.io/key | sudo gpg --dearmor -o /etc/apt/keyrings/pigsty.gpg # Get Debian distribution codename (distro_codename=jammy, focal, bullseye, bookworm), and write the corresponding upstream repository address to the APT List file distro_codename=$(lsb_release -cs) sudo tee /etc/apt/sources.list.d/pigsty-io.list \u003e /dev/null \u003c\u003cEOF deb [signed-by=/etc/apt/keyrings/pigsty.gpg] https://repo.pigsty.io/apt/pgsql/${distro_codename} ${distro_codename} main EOF # Refresh APT repository cache sudo apt update # Use when in mainland China or Cloudflare is unavailable # Add Pigsty's GPG public key to your system keychain to verify package signatures curl -fsSL https://repo.pigsty.cc/key | sudo gpg --dearmor -o /etc/apt/keyrings/pigsty.gpg # Get Debian distribution codename, and write the corresponding upstream repository address to the APT List file distro_codename=$(lsb_release -cs) sudo tee /etc/apt/sources.list.d/pigsty-io.list \u003e /dev/null \u003c\u003cEOF deb [signed-by=/etc/apt/keyrings/pigsty.gpg] https://repo.pigsty.cc/apt/pgsql/${distro_codename} ${distro_codename} main EOF # Refresh APT repository cache sudo apt update DNF You can also enable this repo with dnf/yum directly on EL-compatible systems:\nDefault Mirror # Add Pigsty's GPG public key to your system keychain to verify package signatures curl -fsSL https://repo.pigsty.io/key | sudo tee /etc/pki/rpm-gpg/RPM-GPG-KEY-pigsty \u003e/dev/null # Add Pigsty Repo definition files to /etc/yum.repos.d/ directory, including two repositories sudo tee /etc/yum.repos.d/pigsty-pgsql.repo \u003e /dev/null \u003c\u003c-'EOF' [pigsty-pgsql] name=Pigsty PGSQL For el$releasever.$basearch baseurl=https://repo.pigsty.io/yum/pgsql/el$releasever.$basearch skip_if_unavailable = 1 enabled = 1 priority = 1 gpgcheck = 1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-pigsty module_hotfixes=1 EOF # Refresh YUM/DNF repository cache sudo dnf makecache; # Use when in mainland China or Cloudflare is unavailable # Add Pigsty's GPG public key to your system keychain to verify package signatures curl -fsSL https://repo.pigsty.cc/key | sudo tee /etc/pki/rpm-gpg/RPM-GPG-KEY-pigsty \u003e/dev/null # Add Pigsty Repo definition files to /etc/yum.repos.d/ directory sudo tee /etc/yum.repos.d/pigsty-pgsql.repo \u003e /dev/null \u003c\u003c-'EOF' [pigsty-pgsql] name=Pigsty PGSQL For el$releasever.$basearch baseurl=https://repo.pigsty.cc/yum/pgsql/el$releasever.$basearch skip_if_unavailable = 1 enabled = 1 priority = 1 gpgcheck = 1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-pigsty module_hotfixes=1 EOF # Refresh YUM/DNF repository cache sudo dnf makecache; Source Building specs of this repo is open-sourced on GitHub:\nhttps://github.com/pgsty/rpm https://github.com/pgsty/deb If the platform is not supported, you can also build the packages from source code by yourself.\nThe pig build provides an easy way to build extension RPM/DEB on your own systems.\n","categories":"","description":"The repo for PostgreSQL Extensions \u0026 Kernel Forks","excerpt":"The repo for PostgreSQL Extensions \u0026 Kernel Forks","ref":"/docs/repo/pgsql/","tags":"","title":"PGSQL Repo"},{"body":" 2026-01-16 Name Old New Note etcd_fdw 0.0.0 new pg_ttl_index 0.1.0 new citus 13.2.0 14.0.0 +pg18, pre-release pg_search 0.20.5 0.21.2 +pg18 pg_clickhouse 0.1.0 0.1.2 pg_textsearch 0.1.0 0.4.0 pg_convert 0.0.5 0.1.0 pg_timeseries 0.1.8 0.2.0 biscuit 2.0.1 2.2.2 pgmq 1.8.0 1.8.1 documentdb 0.107 0.109 +pg18, use ms version pg_bulkload 3.1.22 3.1.23 +pg18 age 1.5.0 1.6.0 pgsentinel 1.3.0 1.3.1 pljs - 1.0.4 new pg_partman 5.3.1 5.4.0 use PGDG pgfincore - 1.3.1 +pg18 documentdb_extended_rum 0.109 new mobilitydb_datagen 1.3.0 new 2025-12-25 Name Old Ver New Ver Note pg_duckdb 1.1.0 1.1.1 pg_search 0.20.4 0.20.5 vchord_bm25 0.2.2 0.3.0 pg_semver 0.40.0 0.41.0 pg_timeseries 0.1.7 0.1.8 2025-12-16 Name Old Ver New Ver Note pg_textsearch - 0.1.0 new extension pg_clickhouse - 0.1.0 new extension pg_ai_query - 0.1.1 new extension timescaledb 2.23.1 2.24.0 pg_search 0.20.0 0.20.4 pg_duckdb 1.1.0-1 1.1.0-2 official release pg_biscuit 1.0 2.0.1 switch to new repo pg_convert 0.0.4 0.0.5 removed pg13 support pgdd 0.6.0 0.6.1 removed pg13 support pglinter 1.0.0 1.0.1 pg_session_jwt 0.3.3 0.4.0 pg_anon 2.4.1 2.5.1 pg_enigma 0.4.0 0.5.0 wrappers 0.5.6 0.5.7 pg_vectorize 0.25.0 0.26.0 synchdb - 1.3 EL9 only 2025-11-20 Name Old Ver New Ver Note vchord 0.5.3 1.0.0 pg_later 0.3.1 0.4.0 pgvectorscale 0.8.0 0.9.0 -pg13, +pg18 pglite_fusion 0.0.5 0.0.6 pgx_ulid 0.2.1 0.2.2 pg_search 0.19.5 0.19.7 resume PIGSTY building citus 13.2.0 13.2.0 official tag build timescaledb 2.23.0 2.23.1 pg_profile 4.10 4.11 pglinter 1.0.0 new pg_typeid 0.3.0 align pg18 support pg_enigma 0.4.0 vonng patched pgrx version pg_retry 1.0.0 new, pg17-18 pg_biscuit 1.0 new, pg16-18 pg_weighted_statistics 1.0.0 new, pg13-18 pg_stat_monitor 2.2.0 2.3.0 fix PGDG pg18 missing issue documentdb 0.106 0.107 ferretdb fork polardb 15.15 15.15.5.0-38948055 2025-11-10 Add PostgreSQL 18 support for almost all extensions\nName Old Ver New Ver Note omni_csv - 0.1.1 new ext omni_datasets - 0.1.0 new ext omni_shmem - 0.1.0 new ext pg_csv - 1.0.1 new ext pg_dbms_errlog - 2.2 new ext pg_rrule - 0.2.0 new ext plxslt - 0.20140221 new ext anon 2.3.0 2.4.1 +pg18 collection 1.0.0 1.1.0 +pg18 credcheck 3.0 4.2 +pg18 emaj 4.7.0 4.7.1 +pg18 explain_ui 0.0.1 0.0.2 +pg18 firebird_fdw 1.4.0 1.4.1 +pg18 logerrors 2.1.3 2.1.5 +pg18 multicorn 3.0 3.2 +pg18 omni 0.2.9 0.2.14 +pg18 omni_email 0 0.1.0 +pg18 omni_httpc 0.1.5 0.1.10 +pg18 omni_httpd 0.4.6 0.4.11 +pg18 omni_id 0.4.2 0.4.3 +pg18 omni_kube 0.1.1 0.4.2 +pg18 omni_ledger 0.1.2 0.1.3 +pg18 omni_sql 0.5.1 0.5.3 +pg18 omni_sqlite 0.1.2 0.2.2 +pg18 omni_types 0.3.4 0.3.6 +pg18 omni_vfs 0.2.1 0.2.2 +pg18 omni_worker 0.1.0 0.2.1 +pg18 periods 1.2.2 1.2.3 +pg18 pg_bestmatch 0.0.1 0.0.2 +pg18 pg_cardano 1.0.5 1.1.1 +pg18 pg_checksums 1.1 1.3 +pg18 pg_duckdb 0.3.1 1.1.0 +pg18 pg_failover_slots 1.1.0 1.2.0 +pg18 pg_graphql 1.5.11 1.5.12 +pg18 pg_idkit 0.3.1 0.4.0 +pg18 pg_later 0.3.0 0.3.1 +pg18 pg_mooncake 0.1.2 0.2.0 +pg18 pg_net 0.9.2 0.20.0 +pg18 pg_parquet 0.4.3 0.5.1 +pg18 pg_render 0.1.2 0.1.3 +pg18 pg_session_jwt 0.3.1 0.3.3 +pg18 pg_smtp_client 0.2.0 0.2.1 +pg18 pg_sphere 1.5.1 1.5.2 +pg18 pg_statement_rollback 1.4 1.5 +pg18 pg_store_plans 1.8 1.9 +pg18 pg_tle 1.5.1 1.5.2 +pg18 pg_tokenizer 0.1.0 0.1.1 +pg18 pg_uuidv7 1.6.0 1.7.0 +pg18 pgactive 2.1.6 2.1.7 +pg18 pglogical 2.4.5 2.4.6 +pg18 pglogical_origin 2.4.5 2.4.6 +pg18 pgmq 1.5.1 1.7.0 +pg18 pgsmcrypto 0.1.0 0.1.1 +pg18 pgx_ulid 0.2.0 0.2.1 +pg18 pldbgapi 1.8 1.9 +pg18 pljava 1.6.8 1.6.10 +pg18 plprql 1.0.0 18.0.0 +pg18 roaringbitmap 0.5.4 0.5.5 +pg18 semver 0.32.1 0.40.0 +pg18 supautils 2.10.0 3.0.2 +pg18 tds_fdw 2.0.4 2.0.5 +pg18 timescaledb 2.22.0 2.23.0 +pg18 timescaledb_toolkit 1.21.0 1.22.0 +pg18 timeseries 0.1.6 0.1.7 +pg18 pg_tzf 0.2.2 0.2.3 +pg18 vchord 0.5.1 0.5.3 +pg18 vchord_bm25 0.2.1 0.2.2 +pg18 vectorize 0.22.2 0.25.0 +pg18 wrappers 0.5.4 0.5.6 +pg18 gzip 1.0.1 1.0.0 +pg18 hypopg 1.4.1 1.4.2 +pg18 mobilitydb 1.2.0 1.3.0 +pg18 mongo_fdw 5.5.1 5.5.3 +pg18 orafce 4.14.4 4.14.6 +pg18 pg_hint_plan 1.7.1 1.8.0 +pg18 pg_ivm 1.11 1.13 +pg18 pg_partman 5.2.4 5.3.1 +pg18 pg_search 0.18.1 0.19.2 +pg18 pg_show_plans 2.1.6 2.1.7 +pg18 pgpcre 1 0.20190509 +pg18 pgroonga 4.0.0 4.0.4 +pg18 pgroonga_database 4.0.0 4.0.4 +pg18 plpgsql_check 2.8.2 2.8.3 +pg18 uint 1.20231206 1.20250815 +pg18 uint128 1.1.0 1.1.1 +pg18 omni_* 20250525 20251108 +pg18 acl 1.0.4 +pg18 aggs_for_arrays 1.3.3 +pg18 aggs_for_vecs 1.4.0 +pg18 arraymath 1.1 +pg18 asn1oid 1.6 +pg18 aws_s3 0.0.1 +pg18 base36 1.0.0 +pg18 base62 0.0.1 +pg18 bzip 1.0.0 +pg18 chkpass 1.0 +pg18 convert 0.0.4 +pg18 count_distinct 3.0.2 +pg18 country 0.0.3 +pg18 cryptint 1.0.0 +pg18 currency 0.0.3 +pg18 data_historization 1.1.0 +pg18 db_migrator 1.0.0 +pg18 dbt2 0.61.7 +pg18 ddl_historization 0.0.7 +pg18 ddsketch 1.0.1 +pg18 decoder_raw 1.0 +pg18 decoderbufs 3.2.0 +pg18 emailaddr 0 +pg18 envvar 1.0.1 +pg18 faker 0.5.3 +pg18 financial 1.0.1 +pg18 fio 1.0 +pg18 first_last_agg 0.1.4 +pg18 floatfile 1.3.1 +pg18 floatvec 1.1.1 +pg18 geoip 0.3.0 +pg18 hashlib 1.1 +pg18 hashtypes 0.1.5 +pg18 hll 2.18 +pg18 hunspell_* 1.0 +pg18 imgsmlr 1.0 +pg18 index_advisor 0.2.0 +pg18 kafka_fdw 0.0.3 +pg18 login_hook 1.7 +pg18 oracle_fdw 2.8.0 +pg18 pg_auth_mon 3.0 +pg18 pg_background 1.3 +pg18 pg_bigm 1.2 +pg18 pg_cron 1.6.7 +pg18 pg_profile 4.10 +pg18 pg_stat_kcache 2.3.0 +pg18 pgdd 0.6.0 +pg18 pgjwt 0.2.0 +pg18 pgnodemx 1.7 +pg18 pgsodium 3.1.9 +pg18 pgtap 1.3.3 +pg18 plprofiler 4.2.5 +pg18 plproxy 2.11.0 +pg18 plr 8.4.8 +pg18 plv8 3.2.4 +pg18 pointcloud 1.2.5 +pg18 powa 5.0.1 +pg18 prefix 1.2.10 +pg18 q3c 2.0.1 +pg18 redis_fdw 1.0 +pg18 session_variable 3.4 +pg18 set_user 4.1.0 +pg18 system_stats 3.2 +pg18 temporal_tables 1.2.2 +pg18 topn 2.7.0 +pg18 unit 7.10 +pg18 zhparser 2.3 +pg18 zstd 1.1.2 +pg18 2025-09-04 Name Old Ver New Ver Note timescaledb 2.21.1 2.22.0 citus 13.1.0 13.2.0 documentdb 0.105.0 0.106.0 work with ferretdb 2.5 ddlx 0.29 0.30 + pg18 icu_ext 1.9.0 1.10.0 + pg18 asn1oid 1.5 1.6 + pg18 uint128 1.0.0 1.1.0 + pg18 toastinfo 1.5 1.6 + pg18 vchord 0.4.3 0.5.1 pgrx 0.16.0 pg_idkit 0.3.0 0.3.1 pgrx 0.15.0 pg_search 0.17.3 0.18.0 pgrx 0.15.0 pg_parquet 0.4.0 0.4.3 pgrx 0.16.0 wrappers 0.5.3 0.5.4 pgrx 0.14.3 pg_rewrite - 2.0.0 + Debian/Ubuntu (PGDG) pg_tracing - 0.1.3-2 + pg 14/18 pg_curl 2.4 2.4.5 new version epoch pg_rewrite - 2.0.0 Import from PGDG pg_tracing - 1.3.0 + pg14 / pg18 pgactive 2.1.5 2.1.6 + pg18 pgsentinel 1.1 1.2 1.2 pg_tle 1.5.1-1 1.5.1-2 + pg18 redis_fdw + pg18 pgextwlist 1.17 1.19 + pg18 wal2json 1.6 + pg18 pgvector 0.8.1 + pg18 2025-07-24 Name Old Ver New Ver Note orioledb beta11 1.4 beta12 1.5 pair with oriolepg 17.11 oriolepg 17.9 17.11 pair with orioledb 1.5 beta12 documentdb 0.104.0 0.105.0 pair with ferretdb 2.4 timescaledb 2.20.0 2.21.1 supautils 2.9.2 2.10.0 .so location changed plv8 3.2.3 3.2.4 postgresql_anonymizer 3.1.1 2.3.0 (pgrx 0.14.3) wrappers 0.5.0 0.5.3 (pgrx 0.14.3) pgrx change pgvectorscale 0.7.1 0.8.0 (pgrx 0.12.9) pg_search 0.15.8 0.17.0 fix el icu dep, download 2025-06-24 Name Old Ver New Ver Note citus 13.0.3 13.1.0 timescaledb 2.20.0 2.21.0 vchord 0.3.0 0.4.3 pgactive - 2.1.5 requires pgfeutils documentdb 0.103.0 0.104.0 add arm support 2025-05-26 Name Old Ver New Ver Note pgdd 0.5.0 0.6.0 convert - 0.0.4 pg_idkit 0.2.0 0.3.0 pg_tokenizer - 0.1.0 pg_render - 0.1.2 pgx_ulid - 0.2.0 orioledb 1.4.0b10 1.4.0b11 2025-05-22 Name Old Ver New Ver Note openhalodb - 14.10 spat - 0.1.0a4 pgsentinel - 1.1.0 timescaledb - 2.20.0 sqlite_fdw - 2.5.0 documentdb - 0.103.0 pg_tzf - 0.2.2 pg_vectorize - 0.22.2 wrappers - 0.5.0 2025-05-07 Name Old Ver New Ver Note omnigres - 20250507 citus - 12.0.3 timescaledb - 2.19.3 supautils - 2.9.1 pg_envvar - 1.0.1 pgcollection - 1.0.0 aggs_for_vecs - 1.4.0 pg_tracing - 0.1.3 pgmq - 1.5.1 pg_tzf - 0.2.0 pg_search - 0.15.18 anon - 2.1.1 pg_parquet - 0.4.0 pg_cardano - 1.0.5 pglite_fusion - 0.0.5 vchord_bm25 - 0.2.1 vchord - 0.3.0 timescaledb_toolkit - 1.21.0 pgvectorscale - 0.7.1 pg_session_jwt - 0.3.1 2025-03-20 Name Old Ver New Ver Note timescaledb - 2.19.0 citus - 13.0.2 documentdb - 1.102 pg_analytics - 0.3.7 pg_search - 0.15.8 emaj - 4.6.0 pgsql_tweaks - 0.11.0 pgvectorscale - 0.6.0 pg_session_jwt - 0.2.0 wrappers - 0.4.5 pg_parquet - 0.3.1 vchord - 0.2.2 pg_tle 1.2.0 1.5.0 supautils 2.5.0 2.6.0 sslutils 1.3 1.4 pg_profile 4.7 4.8 pg_jsonschema 0.3.2 0.3.3 pg_incremental 1.1.1 1.2.0 ddl_historization 0.7 0.0.7 pg_sqlog 3.1.7 1.6 pg_random - - pg_stat_monitor 2.1.0 2.1.1 pg_profile 4.7 4.8 2024-10-16 Name Old Ver New Ver Note pg_timeseries - 0.1.6 pgmq - 1.4.4 pg_protobuf - 16 17 pg_uuidv7 - 1.6 pg_readonly - latest pgddl - 0.28 pg_safeupdate - latest pg_stat_monitor - 2.1 pg_profile - 4.7 system_stats - 3.2 pg_auth_mon - 3.0 login_hook - 1.6 logerrors - 2.1.3 pg_orphaned - latest pgnodemx - 1.7 sslutils - 1.4 +pg16, +pg17 ","categories":["Reference"],"description":"PostgreSQL and Extension RPM package changelog and release notes","excerpt":"PostgreSQL and Extension RPM package changelog and release notes","ref":"/docs/repo/pgsql/rpm/","tags":"","title":"DNF Changelog"},{"body":"2026-01-16 Name Old Ver New Ver Note etcd_fdw 0.0.0 new pg_ttl_index 0.1.0 new citus 13.2.0 14.0.0 +pg18, pre-release pg_search 0.20.5 0.21.2 +pg18 pg_clickhouse 0.1.0 0.1.2 pg_textsearch 0.1.0 0.4.0 pg_convert 0.0.5 0.1.0 pg_timeseries 0.1.8 0.2.0 biscuit 2.0.1 2.2.2 pgmq 1.8.0 1.8.1 documentdb 0.107 0.109 +pg18, use MS version pg_bulkload 3.1.22 3.1.23 +pg18 age - 1.6.0 +pg18 use PGDG pgsentinel 1.2.0 1.3.1 use PGDG pljs - 1.0.4 use PGDG pg_partman 5.3.0 5.4.0 use PGDG pgfincore - 1.3.1 use PGDG documentdb_extended_rum 0.109 new mobilitydb_datagen 1.3.0 new 2025-12-25 Name Old Ver New Ver Note pg_duckdb 1.1.0 1.1.1 pg_search 0.20.4 0.20.5 vchord_bm25 0.2.2 0.3.0 pg_semver 0.40.0 0.41.0 pg_timeseries 0.1.7 0.1.8 supautils 3.0.2-1 3.0.2-2 fix pg18 pg_summarize 0.0.1-1 0.0.1-2 fix pg18 2025-12-16 Name Old Ver New Ver Note pg_textsearch - 0.1.0 new pg_clickhouse - 0.1.0 new pg_ai_query - 0.1.1 new timescaledb 2.23.1 2.24.0 pg_search 0.20.0 0.20.4 pg_duckdb 1.1.0-1 1.1.0-2 official release pg_biscuit 1.0 2.0.1 new repo pg_convert 0.0.4 0.0.5 removed pg13 support pgdd 0.6.0 0.6.1 removed pg13 support pglinter 1.0.0 1.0.1 pg_session_jwt 0.3.3 0.4.0 pg_anon 2.4.1 2.5.1 pg_enigma 0.4.0 0.5.0 wrappers 0.5.6 0.5.7 pg_vectorize 0.25.0 0.26.0 fix pg18 pg_tiktoken - - fix pg18 pg_tzf - - fix pg18 pglite_fusion - - fix pg18 pgsmcrypto - - fix pg18 pgx_ulid - - fix pg18 plprql - - fix pg18 synchdb - 1.3 Ubuntu 22/24 only 2025-11-20 Name Old Ver New Ver Note vchord 0.5.3 1.0.0 pg_later 0.3.1 0.4.0 pgvectorscale 0.8.0 0.9.0 -pg13, +pg18 pglite_fusion 0.0.5 0.0.6 pgx_ulid 0.2.1 0.2.2 pg_search 0.19.5 0.19.7 resume PIGSTY building citus 13.2.0 13.2.0 official tag timescaledb 2.23.0 2.23.1 pg_profile 4.10 4.11 pglinter 1.0.0 new pg_typeid 0.3.0 head with pg18 support pg_enigma 0.4.0 vonng patched pgrx version pg_retry 1.0.0 new, pg17-18 pg_biscuit 1.0 new, pg16-18 pg_weighted_statistics 1.0.0 new, pg13-18 documentdb 0.106 0.107 ferretdb fork polardb 15.15 15.15.5.0-38948055 2025-11-10 Add PostgreSQL 18 support for almost all extensions\nName Old Ver New Ver Note omni_csv - 0.1.1 new omni_datasets - 0.1.0 new omni_shmem - 0.1.0 new pg_csv - 1.0.1 new pljs - 1.0.3 new plxslt - 0.20140221 new credcheck 3.0 4.2 +pg18 dbt2 0.45.0 0.61.7 +pg18 h3 4.1.3 4.2.3 +pg18 h3_postgis 4.1.3 4.2.3 +pg18 mongo_fdw 1.1 5.5.3 +pg18 multicorn 3.0 3.2 +pg18 orafce 4.14.4 4.14.6 +pg18 pg_hint_plan 1.7.0 1.8.0 +pg18 pg_search 0.18.1 0.19.2 +pg18 pg_show_plans 2.1.6 2.1.7 +pg18 pgactive 2.1.6 2.1.7 +pg18 pgpcre 1 0.20190509 +pg18 plpgsql_check 2.8.2 2.8.3 +pg18 roaringbitmap 0.5.4 0.5.5 +pg18 uint 1.20231206 1.20250815 +pg18 uint128 1.1.0 1.1.1 +pg18 anon 2.3.0 2.4.1 +pg18 collection 1.0.0 1.1.0 +pg18 emaj 4.7.0 4.7.1 +pg18 explain_ui 0.0.1 0.0.2 +pg18 firebird_fdw 1.4.0 1.4.1 +pg18 login_hook 1.6 1.7 +pg18 logerrors 2.1.3 2.1.5 +pg18 mobilitydb 1.2.0 1.3.0 +pg18 omni 0.2.9 0.2.14 +pg18 omni_httpc 0.1.5 0.1.10 +pg18 omni_httpd 0.4.6 0.4.11 +pg18 omni_kube 0.1.1 0.4.2 +pg18 omni_sql 0.5.1 0.5.3 +pg18 omni_sqlite 0.1.2 0.2.2 +pg18 omni_worker 0.1.0 0.2.1 +pg18 pg_cardano 1.0.5 1.1.1 +pg18 pg_checksums 1.2 1.3 +pg18 pg_cron 1.6.5 1.6.7 +pg18 pg_duckdb 0.3.1 1.1.0 +pg18 pg_failover_slots 1.1.0 1.2.0 +pg18 pg_graphql 1.5.11 1.5.12 +pg18 pg_idkit 0.3.1 0.4.0 +pg18 pg_mooncake 0.1.2 0.2.0 +pg18 pg_net 0.9.2 0.20.0 +pg18 pg_parquet 0.4.3 0.5.1 +pg18 pg_partman 5.2.4 5.3.0 +pg18 pg_session_jwt 0.3.1 0.3.3 +pg18 pg_sphere 1.5.1 1.5.2 +pg18 pg_stat_monitor 2.2.0 2.3.0 +pg18 pg_statement_rollback 1.4 1.5 +pg18 pg_store_plans 1.8 1.9 +pg18 pg_task 1.0.0 2.1.12 +pg18 pg_tle 1.5.1 1.5.2 +pg18 pg_uuidv7 1.6.0 1.7.0 +pg18 pglogical 2.4.5 2.4.6 +pg18 pgmq 1.5.1 1.7.0 +pg18 pgroonga 4.0.0 4.0.4 +pg18 pgsql_tweaks 0.11.3 1.0.2 +pg18 pldbgapi 1.8 1.9 +pg18 plprql 1.0.0 18.0.0 +pg18 supautils 2.10.0 3.0.2 +pg18 timescaledb 2.22.0 2.23.0 +pg18 timescaledb_toolkit 1.21.0 1.22.0 +pg18 vchord 0.5.1 0.5.3 +pg18 vectorize 0.22.2 0.25.0 +pg18 wrappers 0.5.4 0.5.6 +pg18 acl 1.0.4 - +pg18 aggs_for_arrays 1.3.3 - +pg18 aggs_for_vecs 1.4.0 - +pg18 base36 1.0.0 - +pg18 hashlib 1.1 - +pg18 hll 2.18 - +pg18 imgsmlr 1.0 - +pg18 index_advisor 0.2.0 - +pg18 kafka_fdw 0.0.3 - +pg18 pg_auth_mon 3.0 - +pg18 pg_background 1.3 - +pg18 pg_bigm 1.2 - +pg18 pg_profile 4.10 - +pg18 pg_stat_kcache 2.3.0 - +pg18 pgdd 0.6.0 - +pg18 pgjwt 0.2.0 - +pg18 pgmp 1.0.5 - +pg18 plprofiler 4.2.5 - +pg18 plv8 3.2.4 - +pg18 redis_fdw 1.0 - +pg18 repmgr 5.5.0 - +pg18 system_stats 3.2 - +pg18 topn 2.7.0 - +pg18 zhparser 2.3 - +pg18 2025-09-06 Name Old Ver New Ver Note timesacledb 2.21.1 2.22.0 citus 13.1.0 13.2.0 documentdb 0.105.0 0.106.0 work with ferretdb 2.5 ddlx 0.29 0.30 + pg18 uint128 1.0.0 1.1.0 + pg18 vchord 0.4.3 0.5.1 pgrx 0.16.0 pg_idkit 0.3.0 0.3.1 pgrx 0.15.0 pg_search 0.17.3 0.18.0 pgrx 0.15.0 pg_parquet 0.4.0 0.4.3 pgrx 0.16.0 wrappers 0.5.3 0.5.4 pgrx 0.14.3 pg_rewrite - 2.0.0 + Debian/Ubuntu pg_tracing - 0.1.3-2 + pg 14/18 pg_curl 2.4 2.4.5 pg_ivm 1.11 1.12 + pg18 pg_rewrite - 2.0.0 new extension pg_tracing - 1.3.0 + pg14 / pg18 pgactive 2.1.5 2.1.6 + pg18 pgsentinel 1.1 1.2 1.2 pg_tle 1.5.1-1 1.5.1-2 + pg18 redis_fdw + pg18 emaj 4.6 4.7 table_version 1.11.0 1.11.1 2025-07-24 Name Old Ver New Ver Note orioledb beta11 1.4 beta12 1.5 pair with oriolepg 17.11 oriolepg 17.9 17.11 pair with orioledb 1.5 beta12 documentdb 0.104.0 0.105.0 pair with ferretdb 2.4 timescaledb 2.20.0 2.21.1 supautils 2.9.2 2.10.0 .so location changed plv8 3.2.3 3.2.4 postgresql_anonymizer 3.1.1 2.3.0 (pgrx 0.14.3) wrappers 0.5.0 0.5.3 (pgrx 0.14.3) pgrx version change pgvectorscale 0.7.1 0.8.0 (pgrx 0.12.9) pg_search 0.15.8 0.17.0 (download) fix el icu dep issue pg_profile 4.8.0 4.10.0 2025-07-04 Name Old Ver New Ver Note orioledb 1.4 beta11 rebuilt pgvectorscale 0.7.1 0.7.1 rebuilt fix bug pg_stat_monitor 2.1.1 2.2.0 pgsql-tweaks 0.11.1 0.11.3 pg_tle 1.5.0 1.5.1 pg_curl 2.4 2.4.5 2025-06-24 Name Old Ver New Ver Note citus 13.0.3 13.1.0 timescaledb 2.20.0 2.21.0 vchord 0.3.0 0.4.3 pgactive - 2.1.5 requires pgfeutils documentdb 0.103.0 0.104.0 add arm support 2025-05-26 Name Old Ver New Ver Note pgdd 0.5.0 0.6.0 convert - 0.0.4 pg_idkit 0.2.0 0.3.0 pg_tokenizer.rs - 0.1.0 pg_render - 0.1.2 pgx_ulid - 0.2.0 pg_ivm 1.10.0 1.11.0 orioledb 1.4.0b10 1.4.0b11 2025-05-22 Name Old Ver New Ver Note openhanded - 14.10 spat - 0.1.0a4 pgsentinel - 1.1.0 timescaledb - 2.20.0 sqlite_fdw - 2.5.0 documentdb - 0.103.0 tzf - 0.2.2 pg_vectorize - 0.22.2 wrappers - 0.5.0 2025-05-07 Name Old Ver New Ver Note omnigres - 20250507 citus - 12.0.3 timescaledb - 2.19.3 supautils - 2.9.1 pg_envvar - 1.0.1 pgcollection - 1.0.0 aggs_for_vecs - 1.4.0 pg_tracing - 0.1.3 pgmq - 1.5.1 tzf-pg - 0.2.0 pg_search - 0.15.18 anon - 2.1.1 pg_parquet - 0.4.0 pg_cardano - 1.0.5 pglite_fusion - 0.0.5 vchord_bm25 - 0.2.1 vchord - 0.3.0 timescaledb-toolkit - 1.21.0 pgvectorscale - 0.7.1 pg_session_jwt - 0.3.1 2025-03-20 Name Old Ver New Ver Note timescaledb - 2.19.0 citus - 13.0.2 documentdb - 1.102 pg_analytics - 0.3.7 pg_search - 0.15.8 pg_ivm - 1.10 emaj - 4.6.0 pgsql_tweaks - 0.11.0 pgvectorscale - 0.6.0 pg_session_jwt - 0.2.0 wrappers - 0.4.5 pg_parquet - 0.3.1 vchord - 0.2.2 pg_tle 1.2.0 1.5.0 supautils 2.5.0 2.6.0 sslutils 1.3 1.4 pg_profile 4.7 4.8 pg_jsonschema 0.3.2 0.3.3 pg_incremental 1.1.1 1.2.0 ddl_historization 0.7 0.0.7 pg_sqlog 3.1.7 1.6 pg_random - - pg_stat_monitor 2.1.0 2.1.1 pg_profile 4.7 4.8 2024-10-16 Name Old Ver New Ver Note pg_ivm - 1.9 pg_timeseries - 0.1.6 pgmq - 1.4.4 pg_protobuf - 16 17 pg_uuidv7 - 1.6 pg_readonly - latest pgddl - 0.28 pg_safeupdate - latest pg_stat_monitor - 2.1 pg_profile - 4.7 system_stats - 3.2 pg_auth_mon - 3.0 login_hook - 1.6 logerrors - 2.1.3 pg-orphaned - latest pgnodemx - 1.7 sslutils - 1.4 (+16,17) ","categories":["Reference"],"description":"PostgreSQL and Extension DEB package changelog and release notes","excerpt":"PostgreSQL and Extension DEB package changelog and release notes","ref":"/docs/repo/pgsql/deb/","tags":"","title":"APT Changelog"},{"body":"The ultimate monitoring experience for PostgreSQL with 600+ metrics, declarative configuration, and dynamic planning capabilities.\nGet Started | GitHub | Live Demo\nFeatures Feature Description Comprehensive Metrics Monitor PostgreSQL (10-18+) and pgBouncer (1.8-1.24+) with 600+ metrics and ~3K time series per instance Declarative Configuration Define custom metrics through YAML configs with fine-grained control over timeout, caching, and skip conditions Custom Collectors Define your own metrics with declarative YAML configuration and dynamic query planning Auto-Discovery Automatically discover and monitor multiple databases within a PostgreSQL instance Dynamic Planning Automatically adapt metric collection based on PostgreSQL version, extensions, and server characteristics Production Ready Battle-tested in real-world environments across 12K+ cores for 6+ years with enterprise reliability Health Check APIs Comprehensive HTTP endpoints for service health and traffic routing with primary/replica detection Smart Caching Built-in caching mechanism with configurable TTL to reduce database load and improve performance Extension Aware Native support for TimescaleDB, Citus, pg_stat_statements, pg_wait_sampling and automatic detection Installation PG Exporter provides multiple install methods to fit your infrastructure:\nInstallation Docker YUM APT Binary Source docker run -d --name pg_exporter -p 9630:9630 -e PG_EXPORTER_URL=\"postgres://user:pass@host:5432/postgres\" pgsty/pg_exporter:latest # RPM-based systems sudo tee /etc/yum.repos.d/pigsty-infra.repo \u003e /dev/null \u003c\u003c-'EOF' [pigsty-infra] name=Pigsty Infra for $basearch baseurl=https://repo.pigsty.io/yum/infra/$basearch enabled = 1 gpgcheck = 0 module_hotfixes=1 EOF sudo yum makecache; sudo yum install -y pg_exporter sudo tee /etc/apt/sources.list.d/pigsty-infra.list \u003e /dev/null \u003c\u003cEOF deb [trusted=yes] https://repo.pigsty.io/apt/infra generic main EOF sudo apt update; sudo apt install -y pg-exporter wget https://github.com/pgsty/pg_exporter/releases/download/v1.1.2/pg_exporter-1.1.2.linux-amd64.tar.gz tar -xf pg_exporter-1.1.2.linux-amd64.tar.gz sudo install pg_exporter-1.1.2.linux-amd64/pg_exporter /usr/bin/ sudo install pg_exporter-1.1.2.linux-amd64/pg_exporter.yml /etc/pg_exporter.yml # Build from source git clone https://github.com/pgsty/pg_exporter.git cd pg_exporter make build Quick Start Get PG Exporter up and running in minutes, Getting Started with:\n# Run with PostgreSQL URL PG_EXPORTER_URL='postgres://user:pass@localhost:5432/postgres' pg_exporter # Access metrics curl http://localhost:9630/metrics Documentation Getting Started - Quick start guide and basic concepts Installation - Installation instructions for all platforms Configuration - Configuration reference and examples Deployment - Production deployment best practices Collectors - Detailed documentation for all metric collectors Live Demo Experience PG Exporter in action with our live demo environment: https://g.pgsty.com\nThe demo showcases real PostgreSQL clusters monitored by PG Exporter, featuring:\nReal-time metrics visualization with Grafana Multiple PostgreSQL versions and configurations Extension-specific metrics and monitoring Complete observability stack powered by Pigsty Community \u0026 Support GitHub - Source code, issues, and contributions Discussions - Ask questions and share experiences Pigsty - Complete PostgreSQL Distro with PG Exporter License PG Exporter is open-source software licensed under the Apache License 2.0.\nCopyright 2018-2025 © Ruohang Feng / rh@vonng.com\n","categories":"","description":"Advanced PostgreSQL \u0026 pgBouncer Metrics Exporter for Prometheus","excerpt":"Advanced PostgreSQL \u0026 pgBouncer Metrics Exporter for Prometheus","ref":"/docs/pg_exporter/","tags":"","title":"PG Exporter"},{"body":"PG Exporter is an advanced PostgreSQL and pgBouncer metrics exporter for Prometheus. This guide will help you get up and running quickly.\nPrerequisites Before you begin, ensure you have:\nPostgreSQL 10+ or pgBouncer 1.8+ instance to monitor A user account with appropriate permissions for monitoring Prometheus Compatible System (for metrics scraping) Basic understanding of PostgreSQL connection strings Quick Start The fastest way to get started with PG Exporter:\n# Download and install the latest release curl -L https://github.com/pgsty/pg_exporter/releases/latest/download/pg_exporter-$(uname -s)-$(uname -m).tar.gz | tar xz sudo install pg_exporter /usr/bin/ # Run with PostgreSQL connection URL PG_EXPORTER_URL='postgres://user:pass@localhost:5432/postgres' pg_exporter # Verify metrics are available curl http://localhost:9630/metrics Understanding the Basics Connection String PG Exporter uses standard PostgreSQL connection URLs:\npostgres://[user][:password]@[host][:port]/[database][?param=value] Examples:\nLocal PostgreSQL: postgres:///postgres Remote with auth: postgres://monitor:password@db.example.com:5432/postgres With SSL: postgres://user:pass@host/db?sslmode=require pgBouncer: postgres://pgbouncer:password@localhost:6432/pgbouncer Built-in Metrics PG Exporter provides 4 core built-in metrics out of the box:\nMetric Type Description pg_up Gauge 1 if exporter can connect to PostgreSQL, 0 otherwise pg_version Gauge PostgreSQL server version number pg_in_recovery Gauge 1 if server is in recovery mode (replica), 0 if primary pg_exporter_build_info Gauge Exporter version and build information Configuration File All other metrics (600+) are defined in the pg_exporter.yml configuration file. By default, PG Exporter looks for this file in:\nPath specified by --config flag Path in PG_EXPORTER_CONFIG environment variable Current directory (./pg_exporter.yml) System config (/etc/pg_exporter.yml or /etc/pg_exporter/) Your First Monitoring Setup Step 1: Create a Monitoring User Create a dedicated PostgreSQL user for monitoring:\n-- Create monitoring user CREATE USER pg_monitor WITH PASSWORD 'secure_password'; -- Grant necessary permissions GRANT pg_monitor TO pg_monitor; GRANT CONNECT ON DATABASE postgres TO pg_monitor; -- For PostgreSQL 10+, pg_monitor role provides read access to monitoring views -- For older versions, you may need additional grants Step 2: Test Connection Verify the exporter can connect to your database:\n# Set connection URL export PG_EXPORTER_URL='postgres://pg_monitor:secure_password@localhost:5432/postgres' # Run in dry-run mode to test configuration pg_exporter --dry-run Step 3: Run the Exporter Start PG Exporter:\n# Run with default settings pg_exporter # Or with custom flags pg_exporter \\ --url='postgres://pg_monitor:secure_password@localhost:5432/postgres' \\ --web.listen-address=':9630' \\ --log.level=info Step 4: Configure Prometheus Add PG Exporter as a target in your prometheus.yml:\nscrape_configs: - job_name: 'postgresql' static_configs: - targets: ['localhost:9630'] labels: instance: 'postgres-primary' Step 5: Verify Metrics Check that metrics are being collected:\n# View raw metrics curl http://localhost:9630/metrics | grep pg_ # Check exporter statistics curl http://localhost:9630/stat # Verify server detection curl http://localhost:9630/explain Auto-Discovery Mode PG Exporter can automatically discover and monitor all databases in a PostgreSQL instance:\n# Enable auto-discovery (default behavior) pg_exporter --auto-discovery # Exclude specific databases pg_exporter --auto-discovery \\ --exclude-database=\"template0,template1,postgres\" # Include only specific databases pg_exporter --auto-discovery \\ --include-database=\"app_db,analytics_db\" When auto-discovery is enabled:\nCluster-level metrics (1xx-5xx) are collected once per instance Database-level metrics (6xx-8xx) are collected for each discovered database Metrics are labeled with datname to distinguish between databases Monitoring pgBouncer To monitor pgBouncer instead of PostgreSQL:\n# Connect to pgBouncer admin database PG_EXPORTER_URL='postgres://pgbouncer:password@localhost:6432/pgbouncer' \\ pg_exporter --config=/etc/pg_exporter.yml The exporter automatically detects pgBouncer and:\nUses pgbouncer namespace for metrics Executes pgBouncer-specific collectors (9xx series) Provides pgBouncer-specific health checks Using Docker Run PG Exporter in a container:\ndocker run -d \\ --name pg_exporter \\ -p 9630:9630 \\ -e PG_EXPORTER_URL=\"postgres://user:pass@host.docker.internal:5432/postgres\" \\ pgsty/pg_exporter:latest With custom configuration:\ndocker run -d \\ --name pg_exporter \\ -p 9630:9630 \\ -v /path/to/pg_exporter.yml:/etc/pg_exporter.yml \\ -e PG_EXPORTER_URL=\"postgres://user:pass@db:5432/postgres\" \\ pgsty/pg_exporter:latest Health Checks PG Exporter provides health check endpoints for load balancers and orchestrators:\n# Basic health check curl http://localhost:9630/up # Returns: 200 if connected, 503 if not # Primary detection curl http://localhost:9630/primary # Returns: 200 if primary, 404 if replica, 503 if unknown # Replica detection curl http://localhost:9630/replica # Returns: 200 if replica, 404 if primary, 503 if unknown Troubleshooting Connection Issues # Test with detailed logging pg_exporter --log.level=debug --dry-run # Check server planning pg_exporter --explain Permission Errors Ensure the monitoring user has necessary permissions:\n-- Check current permissions SELECT * FROM pg_roles WHERE rolname = 'pg_monitor'; -- Grant additional permissions if needed GRANT USAGE ON SCHEMA pg_catalog TO pg_monitor; GRANT SELECT ON ALL TABLES IN SCHEMA pg_catalog TO pg_monitor; Slow Scrapes If scrapes are timing out:\nCheck slow queries: curl http://localhost:9630/stat Adjust collector timeouts in configuration Use caching for expensive queries (set ttl in collector config) Disable expensive collectors if not needed Next Steps Installation Guide - Detailed installation instructions for all platforms Configuration Reference - Complete configuration documentation Deployment Guide - Production deployment best practices API Reference - Full API endpoint documentation ","categories":"","description":"","excerpt":"PG Exporter is an advanced PostgreSQL and pgBouncer metrics exporter …","ref":"/docs/pg_exporter/start/","tags":"","title":"Getting Started"},{"body":"PG Exporter provides multiple installation methods to suit different deployment scenarios. This guide covers all available installation options with detailed instructions for each platform.\nPigsty The easiest way to get started with pg_exporter is to use Pigsty, which is a complete PostgreSQL distribution with built-in Observability best practices based on pg_exporter, Prometheus, and Grafana. You don’t even need to know any details about pg_exporter, it just gives you all the metrics and dashboard panels\ncurl -fsSL https://repo.pigsty.io/get | bash; cd ~/pigsty; Release You can also download pg_exporter package (RPM/DEB/ Tarball) directly from the Latest GitHub Release Page:\nv1.1.2 Release Files:\nType File DEB (amd64) pg-exporter_1.1.2-1_amd64.deb DEB (arm64) pg-exporter_1.1.2-1_arm64.deb DEB (ppc64le) pg-exporter_1.1.2-1_ppc64le.deb RPM (aarch64) pg_exporter-1.1.2-1.aarch64.rpm RPM (x86_64) pg_exporter-1.1.2-1.x86_64.rpm RPM (ppc64le) pg_exporter-1.1.2-1.ppc64le.rpm Tarball (Linux amd64) pg_exporter-1.1.2.linux-amd64.tar.gz Tarball (Linux arm64) pg_exporter-1.1.2.linux-arm64.tar.gz Tarball (Linux ppc64le) pg_exporter-1.1.2.linux-ppc64le.tar.gz Tarball (macOS amd64) pg_exporter-1.1.2.darwin-amd64.tar.gz Tarball (macOS arm64) pg_exporter-1.1.2.darwin-arm64.tar.gz Tarball (Windows amd64) pg_exporter-1.1.2.windows-amd64.tar.gz You can install it directly with your OS package manager (rpm/dpkg), or even put the binary in your $PATH.\nRepository The pig package is also available in the pigsty-infra repo, You can add the repo to your system, and install it with OS package manager:\nYUM For EL distribution such as RHEL，RockyLinux，CentOS，Alma Linux，OracleLinux,…:\nsudo tee /etc/yum.repos.d/pigsty-infra.repo \u003e /dev/null \u003c\u003c-'EOF' [pigsty-infra] name=Pigsty Infra for $basearch baseurl=https://repo.pigsty.io/yum/infra/$basearch enabled = 1 gpgcheck = 0 module_hotfixes=1 EOF sudo yum makecache; sudo yum install -y pg_exporter APT For Debian, Ubuntu and compatible Linux Distributions:\nsudo tee /etc/apt/sources.list.d/pigsty-infra.list \u003e /dev/null \u003c\u003cEOF deb [trusted=yes] https://repo.pigsty.io/apt/infra generic main EOF sudo apt update; sudo apt install -y pg-exporter Docker We have prebuilt docker images for amd64 and arm64 architectures on docker hub: pgsty/pg_exporter.\n# Basic usage docker run -d \\ --name pg_exporter \\ -p 9630:9630 \\ -e PG_EXPORTER_URL=\"postgres://user:password@host:5432/postgres\" \\ pgsty/pg_exporter:latest # With custom configuration docker run -d \\ --name pg_exporter \\ -p 9630:9630 \\ -v /path/to/pg_exporter.yml:/etc/pg_exporter.yml:ro \\ -e PG_EXPORTER_CONFIG=\"/etc/pg_exporter.yml\" \\ -e PG_EXPORTER_URL=\"postgres://user:password@host:5432/postgres\" \\ pgsty/pg_exporter:latest # With auto-discovery enabled docker run -d \\ --name pg_exporter \\ -p 9630:9630 \\ -e PG_EXPORTER_URL=\"postgres://user:password@host:5432/postgres\" \\ -e PG_EXPORTER_AUTO_DISCOVERY=\"true\" \\ -e PG_EXPORTER_EXCLUDE_DATABASE=\"template0,template1\" \\ pgsty/pg_exporter:latest Binary The pg_exporter can be installed as a standalone binary.\nCompatibility The current pg_exporter support PostgreSQL version 10 and above. While it is designed to work with any PostgreSQL major version (back to 9.x).\nThe only problem to use with legacy version (9.6 and below) is that we removed older metrics collector branches definition due to EOL.\nYou can always retrieve these legacy version of config files and use against historic versions of PostgreSQL\nPostgreSQL Version Support Status 10 ~ 17 ✅ Full Support 9.6- ⚠️ Legacy Conf pg_exporter works with pgbouncer 1.8+, Since v1.8 is the first version with SHOW command support.\npgBouncer Version Support Status 1.8.x ~ 1.24.x ✅ Full Support before 1.8.x ⚠️ No Metrics ","categories":"","description":"How to download and install the pg_exporter","excerpt":"How to download and install the pg_exporter","ref":"/docs/pg_exporter/install/","tags":"","title":"Installation"},{"body":"PG Exporter uses a powerful and flexible configuration system that allows you to define custom metrics, control collection behavior, and optimize performance. This guide covers all aspects of configuration from basic setup to advanced customization.\nMetrics Collectors PG Exporter uses a declarative YAML configuration system that provides incredible flexibility and control over metric collection. This guide covers all aspects of configuring PG Exporter for your specific monitoring needs.\nConfiguration Overview PG Exporter’s configuration is centered around collectors - individual metric queries with associated metadata. The configuration can be:\nA single monolithic YAML file (pg_exporter.yml) A directory containing multiple YAML files (merged alphabetically) Custom path specified via command-line or environment variable Configuration Loading PG Exporter searches for configuration in the following order:\nCommand-line argument: --config=/path/to/config Environment variable: PG_EXPORTER_CONFIG=/path/to/config Current directory: ./pg_exporter.yml System config file: /etc/pg_exporter.yml System config directory: /etc/pg_exporter/ Collector Structure Each collector is a top-level object in the YAML configuration with a unique name and various properties:\ncollector_branch_name: # Unique identifier for this collector name: metric_namespace # Metric prefix (defaults to branch name) desc: \"Collector description\" # Human-readable description query: | # SQL query to execute SELECT column1, column2 FROM table # Execution Control ttl: 10 # Cache time-to-live in seconds timeout: 0.1 # Query timeout in seconds fatal: false # If true, failure fails entire scrape skip: false # If true, collector is disabled # Version Compatibility min_version: 100000 # Minimum PostgreSQL version (inclusive) max_version: 999999 # Maximum PostgreSQL version (exclusive) # Execution Tags tags: [cluster, primary] # Conditions for execution # Predicate Queries (optional) predicate_queries: - name: \"check_function\" predicate_query: | SELECT EXISTS (...) # Metric Definitions metrics: - column_name: usage: GAUGE # GAUGE, COUNTER, LABEL, or DISCARD rename: metric_name # Optional: rename the metric description: \"Help text\" # Metric description default: 0 # Default value if NULL scale: 1000 # Scale factor for the value Core Configuration Elements Collector Branch Name The top-level key uniquely identifies a collector across the entire configuration:\npg_stat_database: # Must be unique name: pg_db # Actual metric namespace Query Definition The SQL query that retrieves metrics:\nquery: | SELECT datname, numbackends, xact_commit, xact_rollback, blks_read, blks_hit FROM pg_stat_database WHERE datname NOT IN ('template0', 'template1') Metric Types Each column in the query result must be mapped to a metric type:\nUsage Description Example GAUGE Instantaneous value that can go up or down Current connections COUNTER Cumulative value that only increases Total transactions LABEL Use as a Prometheus label Database name DISCARD Ignore this column Internal values Cache Control (TTL) The ttl parameter controls result caching:\n# Fast queries - minimal caching pg_stat_activity: ttl: 1 # Cache for 1 second # Expensive queries - longer caching pg_table_bloat: ttl: 3600 # Cache for 1 hour Best practices:\nSet TTL less than your scrape interval Use longer TTL for expensive queries TTL of 0 disables caching Timeout Control Prevent queries from running too long:\ntimeout: 0.1 # 100ms default timeout: 1.0 # 1 second for complex queries timeout: -1 # Disable timeout (not recommended) Version Compatibility Control which PostgreSQL versions can run this collector:\nmin_version: 100000 # PostgreSQL 10.0+ max_version: 140000 # Below PostgreSQL 14.0 Version format: MMMMMMPP00 where:\nMMMMMM = Major version (6 digits) PP = Minor version (2 digits) Examples: 100000 = 10.0, 130200 = 13.2, 160100 = 16.1 Tag System Tags control when and where collectors execute:\nBuilt-in Tags Tag Description cluster Execute once per PostgreSQL cluster primary / master Only on primary servers standby / replica Only on replica servers pgbouncer Only for pgBouncer connections Prefixed Tags Prefix Example Description dbname: dbname:postgres Only on specific database username: username:monitor Only with specific user extension: extension:pg_stat_statements Only if extension installed schema: schema:public Only if schema exists not: not:slow NOT when exporter has tag Custom Tags Pass custom tags to the exporter:\npg_exporter --tag=\"production,critical\" Then use in configuration:\nexpensive_metrics: tags: [critical] # Only runs with 'critical' tag Predicate Queries Execute conditional checks before main query:\npredicate_queries: - name: \"Check pg_stat_statements\" predicate_query: | SELECT EXISTS ( SELECT 1 FROM pg_extension WHERE extname = 'pg_stat_statements' ) The main query only executes if all predicates return true.\nMetric Definition Basic Definition metrics: - numbackends: usage: GAUGE description: \"Number of backends connected\" Advanced Options metrics: - checkpoint_write_time: usage: COUNTER rename: write_time # Rename metric scale: 0.001 # Convert ms to seconds default: 0 # Use 0 if NULL description: \"Checkpoint write time in seconds\" Collector Organization PG Exporter ships with pre-organized collectors:\nRange Category Description 0xx Documentation Examples and documentation 1xx Basic Server info, settings, metadata 2xx Replication Replication, slots, receivers 3xx Persistence I/O, checkpoints, WAL 4xx Activity Connections, locks, queries 5xx Progress Vacuum, index creation progress 6xx Database Per-database statistics 7xx Objects Tables, indexes, functions 8xx Optional Expensive/optional metrics 9xx pgBouncer Connection pooler metrics 10xx+ Extensions Extension-specific metrics Real-World Examples Simple Gauge Collector pg_connections: desc: \"Current database connections\" query: | SELECT count(*) as total, count(*) FILTER (WHERE state = 'active') as active, count(*) FILTER (WHERE state = 'idle') as idle, count(*) FILTER (WHERE state = 'idle in transaction') as idle_in_transaction FROM pg_stat_activity WHERE pid != pg_backend_pid() ttl: 1 metrics: - total: {usage: GAUGE, description: \"Total connections\"} - active: {usage: GAUGE, description: \"Active connections\"} - idle: {usage: GAUGE, description: \"Idle connections\"} - idle_in_transaction: {usage: GAUGE, description: \"Idle in transaction\"} Counter with Labels pg_table_stats: desc: \"Table statistics\" query: | SELECT schemaname, tablename, n_tup_ins, n_tup_upd, n_tup_del, n_live_tup, n_dead_tup FROM pg_stat_user_tables ttl: 10 metrics: - schemaname: {usage: LABEL} - tablename: {usage: LABEL} - n_tup_ins: {usage: COUNTER, description: \"Tuples inserted\"} - n_tup_upd: {usage: COUNTER, description: \"Tuples updated\"} - n_tup_del: {usage: COUNTER, description: \"Tuples deleted\"} - n_live_tup: {usage: GAUGE, description: \"Live tuples\"} - n_dead_tup: {usage: GAUGE, description: \"Dead tuples\"} Version-Specific Collector pg_wal_stats: desc: \"WAL statistics (PG 14+)\" min_version: 140000 query: | SELECT wal_records, wal_bytes, wal_buffers_full, wal_write_time, wal_sync_time FROM pg_stat_wal ttl: 10 tags: [cluster] metrics: - wal_records: {usage: COUNTER} - wal_bytes: {usage: COUNTER} - wal_buffers_full: {usage: COUNTER} - wal_write_time: {usage: COUNTER, scale: 0.001} - wal_sync_time: {usage: COUNTER, scale: 0.001} Extension-Dependent Collector pg_stat_statements_metrics: desc: \"Query performance statistics\" tags: [extension:pg_stat_statements] query: | SELECT sum(calls) as total_calls, sum(total_exec_time) as total_time, sum(mean_exec_time * calls) / sum(calls) as mean_time FROM pg_stat_statements ttl: 60 metrics: - total_calls: {usage: COUNTER} - total_time: {usage: COUNTER, scale: 0.001} - mean_time: {usage: GAUGE, scale: 0.001} Custom Collectors Creating Your Own Metrics Create a new YAML file in your config directory: # /etc/pg_exporter/custom_metrics.yml app_metrics: desc: \"Application-specific metrics\" query: | SELECT (SELECT count(*) FROM users WHERE active = true) as active_users, (SELECT count(*) FROM orders WHERE created_at \u003e NOW() - '1 hour'::interval) as recent_orders, (SELECT avg(processing_time) FROM jobs WHERE completed_at \u003e NOW() - '5 minutes'::interval) as avg_job_time ttl: 30 metrics: - active_users: {usage: GAUGE, description: \"Currently active users\"} - recent_orders: {usage: GAUGE, description: \"Orders in last hour\"} - avg_job_time: {usage: GAUGE, description: \"Average job processing time\"} Test your collector: pg_exporter --explain --config=/etc/pg_exporter/ Conditional Metrics Use predicate queries for conditional metrics:\npartition_metrics: desc: \"Partitioned table metrics\" predicate_queries: - name: \"Check if partitioning is used\" predicate_query: | SELECT EXISTS ( SELECT 1 FROM pg_class WHERE relkind = 'p' LIMIT 1 ) query: | SELECT parent.relname as parent_table, count(*) as partition_count, sum(pg_relation_size(child.oid)) as total_size FROM pg_inherits JOIN pg_class parent ON parent.oid = pg_inherits.inhparent JOIN pg_class child ON child.oid = pg_inherits.inhrelid WHERE parent.relkind = 'p' GROUP BY parent.relname ttl: 300 metrics: - parent_table: {usage: LABEL} - partition_count: {usage: GAUGE} - total_size: {usage: GAUGE} Performance Optimization Query Optimization Tips Use appropriate TTL values:\nFast queries: 1-10 seconds Medium queries: 10-60 seconds Expensive queries: 300-3600 seconds Set realistic timeouts:\nDefault: 100ms Complex queries: 500ms-1s Never disable timeout in production Use cluster-level tags:\ntags: [cluster] # Run once per cluster, not per database Disable expensive collectors:\npg_table_bloat: skip: true # Disable if not needed Monitoring Collector Performance Check collector execution statistics:\n# View collector statistics curl http://localhost:9630/stat # Check which collectors are slow curl http://localhost:9630/metrics | grep pg_exporter_collector_duration Troubleshooting Configuration Validate Configuration # Dry run - shows parsed configuration pg_exporter --dry-run # Explain - shows planned queries pg_exporter --explain Common Issues Problem Solution Metrics missing Check tags and version compatibility Slow scrapes Increase TTL, add timeout, disable expensive queries High memory usage Reduce result set size, use LIMIT Permission errors Verify query permissions for monitoring user Debug Logging Enable debug logging to troubleshoot:\npg_exporter --log.level=debug ","categories":"","description":"","excerpt":"PG Exporter uses a powerful and flexible configuration system that …","ref":"/docs/pg_exporter/config/","tags":"","title":"Configuration"},{"body":"PG Exporter provides a comprehensive REST API for metrics collection, health checking, traffic routing, and operational control. All endpoints are exposed via HTTP on the configured port (default: 9630).\nEndpoint Overview Endpoint Method Description /metrics GET Prometheus metrics endpoint /up GET Basic aliveness check /health GET Detailed health status /primary GET Primary server check /replica GET Replica server check /read GET Read traffic routing /reload GET Reload configuration /explain GET Explain query planning /stat GET Runtime statistics Metrics Endpoint GET /metrics The primary endpoint that exposes all collected metrics in Prometheus format.\nRequest curl http://localhost:9630/metrics Response # HELP pg_up PostgreSQL server is up and accepting connections # TYPE pg_up gauge pg_up 1 # HELP pg_version PostgreSQL server version number # TYPE pg_version gauge pg_version 140000 # HELP pg_in_recovery PostgreSQL server is in recovery mode # TYPE pg_in_recovery gauge pg_in_recovery 0 # HELP pg_exporter_build_info PG Exporter build information # TYPE pg_exporter_build_info gauge pg_exporter_build_info{version=\"1.1.2\",branch=\"main\",revision=\"abc123\"} 1 # ... additional metrics Response Format Metrics follow the Prometheus exposition format:\n# HELP \u003cmetric_name\u003e \u003cdescription\u003e # TYPE \u003cmetric_name\u003e \u003ctype\u003e \u003cmetric_name\u003e{\u003clabel_name\u003e=\"\u003clabel_value\u003e\",...} \u003cvalue\u003e \u003ctimestamp\u003e Health Check Endpoints Health check endpoints provide various ways to monitor PG Exporter and the target database status.\nGET /up Simple binary health check.\nResponse Codes Code Status Description 200 OK Exporter and database are up 503 Service Unavailable Database is down or unreachable Example # Check if service is up curl -I http://localhost:9630/up HTTP/1.1 200 OK Content-Type: text/plain; charset=utf-8 GET /health Alias for /up with same behavior.\ncurl http://localhost:9630/health GET /liveness Kubernetes liveness probe endpoint.\n# Liveness probe configuration livenessProbe: httpGet: path: /liveness port: 9630 initialDelaySeconds: 30 periodSeconds: 10 GET /readiness Kubernetes readiness probe endpoint.\n# Readiness probe configuration readinessProbe: httpGet: path: /readiness port: 9630 initialDelaySeconds: 5 periodSeconds: 5 Traffic Routing Endpoints These endpoints are designed for load balancers and proxies to route traffic based on server role.\nGET /primary Check if the server is a primary (master) instance.\nResponse Codes Code Status Description 200 OK Server is primary and accepting writes 404 Not Found Server is not primary (is replica) 503 Service Unavailable Server is down Aliases /leader /master /read-write /rw Example # Check if server is primary curl -I http://localhost:9630/primary # Use in HAProxy configuration backend pg_primary option httpchk GET /primary server pg1 10.0.0.1:5432 check port 9630 server pg2 10.0.0.2:5432 check port 9630 GET /replica Check if the server is a replica (standby) instance.\nResponse Codes Code Status Description 200 OK Server is replica and in recovery 404 Not Found Server is not replica (is primary) 503 Service Unavailable Server is down Aliases /standby /slave /read-only /ro Example # Check if server is replica curl -I http://localhost:9630/replica # Use in load balancer configuration backend pg_replicas option httpchk GET /replica server pg2 10.0.0.2:5432 check port 9630 server pg3 10.0.0.3:5432 check port 9630 GET /read Check if the server can handle read traffic (both primary and replica).\nResponse Codes Code Status Description 200 OK Server is up and can handle reads 503 Service Unavailable Server is down Example # Check if server can handle reads curl -I http://localhost:9630/read # Route read traffic to any available server backend pg_read option httpchk GET /read server pg1 10.0.0.1:5432 check port 9630 server pg2 10.0.0.2:5432 check port 9630 server pg3 10.0.0.3:5432 check port 9630 Operational Endpoints POST /reload Reload configuration without restarting the exporter.\nRequest curl -X POST http://localhost:9630/reload Response { \"status\": \"success\", \"message\": \"Configuration reloaded successfully\", \"timestamp\": \"2024-01-15T10:30:00Z\" } Response Codes Code Status Description 200 OK Configuration reloaded successfully 500 Internal Server Error Reload failed Use Cases Update collector definitions Change query parameters Modify cache TTL values Add or remove collectors Note Configuration reload does not affect the database connection. To change connection parameters, restart the exporter.\nGET /explain Display query execution planning information for all configured collectors.\nRequest curl http://localhost:9630/explain Response Collector: pg_stat_database Query: SELECT datname, numbackends FROM pg_stat_database Tags: [cluster] TTL: 10s Timeout: 100ms Version: 100000-999999 Status: Active Collector: pg_stat_replication Query: SELECT client_addr, state FROM pg_stat_replication Tags: [primary] TTL: 5s Timeout: 100ms Version: 100000-999999 Status: Active (primary only) ... ","categories":"","description":"","excerpt":"PG Exporter provides a comprehensive REST API for metrics collection, …","ref":"/docs/pg_exporter/api/","tags":"","title":"API Reference"},{"body":"This guide covers production deployment strategies, best practices, and real-world configurations for PG Exporter.\npg_exporter itself can be configured through:\nCommand-line arguments (higher priority) Environment variables (lower priority) The metrics collectors are configured with a YAML configuration file (dir/files):\n/etc/pg_exporter.yml (default) /etc/pg_exporter/ (directory with multiple files) The configuration file uses YAML format and consists of collector definitions that specify what metrics to collect and how to collect them.\nCLI Arg All configuration options can be specified via command-line flags:\npg_exporter \\ --url=\"postgres://user:pass@localhost:5432/postgres\" \\ --config=\"/etc/pg_exporter/pg_exporter.yml\" \\ --web.listen-address=\":9630\" \\ --auto-discovery \\ --exclude-database=\"template0,template1\" \\ --log.level=\"info\" Run pg_exporter --help for a complete list of available flags:\nFlags: -h, --[no-]help Show context-sensitive help (also try --help-long and --help-man). -u, --url=URL postgres target url -c, --config=CONFIG path to config dir or file --[no-]web.systemd-socket Use systemd socket activation listeners instead of port listeners (Linux only). --web.listen-address=:9630 ... Addresses on which to expose metrics and web interface. Repeatable for multiple addresses. Examples: `:9100` or `[::1]:9100` for http, `vsock://:9100` for vsock --web.config.file=\"\" Path to configuration file that can enable TLS or authentication. See: https://github.com/prometheus/exporter-toolkit/blob/master/docs/web-configuration.md -l, --label=\"\" constant lables:comma separated list of label=value pair ($PG_EXPORTER_LABEL) -t, --tag=\"\" tags,comma separated list of server tag ($PG_EXPORTER_TAG) -C, --[no-]disable-cache force not using cache ($PG_EXPORTER_DISABLE_CACHE) -m, --[no-]disable-intro disable collector level introspection metrics ($PG_EXPORTER_DISABLE_INTRO) -a, --[no-]auto-discovery automatically scrape all database for given server ($PG_EXPORTER_AUTO_DISCOVERY) -x, --exclude-database=\"template0,template1,postgres\" excluded databases when enabling auto-discovery ($PG_EXPORTER_EXCLUDE_DATABASE) -i, --include-database=\"\" included databases when enabling auto-discovery ($PG_EXPORTER_INCLUDE_DATABASE) -n, --namespace=\"\" prefix of built-in metrics, (pg|pgbouncer) by default ($PG_EXPORTER_NAMESPACE) -f, --[no-]fail-fast fail fast instead of waiting during start-up ($PG_EXPORTER_FAIL_FAST) -T, --connect-timeout=100 connect timeout in ms, 100 by default ($PG_EXPORTER_CONNECT_TIMEOUT) -P, --web.telemetry-path=\"/metrics\" URL path under which to expose metrics. ($PG_EXPORTER_TELEMETRY_PATH) -D, --[no-]dry-run dry run and print raw configs -E, --[no-]explain explain server planned queries --log.level=\"info\" log level: debug|info|warn|error] --log.format=\"logfmt\" log format: logfmt|json --[no-]version Show application version. Environment Variables All command-line arguments have corresponding environment variables:\nPG_EXPORTER_URL='postgres://:5432/?sslmode=disable' PG_EXPORTER_CONFIG=/etc/pg_exporter.yml PG_EXPORTER_LABEL=\"\" PG_EXPORTER_TAG=\"\" PG_EXPORTER_DISABLE_CACHE=false PG_EXPORTER_AUTO_DISCOVERY=true PG_EXPORTER_EXCLUDE_DATABASE=\"template0,template1,postgres\" PG_EXPORTER_INCLUDE_DATABASE=\"\" PG_EXPORTER_NAMESPACE=\"pg\" PG_EXPORTER_FAIL_FAST=false PG_EXPORTER_CONNECT_TIMEOUT=100 PG_EXPORTER_TELEMETRY_PATH=\"/metrics\" PG_EXPORTER_OPTS='--log.level=info' pg_exporter Deployment Architecture The simplest deployment with one exporter per PostgreSQL instance:\n┌─────────────┐ ┌──────────────┐ ┌────────────┐ │ Prometheus │────▶│ PG Exporter │────▶│ PostgreSQL │ └─────────────┘ └──────────────┘ └────────────┘ :9630 :5432 Multi-Database Environment Using auto-discovery to monitor multiple databases, which is enabled by default\n┌─────────────┐ ┌────────────────┐ ┌────────────┐ │ Prometheus │────▶│ PG Exporter │────▶│ PostgreSQL │ └─────────────┘ │ with │ │ ├─ db1 │ │ auto-discovery │ │ ├─ db2 │ └────────────────┘ │ └─ db3 │ └────────────┘ Production Configuration PostgreSQL User Setup Create a dedicated monitoring user with minimal required permissions:\n-- Create monitoring role CREATE ROLE pg_monitor WITH LOGIN PASSWORD 'strong_password' CONNECTION LIMIT 5; -- Grant necessary permissions GRANT pg_monitor TO pg_monitor; -- PostgreSQL 10+ built-in role GRANT CONNECT ON DATABASE postgres TO pg_monitor; -- For specific databases GRANT CONNECT ON DATABASE app_db TO pg_monitor; GRANT USAGE ON SCHEMA public TO pg_monitor; -- Additional permissions for extended monitoring GRANT SELECT ON ALL TABLES IN SCHEMA pg_catalog TO pg_monitor; GRANT SELECT ON ALL SEQUENCES IN SCHEMA pg_catalog TO pg_monitor; Connection Security Using SSL/TLS # Connection string with SSL PG_EXPORTER_URL='postgres://pg_monitor:password@db.example.com:5432/postgres?sslmode=require\u0026sslcert=/path/to/client.crt\u0026sslkey=/path/to/client.key\u0026sslrootcert=/path/to/ca.crt' Using .pgpass File # Create .pgpass file echo \"db.example.com:5432:*:pg_monitor:password\" \u003e ~/.pgpass chmod 600 ~/.pgpass # Use without password in URL PG_EXPORTER_URL='postgres://pg_monitor@db.example.com:5432/postgres' Systemd Service Configuration Complete production systemd setup:\n[Unit] Description=Prometheus exporter for PostgreSQL/Pgbouncer server metrics Documentation=https://github.com/pgsty/pg_exporter After=network.target [Service] EnvironmentFile=-/etc/default/pg_exporter User=prometheus ExecStart=/usr/bin/pg_exporter $PG_EXPORTER_OPTS Restart=on-failure [Install] WantedBy=multi-user.target Environment file /etc/default/pg_exporter:\nPG_EXPORTER_URL='postgres://:5432/?sslmode=disable' PG_EXPORTER_CONFIG=/etc/pg_exporter.yml PG_EXPORTER_LABEL=\"\" PG_EXPORTER_TAG=\"\" PG_EXPORTER_DISABLE_CACHE=false PG_EXPORTER_AUTO_DISCOVERY=true PG_EXPORTER_EXCLUDE_DATABASE=\"template0,template1,postgres\" PG_EXPORTER_INCLUDE_DATABASE=\"\" PG_EXPORTER_NAMESPACE=\"pg\" PG_EXPORTER_FAIL_FAST=false PG_EXPORTER_CONNECT_TIMEOUT=100 PG_EXPORTER_TELEMETRY_PATH=\"/metrics\" PG_EXPORTER_OPTS='--log.level=info' ","categories":"","description":"","excerpt":"This guide covers production deployment strategies, best practices, …","ref":"/docs/pg_exporter/deploy/","tags":"","title":"Deployment"},{"body":"The latest stable version of pg_exporter is v1.1.2\nVersion Date Summary GitHub v1.1.2 2026-01-16 fix pg_timeline conf issue, build with latest deps v1.1.2 v1.1.1 2025-12-30 New pg_timeline collector, pg_sub_16 branch, bug fixes v1.1.1 v1.1.0 2025-12-15 Update default metrics collectors, bump to go 1.25.5 v1.1.0 v1.0.3 2025-11-20 Routine update on 1.25.4, fix unsupported libpq env v1.0.3 v1.0.2 2025-08-14 Build for more os arch with goreleaser v1.0.2 v1.0.1 2025-07-17 DockerHub images, Go 1.24.5, disable pg_tsdb_hypertable v1.0.1 v1.0.0 2025-05-06 PostgreSQL 18 support, new WAL/checkpointer/I/O metrics v1.0.0 v0.9.0 2025-04-26 TimescaleDB, Citus, pg_wait_sampling collectors v0.9.0 v0.8.1 2025-02-14 Dependencies update, docker image tags v0.8.1 v0.8.0 2025-02-14 PgBouncer 1.24 support, Go 1.24, logging refactor v0.8.0 v0.7.1 2024-12-29 Routine update, configuration as Reader support v0.7.1 v0.7.0 2024-08-13 PostgreSQL 17 support, predicate queries feature v0.7.0 v0.6.0 2023-10-18 PostgreSQL 16 support, ARM64 packages, security fixes v0.6.0 v0.5.0 2022-04-27 RPM/DEB builds, column scaling, metrics enhancements v0.5.0 v0.4.1 2022-03-08 Collector updates, connect-timeout parameter v0.4.1 v0.4.0 2021-07-12 PostgreSQL 14 support, auto-discovery feature v0.4.0 v0.3.2 2021-02-01 Shadow DSN fixes, documentation updates v0.3.2 v0.3.1 2020-12-04 Configuration fixes for older PostgreSQL versions v0.3.1 v0.3.0 2020-10-29 PostgreSQL 13 support, REST APIs, dummy server v0.3.0 v0.2.0 2020-03-21 YUM packages, configuration reload support v0.2.0 v0.1.2 2020-02-20 Dynamic configuration reload, bulky mode v0.1.2 v0.1.1 2020-01-10 Startup hang bug fix v0.1.1 v0.1.0 2020-01-08 Initial stable release v0.1.0 v0.0.4 2019-12-20 Production tested release v0.0.4 v0.0.3 2019-12-14 Production environment testing v0.0.3 v0.0.2 2019-12-09 Early testing release v0.0.2 v0.0.1 2019-12-06 Initial release with PgBouncer mode v0.0.1 v1.1.2 Minor release fixing pg_timeline configuration issue and building with latest go deps\nChecksums\nhttps://github.com/pgsty/pg_exporter/releases/download/v1.1.2/checksums.txt\n8cddd57a843914a3145a80a3220bc875047b9bcac0664357c01ba86485436236 pg-exporter_1.1.2-1_amd64.deb f5b25a8ae5c022867a54c17ba1c6493eba20dcb292340460390289336df24f04 pg-exporter_1.1.2-1_arm64.deb 4da2c287f6717681b25befda0d59a89b9d1b258281ce94f3a6bc21d02f70c83c pg-exporter_1.1.2-1_ppc64le.deb b26355f3c1a5b8a147291a51e2d7ada204deed6d52877c146a8b3e499defa5e8 pg_exporter-1.1.2-1.aarch64.rpm 42ef89716ba99dd918b0e9c77ef3236129d613f68bb8ae5929668a5a2596cca5 pg_exporter-1.1.2-1.ppc64le.rpm a8f4a2d5c7b6701c7bac788a7ed7183b6c4b74a334326cd389f3a695fb77675d pg_exporter-1.1.2-1.x86_64.rpm 775f5ea3188a6acb1327c001c4ba9a0651424c3bb37d800e6f67972c904c4750 pg_exporter-1.1.2.darwin-amd64.tar.gz 7f2bbcc2db1e16dc78c3edd8e67e20e4ec81f2972c8c37135cba6f6afbf91003 pg_exporter-1.1.2.darwin-arm64.tar.gz 33c34b1f9ef6b6e7615f241a95059a8137a2337a454930b668180a9329d12b98 pg_exporter-1.1.2.linux-amd64.tar.gz 2b91a5818d780e38692ab6446cacb496695e67388676c18012be582e8ddfbdd8 pg_exporter-1.1.2.linux-arm64.tar.gz adcb5f229f4a5d641f6430b9a2dfb0377a2e4310efad242730867d6cdf5e27ee pg_exporter-1.1.2.linux-ppc64le.tar.gz 90b7c7e4b2b94936b5faa3cf2d35509b62ebc0d60b3afe1abaaf03efcd415a4a pg_exporter-1.1.2.windows-amd64.tar.gz https://github.com/pgsty/pg_exporter/releases/tag/v1.1.2\nv1.1.1 Minor release with new collectors and bug fixes.\nNew Features:\nNew pg_timeline collector for timeline monitoring New pg_sub_16 collector branch to exclude parallel operations in subscriptions (PostgreSQL 16+ compatibility) Bug Fixes:\nFix: Add coalesce for slotname in pg_recv collector to handle NULL values Checksums\nhttps://github.com/pgsty/pg_exporter/releases/download/v1.1.1/checksums.txt\nfd5ee96511676fc11b975115a4870ed0c811056519f79ad7f24ab7ec538fa278 pg-exporter_1.1.1-1_amd64.deb b90a08d16a6e4707d82f8f3ae282cb76acb331de607e7544532fd0b774b7aa27 pg-exporter_1.1.1-1_arm64.deb 163955f59a71da48901ffa26bb2f2db0712d31d8aeb1ab3fa463683f719a6d3a pg-exporter_1.1.1-1_ppc64le.deb cf4f8bc12bb8a2d1e55553f891fd31c43324e4348249727972eb44f82cd4e6c8 pg_exporter-1.1.1-1.aarch64.rpm 5a425b2f61f308b32f2d107372830c34eb685bfb312ee787f11877a20f1c4a2e pg_exporter-1.1.1-1.ppc64le.rpm 23606ccea565368971ac2e7f39766455b507021f09457bcf61db13cb10501a16 pg_exporter-1.1.1-1.x86_64.rpm ce74624eba92573318f50764cee4f355fa1f35697d209f70a4240f8f9d976188 pg_exporter-1.1.1.darwin-amd64.tar.gz 35fba12521dbdcc54a3792278ed4822e4ca9e951665b5e53dff7c2a0f7014ae3 pg_exporter-1.1.1.darwin-arm64.tar.gz 7699bdef15dd306289645beee8d40a123ca75dc988e46d89cdd75a1c1f650bef pg_exporter-1.1.1.linux-amd64.tar.gz f4baba59d27a8eb67f0c5209fed7b9f00f78db796e583cc3487701e7803671c6 pg_exporter-1.1.1.linux-arm64.tar.gz 810c3817c27358fa667714f8bfe8d52840a7ea010035e29547919ccb7c9fa781 pg_exporter-1.1.1.linux-ppc64le.tar.gz 3f6df693b3eb92fdaeaeccf99ea7e5977b2c65028a4f00bdfabbc0405b9f5f93 pg_exporter-1.1.1.windows-amd64.tar.gz https://github.com/pgsty/pg_exporter/releases/tag/v1.1.1\nv1.1.0 Build with Go 1.25.5 and latest dependencies, collector updates:\nCollector Changes:\npg_setting: Major refactor for PG10-18 compatibility with missing_ok support Add 13 new metrics: max_parallel_workers, max_parallel_workers_per_gather, max_parallel_maintenance_workers, shared_buffers, maintenance_work_mem, effective_cache_size, fsync, full_page_writes, autovacuum, autovacuum_max_workers, checkpoint_timeout, checkpoint_completion_target, hot_standby, synchronous_commit, io_method Rename work_memory_size to work_mem Change min_version from 9.6 to 10, explicit ::int type casting pg_size: Fix log directory size detection, use logging_collector check instead of path pattern matching pg_table: Performance optimization, replace LATERAL subqueries with JOIN for better query performance; fix tuples and frozenxid metric type from COUNTER to GAUGE; increase timeout from 1s to 2s pg_vacuuming: Add PG17 collector branch with new metrics indexes_total, indexes_processed, dead_tuple_bytes for index vacuum progress tracking pg_query: Increase timeout from 1s to 2s for high-load scenarios pg_io: Fix typo in reuses description (“in reused” -\u003e “is reused”) pg_checkpointer: Fix description for pg_checkpointer_10 (“9.4+” -\u003e “9.4-17”) pg_db_confl: Fix description for pg_db_confl_15 (“9.1 - 16” -\u003e “9.1 - 15”) Format alignment fixes for pg_db, pg_indexing, pg_clustering, pg_backup Other Changes:\nFix release year by @anayrat Checksums\nhttps://github.com/pgsty/pg_exporter/releases/download/v1.0.3/checksums.txt\n9c65f43e76213bb8a49d1eab2c76a27d9ab694e67bc79f0ad12769ea362b5ca2 pg-exporter_1.1.0-1_amd64.deb bcd2cacb4febc5fb92f9eda8e733c161c8c6721416e16ec91a773503241c972d pg-exporter_1.1.0-1_arm64.deb 2c9d4a9cb06d07af0b6dd9dd6e568af073dc9f6775abde63b45f0aae34d171b1 pg-exporter_1.1.0-1_ppc64le.deb 2934ab5b0fb16dca5a96ec1e8f230e32c72b30ca076b5e5ddf8ec553c821f7b8 pg_exporter-1.1.0-1.aarch64.rpm 3c9955f31ba93532cc7f95ff60b0658f4b6eca6a827710e2f70c0716b34eab43 pg_exporter-1.1.0-1.ppc64le.rpm 9fdefbd8e7660dcb130207901a27762e0a381857ba8cf12b63184744f92dea05 pg_exporter-1.1.0-1.x86_64.rpm 7159002016754309e0ed625a9a48049d21177883fa11d1e448eb7655ceb690cc pg_exporter-1.1.0.darwin-amd64.tar.gz 7d55ac5cda0b1fd8ffbd5e76b9c1c1784ac8e353104a206caaadce89adda6d65 pg_exporter-1.1.0.darwin-arm64.tar.gz 8211ec24277554b9b1a36920d7865153e21c2621031d3d08f22d94cdd2ddf02f pg_exporter-1.1.0.linux-amd64.tar.gz d17ab7f9bf04442e642483d432d005d25bb62e0c9caa73cb7e69ee19eb89b3ae pg_exporter-1.1.0.linux-arm64.tar.gz c074aeb345cc30f7b6e16aa153ae3d9a12789e4425987590c3fd77c4e68a40b6 pg_exporter-1.1.0.linux-ppc64le.tar.gz 13d653e2abb023ce9526bdc2815135b82f49c044d237030f3f56b09fb016fcb7 pg_exporter-1.1.0.windows-amd64.tar.gz https://github.com/pgsty/pg_exporter/releases/tag/v1.1.0\nv1.0.3 Build with Go 1.25.4 and latest dependencies Fix #80 Conflict with libpq env variables Chanage default value of auto-discovery to true by @kadaffy Checksums\nhttps://github.com/pgsty/pg_exporter/releases/download/v1.0.3/checksums.txt\n7efa1a77dfd5b94813c32c7ac015b1d479b1f04fb958f6b1ed5af333e354d015 pg-exporter_1.0.3-1_amd64.deb 41e18bf18eba2ab90ac371bfb46e9152da9fe628ebd8e26766cac08325eb3b07 pg-exporter_1.0.3-1_arm64.deb 7da8ed738d254c120d42aa51d6137f84e7f4e3188bc764d4f9a1438220363a43 pg-exporter_1.0.3-1_ppc64le.deb a214b555981156da7b7d248b1f728f8ac88a07ac8f77a66c5d8e43b40670d6b4 pg_exporter-1.0.3-1.aarch64.rpm d876fc66e208612ebffe3c43dabce88b088d915f92584260d710b85a3a131413 pg_exporter-1.0.3-1.ppc64le.rpm 75f62d314fec50c836c534996c884d25ecea77810ab33e7ba0e9c4b783e775b4 pg_exporter-1.0.3-1.x86_64.rpm 47829a19707284bcee1b8dc47cc7d0172398bb533e6b4043950f787486712769 pg_exporter-1.0.3.darwin-amd64.tar.gz 38b6ccb72315cadea542b1f2a7b7022d0e8d48ffd4ab177bb69a0a909b99af6b pg_exporter-1.0.3.darwin-arm64.tar.gz 36e8dff84d61a7593ff1fcec567ca4ffeaecd0be2f9eabd227ceac71b12a919a pg_exporter-1.0.3.linux-amd64.tar.gz 6477e8ef873773a09c4f39a29444f21b5b2c71e717e52ca425bcc8e8e5448791 pg_exporter-1.0.3.linux-arm64.tar.gz a083b51ebed2b280e2eaa0f19558494e7fa6f122a0a86a1d117206fcd090820c pg_exporter-1.0.3.linux-ppc64le.tar.gz a1f9b27b7190f478726d96f270a72d9dc4d3f2bcc3b0326b7c4a2607e62ea588 pg_exporter-1.0.3.windows-amd64.tar.gz https://github.com/pgsty/pg_exporter/releases/tag/v1.0.3\nv1.0.2 Build with Go 1.25.0 and latest dependencies Dedicate website and homepage: https://exp.pgsty.com Release with goreleaser for more os/arch with CI/CD pipeline: add windows amd64 support add linux ppc64le support Checksums\nhttps://github.com/pgsty/pg_exporter/releases/download/v1.0.2/checksums.txt\n683bf97f22173f2f2ec319a88e136939c2958a1f5ced4f4aa09a1357fc1c44c5 pg-exporter_1.0.2-1_amd64.deb f62d479a92be2d03211c162b8419f968cea87ceef5b1f25f2bcd390e0b72ccb5 pg-exporter_1.0.2-1_arm64.deb e1bbfc5a4c1b93e6f92bc7adcb4364583ab763e76e156aa5c979d6d1040f4c7a pg-exporter_1.0.2-1_ppc64le.deb f51d5b45448e6bbec3467d1d1dc049b1e16976f723af713c4262541ac55a039c pg_exporter-1.0.2-1.aarch64.rpm 18380011543674e4c48b2410266b41165974d780cbc8918fc562152ba623939e pg_exporter-1.0.2-1.ppc64le.rpm 198372d894b9598c166a0e91ca36d3c9271cb65298415f63dbffcf6da611f2bb pg_exporter-1.0.2-1.x86_64.rpm cbe7e07df6d180507c830cdab4cf86d40ccd62774723946307b5331d4270477d pg_exporter-1.0.2.darwin-amd64.tar.gz 20c4a35fa244287766c1d1a19cd2e393b3fa451a96a81e5635401e69bef04b97 pg_exporter-1.0.2.darwin-arm64.tar.gz d742111185f6a89fff34bfd304b851c8eb7a8e38444f0220786e11ed1934eff1 pg_exporter-1.0.2.linux-amd64.tar.gz 0b1f4c97c1089c4767d92eb22419b8f29c9f46fb90ddfd1e8514cc42dc41054f pg_exporter-1.0.2.linux-arm64.tar.gz 895083fd2c7fc5409cc1a2dbaaef1e47ac7aa6a3fd5db2359012922d90bcdcc3 pg_exporter-1.0.2.linux-ppc64le.tar.gz 5f751228e7120604af9a482fb70197489fa633c38a0f2b6a3489393fbc6a10aa pg_exporter-1.0.2.windows-amd64.tar.gz https://github.com/pgsty/pg_exporter/releases/tag/v1.0.2\nv1.0.1 Add dockerhub images: pgsty/pg_exporter Bump go dependencies to the latest version, build with go 1.24.5 Disable pg_tsdb_hypertable collector by default, since timescaledb catalog is changed. Checksums\nd5e2d6a656eef0ae1b29cd49695f9773 pg_exporter-1.0.1-1.aarch64.rpm cb01bb78d7b216a235363e9342803cb3 pg_exporter-1.0.1-1.x86_64.rpm 67093a756b04845f69ad333b6d458e81 pg_exporter-v1.0.1.darwin-amd64.tar.gz 2d3fdc10045d1cf494b9c1ee7f94f127 pg_exporter-v1.0.1.darwin-arm64.tar.gz e242314461becfa99c3978ae72838ab0 pg_exporter-v1.0.1.linux-amd64.tar.gz 63de91da9ef711a53718bc60b89c82a6 pg_exporter-v1.0.1.linux-arm64.tar.gz 718f6afc004089f12c1ca6553f9b9ba5 pg-exporter_1.0.1_amd64.deb 57da7a8005cdf91ba8c1fb348e0d7367 pg-exporter_1.0.1_arm64.deb https://github.com/pgsty/pg_exporter/releases/tag/v1.0.1\nv1.0.0 Add PostgreSQL 18 metrics support\nnew collector branch pg_wal_18: remove write, sync, write_time, sync_time metrics move to pg_stat_io new collector branch pg_checkpointer_18: new metric num_done new metric slru_written new collector branch pg_db_18: new metric parallel_workers_to_launch new metric parallel_workers_launched new collector branch pg_table_18: table_parallel_workers_to_launch table_parallel_workers_launched new collector branch pg_io_18: new series about WAL statistics new metric read_bytes new metric write_bytes new metric extend_bytes remove op_bytes due to fixed value new collector branch pg_vacuuming_18 new metric delay_time 8637bc1a05b93eedfbfd3816cca468dd pg_exporter-1.0.0-1.aarch64.rpm a28c4c0dcdd3bf412268a2dbff79f5b9 pg_exporter-1.0.0-1.x86_64.rpm 229129209b8e6bc356c28043c7c22359 pg_exporter-v1.0.0.darwin-amd64.tar.gz d941c2c28301269e62a8853c93facf12 pg_exporter-v1.0.0.darwin-arm64.tar.gz 5bbb94db46cacca4075d4c341c54db37 pg_exporter-v1.0.0.linux-amd64.tar.gz da9ad428a50546a507a542d808f1c0fa pg_exporter-v1.0.0.linux-arm64.tar.gz 0fa2395d9d7a43ab87e5c87e5b06ffcc pg-exporter_1.0.0_amd64.deb fed56f8a37e30cc59e85f03c81fce3f5 pg-exporter_1.0.0_arm64.deb https://github.com/pgsty/pg_exporter/releases/tag/v1.0.0\nv0.9.0 Default Collectors\nnew metrics collector for timescaledb hypertable new metrics collector for citus dist node new metrics collector for pg_wait_sampling wait event profile pg_slot overhaul: Add 16/17 pg_replication_slot metrics allow pg_slot collector run on replica since 16/17 refactor pg_wait collector to agg from all processes restrict pg_clustering, pg_indexing, pg_vacuuming run on primary mark all reset_time as GAUGE rather than COUNTER fix pg_recovery_prefetch_skip_fpw type from GAUGE to COUNTER fix pg_recv.state type from LABEL to GAUGE Format collector in compact mode new default metric pg_exporter_build_info / pgbouncer_exporter_build_info add server_encoding to pg_meta collector add 12 new setting metrics to pg_setting collector wal_block_size segment_size wal_segment_size wal_level wal_log_hints work_mem hugepage_count hugepage_status max_wal_size min_wal_size max_slot_wal_keep_size Exporter Codebase\nnormalize collector branch name with min pg ver suffix Add license file to binary packages move pgsty/pg_exporter repo to pgsty/pg_exporter refactor server.go to reduce Compatible and PostgresPrecheck complexity rename metrics collector with extra number prefix for better sorting bump dependencies to the latest version execute fatal collectors ahead of all non-fatal collectors, and fail fast https://github.com/pgsty/pg_exporter/releases/tag/v0.9.0\nv0.8.1 Bump dependencies to the latest version Bump golang.org/x/net from 0.35.0 to 0.36.0 #67 Update docker images building tags https://github.com/pgsty/pg_exporter/releases/tag/v0.8.1\nv0.8.0 Add PgBouncer 1.24 new metrics support (stat, pool, database) Fix: 310-pg_size.yml fails if log dir not set properly #64 by @Süleyman Vurucu Build with the latest Go 1.24 and bump all dependencies Refactor logging with the standard log/slog instead of go-kit Full Changelog: https://github.com/pgsty/pg_exporter/compare/v0.7.1...v0.8.0 https://github.com/pgsty/pg_exporter/releases/tag/v0.8.0\nv0.7.1 Routine update with dependabot\nFeat: support specifying configuration as Reader by @ringerc in #62 Bump golang.org/x/crypto from 0.21.0 to 0.31.0 by @dependabot in #63 Fix some typos Full Changelog: https://github.com/pgsty/pg_exporter/compare/v0.7.0...v0.7.1 https://github.com/pgsty/pg_exporter/releases/tag/v0.7.1\nv0.7.0 Refactor codebase for the latest go version.\nPostgreSQL 17 Metrics Support by @Vonng pg_exporter: predicate queries feature by @ringerc Do a clean build in the dockerfile by @ringerc by @ringerc pg_exporter: don’t panic after “bind: address already in use” by @ringerc pg_exporter: fix /stat endpoint formatting by @ringerc pg_exporter: omit default query properties on yaml export by @ringerc Exclude template DBs from discovery and schema-qualify discovery query by @ringerc Fix some typos and some metric description mistakes by @ringerc Switch from unmaintained lib/pq driver to pgx with stdlib wrapper by @ringerc https://github.com/pgsty/pg_exporter/releases/tag/v0.7.0\nv0.6.0 Security Enhancement: Fix security dependent-bot issue\nAdd pg16 collectors\nAdd arm64 \u0026 aarch64 packages\nRemove the monitor schema requirement for pg_query collectors (you have to ensure it with search_path or just install pg_stat_statements in the default public schema)\nFix pgbouncer version parsing message level from info to debug\nFix pg_table_10_12 collector missing relid issue.\nRecognize the files with yml suffix in config directory by @Japin Li\nSupport PostgreSQL 15 and higher by @Japin Li\nFix connect-timeout propagation by @mouchar\nhttps://github.com/pgsty/pg_exporter/releases/tag/v0.6.0\nv0.5.0 Exporter Enhancement\nBuild rpm \u0026 deb with nfpm Add column.default, replace when metric value is NULL Add column.scale, multiply scale factor when metric value is float/int (e.g µs to second) Fix /stat endpoint output Add docker container pgsty/pg_exporter Metrics Collector\nscale bgwriter \u0026 pg_wal time unit to second remove pg_class collector and move it to pg_table \u0026 pg_inex add pg_class metrics to pg_table add pg_class metrics to pg_index enable pg_table_size by default scale pg_query pg_db pg_bgwriter pg_ssl pgbouncer_stat time metrics to second https://github.com/pgsty/pg_exporter/releases/tag/v0.5.0\nv0.4.1 update default collectors omit citus \u0026 timescaledb schemas on object monitoring avoid duplicate pg_statio tuples support pgbouncer v1.16 bug fix: pg_repl collector overlap on pg 12 new parameter: -T connect-timeout PG_EXPORTER_CONNECT_TIMEOUT this can be useful when monitoring remote Postgres instances. now pg_exporter.yaml are renamed as pg_exporter.yml in rpm package. https://github.com/pgsty/pg_exporter/releases/tag/v0.4.1\nv0.4.0 Add PG 14 support Default metrics configuration overhaul. (BUT you can still use the old configuration) add auto-discovery , include-database and exclude-database option Add multiple database monitoring implementations (with auto-discovery = on) https://github.com/pgsty/pg_exporter/releases/tag/v0.4.0\nv0.3.2 fix shadow DSN corner case fix typo \u0026 docs https://github.com/pgsty/pg_exporter/releases/tag/v0.3.2\nv0.3.1 fix default configuration problems (especially for versions lower than 13)\nsetting primary_conninfo not exists until PG13 add funcid label to pg_func collector to avoid func name duplicate label fix version string to pg_exporter https://github.com/pgsty/pg_exporter/releases/tag/v0.3.1\nv0.3.0 https://github.com/pgsty/pg_exporter/releases/tag/v0.3.0\nChange default configuration, Support PostgreSQL 13 new metrics (pg_slru, pg_shmem, pg_query13,pg_backup, etc…) Add a series of new REST APIs for health / recovery status check Add a dummy server with fake pg_up 0 metric, which serves before PgExporter is initialized. Add sslmode=disable to URL if sslmode is not given fix typos and bugs v0.2.0 add yum package and linux service definition add a ‘skip’ flag into query config fix pgbouncer_up metrics add conf reload support https://github.com/pgsty/pg_exporter/releases/tag/v0.2.0\nv0.1.2 fix pgbouncer_up metrics add dynamic configuration reload remove ‘shard’ related logic add a ‘bulky’ mode to default settings https://github.com/pgsty/pg_exporter/releases/tag/v0.1.2\nv0.1.1 Fix the bug that pg_exporter will hang during start-up if any query is failed.\nhttps://github.com/pgsty/pg_exporter/releases/tag/v0.1.1\nv0.1.0 It works, looks good to me.\nhttps://github.com/pgsty/pg_exporter/releases/tag/v0.1.0\nv0.0.4 Tested in real world production environment with 200+ nodes for about 2 weeks. Looks good !\nhttps://github.com/pgsty/pg_exporter/releases/tag/v0.0.4\nv0.0.3 v0.0.3 Release, Tested in Production Environment\nThis version is already tested in a production environment.\nThis project is still under rapid evolution, I would say if you want use it in production , try with caution.\nhttps://github.com/pgsty/pg_exporter/releases/tag/v0.0.3\nv0.0.2 It’s ok to try now\nhttps://github.com/pgsty/pg_exporter/releases/tag/v0.0.2\nv0.0.1 Add pgbouncer mode\nhttps://github.com/pgsty/pg_exporter/releases/tag/v0.0.1\n","categories":"","description":"","excerpt":"The latest stable version of pg_exporter is v1.1.2\nVersion Date …","ref":"/docs/pg_exporter/release/","tags":"","title":"Release Notes"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/pigsty/","tags":"","title":"Pigsty"},{"body":"\nVictoriaMetrics/Logs replace Prometheus/Loki for 10x observability performance, Vector handles logs, unified UI, firewall/SELinux/credential hardening. Read more\n","categories":"","description":"VictoriaMetrics/Logs replace Prometheus/Loki for 10x observability performance, Vector handles logs, unified UI, firewall/SELinux/credential hardening.\n","excerpt":"VictoriaMetrics/Logs replace Prometheus/Loki for 10x observability …","ref":"/blog/pigsty/v4.0/","tags":["Pigsty"],"title":"Pigsty v4.0: Victoria Stack + Security Hardening"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/","tags":"","title":"Tags"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/agent/","tags":"","title":"Agent"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/ai/","tags":"","title":"AI"},{"body":"\nHow to install and use Claude Code? How to achieve similar results at 1/10 of Claude’s cost with alternative models? A one-liner to get CC up and running! Read more\n","categories":"","description":"How to install and use Claude Code? How to achieve similar results at 1/10 of Claude's cost with alternative models? A one-liner to get CC up and running!\n","excerpt":"How to install and use Claude Code? How to achieve similar results at …","ref":"/blog/db/claude-code-intro/","tags":["AI","Agent"],"title":"Claude Code Quick Start: Using Alternative LLMs at 1/10 the Cost"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/development/","tags":"","title":"Development"},{"body":"\nHow to instantly clone a massive PostgreSQL database without consuming extra storage? PostgreSQL 18 and XFS can spark some serious magic. Read more\n","categories":"","description":"How to instantly clone a massive PostgreSQL database without consuming extra storage? PostgreSQL 18 and XFS can spark some serious magic.\n","excerpt":"How to instantly clone a massive PostgreSQL database without consuming …","ref":"/blog/pg/pg-clone/","tags":["PostgreSQL","Development"],"title":"Git for Data: Instant PostgreSQL Database Cloning"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/postgresql/","tags":"","title":"PostgreSQL"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/aliyun/","tags":"","title":"Aliyun"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/cloud-exit/","tags":"","title":"Cloud-Exit"},{"body":"\nWhen a company that was “born on the cloud” goes “self-host first,” does that count as cloud exit? A repost of a deleted piece on the infrastructure coming-of-age for Chinas internet giants. Read more\n","categories":"","description":"When a company that was \"born on the cloud\" goes \"self-host first,\" does that count as cloud exit? A repost of a deleted piece on the infrastructure coming-of-age for Chinas internet giants.\n","excerpt":"When a company that was \"born on the cloud\" goes \"self-host first,\" …","ref":"/blog/cloud/rednote-cloud-exit/","tags":["Cloud-Exit","Aliyun","RedNote"],"title":"Did RedNote Exit the Cloud?"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/rednote/","tags":"","title":"RedNote"},{"body":"\nA conversation between Mike Stonebraker (Turing Award Winner, Creator of PostgreSQL), Andy Pavlo (Carnegie Mellon), and the DBOS team. Read more\n","categories":"","description":"A conversation between Mike Stonebraker (Turing Award Winner, Creator of PostgreSQL), Andy Pavlo (Carnegie Mellon), and the DBOS team.\n","excerpt":"A conversation between Mike Stonebraker (Turing Award Winner, Creator …","ref":"/blog/db/db-year-review-2025/","tags":["Database","DBOS","PostgreSQL"],"title":"Data 2025: Year in Review with Mike Stonebraker"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/database/","tags":"","title":"Database"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/dbos/","tags":"","title":"DBOS"},{"body":"\nThe bottleneck for AI Agents isnt in database engines but in upper-layer integration. Muscle memory, associative memory, and trial-and-error courage will be key. Read more\n","categories":"","description":"The bottleneck for AI Agents isnt in database engines but in upper-layer integration. Muscle memory, associative memory, and trial-and-error courage will be key.\n","excerpt":"The bottleneck for AI Agents isnt in database engines but in …","ref":"/blog/db/agent-native-db/","tags":["Database","PostgreSQL","AI","Agent"],"title":"What Database Does AI Agent Need?"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/mysql/","tags":"","title":"MySQL"},{"body":"\nMySQL is to the internet what baijiu is to China: harsh, hard to swallow, yet worshipped because culture demands obedience. Both are loyalty tests—will you endure discomfort to fit in? Read more\n","categories":"","description":"MySQL is to the internet what baijiu is to China: harsh, hard to swallow, yet worshipped because culture demands obedience. Both are loyalty tests—will you endure discomfort to fit in?\n","excerpt":"MySQL is to the internet what baijiu is to China: harsh, hard to …","ref":"/blog/db/mysql-baijiu/","tags":["MySQL","Database"],"title":"MySQL and Baijiu: The Internet’s Obedience Test"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/observability/","tags":"","title":"Observability"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/victoria/","tags":"","title":"Victoria"},{"body":"\nVictoriaMetrics is brutally efficient—using a fraction of Prometheus + Loki’s resources for multiples of the performance. Pigsty v4 swaps to the Victoria stack; here’s the beta for anyone eager to try it. Read more\n","categories":"","description":"VictoriaMetrics is brutally efficient—using a fraction of Prometheus + Loki’s resources for multiples of the performance. Pigsty v4 swaps to the Victoria stack; here’s the beta for anyone eager to try it.\n","excerpt":"VictoriaMetrics is brutally efficient—using a fraction of Prometheus + …","ref":"/blog/db/victoria-stack/","tags":["Victoria","Observability"],"title":"Victoria: The Observability Stack That Slaps the Industry"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/minio/","tags":"","title":"MinIO"},{"body":"\nMinIO just entered maintenance mode. What replaces it? Can RustFS step in? I tested the contenders so you don’t have to. Read more\n","categories":"","description":"MinIO just entered maintenance mode. What replaces it? Can RustFS step in? I tested the contenders so you don’t have to.\n","excerpt":"MinIO just entered maintenance mode. What replaces it? Can RustFS step …","ref":"/blog/db/minio-alternative/","tags":["Database","MinIO"],"title":"MinIO Is Dead. Who Picks Up the Pieces?"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/alibaba-cloud/","tags":"","title":"Alibaba-Cloud"},{"body":"\nDec 4, 2025, Taobao, Alipay, and Xianyu all cratered. Users got charged while orders still showed “unpaid,” a carbon copy of the 2024 Double-11 fiasco. Read more\n","categories":"","description":"Dec 4, 2025, Taobao, Alipay, and Xianyu all cratered. Users got charged while orders still showed “unpaid,” a carbon copy of the 2024 Double-11 fiasco.\n","excerpt":"Dec 4, 2025, Taobao, Alipay, and Xianyu all cratered. Users got …","ref":"/blog/cloud/alipay-crash/","tags":["Cloud-Exit","Alibaba-Cloud","Cloud-Outage"],"title":"Alipay, Taobao, Xianyu Went Dark. Smells Like a Message Queue Meltdown."},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/cloud-outage/","tags":"","title":"Cloud-Outage"},{"body":"\nMinIO announces it is entering maintenance mode, the dragon-slayer has become the dragon – how MinIO transformed from an open-source S3 alternative to just another commercial software company Read more\n","categories":"","description":"MinIO announces it is entering maintenance mode, the dragon-slayer has become the dragon – how MinIO transformed from an open-source S3 alternative to just another commercial software company\n","excerpt":"MinIO announces it is entering maintenance mode, the dragon-slayer has …","ref":"/blog/db/minio-is-dead/","tags":["Database","MinIO","Open-Source"],"title":"MinIO is Dead"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/open-source/","tags":"","title":"Open-Source"},{"body":"\nPostgreSQL 18 becomes the default version, EL10 and Debian 13 support added, extensions reach 437, and Pigsty wins the PostgreSQL Magneto Award. Read more\n","categories":"","description":"PostgreSQL 18 becomes the default version, EL10 and Debian 13 support added, extensions reach 437, and Pigsty wins the PostgreSQL Magneto Award.\n","excerpt":"PostgreSQL 18 becomes the default version, EL10 and Debian 13 support …","ref":"/blog/pigsty/v3.7/","tags":["Pigsty"],"title":"Pigsty v3.7: PostgreSQL Magneto Award, PG18 Deep Support"},{"body":"\nYour ability to ask questions—and your taste in what to ask—determines your position in the AI era. When answers become commodities, good questions become the new wealth. We are living in the moment this prophecy comes true. Read more\n","categories":"","description":"Your ability to ask questions—and your taste in what to ask—determines your position in the AI era. When answers become commodities, good questions become the new wealth. We are living in the moment this prophecy comes true.\n","excerpt":"Your ability to ask questions—and your taste in what to ask—determines …","ref":"/blog/db/ai-question/","tags":["AI"],"title":"When Answers Become Abundant, Questions Become the New Currency"},{"body":"\nContext window economics, the polyglot persistence problem, and the triumph of zero-glue architecture make PostgreSQL the database king of the AI era. Read more\n","categories":"","description":"Context window economics, the polyglot persistence problem, and the triumph of zero-glue architecture make PostgreSQL the database king of the AI era.\n","excerpt":"Context window economics, the polyglot persistence problem, and the …","ref":"/blog/pg/ai-db-king/","tags":["PostgreSQL","AI","Database"],"title":"Why PostgreSQL Will Dominate the AI Era"},{"body":"\nPostgreSQL already won. The real battle is the distro layer. Will Chinese developers watch from the sideline or craft a PG “Ubuntu” for the world? Read more\n","categories":"","description":"PostgreSQL already won. The real battle is the distro layer. Will Chinese developers watch from the sideline or craft a PG “Ubuntu” for the world?\n","excerpt":"PostgreSQL already won. The real battle is the distro layer. Will …","ref":"/blog/pg/forge-a-pg-distro/","tags":["PostgreSQL","Pigsty"],"title":"Forging a China-Rooted, Global PostgreSQL Distro"},{"body":"\nIn serious production you can’t rely on an upstream that explicitly says “no guarantees.” When someone says “don’t count on me,” the right answer is “then I’ll run it myself.” Read more\n","categories":"","description":"In serious production you can’t rely on an upstream that explicitly says “no guarantees.” When someone says “don’t count on me,” the right answer is “then I’ll run it myself.”\n","excerpt":"In serious production you can’t rely on an upstream that explicitly …","ref":"/blog/db/tuna-mirror-site/","tags":["Open-Source","Supply Chain"],"title":"On Trusting Open-Source Supply Chains"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/supply-chain/","tags":"","title":"Supply Chain"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/docker/","tags":"","title":"Docker"},{"body":"\nTons of users running the official docker postgres image got burned during recent minor version upgrades. A friendly reminder: think twice before containerizing production databases. Read more\n","categories":"","description":"Tons of users running the official docker postgres image got burned during recent minor version upgrades. A friendly reminder: think twice before containerizing production databases.\n","excerpt":"Tons of users running the official docker postgres image got burned …","ref":"/blog/db/no-docker-pg/","tags":["PostgreSQL","Docker"],"title":"Don't Run Docker Postgres for Production!"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/cloudflare/","tags":"","title":"Cloudflare"},{"body":"\nA ClickHouse permission tweak doubled a feature file, tripped a Rust hard limit, and froze Cloudflare’s core traffic for six hours—their worst outage since 2019. Here’s the full translation plus commentary. Read more\n","categories":"","description":"A ClickHouse permission tweak doubled a feature file, tripped a Rust hard limit, and froze Cloudflare’s core traffic for six hours—their worst outage since 2019. Here’s the full translation plus commentary.\n","excerpt":"A ClickHouse permission tweak doubled a feature file, tripped a Rust …","ref":"/blog/cloud/cf-ck-down/","tags":["Cloudflare","Cloud-Outage"],"title":"Cloudflare’s Nov 18 Outage, Translated and Dissected"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/extension/","tags":"","title":"Extension"},{"body":"\nFree, open, no VPN. Install PostgreSQL and 431 extensions on 14 Linux distros × 6 PG versions via native RPM/DEB—and a tiny CLI. Read more\n","categories":"","description":"Free, open, no VPN. Install PostgreSQL and 431 extensions on 14 Linux distros × 6 PG versions via native RPM/DEB—and a tiny CLI.","excerpt":"Free, open, no VPN. Install PostgreSQL and 431 extensions on 14 Linux …","ref":"/blog/pg/pgext-cloud/","tags":["PostgreSQL","Extension"],"title":"PG Extension Cloud: Unlocking PostgreSQL’s Entire Ecosystem"},{"body":"\nFounders here get asked the same question over and over: what if Alibaba builds the same thing? Alicloud RDS just launched Supabase as a managed service. Exhibit A. Read more\n","categories":"","description":"Founders here get asked the same question over and over: what if Alibaba builds the same thing? Alicloud RDS just launched Supabase as a managed service. Exhibit A.","excerpt":"Founders here get asked the same question over and over: what if …","ref":"/blog/cloud/aliyun-supabase/","tags":["Alibaba-Cloud","Open-Source","Supabase"],"title":"Alicloud “Borrowed” Supabase. This Is What Happens When Giants Strip-Mine Open-Source."},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/supabase/","tags":"","title":"Supabase"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/aws/","tags":"","title":"AWS"},{"body":"\nAWS finally published the Oct 20 us-east-1 postmortem. I translated the key parts and added commentary on how one DNS bug toppled half the internet. Read more\n","categories":"","description":"AWS finally published the Oct 20 us-east-1 postmortem. I translated the key parts and added commentary on how one DNS bug toppled half the internet.\n","excerpt":"AWS finally published the Oct 20 us-east-1 postmortem. I translated …","ref":"/blog/cloud/aws-postmotem/","tags":["Cloud-Outage","AWS"],"title":"AWS’s Official DynamoDB Outage Postmortem"},{"body":"\nus-east-1’s DNS control plane faceplanted for 15 hours and dragged 142 AWS services—and a good chunk of the public internet—down with it. Here’s the forensic tour. Read more\n","categories":"","description":"us-east-1’s DNS control plane faceplanted for 15 hours and dragged 142 AWS services—and a good chunk of the public internet—down with it. Here’s the forensic tour.\n","excerpt":"us-east-1’s DNS control plane faceplanted for 15 hours and dragged 142 …","ref":"/blog/cloud/aws-dns-failure/","tags":["Cloud-Outage","AWS"],"title":"How One AWS DNS Failure Cascaded Across Half the Internet"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/pg-admin/","tags":"","title":"PG-Admin"},{"body":"\nPostgreSQL official repos cut off global mirror sync channels, open-source binaries supply disrupted, revealing the true colors of various database and cloud vendors. Read more\n","categories":"","description":"PostgreSQL official repos cut off global mirror sync channels, open-source binaries supply disrupted, revealing the true colors of various database and cloud vendors.\n","excerpt":"PostgreSQL official repos cut off global mirror sync channels, …","ref":"/blog/pg/pg-mirror-pigsty/","tags":["PostgreSQL","PG-Admin"],"title":"The PostgreSQL 'Supply Cut' and Trust Issues in Software Supply Chain"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/ddia/","tags":"","title":"DDIA"},{"body":"\nThe second edition of Designing Data-Intensive Applications has released ten chapters. I translated them into Chinese and rebuilt a clean Hugo/Hextra web version for the community. Read more\n","categories":"","description":"The second edition of Designing Data-Intensive Applications has released ten chapters. I translated them into Chinese and rebuilt a clean Hugo/Hextra web version for the community.\n","excerpt":"The second edition of Designing Data-Intensive Applications has …","ref":"/blog/db/ddia-v2/","tags":["DDIA"],"title":"DDIA 2nd Edition, Chinese Translation"},{"body":"\nA whole generation of developers has been told “cloud-first.” This column collects data, case studies, and analysis on the cloud exit movement. Read more\n","categories":"","description":"A whole generation of developers has been told \"cloud-first.\" This column collects data, case studies, and analysis on the cloud exit movement.\n","excerpt":"A whole generation of developers has been told \"cloud-first.\" This …","ref":"/blog/cloud/exit/","tags":["Cloud-Exit"],"title":"Column: Cloud-Exit"},{"body":"\nThe database world is full of hype and marketing fog. This column cuts through it with blunt commentary on industry trends and product reality. Read more\n","categories":"","description":"The database world is full of hype and marketing fog. This column cuts through it with blunt commentary on industry trends and product reality.\n","excerpt":"The database world is full of hype and marketing fog. This column cuts …","ref":"/blog/db/guru/","tags":["Database"],"title":"Column: Database Guru"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/pg-ecosystem/","tags":"","title":"PG-Ecosystem"},{"body":"\nThe same forces that once led MongoDB and MySQL toward closure are now at work in the PostgreSQL ecosystem. The PG world needs a distribution that represents “software freedom” values. Read more\n","categories":"","description":"The same forces that once led MongoDB and MySQL toward closure are now at work in the PostgreSQL ecosystem. The PG world needs a distribution that represents \"software freedom\" values.\n","excerpt":"The same forces that once led MongoDB and MySQL toward closure are now …","ref":"/blog/pg/proprity-pg/","tags":["PostgreSQL","PG-Ecosystem"],"title":"PostgreSQL Dominates Database World, but Who Will Devour PG?"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kubernetes/","tags":"","title":"Kubernetes"},{"body":"\nDeleting images and running away - this isn’t about commercial closed-source issues, but supply cut problems that directly destroy years of accumulated community trust. Read more\n","categories":"","description":"Deleting images and running away - this isn't about commercial closed-source issues, but supply cut problems that directly destroy years of accumulated community trust.","excerpt":"Deleting images and running away - this isn't about commercial …","ref":"/blog/cloud/kubesphere-rugpull/","tags":["Open-Source","Kubernetes"],"title":"KubeSphere: Trust Crisis Behind Open-Source Supply Cut"},{"body":"\nThe 2025 SO global developer survey results are fresh out, and PostgreSQL has become the most popular, most loved, and most wanted database for the third consecutive year. Nothing can stop PostgreSQL from consolidating the entire database world! Read more\n","categories":"","description":"The 2025 SO global developer survey results are fresh out, and PostgreSQL has become the most popular, most loved, and most wanted database for the third consecutive year. Nothing can stop PostgreSQL from consolidating the entire database world!","excerpt":"The 2025 SO global developer survey results are fresh out, and …","ref":"/blog/pg/so2025-pg/","tags":["PostgreSQL","PG-Ecosystem"],"title":"PostgreSQL Has Dominated the Database World"},{"body":"\nImagine a “closed-course” shootout for domestic databases and clouds, the way Dongchedi just humiliated 30+ autonomous cars. This industry needs its own stress test. Read more\n","categories":"","description":"Imagine a “closed-course” shootout for domestic databases and clouds, the way Dongchedi just humiliated 30+ autonomous cars. This industry needs its own stress test.\n","excerpt":"Imagine a “closed-course” shootout for domestic databases and clouds, …","ref":"/blog/db/car-autopilot-test/","tags":["Database","PostgreSQL"],"title":"Dongchedi Just Exposed “Smart Driving.” Where’s Our Dongku-Di?"},{"body":"\nNew doc site, PITR playbook, Percona PG TDE kernel support, and Supabase self-hosting optimization make v3.6 the last major release before 4.0. Read more\n","categories":"","description":"New doc site, PITR playbook, Percona PG TDE kernel support, and Supabase self-hosting optimization make v3.6 the last major release before 4.0.\n","excerpt":"New doc site, PITR playbook, Percona PG TDE kernel support, and …","ref":"/blog/pigsty/v3.6/","tags":["Pigsty"],"title":"Pigsty v3.6: The Ultimate PostgreSQL Distribution"},{"body":"\nGoogle recently launched a database MCP toolbox, perhaps the first production-ready solution. Read more\n","categories":"","description":"Google recently launched a database MCP toolbox, perhaps the first production-ready solution.\n","excerpt":"Google recently launched a database MCP toolbox, perhaps the first …","ref":"/blog/db/google-mcp/","tags":["MCP","Database","PostgreSQL"],"title":"Google AI Toolbox: Production-Ready Database MCP is Here?"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/mcp/","tags":"","title":"MCP"},{"body":"\nPGDG cuts off FTP rsync sync channels, global mirror sites universally disconnected - this time they really strangled global users’ supply chain. Read more\n","categories":"","description":"PGDG cuts off FTP rsync sync channels, global mirror sites universally disconnected - this time they really strangled global users' supply chain.\n","excerpt":"PGDG cuts off FTP rsync sync channels, global mirror sites universally …","ref":"/blog/pg/pg-mirror-break/","tags":["PostgreSQL","PG-Admin"],"title":"PGDG Cuts Off Mirror Sync Channel"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/dba/","tags":"","title":"DBA"},{"body":"\nWho will be revolutionized first - OLTP or OLAP? Integration vs specialization, how to choose? Where will DBAs go in the AI era? Feng’s views from the HOW 2025 conference roundtable, organized and published. Read more\n","categories":"","description":"Who will be revolutionized first - OLTP or OLAP? Integration vs specialization, how to choose? Where will DBAs go in the AI era? Feng's views from the HOW 2025 conference roundtable, organized and published.\n","excerpt":"Who will be revolutionized first - OLTP or OLAP? Integration vs …","ref":"/blog/db/ai-dba-job/","tags":["Database","AI","DBA"],"title":"Where Will Databases and DBAs Go in the AI Era?"},{"body":"\nThe database for the AI era has been settled. Capital markets are making intensive moves on PostgreSQL targets, with PG having become the default database for the AI era. Read more\n","categories":"","description":"The database for the AI era has been settled. Capital markets are making intensive moves on PostgreSQL targets, with PG having become the default database for the AI era.\n","excerpt":"The database for the AI era has been settled. Capital markets are …","ref":"/blog/db/db-for-ai/","tags":["Database","AI","PostgreSQL"],"title":"Stop Arguing, The AI Era Database Has Been Settled"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/iceberg/","tags":"","title":"Iceberg"},{"body":" Author: Paul Copplestone (Supabase CEO) | Original: Open Data Standards: Postgres, OTel, and Iceberg\nThree emerging standards in the data world: Postgres, OpenTelemetry, and Iceberg. Postgres is already the de facto standard. Read more\n","categories":"","description":"Three emerging standards in the data world: Postgres, OpenTelemetry, and Iceberg. Postgres is already the de facto standard.\n","excerpt":"Three emerging standards in the data world: Postgres, OpenTelemetry, …","ref":"/blog/db/open-data-standard/","tags":["Database","PostgreSQL","OpenTelemetry","Iceberg"],"title":"Open Data Standards: Postgres, OTel, and Iceberg"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/opentelemetry/","tags":"","title":"OpenTelemetry"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/duckdb/","tags":"","title":"DuckDB"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/olap/","tags":"","title":"OLAP"},{"body":" Author: Hannes Mühleisen (DuckDB Labs) | Original: The Lost Decade of Small Data\nIf DuckDB had launched in 2012, the great migration to distributed analytics might never have happened. Data isn’t that big after all. Read more\n","categories":"","description":"If DuckDB had launched in 2012, the great migration to distributed analytics might never have happened. Data isnt that big after all.\n","excerpt":"If DuckDB had launched in 2012, the great migration to distributed …","ref":"/blog/db/smalldata-decade/","tags":["Database","DuckDB","OLAP"],"title":"The Lost Decade of Small Data"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/openai/","tags":"","title":"OpenAI"},{"body":"\nAt PGConf.Dev 2025, Bohan Zhang from OpenAI shared how they scale PostgreSQL to millions of QPS using a single-primary, multi-replica architecture—proving that PostgreSQL can handle massive read workloads without sharding. Read more\n","categories":"","description":"At PGConf.Dev 2025, Bohan Zhang from OpenAI shared how they scale PostgreSQL to support millions of queries per second using a single-primary, multi-replica architecture without sharding.\n","excerpt":"At PGConf.Dev 2025, Bohan Zhang from OpenAI shared how they scale …","ref":"/blog/select/openai-pg/","tags":["PostgreSQL","OpenAI"],"title":"Scaling Postgres to the Next Level at OpenAI"},{"body":"At PGConf.Dev 2025, Bohan Zhang from OpenAI shared a session titled Scaling Postgres to the next level at OpenAI, giving us a peek into the database usage of a top-tier unicorn.\n“At OpenAl, we’ve proven that PostgreSQL can scale to support massive read-heavy workloads - even without sharding - using a single primary writer”\n—— Bohan Zhang from OpenAI, PGConf.Dev 2025\nBohan Zhang is a member of the OpenAI Infra team, student of Andy Pavlo, and co-found OtterTune with him.\nThis article is based on Bohan’s presentation at the conference. with chinese translation/commentary by Ruohang Feng (Vonng): Author of Pigsty. The original chinese version is available on WeChat Column and Pigsty CN Blog.\nHacker News Discussion: OpenAI: Scaling Postgres to the Next Level\nBackground Postgres is the backbone of our most critical systems at OpenAl. If Postgres goes down, many of OpenAI’s key features go down with it — and there’s plenty of precedent for this. PostgreSQL-related failures have caused several ChatGPT outages in the past.\nOpenAI uses managed PostgreSQL databases on Azure, without sharding. Instead, they employ a classic primary-replica replication architecture with one primary and over dozens of read replicas. For a service with several hundred million active users like OpenAI, scalability is a major concern.\nChallenges In OpenAI’s PostgreSQL architecture, read scalability is excellent, but “write requests” have become the primary bottleneck. OpenAI has already made many optimizations here, such as offloading write workloads wherever possible and avoiding placing new business logic into the main database.\nPostgreSQL’s MVCC design has some known issues, such as table and index bloat. Tuning autovacuum is complex, and every write generates a completely new version of a row. Index access might also require additional heap fetches for visibility checks. These design choices create challenges for scaling read replicas: for instance, more WAL typically leads to greater replication lag, and as the number of replicas grows, network bandwidth can become the new bottleneck.\nMeasures To tackle these issues, we’ve made efforts on multiple fronts:\nReduce Load on Primary The first optimization is to smooth out write spikes on the primary and minimize its load as much as possible, for example:\nOffloading all possible writes. Avoiding unnecessary writes at the application level. Using lazy writes to smooth out write bursts. Controlling the rate of data backfilling. Additionally, OpenAI offloads as many read requests as possible to replicas. The few read requests that cannot be moved from the primary because they are part of read-write transactions are required to be as efficient as possible.\nQuery Optimization The second area is query-level optimization. Since long-running transactions can block garbage collection and consume resources, they use timeout settings to prevent long “idle in transaction” states and set session, statement, and client-level timeouts. They also optimized some multi-way JOIN queries (e.g., joining 12 tables at once). The talk specifically mentioned that using ORMs can easily lead to inefficient queries and should be used with caution.\nMitigating Single Points of Failure The primary is a single point of failure; if it goes down, writes are blocked. In contrast, we have many read-only replicas. If one fails, applications can still read from others. In fact, many critical requests are read-only, so even if the primary goes down, they can continue to serve reads.\nFurthermore, we’ve distinguished between low-priority and high-priority requests. For high-priority requests, OpenAI allocates dedicated read-only replicas to prevent them from being impacted by low-priority ones.\nSchema Management The fourth measure is to allow only lightweight schema changes on this cluster. This means:\nCreating new tables or adding new workloads to it is not allowed. Adding or removing columns is allowed (with a 5-second timeout), but any operation that requires a full table rewrite is forbidden. Creating or removing indexes is allowed, but must be done using CONCURRENTLY. Another issue mentioned was that persistent long-running queries (\u003e1s) would continuously block schema changes, eventually causing them to fail. The solution was to have the application optimize or move these slow queries to replicas.\nResults Scaled PostgreSQL on Azure to millions of QPS, supporting OpenAI’s critical services. Added dozens of replicas without increasing replication lag. Deployed read-only replicas to different geographical regions while maintaining low latency. Only one SEV0 incident related to PostgreSQL in the past nine months. Still have plenty of room for future growth. “At OpenAl, we’ve proven that PostgreSQL can scale to support massive read-heavy workloads - even without sharding - using a single primary writer”\nCase Studies OpenAI also shared a few case studies of failures they’ve faced. The first was a cascading failure caused by a redis outage.\nThe second incident was more interesting: extremely high CPU usage triggered a bug where the WALSender process kept spin-looping instead of sending WAL to replicas, even after CPU levels returned to normal. This led to increased replication lag.\nFeature Suggestions Finally, Bohan raised some questions and feature suggestions to the PostgreSQL developer community:\nFirst, regarding disabling indexes. Unused indexes cause write amplification and extra maintenance overhead. They want to remove useless indexes, but to minimize risk, they wish for a feature to “disable” an index. This would allow them to monitor performance metrics to ensure everything is fine before actually dropping it.\nSecond is about RT observability. Currently, pg_stat_statement only provides the average response time for each query type, but doesn’t directly offer latency metrics like p95 or p99. They hope for more histogram-like and percentile latency metrics.\nThe third point is about schema changes. They want PostgreSQL to record a history of schema change events, such as adding/removing columns and other DDL operations.\nThe fourth case is about the semantics of monitoring views. They found a session with state = 'active' and wait_event = 'ClientRead' that lasted for over two hours. This means a connection remained active long after query_start, and such connections can’t be killed by the idle_in_transaction_timeout. They wanted to know if this is a bug and how to resolve it.\nFinally, a suggestion for optimizing PostgreSQL’s default parameters. The default values are too conservative. Could better defaults be used, or perhaps a heuristic-based configuration rule?\nVonng’s Commentary Although PGConf.Dev 2025 is primarily focused on development, you often see use case presentations from users, like this one from OpenAI on their PostgreSQL scaling practices. These topics are actually quite interesting for core developers, as many of them don’t have a clear picture of how PostgreSQL is used in extreme scenarios, and these talks are very helpful.\nSince late 2017, I managed dozens of PostgreSQL clusters at Tantan, which was one of the largest and most complex PG deployments in the Chinese internet scene: dozens of PG clusters with around 2.5 million QPS. Back then, our largest core primary had a 1-primary-33-replica setup, with a single cluster handling around 400K QPS. The bottleneck was also on single-database writes, which we eventually solved with application-side sharding, similar to Instagram’s approach.\nYou could say I’ve encountered all the problems and used all the solutions OpenAI mentioned in their talk. Of course, the difference is that today’s top-tier hardware is orders of magnitude better than it was eight years ago. This allows a startup like OpenAI to serve its entire business with a single PostgreSQL cluster without sharding. This is undoubtedly another powerful piece of evidence for the argument that “Distributed Databases Are a False Need”.\nDuring the Q\u0026A, I learned that OpenAI uses managed PostgreSQL on Azure with the highest available server hardware specs. They have dozens of replicas, including some in different geographical regions, and this behemoth cluster handles a total of about millions QPS. They use Datadog for monitoring, and the services access the RDS cluster from Kubernetes through a business-side PgBouncer connection pool.\nAs a strategic customer, the Azure PostgreSQL team provides them with dedicated support. But it’s clear that even with top-tier cloud database services, the customer needs to have sufficient knowledge and skill on the application and operations side. Even with the brainpower of OpenAI, they still stumble on some of the practical driving lessons of PostgreSQL.\nDuring the social event after the conference, I had a great chat with Bohan and two other database founders until the wee hours. The off-the-record discussions were fascinating, but I can’t disclose more here, haha.\nVonng’s Q\u0026A Regarding the questions and feature requests Bohan raised, I can offer some answers here.\nMost of the features OpenAI wants already exist in the PostgreSQL ecosystem, they just might not be available in the vanilla PG kernel or in a managed cloud database environment.\nOn Disabling Indexes PostgreSQL actually has a “feature” to disable indexes. You just need to update the indisvalid field in the pg_index system catalog to false. The planner will then stop using the index, but it will continue to be maintained during DML operations. In principle, there’s nothing wrong with this, as concurrent index creation uses these two flags (isready, isvalid). It’s not black magic.\nHowever, I can understand why OpenAI can’t use this method: it’s an undocumented “internal detail” rather than a formal feature. But more importantly, cloud databases usually don’t grant superuser privileges, so you just can’t update the system catalog like this.\nBut back to the original need — fear of accidentally deleting an index. There’s a simpler solution: just confirm from monitoring view (pg_stat_all_indexes) that the index isn’t being used on either the primary or the replicas. If you know an index hasn’t been used for a long time, you can safely delete it.\nMonitoring index switch with Pigsty PGSQL TABLES Dashboard\n-- Create a new index CREATE UNIQUE INDEX CONCURRENTLY pgbench_accounts_pkey2 ON pgbench_accounts USING BTREE(aid); -- Mark the original index as invalid (not used), but still maintained. planner will not use it. UPDATE pg_index SET indisvalid = false WHERE indexrelid = 'pgbench_accounts_pkey'::regclass; On Observability Actually, pg_stat_statements provides the mean and stddev metrics, which you can use with properties of the normal distribution to estimate percentile metrics. But this is only a rough estimate, and you need to reset the counters periodically, otherwise the effectiveness of the full historical statistics will degrade over time.\nRT Distribution with PGSQL QUERY Dashboard from PGSS\nPGSS is unlikely to provide P95, P99 RT percentile metrics anytime soon, because it would increase the extension’s memory footprint by several dozen times. While that’s not a big deal for modern servers, it could be an issue in extremely conservative environments. I asked the maintainer of PGSS about this at the Unconference, and it’s unlikely to happen in the short term. I also asked Jelte, the maintainer of Pgbouncer, if this could be solved at the connection pool level, and a feature like that is not coming soon either.\nHowever, there are other solutions to this problem. First, the pg_stat_monitor extension explicitly provides detailed percentile RT metrics, but you have to consider the performance impact of collecting these metrics on the cluster. A universal, non-intrusive method with no database performance overhead is to add query RT monitoring directly at the application’s Data Access Layer (DAL), but this requires cooperation and effort from the application side.\nAlso, using eBPF for side-channel collection of RT metrics is a great idea, but considering they’re using managed PostgreSQL on Azure, they won’t have server access, so that path is likely blocked.\nOn Schema Change History Actually, PostgreSQL’s logging already provides this option. You just need to set log_statement to ddl (or the more advanced mod or all), and all DDL logs will be preserved. The pgaudit extension also provides similar functionality.\nBut I suspect what they really want isn’t DDL logs, but something like a system view that can be queried via SQL. In that case, another option is CREATE EVENT TRIGGER. You can use an event trigger to log DDL events directly into a data table. The pg_ddl_historization extension provides a more convenient way to do this, and I’ve compiled and packaged this extension as well.\nCreating an event trigger also requires superuser privileges. AWS RDS has some special handling to allow this, but it seems that PostgreSQL on Azure does not support it.\nOn Monitoring View Semantics In OpenAI’s example, pg_stat_activity.state = active means the backend process is still within the lifecycle of a single SQL statement. The WaitEvent = ClientRead means the process is on the CPU waiting for data from the client. When both appear together, a typical example is an idle COPY FROM STDIN, but it could also be TCP blocking or being stuck between BIND / EXECUTE. So it’s hard to say if it’s a bug without knowing what the connection is actually doing.\nSome might argue that waiting for client I/O should be considered “idle” from a CPU perspective. But state tracks the execution state of the statement itself, not whether the CPU is busy. state = 'active' means the PostgreSQL backend considers “this statement is not yet finished.” Resources like row locks, buffer pins, snapshots, and file handles are considered “in use.” This doesn’t mean it’s running on the CPU. When the process is running on the CPU in a loop waiting for client data, the wait event is ClientRead. When it yields the CPU and “waits” in the background, the wait event is NULL.\nBut back to the problem itself, there are other solutions. For example, in Pigsty, when accessing PostgreSQL through HAProxy, we set a connection timeout at the LB level for the primary service, defaulting to 24 hours. More stringent environments would have a shorter timeout, like 1 hour. This means any connection lasting over an hour would be terminated. Of course, this also needs to be configured with a corresponding max lifetime in the application-side connection pool, to proactively close connections rather than having them be cut off. For offline, read-only services, this parameter can be omitted to allow for ultra-long queries that might run for two or three days. This provides a safety net for these active-but-waiting-on-I/O situations.\nBut I also doubt whether Azure PostgreSQL offers this kind of control.\nOn Default Parameters PostgreSQL’s default parameters are quite conservative. For example, it defaults to using 128 MB of memory (the minimum can be set to 128 KB!). On the bright side, this allows its default configuration to run in almost any environment. On the downside, I’ve actually seen a case of a production system with 1TB of physical memory running with the 128 MB default… (thanks to double buffering, it actually ran for a long time).\nBut overall, I think conservative defaults aren’t a bad thing. This issue can be solved in a more flexible, dynamic configuration process. RDS and Pigsty both provide pretty good initial parameter heuristic config rules, which fully address this problem. But this feature could indeed be added to the PG command-line tools, for example, having initdb automatically detect CPU/memory count, disk size, and storage type and set optimized parameter values accordingly.\nSelf-hosted PostgreSQL? The challenges OpenAI raised are not really from PostgreSQL itself, but from the additional limitations of managed cloud services. One solution is to use the IaaS layer and self-host a PostgreSQL cluster on instances with local NVMe SSD storage to bypass these restrictions.\nIn fact, my project Pigsty built for ourselves to solve PostgreSQL challenges at a similar scale. It scales well, having supported Tantan’s 25K vCPU PostgreSQL cluster and 2.5M QPS. It includes solutions for all the problems mentioned above, and even for many that OpenAI hasn’t encountered yet. And in a self-hosting manner, open-source, free, and ready to use out of the box.\nIf OpenAI is interested, I’d certainly be happy to provide some help. But I think when you’re in a phase of hyper-growth, fiddling with database infra is probably not a high-priority item. Fortunately, they still have excellent PostgreSQL DBAs who can continue to forge these paths.\nReferences [1] HackerNews OpenAI: Scaling Postgres to the Next Level: https://news.ycombinator.com/item?id=44071418#44072781\n[2] PostgreSQL is eating the database world: https://pigsty.io/pg/pg-eat-db-world\n[3] Chinese: Scaling Postgres to the Next Level at OpenAI https://pigsty.cc/db/openai-pg/\n[4] The part of PostgreSQL we hate the most: https://www.cs.cmu.edu/~pavlo/blog/2023/04/the-part-of-postgresql-we-hate-the-most.html\n[5] PGConf.Dev 2025: https://2025.pgconf.dev/schedule.html\n[6] Schedule: Scaling Postgres to the next level at OpenAI: https://www.pgevents.ca/events/pgconfdev2025/schedule/session/433-scaling-postgres-to-the-next-level-at-openai/\n[7] Bohan Zhang: https://www.linkedin.com/in/bohan-zhang-52b17714b\n[8] Ruohang Feng / Vonng: https://github.com/Vonng/\n[9] Pigsty: https://pigsty.io\n[10] Instagram’s Sharding IDs: https://instagram-engineering.com/sharding-ids-at-instagram-1cf5a71e5a5c\n[11] Reclaim hardware bouns: https://pigsty.io/cloud//bonus/\n[12] Distributed Databases Are a False Need: https://pigsty.io/db/distributive-bullshit/\n","categories":"","description":"","excerpt":"At PGConf.Dev 2025, Bohan Zhang from OpenAI shared a session titled …","ref":"/blog/db/openai-pg/","tags":["PostgreSQL","OpenAI"],"title":"Scaling Postgres to the Next Level at OpenAI"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/etcd/","tags":"","title":"Etcd"},{"body":"\nPlenty. If you’re rolling your own Kubernetes, odds are you’ll crash because etcd ships with a 2 GB time bomb. Read more\n","categories":"","description":"Plenty. If you’re rolling your own Kubernetes, odds are you’ll crash because etcd ships with a 2 GB time bomb.","excerpt":"Plenty. If you’re rolling your own Kubernetes, odds are you’ll crash …","ref":"/blog/db/bad-etcd/","tags":["Database","etcd"],"title":"How Many Shops Has etcd Torched?"},{"body":"\nPigsty crosses 4K GitHub stars, adds PG18 beta support, pushes extensions to 421, ships new doc site, and completes OrioleDB/OpenHalo full-platform support. Read more\n","categories":"","description":"Pigsty crosses 4K GitHub stars, adds PG18 beta support, pushes extensions to 421, ships new doc site, and completes OrioleDB/OpenHalo full-platform support.\n","excerpt":"Pigsty crosses 4K GitHub stars, adds PG18 beta support, pushes …","ref":"/blog/pigsty/v3.5/","tags":["Pigsty"],"title":"Pigsty v3.5: 4K Stars, PG18 Beta, 421 Extensions"},{"body":"\nFuture software = Agent + Database. No middle tiers, just agents issuing CRUD. Database skills age well, and PostgreSQL is poised to be the agent-era default. Read more\n","categories":"","description":"Future software = Agent + Database. No middle tiers, just agents issuing CRUD. Database skills age well, and PostgreSQL is poised to be the agent-era default.","excerpt":"Future software = Agent + Database. No middle tiers, just agents …","ref":"/blog/db/ai-agent-era/","tags":["Database","PostgreSQL","AI"],"title":"In the AI Era, Software Starts at the Database"},{"body":"\nA 2025 reality check on where PostgreSQL stands relative to MySQL across features, performance, quality, and ecosystem. Read more\n","categories":"","description":"A 2025 reality check on where PostgreSQL stands relative to MySQL across features, performance, quality, and ecosystem.","excerpt":"A 2025 reality check on where PostgreSQL stands relative to MySQL …","ref":"/blog/db/mysql-vs-pgsql/","tags":["Database","MySQL","PostgreSQL"],"title":"MySQL vs. PostgreSQL @ 2025"},{"body":"\nThe annual PostgreSQL developer conference will be held in Montreal in May. Like the first PG Con.Dev, there’s also an additional dedicated event - Postgres Extensions Day Read more\n","categories":"","description":"The annual PostgreSQL developer conference will be held in Montreal in May. Like the first PG Con.Dev, there's also an additional dedicated event - Postgres Extensions Day","excerpt":"The annual PostgreSQL developer conference will be held in Montreal in …","ref":"/blog/pg/pgext-day/","tags":["PostgreSQL","Extension"],"title":"Postgres Extension Day - See You There!"},{"body":"\nA PG kernel fork acquired by Supabase, claiming to solve PG’s XID wraparound problem, eliminate table bloat issues, improve performance by 4x, and support cloud-native storage. Now part of the Pigsty family. Read more\n","categories":"","description":"A PG kernel fork acquired by Supabase, claiming to solve PG's XID wraparound problem, eliminate table bloat issues, improve performance by 4x, and support cloud-native storage. Now part of the Pigsty family.","excerpt":"A PG kernel fork acquired by Supabase, claiming to solve PG's XID …","ref":"/blog/pg/orioledb-is-coming/","tags":["PostgreSQL"],"title":"OrioleDB is Coming! 4x Performance, Eliminates Pain Points, Storage-Compute Separation"},{"body":"\nWhat? PostgreSQL can now be accessed using MySQL clients? That’s right, openHalo, which was open-sourced on April Fool’s Day, provides exactly this capability and has now joined the Pigsty kernel family. Read more\n","categories":"","description":"What? PostgreSQL can now be accessed using MySQL clients? That's right, openHalo, which was open-sourced on April Fool's Day, provides exactly this capability and has now joined the Pigsty kernel family.","excerpt":"What? PostgreSQL can now be accessed using MySQL clients? That's …","ref":"/blog/pg/openhalo-mysql/","tags":["PostgreSQL","MySQL"],"title":"OpenHalo: MySQL Wire-Compatible PostgreSQL is Here!"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/juicefs/","tags":"","title":"JuiceFS"},{"body":"\nLeverage JuiceFS to turn PostgreSQL into a filesystem with PITR capabilities! Read more\n","categories":"","description":"Leverage JuiceFS to turn PostgreSQL into a filesystem with PITR capabilities!","excerpt":"Leverage JuiceFS to turn PostgreSQL into a filesystem with PITR …","ref":"/blog/pg/pgfs/","tags":["PostgreSQL","JuiceFS"],"title":"PGFS: Using Database as a Filesystem"},{"body":"\nPigsty v3.4 adds pgBackRest backup monitoring, cross-cluster PITR restore, automated HTTPS certificates, locale best practices, and full-platform IvorySQL and Apache AGE support. Read more\n","categories":"","description":"Pigsty v3.4 adds pgBackRest backup monitoring, cross-cluster PITR restore, automated HTTPS certificates, locale best practices, and full-platform IvorySQL and Apache AGE support.\n","excerpt":"Pigsty v3.4 adds pgBackRest backup monitoring, cross-cluster PITR …","ref":"/blog/pigsty/v3.4/","tags":["Pigsty"],"title":"Pigsty v3.4: PITR Enhancement, Locale Best Practices, Auto Certificates"},{"body":"\nIf you ask me, we’re on the brink of a cosmic collision in database-land, and Postgres + DuckDB is the meteor we should all be watching. Read more\n","categories":"","description":"If you ask me, we’re on the brink of a cosmic collision in database-land, and Postgres + DuckDB is the meteor we should all be watching.\n","excerpt":"If you ask me, we’re on the brink of a cosmic collision in …","ref":"/blog/db/pg-kiss-duckdb/","tags":["PostgreSQL","DuckDB","Database"],"title":"Database Planet Collision: When PG Falls for DuckDB"},{"body":"\nDoes bolting DuckDB onto RDS suddenly make open-source Postgres ‘trash’? Business and open source should be symbiotic. If a vendor only extracts without giving back, the community will spit it out.\" Read more\n","categories":"","description":"Does bolting DuckDB onto RDS suddenly make open-source Postgres ‘trash’? Business and open source should be symbiotic. If a vendor only extracts without giving back, the community will spit it out.\"\n","excerpt":"Does bolting DuckDB onto RDS suddenly make open-source Postgres …","ref":"/blog/cloud/rds-duckdb/","tags":["PostgreSQL","PG-Ecosystem"],"title":"Alicloud’s rds_duckdb: Tribute or Rip-Off?"},{"body":"\nThe PG community has started punching up: Cybertec’s Laurenz Albe breaks down how Oracle’s transaction system stacks against PostgreSQL. Read more\n","categories":"","description":"The PG community has started punching up: Cybertec’s Laurenz Albe breaks down how Oracle’s transaction system stacks against PostgreSQL.","excerpt":"The PG community has started punching up: Cybertec’s Laurenz Albe …","ref":"/blog/db/oracle-pg-xact/","tags":["Database","PostgreSQL","Oracle"],"title":"Comparing Oracle and PostgreSQL Transaction Systems"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/oracle/","tags":"","title":"Oracle"},{"body":"\nPigsty v3.3 pushes available extensions to 404, adds turnkey app deployment with app.yml, delivers Certbot integration for automated HTTPS, and launches a redesigned website. Read more\n","categories":"","description":"Pigsty v3.3 pushes available extensions to 404, adds turnkey app deployment with app.yml, delivers Certbot integration for automated HTTPS, and launches a redesigned website.\n","excerpt":"Pigsty v3.3 pushes available extensions to 404, adds turnkey app …","ref":"/blog/pigsty/v3.3/","tags":["Pigsty"],"title":"Pigsty v3.3: 404 Extensions, Turnkey Apps, New Website"},{"body":"\nSharing some interesting recent developments in the PG ecosystem. Read more\n","categories":"","description":"Sharing some interesting recent developments in the PG ecosystem.","excerpt":"Sharing some interesting recent developments in the PG ecosystem.","ref":"/blog/pg/pg-frontier/","tags":["PostgreSQL","PG-Ecosystem"],"title":"PostgreSQL Ecosystem Frontier Developments"},{"body":"\nDatabases are the core of business architecture, but what happens if we go further and let databases become the business architecture itself? Read more\n","categories":"","description":"Databases are the core of business architecture, but what happens if we go further and let databases become the business architecture itself?\n","excerpt":"Databases are the core of business architecture, but what happens if …","ref":"/blog/db/db-is-the-arch/","tags":["PostgreSQL"],"title":"Database as Business Architecture"},{"body":"\nA user consulted about distributed databases, but he wasn’t dealing with data bursting through server cabinet doors—rather, he’d fallen into another cloud computing pig-butchering scam. Read more\n","categories":"","description":"A user consulted about distributed databases, but he wasn't dealing with data bursting through server cabinet doors—rather, he'd fallen into another cloud computing pig-butchering scam.\n","excerpt":"A user consulted about distributed databases, but he wasn't dealing …","ref":"/blog/cloud/patsy/","tags":["Cloud-Exit"],"title":"Escaping Cloud Computing Scam Mills: The Big Fool Paying for Pain"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/concept/","tags":"","title":"Concept"},{"body":"","categories":"","description":"","excerpt":"","ref":"/module/","tags":"","title":"Module"},{"body":"","categories":"","description":"","excerpt":"","ref":"/module/pgsql/","tags":"","title":"PGSQL"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/task/","tags":"","title":"Task"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/tutorial/","tags":"","title":"Tutorial"},{"body":"\nWhy would we need yet another package manager for PostgreSQL \u0026 extensions? Read more\n","categories":"","description":"Why would we need yet another package manager for PostgreSQL \u0026 extensions?","excerpt":"Why would we need yet another package manager for PostgreSQL \u0026 …","ref":"/blog/pg/pig/","tags":["PostgreSQL","Tool"],"title":"Pig, The Postgres Extension Wizard"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/tool/","tags":"","title":"Tool"},{"body":"\nPigsty v3.2 introduces the pig CLI for PostgreSQL package management, complete ARM64 extension repository support, and Supabase \u0026 Grafana enhancements. Read more\n","categories":"","description":"Pigsty v3.2 introduces the pig CLI for PostgreSQL package management, complete ARM64 extension repository support, and Supabase \u0026 Grafana enhancements.\n","excerpt":"Pigsty v3.2 introduces the pig CLI for PostgreSQL package management, …","ref":"/blog/pigsty/v3.2/","tags":["Pigsty"],"title":"Pigsty v3.2: The pig CLI, Full ARM Support, Supabase \u0026 Grafana Enhancements"},{"body":"\nEven trillion-dollar unicorns can be a house of cards when operating outside their core expertise. Read more\n","categories":"","description":"Even trillion-dollar unicorns can be a house of cards when operating outside their core expertise.\n","excerpt":"Even trillion-dollar unicorns can be a house of cards when operating …","ref":"/blog/cloud/openai-failure/","tags":["Cloud-Outage"],"title":"OpenAI Global Outage Postmortem: K8S Circular Dependencies"},{"body":"\nIs PostgreSQL the king of boring databases? Here are seven databases worth studying in 2025: PostgreSQL, SQLite, DuckDB, ClickHouse, FoundationDB, TigerBeetle, and CockroachDB—each deserving a week of deep exploration. Read more\n","categories":"","description":"Is PostgreSQL the king of boring databases? Here are seven databases worth studying in 2025: PostgreSQL, SQLite, DuckDB, ClickHouse, FoundationDB, TigerBeetle, and CockroachDB—each deserving a week of deep exploration.\n","excerpt":"Is PostgreSQL the king of boring databases? Here are seven databases …","ref":"/blog/db/7-week-7-db/","tags":["Database","PostgreSQL","SQLite","DuckDB","ClickHouse"],"title":"7 Databases in 7 Weeks (2025)"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/clickhouse/","tags":"","title":"ClickHouse"},{"body":"\nAn interesting but tricky puzzle: solving the 24 game with SQL. The PostgreSQL solution. Read more\n","categories":"","description":"An interesting but tricky puzzle: solving the 24 game with SQL. The PostgreSQL solution.\n","excerpt":"An interesting but tricky puzzle: solving the 24 game with SQL. The …","ref":"/blog/db/poker-24/","tags":["Database","PostgreSQL","MySQL"],"title":"Solving Poker 24 with a Single SQL Query"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/sqlite/","tags":"","title":"SQLite"},{"body":"\nSupabase is great, owning your own Supabase is even better! This tutorial covers how to self-host production-grade Supabase on local/cloud VMs using Pigsty. Read more\n","categories":"","description":"Supabase is great, owning your own Supabase is even better! A comprehensive tutorial for self-hosting production-grade Supabase on local/cloud VMs with Pigsty.\n","excerpt":"Supabase is great, owning your own Supabase is even better! A …","ref":"/blog/select/supabase/","tags":["PostgreSQL","Supabase"],"title":"Self-Hosting Supabase on PostgreSQL"},{"body":"Supabase is great, own your own Supabase is even better. Here’s a comprehensive tutorial for self-hosting production-grade supabase on local/cloud VM/BMs.\nWhat is Supabase? Supabase is an open-source Firebase alternative, a Backend as a Service (BaaS).\nSupabase wraps PostgreSQL kernel and vector extensions, alone with authentication, realtime subscriptions, edge functions, object storage, and instant REST and GraphQL APIs from your postgres schema. It let you skip most backend work, requiring only database design and frontend skills to ship quickly.\nCurrently, Supabase may be the most popular open-source project in the PostgreSQL ecosystem, boasting over 74,000 stars on GitHub. And become quite popular among developers, and startups, since they have a generous free plan, just like cloudflare \u0026 neon.\nWhy Self-Hosting? Supabase’s slogan is: “Build in a weekend, Scale to millions”. It has great cost-effectiveness in small scales (4c8g) indeed. But there is no doubt that when you really grow to millions of users, some may choose to self-hosting their own Supabase —— for functionality, performance, cost, and other reasons.\nThat’s where Pigsty comes in. Pigsty provides a complete one-click self-hosting solution for Supabase. Self-hosted Supabase can enjoy full PostgreSQL monitoring, IaC, PITR, and high availability capability,\nYou can run the latest PostgreSQL 17(,16,15,14) kernels, (supabase is using the 15 currently), alone with 390 PostgreSQL extensions out-of-the-box. Run on mainstream Linus OS distros with production grade HA PostgreSQL, MinIO, Prometheus \u0026 Grafana Stack for observability, and Nginx for reverse proxy.\nTIME: timescaledb timescaledb_toolkit timeseries periods temporal_tables emaj table_version pg_cron pg_later pg_background GIS: postgis postgis_topology postgis_raster postgis_sfcgal postgis_tiger_geocoder address_standardizer address_standardizer_data_us pgrouting pointcloud pointcloud_postgis h3 h3_postgis q3c ogr_fdw geoip pg_polyline pg_geohash mobilitydb earthdistance RAG: vector vectorscale vectorize pg_similarity smlar pg_summarize pg_tiktoken pgml pg4ml FTS: pg_search pg_bigm zhparser hunspell_cs_cz hunspell_de_de hunspell_en_us hunspell_fr hunspell_ne_np hunspell_nl_nl hunspell_nn_no hunspell_pt_pt hunspell_ru_ru hunspell_ru_ru_aot fuzzystrmatch pg_trgm OLAP: citus citus_columnar columnar pg_analytics pg_duckdb pg_mooncake duckdb_fdw pg_parquet pg_fkpart pg_partman plproxy pg_strom tablefunc FEAT: age hll rum pg_graphql pg_jsonschema jsquery pg_hint_plan hypopg index_advisor plan_filter imgsmlr pg_ivm pgmq pgq pg_cardano rdkit bloom LANG: pg_tle plv8 pllua hstore_pllua plluau hstore_plluau plprql pldbgapi plpgsql_check plprofiler plsh pljava plr pgtap faker dbt2 pltcl pltclu plperl bool_plperl hstore_plperl jsonb_plperl plperlu bool_plperlu jsonb_plperlu hstore_plperlu plpgsql plpython3u jsonb_plpython3u ltree_plpython3u hstore_plpython3u TYPE: prefix semver unit md5hash asn1oid roaringbitmap pgfaceting pg_sphere country currency pgmp numeral pg_rational uint uint128 ip4r uri pgemailaddr acl debversion pg_rrule timestamp9 chkpass isn seg cube ltree hstore citext xml2 FUNC: topn gzip zstd http pg_net pg_smtp_client pg_html5_email_address pgsql_tweaks pg_extra_time timeit count_distinct extra_window_functions first_last_agg tdigest aggs_for_vecs aggs_for_arrays arraymath quantile lower_quantile pg_idkit pg_uuidv7 permuteseq pg_hashids sequential_uuids pg_math random base36 base62 pg_base58 floatvec financial pgjwt pg_hashlib shacrypt cryptint pguecc pgpcre icu_ext pgqr envvar pg_protobuf url_encode refint autoinc insert_username moddatetime tsm_system_time dict_xsyn tsm_system_rows tcn uuid-ossp btree_gist btree_gin intarray intagg dict_int unaccent ADMIN: pg_repack pg_squeeze pg_dirtyread pgfincore pgdd ddlx prioritize pg_checksums pg_readonly safeupdate pg_permissions pgautofailover pg_catcheck pre_prepare pgcozy pg_orphaned pg_crash pg_cheat_funcs pg_savior table_log pg_fio pgpool_adm pgpool_recovery pgpool_regclass pgagent vacuumlo pg_prewarm oid2name lo basic_archive basebackup_to_shell old_snapshot adminpack amcheck pg_surgery STAT: pg_profile pg_show_plans pg_stat_kcache pg_stat_monitor pg_qualstats pg_store_plans pg_track_settings pg_wait_sampling system_stats meta pgnodemx pg_proctab pg_sqlog bgw_replstatus pgmeminfo toastinfo explain_ui pg_relusage pg_top pagevis powa pageinspect pgrowlocks sslinfo pg_buffercache pg_walinspect pg_freespacemap pg_visibility pgstattuple auto_explain pg_stat_statements SEC: passwordcheck_cracklib supautils pgsodium supabase_vault pg_session_jwt anon pg_tde pgsmcrypto pgaudit pgauditlogtofile pg_auth_mon credcheck pgcryptokey pg_jobmon logerrors login_hook set_user pg_snakeoil pgextwlist pg_auditor sslutils noset sepgsql auth_delay pgcrypto passwordcheck FDW: wrappers multicorn odbc_fdw jdbc_fdw mysql_fdw oracle_fdw tds_fdw db2_fdw sqlite_fdw pgbouncer_fdw mongo_fdw redis_fdw redis kafka_fdw hdfs_fdw firebird_fdw aws_s3 log_fdw dblink file_fdw postgres_fdw SIM: orafce pgtt session_variable pg_statement_rollback pg_dbms_metadata pg_dbms_lock pg_dbms_job babelfishpg_common babelfishpg_tsql babelfishpg_tds babelfishpg_money pgmemcache ETL: pglogical pglogical_origin pglogical_ticker pgl_ddl_deploy pg_failover_slots wal2json wal2mongo decoderbufs decoder_raw test_decoding mimeo repmgr pg_fact_loader pg_bulkload\nSince most of the supabase maintained extensions are not available in the official PGDG repo, we have compiled all the RPM/DEBs for these extensions and put them in the Pigsty repo: pg_graphql, pg_jsonschema, wrappers, index_advisor, pg_net, vault, pgjwt, supautils, pg_plan_filter,\nEverything is under your control, you have the ability and freedom to scale PGSQL, MinIO, and Supabase itself. And take full advantage of the performance and cost advantages of modern hardware like Gen5 NVMe SSD.\nAll you need is prepare a VM with several commands and wait for 10 minutes….\nGet Started First, download \u0026 install pigsty as usual, with the supa config template:\ncurl -fsSL https://repo.pigsty.io/get | bash ./bootstrap # install deps (ansible) ./configure -c supa # use supa config template (IMPORTANT: CHANGE PASSWORDS!) ./deploy.yml # install pigsty, create ha postgres \u0026 minio clusters Please change the pigsty.yml config file according to your need before deploying Supabase. (Credentials) For dev/test/demo purposes, we will just skip that, and comes back later.\nThen, run the supabase.yml to launch stateless part of supabase.\n./supabase.yml # launch stateless supabase containers with docker compose You can access the supabase API / Web UI through the 8000/8443 directly.\nwith configured DNS, or a local /etc/hosts entry, you can also use the default supa.pigsty domain name via the 80/443 infra portal.\nCredentials for Supabase Studio: supabase : pigsty\nArchitecture Pigsty’s supabase is based on the Supabase Docker Compose Template, with some slight modifications to fit-in Pigsty’s default ACL model.\nThe stateful part of this template is replaced by Pigsty’s managed PostgreSQL cluster and MinIO cluster. The container part are stateless, so you can launch / destroy / run multiple supabase containers on the same stateful PGSQL / MINIO cluster simultaneously to scale out.\nThe built-in supa.yml config template will create a single-node supabase, with a singleton PostgreSQL and SNSD MinIO server. You can use Multinode PostgreSQL Clusters and MNMD MinIO Clusters / external S3 service instead in production, we will cover that later.\nConfig Detail Here are checklists for self-hosting\nHardware: necessary VM/BM resources, one node at least, 3-4 are recommended for HA. Linux OS: Linux x86_64 server with fresh installed Linux, check compatible distro Network: Static IPv4 address which can be used as node identity Admin User: nopass ssh \u0026 sudo are recommended for admin user Conf Template: Use the supa config template, if you don’t know how to manually configure pigsty The built-in supa.yml config template is shown below.\nThe supa Config Template all: children: # infra cluster for proxy, monitor, alert, etc.. infra: { hosts: { 10.10.10.10: { infra_seq: 1 } } } # etcd cluster for ha postgres etcd: { hosts: { 10.10.10.10: { etcd_seq: 1 } }, vars: { etcd_cluster: etcd } } # minio cluster, s3 compatible object storage minio: { hosts: { 10.10.10.10: { minio_seq: 1 } }, vars: { minio_cluster: minio } } # pg-meta, the underlying postgres database for supabase pg-meta: hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } } vars: pg_cluster: pg-meta pg_users: # supabase roles: anon, authenticated, dashboard_user - { name: anon ,login: false } - { name: authenticated ,login: false } - { name: dashboard_user ,login: false ,replication: true ,createdb: true ,createrole: true } - { name: service_role ,login: false ,bypassrls: true } # supabase users: please use the same password - { name: supabase_admin ,password: 'DBUser.Supa' ,pgbouncer: true ,inherit: true ,roles: [ dbrole_admin ] ,superuser: true ,replication: true ,createdb: true ,createrole: true ,bypassrls: true } - { name: authenticator ,password: 'DBUser.Supa' ,pgbouncer: true ,inherit: false ,roles: [ dbrole_admin, authenticated ,anon ,service_role ] } - { name: supabase_auth_admin ,password: 'DBUser.Supa' ,pgbouncer: true ,inherit: false ,roles: [ dbrole_admin ] ,createrole: true } - { name: supabase_storage_admin ,password: 'DBUser.Supa' ,pgbouncer: true ,inherit: false ,roles: [ dbrole_admin, authenticated ,anon ,service_role ] ,createrole: true } - { name: supabase_functions_admin ,password: 'DBUser.Supa' ,pgbouncer: true ,inherit: false ,roles: [ dbrole_admin ] ,createrole: true } - { name: supabase_replication_admin ,password: 'DBUser.Supa' ,replication: true ,roles: [ dbrole_admin ]} - { name: supabase_read_only_user ,password: 'DBUser.Supa' ,bypassrls: true ,roles: [ dbrole_readonly, pg_read_all_data ] } pg_databases: - name: postgres baseline: supabase.sql owner: supabase_admin comment: supabase postgres database schemas: [ extensions ,auth ,realtime ,storage ,graphql_public ,supabase_functions ,_analytics ,_realtime ] extensions: - { name: pgcrypto ,schema: extensions } # 1.3 : cryptographic functions - { name: pg_net ,schema: extensions } # 0.9.2 : async HTTP - { name: pgjwt ,schema: extensions } # 0.2.0 : json web token API for postgres - { name: uuid-ossp ,schema: extensions } # 1.1 : generate universally unique identifiers (UUIDs) - { name: pgsodium } # 3.1.9 : pgsodium is a modern cryptography library for Postgres. - { name: supabase_vault } # 0.2.8 : Supabase Vault Extension - { name: pg_graphql } # 1.5.9 : pg_graphql: GraphQL support - { name: pg_jsonschema } # 0.3.3 : pg_jsonschema: Validate json schema - { name: wrappers } # 0.4.3 : wrappers: FDW collections - { name: http } # 1.6 : http: allows web page retrieval inside the database. - { name: pg_cron } # 1.6 : pg_cron: Job scheduler for PostgreSQL - { name: timescaledb } # 2.17 : timescaledb: Enables scalable inserts and complex queries for time-series data - { name: pg_tle } # 1.2 : pg_tle: Trusted Language Extensions for PostgreSQL - { name: vector } # 0.8.0 : pgvector: the vector similarity search # supabase required extensions pg_libs: 'pg_stat_statements, plpgsql, plpgsql_check, pg_cron, pg_net, timescaledb, auto_explain, pg_tle, plan_filter' pg_extensions: # extensions to be installed on this cluster - supabase # essential extensions for supabase - timescaledb postgis pg_graphql pg_jsonschema wrappers pg_search pg_analytics pg_parquet plv8 duckdb_fdw pg_cron pg_timetable pgqr - supautils pg_plan_filter passwordcheck plpgsql_check pgaudit pgsodium pg_vault pgjwt pg_ecdsa pg_session_jwt index_advisor - pgvector pgvectorscale pg_summarize pg_tiktoken pg_tle pg_stat_monitor hypopg pg_hint_plan pg_http pg_net pg_smtp_client pg_idkit pg_parameters: cron.database_name: postgres pgsodium.enable_event_trigger: off pg_hba_rules: # supabase hba rules, require access from docker network - { user: all ,db: postgres ,addr: intra ,auth: pwd ,title: 'allow supabase access from intranet' } - { user: all ,db: postgres ,addr: 172.17.0.0/16 ,auth: pwd ,title: 'allow access from local docker network' } node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] # make a full backup every 1am # launch supabase stateless part with docker compose: ./supabase.yml supabase: hosts: 10.10.10.10: { supa_seq: 1 } # instance id vars: supa_cluster: supa # cluster name docker_enabled: true # enable docker # use these to pull docker images via proxy and mirror registries #docker_registry_mirrors: ['https://docker.xxxxx.io'] #proxy_env: # add [OPTIONAL] proxy env to /etc/docker/daemon.json configuration file # no_proxy: \"localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,*.pigsty,*.aliyun.com,mirrors.*,*.myqcloud.com,*.tsinghua.edu.cn\" # #all_proxy: http://user:pass@host:port # these configuration entries will OVERWRITE or APPEND to /opt/supabase/.env file (src template: app/supabase/.env) # check https://github.com/pgsty/pigsty/blob/main/app/supabase/.env for default values supa_config: # IMPORTANT: CHANGE JWT_SECRET AND REGENERATE CREDENTIAL ACCORDING!!!!!!!!!!! # https://supabase.com/docs/guides/self-hosting/docker#securing-your-services jwt_secret: your-super-secret-jwt-token-with-at-least-32-characters-long anon_key: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJhbm9uIiwKICAgICJpc3MiOiAic3VwYWJhc2UtZGVtbyIsCiAgICAiaWF0IjogMTY0MTc2OTIwMCwKICAgICJleHAiOiAxNzk5NTM1NjAwCn0.dc_X5iR_VP_qT0zsiyj_I_OZ2T9FtRU2BBNWN8Bu4GE service_role_key: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJzZXJ2aWNlX3JvbGUiLAogICAgImlzcyI6ICJzdXBhYmFzZS1kZW1vIiwKICAgICJpYXQiOiAxNjQxNzY5MjAwLAogICAgImV4cCI6IDE3OTk1MzU2MDAKfQ.DaYlNEoUrrEn2Ig7tqibS-PHK5vgusbcbo7X36XVt4Q dashboard_username: supabase dashboard_password: pigsty # postgres connection string (use the correct ip and port) postgres_host: 10.10.10.10 postgres_port: 5436 # access via the 'default' service, which always route to the primary postgres postgres_db: postgres postgres_password: DBUser.Supa # password for supabase_admin and multiple supabase users # expose supabase via domain name site_url: http://supa.pigsty api_external_url: http://supa.pigsty supabase_public_url: http://supa.pigsty # if using s3/minio as file storage s3_bucket: supa s3_endpoint: https://sss.pigsty:9000 s3_access_key: supabase s3_secret_key: S3User.Supabase s3_force_path_style: true s3_protocol: https s3_region: stub minio_domain_ip: 10.10.10.10 # sss.pigsty domain name will resolve to this ip statically # if using SMTP (optional) #smtp_admin_email: admin@example.com #smtp_host: supabase-mail #smtp_port: 2500 #smtp_user: fake_mail_user #smtp_pass: fake_mail_password #smtp_sender_name: fake_sender #enable_anonymous_users: false vars: version: v3.1.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: default # upstream mirror region: default|china|europe node_tune: oltp # node tuning specs: oltp,olap,tiny,crit pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml infra_portal: # domain names and upstream servers home : { domain: h.pigsty } grafana : { domain: g.pigsty ,endpoint: \"${admin_ip}:3000\" , websocket: true } prometheus : { domain: p.pigsty ,endpoint: \"${admin_ip}:9090\" } alertmanager : { domain: a.pigsty ,endpoint: \"${admin_ip}:9093\" } minio : { domain: m.pigsty ,endpoint: \"10.10.10.10:9001\", https: true, websocket: true } blackbox : { endpoint: \"${admin_ip}:9115\" } loki : { endpoint: \"${admin_ip}:3100\" } # expose supa studio UI and API via nginx supa : { domain: supa.pigsty ,endpoint: \"10.10.10.10:8000\", websocket: true } #----------------------------------# # Credential: CHANGE THESE PASSWORDS #----------------------------------# #grafana_admin_username: admin grafana_admin_password: pigsty #pg_admin_username: dbuser_dba pg_admin_password: DBUser.DBA #pg_monitor_username: dbuser_monitor pg_monitor_password: DBUser.Monitor #pg_replication_username: replicator pg_replication_password: DBUser.Replicator #patroni_username: postgres patroni_password: Patroni.API #haproxy_admin_username: admin haproxy_admin_password: pigsty # use minio as supabase file storage, single node single driver mode for demonstration purpose minio_access_key: minioadmin # root access key, `minioadmin` by default minio_secret_key: minioadmin # root secret key, `minioadmin` by default minio_buckets: [ { name: pgsql }, { name: supa } ] minio_users: - { access_key: dba , secret_key: S3User.DBA, policy: consoleAdmin } - { access_key: pgbackrest , secret_key: S3User.Backup, policy: readwrite } - { access_key: supabase , secret_key: S3User.Supabase, policy: readwrite } minio_endpoint: https://sss.pigsty:9000 # explicit overwrite minio endpoint with haproxy port node_etc_hosts: [\"10.10.10.10 sss.pigsty\"] # domain name to access minio from all nodes (required) # use minio as default backup repo for PostgreSQL pgbackrest_method: minio # pgbackrest repo method: local,minio,[user-defined...] pgbackrest_repo: # pgbackrest repo: https://pgbackrest.org/configuration.html#section-repository local: # default pgbackrest repo with local posix fs path: /pg/backup # local backup directory, `/pg/backup` by default retention_full_type: count # retention full backups by count retention_full: 2 # keep 2, at most 3 full backup when using local fs repo minio: # optional minio repo for pgbackrest type: s3 # minio is s3-compatible, so s3 is used s3_endpoint: sss.pigsty # minio endpoint domain name, `sss.pigsty` by default s3_region: us-east-1 # minio region, us-east-1 by default, useless for minio s3_bucket: pgsql # minio bucket name, `pgsql` by default s3_key: pgbackrest # minio user access key for pgbackrest s3_key_secret: S3User.Backup # minio user secret key for pgbackrest s3_uri_style: path # use path style uri for minio rather than host style path: /pgbackrest # minio backup path, default is `/pgbackrest` storage_port: 9000 # minio port, 9000 by default storage_ca_file: /pg/cert/ca.crt # minio ca file path, `/pg/cert/ca.crt` by default bundle: y # bundle small files into a single file cipher_type: aes-256-cbc # enable AES encryption for remote backup repo cipher_pass: pgBackRest # AES encryption password, default is 'pgBackRest' retention_full_type: time # retention full backup by time on minio repo retention_full: 14 # keep full backup for last 14 days # download docker and supabase related extensions pg_version: 17 repo_modules: node,pgsql,infra,docker repo_packages: [node-bootstrap, infra-package, infra-addons, node-package1, node-package2, pgsql-utility, docker ] repo_extra_packages: - pgsql-main - supabase # essential extensions for supabase - timescaledb postgis pg_graphql pg_jsonschema wrappers pg_search pg_analytics pg_parquet plv8 duckdb_fdw pg_cron pg_timetable pgqr - supautils pg_plan_filter passwordcheck plpgsql_check pgaudit pgsodium pg_vault pgjwt pg_ecdsa pg_session_jwt index_advisor - pgvector pgvectorscale pg_summarize pg_tiktoken pg_tle pg_stat_monitor hypopg pg_hint_plan pg_http pg_net pg_smtp_client pg_idkit For advanced topics, we may need to modify the configuration file to fit our needs.\nSecurity Enhancement Domain Name and HTTPS Sending Mail with SMTP MinIO or External S3 True High Availability Security Enhancement For security reasons, you should change the default passwords in the pigsty.yml config file.\ngrafana_admin_password: pigsty, Grafana admin password pg_admin_password: DBUser.DBA, PGSQL superuser password pg_monitor_password: DBUser.Monitor, PGSQL monitor user password pg_replication_password: DBUser.Replicator, PGSQL replication user password patroni_password: Patroni.API, Patroni HA Agent Password haproxy_admin_password: pigsty, Load balancer admin password minio_access_key: minioadmin, MinIO root username minio_secret_key: minioadmin, MinIO root password Supabase will use PostgreSQL \u0026 MinIO as its backend, so also change the following passwords for supabase business users:\npg_users: password for supabase business users in postgres minio_users: minioadmin, MinIO business user’s password The pgbackrest will take backups and WALs to MinIO, so also change the following passwords reference\npgbackrest_repo: refer to the PLEASE check the Supabase Self-Hosting: Generate API Keys to generate supabase credentials:\njwt_secret: a secret key with at least 40 characters anon_key: a jwt token generate for anonymous users, based on jwt_secret service_role_key: a jwt token generate for elevated service roles, based on jwt_secret dashboard_username: supabase studio web portal username, supabase by default dashboard_password: supabase studio web portal password, pigsty by default If you have chanaged the default password for PostgreSQL and MinIO, you have to update the following parameters as well:\npostgres_password, according to pg_users s3_access_key and s3_secret_key, according to minio_users Domain Name and HTTPS For local or intranet use, you can connect directly to Kong port on http://\u003cIP\u003e:8000 or 8443 for https. This works but isn’t ideal. Using a domain with HTTPS is strongly recommended when serving Supabase to the public.\nPigsty has a Nginx server installed \u0026 configured on the admin node to act as a reverse proxy for all web based service. which is configured via the infra_portal parameter.\nall: vars: # global vars #..... infra_portal: # domain names and upstream servers home : { domain: h.pigsty } grafana : { domain: g.pigsty ,endpoint: \"${admin_ip}:3000\" , websocket: true } prometheus : { domain: p.pigsty ,endpoint: \"${admin_ip}:9090\" } alertmanager : { domain: a.pigsty ,endpoint: \"${admin_ip}:9093\" } minio : { domain: m.pigsty ,endpoint: \"10.10.10.10:9001\", https: true, websocket: true } blackbox : { endpoint: \"${admin_ip}:9115\" } loki : { endpoint: \"${admin_ip}:3100\" } # expose supa studio UI and API via nginx supa : { domain: supa.pigsty ,endpoint: \"10.10.10.10:8000\", websocket: true } On the client side, you can use the domain supa.pigsty to access the Supabase Studio management interface. You can add this domain to your local /etc/hosts file or use a local DNS server to resolve it to the server’s external IP address.\nTo use a real domain with HTTPS, you will need to modify the all.vars.infra_portal.supa with updated domain name (such as supa.pigsty.cc here). You can obtain a free HTTPS certificate from Let’s Encrypt, and just put the cert/key files in the specified path.\n#supa : { domain: supa.pigsty ,endpoint: \"10.10.10.10:8000\", websocket: true } # add your HTTPS certs/keys and specify the path supa : { domain: supa.pigsty.cc ,endpoint: \"10.10.10.10:8000\", websocket: true ,cert: /etc/cert/suap.pigsty.cc.crt ,key: /etc/cert/supa.pigsty.cc.key } To reload the new configuration after installation, use the infra.yml playbook:\n./infra.yml -t nginx_config,nginx_launch # reload nginx config You also have to update the all.children.supabase.vars.supa_config to tell supabase to use the new domain name:\nall: children: # clusters supabase: # supabase group vars: # supabase param supa_config: # supabase config # update supabase domain names here site_url: http://supa.pigsty.cc api_external_url: http://supa.pigsty.cc supabase_public_url: http://supa.pigsty.cc And reload the supabase service to apply the new configuration:\n./supabase.yml -t supa_config,supa_launch # reload supabase config Sending Mail with SMTP Some Supabase features require email. For production use, I’d recommend using an external SMTP service. Since self-hosted SMTP servers often result in rejected or spam-flagged emails.\nTo do this, modify the Supabase configuration and add SMTP credentials:\nall: children: supabase: vars: supa_config: smtp_host: smtpdm.aliyun.com:80 smtp_port: 80 smtp_user: no_reply@mail.your.domain.com smtp_pass: your_email_user_password smtp_sender_name: MySupabase smtp_admin_email: adminxxx@mail.your.domain.com enable_anonymous_users: false And don’t forget to reload the supabase service with ./supabase.yml -t supa_config,supa_launch\nMinIO or External S3 Pigsty’s self-hosting supabase will use a local SNSD MinIO server, which is used by Supabase itself for object storage, and by PostgreSQL for backups. For production use, you should consider using a HA MNMD MinIO cluster or an external S3 compatible service instead.\nWe recommend using an external S3 when:\nyou just have one single server available, then external s3 gives you a minimal disaster recovery guarantee, with RTO in hours and RPO in MBs. you are operating in the cloud, then using S3 directly is recommended rather than wrap expensively EBS with MinIO The terraform/spec/aliyun-meta-s3.tf provides an example of how to provision a single node alone with an S3 bucket.\nTo use an external S3 compatible service, you’ll have to update two related references in the pigsty.yml config.\nFor example, to use Aliyun OSS as the object storage for Supabase, you can modify the all.children.supabase.vars.supa_config to point to the Aliyun OSS bucket:\nall: children: supabase: vars: supa_config: s3_bucket: pigsty-oss s3_endpoint: https://oss-cn-beijing-internal.aliyuncs.com s3_access_key: xxxxxxxxxxxxxxxx s3_secret_key: xxxxxxxxxxxxxxxx s3_force_path_style: false s3_protocol: https s3_region: oss-cn-beijing Reload the supabase service with ./supabase.yml -t supa_config,supa_launch again.\nThe next reference is in the PostgreSQL backup repo:\nall: vars: # use minio as default backup repo for PostgreSQL pgbackrest_method: minio # pgbackrest repo method: local,minio,[user-defined...] pgbackrest_repo: # pgbackrest repo: https://pgbackrest.org/configuration.html#section-repository local: # default pgbackrest repo with local posix fs path: /pg/backup # local backup directory, `/pg/backup` by default retention_full_type: count # retention full backups by count retention_full: 2 # keep 2, at most 3 full backup when using local fs repo minio: # optional minio repo for pgbackrest type: s3 # minio is s3-compatible, so s3 is used # update your credentials here s3_endpoint: oss-cn-beijing-internal.aliyuncs.com s3_region: oss-cn-beijing s3_bucket: pigsty-oss s3_key: xxxxxxxxxxxxxx s3_key_secret: xxxxxxxx s3_uri_style: host path: /pgbackrest # minio backup path, default is `/pgbackrest` storage_port: 9000 # minio port, 9000 by default storage_ca_file: /pg/cert/ca.crt # minio ca file path, `/pg/cert/ca.crt` by default bundle: y # bundle small files into a single file cipher_type: aes-256-cbc # enable AES encryption for remote backup repo cipher_pass: pgBackRest # AES encryption password, default is 'pgBackRest' retention_full_type: time # retention full backup by time on minio repo retention_full: 14 # keep full backup for last 14 days After updating the pgbackrest_repo, you can reset the pgBackrest backup with ./pgsql.yml -t pgbackrest.\nTrue High Availability The default single-node deployment (with external S3) provide a minimal disaster recovery guarantee, with RTO in hours and RPO in MBs.\nTo achieve RTO \u003c 30s and zero data loss, you need a multi-node high availability cluster with at least 3-nodes.\nWhich involves high availability for these components:\nETCD: DCS requires at least three nodes to tolerate one node failure. PGSQL: PGSQL synchronous commit mode recommends at least three nodes. INFRA: It’s good to have two or three copies of observability stack. Supabase itself can also have multiple replicas to achieve high availability. We recommend you to refer to the trio and safe config to upgrade your cluster to three nodes or more.\nIn this case, you also need to modify the access points for PostgreSQL and MinIO to use the DNS / L2 VIP / HAProxy HA access points.\nall: children: supabase: hosts: 10.10.10.10: { supa_seq: 1 } 10.10.10.11: { supa_seq: 2 } 10.10.10.12: { supa_seq: 3 } vars: supa_cluster: supa # cluster name supa_config: postgres_host: 10.10.10.2 # use the PG L2 VIP postgres_port: 5433 # use the 5433 port to access the primary instance through pgbouncer s3_endpoint: https://sss.pigsty:9002 # If you are using MinIO through the haproxy lb port 9002 minio_domain_ip: 10.10.10.3 # use the L2 VIP binds to all proxy nodes The 3-Node HA Supabase Config Template all: #==============================================================# # Clusters, Nodes, and Modules #==============================================================# children: # infra cluster for proxy, monitor, alert, etc.. infra: hosts: 10.10.10.10: { infra_seq: 1 ,nodename: infra-1 } 10.10.10.11: { infra_seq: 2 ,nodename: infra-2, repo_enabled: false, grafana_enabled: false } 10.10.10.12: { infra_seq: 3 ,nodename: infra-3, repo_enabled: false, grafana_enabled: false } vars: vip_enabled: true vip_vrid: 128 vip_address: 10.10.10.3 vip_interface: eth1 haproxy_services: - name: minio # [REQUIRED] service name, unique port: 9002 # [REQUIRED] service port, unique balance: leastconn # [OPTIONAL] load balancer algorithm options: # [OPTIONAL] minio health check - option httpchk - option http-keep-alive - http-check send meth OPTIONS uri /minio/health/live - http-check expect status 200 servers: - { name: minio-1 ,ip: 10.10.10.10 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-2 ,ip: 10.10.10.11 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } - { name: minio-3 ,ip: 10.10.10.12 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' } etcd: # dcs service for postgres/patroni ha consensus hosts: # 1 node for testing, 3 or 5 for production 10.10.10.10: { etcd_seq: 1 } # etcd_seq required 10.10.10.11: { etcd_seq: 2 } # assign from 1 ~ n 10.10.10.12: { etcd_seq: 3 } # odd number please vars: # cluster level parameter override roles/etcd etcd_cluster: etcd # mark etcd cluster name etcd etcd_safeguard: false # safeguard against purging etcd_clean: true # purge etcd during init process # minio cluster 4-node minio: hosts: 10.10.10.10: { minio_seq: 1 , nodename: minio-1 } 10.10.10.11: { minio_seq: 2 , nodename: minio-2 } 10.10.10.12: { minio_seq: 3 , nodename: minio-3 } vars: minio_cluster: minio minio_data: '/data{1...4}' minio_buckets: [ { name: pgsql }, { name: supa } ] minio_users: - { access_key: dba , secret_key: S3User.DBA, policy: consoleAdmin } - { access_key: pgbackrest , secret_key: S3User.Backup, policy: readwrite } - { access_key: supabase , secret_key: S3User.Supabase, policy: readwrite } # pg-meta, the underlying postgres database for supabase pg-meta: hosts: 10.10.10.10: { pg_seq: 1, pg_role: primary } 10.10.10.11: { pg_seq: 2, pg_role: replica } 10.10.10.12: { pg_seq: 3, pg_role: replica } vars: pg_cluster: pg-meta pg_users: # supabase roles: anon, authenticated, dashboard_user - { name: anon ,login: false } - { name: authenticated ,login: false } - { name: dashboard_user ,login: false ,replication: true ,createdb: true ,createrole: true } - { name: service_role ,login: false ,bypassrls: true } # supabase users: please use the same password - { name: supabase_admin ,password: 'DBUser.Supa' ,pgbouncer: true ,inherit: true ,roles: [ dbrole_admin ] ,superuser: true ,replication: true ,createdb: true ,createrole: true ,bypassrls: true } - { name: authenticator ,password: 'DBUser.Supa' ,pgbouncer: true ,inherit: false ,roles: [ dbrole_admin, authenticated ,anon ,service_role ] } - { name: supabase_auth_admin ,password: 'DBUser.Supa' ,pgbouncer: true ,inherit: false ,roles: [ dbrole_admin ] ,createrole: true } - { name: supabase_storage_admin ,password: 'DBUser.Supa' ,pgbouncer: true ,inherit: false ,roles: [ dbrole_admin, authenticated ,anon ,service_role ] ,createrole: true } - { name: supabase_functions_admin ,password: 'DBUser.Supa' ,pgbouncer: true ,inherit: false ,roles: [ dbrole_admin ] ,createrole: true } - { name: supabase_replication_admin ,password: 'DBUser.Supa' ,replication: true ,roles: [ dbrole_admin ]} - { name: supabase_read_only_user ,password: 'DBUser.Supa' ,bypassrls: true ,roles: [ dbrole_readonly, pg_read_all_data ] } pg_databases: - name: postgres baseline: supabase.sql owner: supabase_admin comment: supabase postgres database schemas: [ extensions ,auth ,realtime ,storage ,graphql_public ,supabase_functions ,_analytics ,_realtime ] extensions: - { name: pgcrypto ,schema: extensions } # 1.3 : cryptographic functions - { name: pg_net ,schema: extensions } # 0.9.2 : async HTTP - { name: pgjwt ,schema: extensions } # 0.2.0 : json web token API for postgres - { name: uuid-ossp ,schema: extensions } # 1.1 : generate universally unique identifiers (UUIDs) - { name: pgsodium } # 3.1.9 : pgsodium is a modern cryptography library for Postgres. - { name: supabase_vault } # 0.2.8 : Supabase Vault Extension - { name: pg_graphql } # 1.5.9 : pg_graphql: GraphQL support - { name: pg_jsonschema } # 0.3.3 : pg_jsonschema: Validate json schema - { name: wrappers } # 0.4.3 : wrappers: FDW collections - { name: http } # 1.6 : http: allows web page retrieval inside the database. - { name: pg_cron } # 1.6 : pg_cron: Job scheduler for PostgreSQL - { name: timescaledb } # 2.17 : timescaledb: Enables scalable inserts and complex queries for time-series data - { name: pg_tle } # 1.2 : pg_tle: Trusted Language Extensions for PostgreSQL - { name: vector } # 0.8.0 : pgvector: the vector similarity search # supabase required extensions pg_libs: 'pg_stat_statements, plpgsql, plpgsql_check, pg_cron, pg_net, timescaledb, auto_explain, pg_tle, plan_filter' pg_extensions: # extensions to be installed on this cluster - supabase # essential extensions for supabase - timescaledb postgis pg_graphql pg_jsonschema wrappers pg_search pg_analytics pg_parquet plv8 duckdb_fdw pg_cron pg_timetable pgqr - supautils pg_plan_filter passwordcheck plpgsql_check pgaudit pgsodium pg_vault pgjwt pg_ecdsa pg_session_jwt index_advisor - pgvector pgvectorscale pg_summarize pg_tiktoken pg_tle pg_stat_monitor hypopg pg_hint_plan pg_http pg_net pg_smtp_client pg_idkit pg_parameters: cron.database_name: postgres pgsodium.enable_event_trigger: off pg_hba_rules: # supabase hba rules, require access from docker network - { user: all ,db: postgres ,addr: intra ,auth: pwd ,title: 'allow supabase access from intranet' } - { user: all ,db: postgres ,addr: 172.17.0.0/16 ,auth: pwd ,title: 'allow access from local docker network' } pg_vip_enabled: true pg_vip_address: 10.10.10.2/24 pg_vip_interface: eth1 node_crontab: [ '00 01 * * * postgres /pg/bin/pg-backup full' ] # make a full backup every 1am # launch supabase stateless part with docker compose: ./supabase.yml supabase: hosts: 10.10.10.10: { supa_seq: 1 } # instance 1 10.10.10.11: { supa_seq: 2 } # instance 2 10.10.10.12: { supa_seq: 3 } # instance 3 vars: supa_cluster: supa # cluster name docker_enabled: true # enable docker # use these to pull docker images via proxy and mirror registries #docker_registry_mirrors: ['https://docker.xxxxx.io'] #proxy_env: # add [OPTIONAL] proxy env to /etc/docker/daemon.json configuration file # no_proxy: \"localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,*.pigsty,*.aliyun.com,mirrors.*,*.myqcloud.com,*.tsinghua.edu.cn\" # #all_proxy: http://user:pass@host:port # these configuration entries will OVERWRITE or APPEND to /opt/supabase/.env file (src template: app/supabase/.env) # check https://github.com/pgsty/pigsty/blob/main/app/supabase/.env for default values supa_config: # IMPORTANT: CHANGE JWT_SECRET AND REGENERATE CREDENTIAL ACCORDING!!!!!!!!!!! # https://supabase.com/docs/guides/self-hosting/docker#securing-your-services jwt_secret: your-super-secret-jwt-token-with-at-least-32-characters-long anon_key: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJhbm9uIiwKICAgICJpc3MiOiAic3VwYWJhc2UtZGVtbyIsCiAgICAiaWF0IjogMTY0MTc2OTIwMCwKICAgICJleHAiOiAxNzk5NTM1NjAwCn0.dc_X5iR_VP_qT0zsiyj_I_OZ2T9FtRU2BBNWN8Bu4GE service_role_key: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJzZXJ2aWNlX3JvbGUiLAogICAgImlzcyI6ICJzdXBhYmFzZS1kZW1vIiwKICAgICJpYXQiOiAxNjQxNzY5MjAwLAogICAgImV4cCI6IDE3OTk1MzU2MDAKfQ.DaYlNEoUrrEn2Ig7tqibS-PHK5vgusbcbo7X36XVt4Q dashboard_username: supabase dashboard_password: pigsty # postgres connection string (use the correct ip and port) postgres_host: 10.10.10.3 # use the pg_vip_address rather than single node ip postgres_port: 5433 # access via the 'default' service, which always route to the primary postgres postgres_db: postgres postgres_password: DBUser.Supa # password for supabase_admin and multiple supabase users # expose supabase via domain name site_url: http://supa.pigsty api_external_url: http://supa.pigsty supabase_public_url: http://supa.pigsty # if using s3/minio as file storage s3_bucket: supa s3_endpoint: https://sss.pigsty:9002 s3_access_key: supabase s3_secret_key: S3User.Supabase s3_force_path_style: true s3_protocol: https s3_region: stub minio_domain_ip: 10.10.10.3 # sss.pigsty domain name will resolve to this l2 vip that bind to all nodes # if using SMTP (optional) #smtp_admin_email: admin@example.com #smtp_host: supabase-mail #smtp_port: 2500 #smtp_user: fake_mail_user #smtp_pass: fake_mail_password #smtp_sender_name: fake_sender #enable_anonymous_users: false #==============================================================# # Global Parameters #==============================================================# vars: version: v3.1.0 # pigsty version string admin_ip: 10.10.10.10 # admin node ip address region: china # upstream mirror region: default|china|europe node_tune: oltp # node tuning specs: oltp,olap,tiny,crit pg_conf: oltp.yml # pgsql tuning specs: {oltp,olap,tiny,crit}.yml infra_portal: # domain names and upstream servers home : { domain: h.pigsty } grafana : { domain: g.pigsty ,endpoint: \"${admin_ip}:3000\" , websocket: true } prometheus : { domain: p.pigsty ,endpoint: \"${admin_ip}:9090\" } alertmanager : { domain: a.pigsty ,endpoint: \"${admin_ip}:9093\" } minio : { domain: m.pigsty ,endpoint: \"10.10.10.10:9001\", https: true, websocket: true } blackbox : { endpoint: \"${admin_ip}:9115\" } loki : { endpoint: \"${admin_ip}:3100\" } # expose supa studio UI and API via nginx supa : { domain: supa.pigsty ,endpoint: \"10.10.10.10:8000\", websocket: true } #----------------------------------# # Credential: CHANGE THESE PASSWORDS #----------------------------------# #grafana_admin_username: admin grafana_admin_password: pigsty #pg_admin_username: dbuser_dba pg_admin_password: DBUser.DBA #pg_monitor_username: dbuser_monitor pg_monitor_password: DBUser.Monitor #pg_replication_username: replicator pg_replication_password: DBUser.Replicator #patroni_username: postgres patroni_password: Patroni.API #haproxy_admin_username: admin haproxy_admin_password: pigsty # use minio as supabase file storage, single node single driver mode for demonstration purpose minio_access_key: minioadmin # root access key, `minioadmin` by default minio_secret_key: minioadmin # root secret key, `minioadmin` by default minio_buckets: [ { name: pgsql }, { name: supa } ] minio_users: - { access_key: dba , secret_key: S3User.DBA, policy: consoleAdmin } - { access_key: pgbackrest , secret_key: S3User.Backup, policy: readwrite } - { access_key: supabase , secret_key: S3User.Supabase, policy: readwrite } minio_endpoint: https://sss.pigsty:9000 # explicit overwrite minio endpoint with haproxy port node_etc_hosts: [\"10.10.10.3 sss.pigsty\"] # domain name to access minio from all nodes (required) # use minio as default backup repo for PostgreSQL pgbackrest_method: minio # pgbackrest repo method: local,minio,[user-defined...] pgbackrest_repo: # pgbackrest repo: https://pgbackrest.org/configuration.html#section-repository local: # default pgbackrest repo with local posix fs path: /pg/backup # local backup directory, `/pg/backup` by default retention_full_type: count # retention full backups by count retention_full: 2 # keep 2, at most 3 full backup when using local fs repo minio: # optional minio repo for pgbackrest type: s3 # minio is s3-compatible, so s3 is used s3_endpoint: sss.pigsty # minio endpoint domain name, `sss.pigsty` by default s3_region: us-east-1 # minio region, us-east-1 by default, useless for minio s3_bucket: pgsql # minio bucket name, `pgsql` by default s3_key: pgbackrest # minio user access key for pgbackrest s3_key_secret: S3User.Backup # minio user secret key for pgbackrest s3_uri_style: path # use path style uri for minio rather than host style path: /pgbackrest # minio backup path, default is `/pgbackrest` storage_port: 9002 # minio port, 9000 by default storage_ca_file: /pg/cert/ca.crt # minio ca file path, `/pg/cert/ca.crt` by default bundle: y # bundle small files into a single file cipher_type: aes-256-cbc # enable AES encryption for remote backup repo cipher_pass: pgBackRest # AES encryption password, default is 'pgBackRest' retention_full_type: time # retention full backup by time on minio repo retention_full: 14 # keep full backup for last 14 days # download docker and supabase related extensions pg_version: 17 repo_modules: node,pgsql,infra,docker repo_packages: [node-bootstrap, infra-package, infra-addons, node-package1, node-package2, pgsql-utility, docker ] repo_extra_packages: - pgsql-main - supabase # essential extensions for supabase - timescaledb postgis pg_graphql pg_jsonschema wrappers pg_search pg_analytics pg_parquet plv8 duckdb_fdw pg_cron pg_timetable pgqr - supautils pg_plan_filter passwordcheck plpgsql_check pgaudit pgsodium pg_vault pgjwt pg_ecdsa pg_session_jwt index_advisor - pgvector pgvectorscale pg_summarize pg_tiktoken pg_tle pg_stat_monitor hypopg pg_hint_plan pg_http pg_net pg_smtp_client pg_idkit ","categories":"","description":"","excerpt":"Supabase is great, own your own Supabase is even better. Here’s a …","ref":"/blog/db/supabase/","tags":["Database","Supabase"],"title":"Self-Hosting Supabase on PostgreSQL"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/hardware/","tags":"","title":"Hardware"},{"body":" Author: Alex Miller (Snowflake, Apple, Google) | Original: Modern Hardware for Future Databases\nA survey on how hardware developments affect database design, covering key advancements in networking, storage, and computing. Read more\n","categories":"","description":"A survey on how hardware developments affect database design, covering key advancements in networking, storage, and computing.\n","excerpt":"A survey on how hardware developments affect database design, covering …","ref":"/blog/db/future-hardware/","tags":["Database","Hardware","Performance"],"title":"Modern Hardware for Future Databases"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/performance/","tags":"","title":"Performance"},{"body":"\nNever deploy on Friday, or you’ll be working all weekend! PostgreSQL minor releases were pulled on the day of release, requiring emergency rollback. Read more\n","categories":"","description":"Never deploy on Friday, or you'll be working all weekend! PostgreSQL minor releases were pulled on the day of release, requiring emergency rollback.\n","excerpt":"Never deploy on Friday, or you'll be working all weekend! PostgreSQL …","ref":"/blog/pg/pg-faint/","tags":["PostgreSQL"],"title":"Don't Upgrade! Released and Immediately Pulled - Even PostgreSQL Isn't Immune to Epic Fails"},{"body":"\nPG17 achieved extension ecosystem adaptation in half the time of PG16, with 300 available extensions ready for production use. PG 12 officially exits support lifecycle. Read more\n","categories":"","description":"PG17 achieved extension ecosystem adaptation in half the time of PG16, with 300 available extensions ready for production use. PG 12 officially exits support lifecycle.\n","excerpt":"PG17 achieved extension ecosystem adaptation in half the time of PG16, …","ref":"/blog/pg/pg12-eol-pg17-up/","tags":["PostgreSQL"],"title":"PostgreSQL 12 End-of-Life, PG 17 Takes the Throne"},{"body":" Author: Peter Zaitsev (Percona Founder) | Original: Can MySQL Catch Up with PostgreSQL?\nPercona founder Peter Zaitsev discusses whether MySQL can still keep up with PostgreSQL. His views largely represent the MySQL community’s perspective. Read more\n","categories":"","description":"Percona founder Peter Zaitsev discusses whether MySQL can still keep up with PostgreSQL. His views largely represent the MySQL communitys perspective.\n","excerpt":"Percona founder Peter Zaitsev discusses whether MySQL can still keep …","ref":"/blog/db/can-mysql-catchup/","tags":["Database","MySQL","PostgreSQL"],"title":"Can MySQL Still Catch Up with PostgreSQL?"},{"body":"\nPostgreSQL Is Eating the Database World through the power of extensibility. With 390 extensions powering PG, we may not say it’s invincible, but it’s definitely getting much closer. Read more\n","categories":"","description":"PostgreSQL Is Eating the Database World through the power of extensibility. With 390 extensions powering PG, we may not say it's invincible, but it’s definitely getting much closer.\n","excerpt":"PostgreSQL Is Eating the Database World through the power of …","ref":"/blog/pg/pg-ext-repo/","tags":["PostgreSQL","PG-Ecosystem","Extension"],"title":"The ideal way to deliver PostgreSQL Extensions"},{"body":"\nPigsty v3.1 makes PostgreSQL 17 the default, delivers one-click Supabase self-hosting, adds ARM64 and Ubuntu 24.04 support, and simplifies configuration management. Read more\n","categories":"","description":"Pigsty v3.1 makes PostgreSQL 17 the default, delivers one-click Supabase self-hosting, adds ARM64 and Ubuntu 24.04 support, and simplifies configuration management.\n","excerpt":"Pigsty v3.1 makes PostgreSQL 17 the default, delivers one-click …","ref":"/blog/pigsty/v3.1/","tags":["Pigsty"],"title":"Pigsty v3.1: One-Click Supabase, PG17 Default, ARM \u0026 Ubuntu 24"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/linux/","tags":"","title":"Linux"},{"body":"\nThe Linux community is essentially imperial — and Linus himself is the earliest and most successful technical dictator. People are used to Linus’s generosity but forget this point. Read more\n","categories":"","description":"The Linux community is essentially imperial — and Linus himself is the earliest and most successful technical dictator. People are used to Linus's generosity but forget this point.\n","excerpt":"The Linux community is essentially imperial — and Linus himself is the …","ref":"/blog/db/linus-ban-ru/","tags":["Linux","Open-Source"],"title":"Open-Source \"Tyrant\" Linus's Purge"},{"body":"\nWhen open source ideals meet commercial conflicts, what insights can this conflict between open source software communities and cloud vendors bring? On the importance of community boundary demarcation. Read more\n","categories":"","description":"When open source ideals meet commercial conflicts, what insights can this conflict between open source software communities and cloud vendors bring? On the importance of community boundary demarcation.\n","excerpt":"When open source ideals meet commercial conflicts, what insights can …","ref":"/blog/cloud/wordpress-drama/","tags":["Open-Source"],"title":"WordPress Community Civil War: On Community Boundary Demarcation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/pg-development/","tags":"","title":"PG-Development"},{"body":"\nNo rules, no standards. Some developer conventions for PostgreSQL 16. Read more\n","categories":"","description":"No rules, no standards. Some developer conventions for PostgreSQL 16.\n","excerpt":"No rules, no standards. Some developer conventions for PostgreSQL 16.\n","ref":"/blog/pg/pg-convention/","tags":["PostgreSQL","PG-Development"],"title":"PostgreSQL Convention 2024"},{"body":"\nThe paradigm shift brought by RDS, whether cloud databases are overpriced cafeteria meals. Quality, security, efficiency, and cost analysis, cloud exit database self-building: how to implement in practice! Read more\n","categories":"","description":"The paradigm shift brought by RDS, whether cloud databases are overpriced cafeteria meals. Quality, security, efficiency, and cost analysis, cloud exit database self-building: how to implement in practice!\n","excerpt":"The paradigm shift brought by RDS, whether cloud databases are …","ref":"/blog/cloud/rds-scam/","tags":["Cloud-Exit","RDS"],"title":"Cloud Database: Michelin Prices for Cafeteria Pre-made Meals"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/rds/","tags":"","title":"RDS"},{"body":"\nPigsty v3.0 ships 340 extensions across EL/Deb with full parity, adds pluggable kernels (Babelfish, IvorySQL, PolarDB) for MSSQL/Oracle compatibility, and delivers a local-first state-of-the-art RDS experience. Read more\n","categories":"","description":"Pigsty v3.0 ships 340 extensions across EL/Deb with full parity, adds pluggable kernels (Babelfish, IvorySQL, PolarDB) for MSSQL/Oracle compatibility, and delivers a local-first state-of-the-art RDS experience.\n","excerpt":"Pigsty v3.0 ships 340 extensions across EL/Deb with full parity, adds …","ref":"/blog/pigsty/v3.0/","tags":["Pigsty"],"title":"Pigsty v3.0: Pluggable Kernels \u0026 340 Extensions"},{"body":"\nPostgreSQL is now the world’s most advanced open-source database and has become the preferred open-source database for organizations of all sizes, matching or exceeding top commercial databases. Read more\n","categories":"","description":"PostgreSQL is now the world's most advanced open-source database and has become the preferred open-source database for organizations of all sizes, matching or exceeding top commercial databases.\n","excerpt":"PostgreSQL is now the world's most advanced open-source database and …","ref":"/blog/pg/pg-17/","tags":["PostgreSQL"],"title":"PostgreSQL 17 Released: No More Pretending!"},{"body":"\nSeven days after Singapore Zone C failure, availability not even reaching 8, let alone multiple 9s. But compared to data loss, availability is just a minor issue Read more\n","categories":"","description":"Seven days after Singapore Zone C failure, availability not even reaching 8, let alone multiple 9s. But compared to data loss, availability is just a minor issue","excerpt":"Seven days after Singapore Zone C failure, availability not even …","ref":"/blog/cloud/aliyun-ha/","tags":["Cloud-Exit","Alibaba-Cloud","Cloud-Outage"],"title":"Alibaba-Cloud: High Availability Disaster Recovery Myth Shattered"},{"body":"\nProgrammers are expensive, scarce biological computing cores, the anchor point of software costs — please prioritize optimizing biological cores before optimizing CPU cores. Read more\n","categories":"","description":"Programmers are expensive, scarce biological computing cores, the anchor point of software costs — please prioritize optimizing biological cores before optimizing CPU cores.\n","excerpt":"Programmers are expensive, scarce biological computing cores, the …","ref":"/blog/db/bio-core-cpu-core/","tags":["Database"],"title":"Optimize Bio Cores First, CPU Cores Second"},{"body":"\nMongoDB has a terrible track record on integrity, lackluster products and technology, gets beaten by PG in correctness, performance, and functionality, with collapsing developer reputation, declining popularity, stock price halving, and expanding losses. Provocative marketing against PG can’t save it with “good marketing.” Read more\n","categories":"","description":"MongoDB has a terrible track record on integrity, lackluster products and technology, gets beaten by PG in correctness, performance, and functionality, with collapsing developer reputation, declining popularity, stock price halving, and expanding losses. Provocative marketing against PG can't save it with \"good marketing.\"\n","excerpt":"MongoDB has a terrible track record on integrity, lackluster products …","ref":"/blog/db/bad-mongo/","tags":["MongoDB","PostgreSQL","PG-Ecosystem"],"title":"MongoDB Has No Future: Good Marketing Can't Save a Rotten Mango"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/mongodb/","tags":"","title":"MongoDB"},{"body":"\nMongoDB 3.2’s analytics subsystem turned out to be an embedded PostgreSQL database? A whistleblowing story from MongoDB’s partner about betrayal and disillusionment. Read more\n","categories":"","description":"MongoDB 3.2's analytics subsystem turned out to be an embedded PostgreSQL database? A whistleblowing story from MongoDB's partner about betrayal and disillusionment.\n","excerpt":"MongoDB 3.2's analytics subsystem turned out to be an embedded …","ref":"/blog/db/mongo-powered-by-pg/","tags":["Database"],"title":"MongoDB: Now Powered by PostgreSQL?"},{"body":"\nPostgreSQL can directly replace Oracle, SQL Server, and MongoDB at the kernel level. Of course, the most thorough replacement is SQL Server - AWS’s Babelfish provides wire-protocol-level compatibility. Read more\n","categories":"","description":"PostgreSQL can directly replace Oracle, SQL Server, and MongoDB at the kernel level. Of course, the most thorough replacement is SQL Server - AWS's Babelfish provides wire-protocol-level compatibility.\n","excerpt":"PostgreSQL can directly replace Oracle, SQL Server, and MongoDB at the …","ref":"/blog/pg/pg-replace-mssql/","tags":["PostgreSQL","PG-Ecosystem","MSSQL"],"title":"Can PostgreSQL Replace Microsoft SQL Server?"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/mssql/","tags":"","title":"MSSQL"},{"body":"\nA customer experienced an outrageous cascade of failures on cloud database last week: a high-availability PG RDS cluster went down completely - both primary and replica servers - after attempting a simple memory expansion, troubleshooting until dawn. Poor recommendations abounded during the incident, and the postmortem was equally perfunctory. I share this case study here for reference and review. Read more\n","categories":"","description":"A customer experienced an outrageous cascade of failures on cloud database last week: a high-availability PG RDS cluster went down completely - both primary and replica servers - after attempting a simple memory expansion, troubleshooting until dawn. Poor recommendations abounded during the incident, and the postmortem was equally perfunctory. I share this case study here for reference and review.\n","excerpt":"A customer experienced an outrageous cascade of failures on cloud …","ref":"/blog/cloud/rds-failure/","tags":["Cloud-Exit","RDS"],"title":"Amateur Hour Opera: Alibaba-Cloud PostgreSQL Disaster Chronicle"},{"body":"\nNetEase Cloud Music experienced a two-and-a-half-hour outage this afternoon. Based on circulating online clues, we can deduce that the real cause behind this incident was… Read more\n","categories":"","description":"NetEase Cloud Music experienced a two-and-a-half-hour outage this afternoon. Based on circulating online clues, we can deduce that the real cause behind this incident was...\n","excerpt":"NetEase Cloud Music experienced a two-and-a-half-hour outage this …","ref":"/blog/cloud/netease/","tags":["Cloud-Outage"],"title":"What Can We Learn from NetEase Cloud Music's Outage?"},{"body":"\nJust like the vector database extension race two years ago, the current PostgreSQL ecosystem extension competition has begun revolving around DuckDB. MotherDuck’s official entry into the PostgreSQL extension space undoubtedly signals that competition has entered white-hot territory. Read more\n","categories":"","description":"Just like the vector database extension race two years ago, the current PostgreSQL ecosystem extension competition has begun revolving around DuckDB. MotherDuck's official entry into the PostgreSQL extension space undoubtedly signals that competition has entered white-hot territory.\n","excerpt":"Just like the vector database extension race two years ago, the …","ref":"/blog/pg/pg-duckdb/","tags":["PostgreSQL","PG-Ecosystem"],"title":"Whoever Integrates DuckDB Best Wins the OLAP World"},{"body":"\nThe 2024 StackOverflow Global Developer Survey results are fresh out, and PostgreSQL has become the most popular, most loved, and most wanted database globally for the second consecutive year. Nothing can stop PostgreSQL from devouring the entire database world anymore! Read more\n","categories":"","description":"The 2024 StackOverflow Global Developer Survey results are fresh out, and PostgreSQL has become the most popular, most loved, and most wanted database globally for the second consecutive year. Nothing can stop PostgreSQL from devouring the entire database world anymore!\n","excerpt":"The 2024 StackOverflow Global Developer Survey results are fresh out, …","ref":"/blog/pg/pg-is-no1-again/","tags":["PostgreSQL","PG-Ecosystem"],"title":"StackOverflow 2024 Survey: PostgreSQL Has Gone Completely Berserk"},{"body":"\nSwitzerland’s government leads the way with open source legislation, showing IT developing countries how to ensure software sovereignty and control. True autonomy and control stem from “open source communities,” not some “nationalist” style “domestic software.” Read more\n","categories":"","description":"Switzerland's government leads the way with open source legislation, showing IT developing countries how to ensure software sovereignty and control. True autonomy and control stem from \"open source communities,\" not some \"nationalist\" style \"domestic software.\"\n","excerpt":"Switzerland's government leads the way with open source legislation, …","ref":"/blog/db/oss-gov/","tags":["Open-Source"],"title":"Switzerland Mandates Open-Source for Government Software"},{"body":"\nBoth client and vendor failed to control blast radius, leading to this epic global security incident that will greatly benefit local-first software philosophy. Read more\n","categories":"","description":"Both client and vendor failed to control blast radius, leading to this epic global security incident that will greatly benefit local-first software philosophy.\n","excerpt":"Both client and vendor failed to control blast radius, leading to this …","ref":"/blog/cloud/bsod-friday/","tags":["Cloud-Exit","Cloudflare"],"title":"Blue Screen Friday: Amateur Hour on Both Sides"},{"body":"\nPigsty v2.7 bundles 255 PostgreSQL extensions, plus Docker templates for Odoo, Supabase, PolarDB, and Jupyter, with new PITR dashboards. Read more\n","categories":"","description":"Pigsty v2.7 bundles 255 PostgreSQL extensions, plus Docker templates for Odoo, Supabase, PolarDB, and Jupyter, with new PITR dashboards.\n","excerpt":"Pigsty v2.7 bundles 255 PostgreSQL extensions, plus Docker templates …","ref":"/blog/pigsty/v2.7/","tags":["Pigsty"],"title":"Pigsty v2.7: The Extension Superpack"},{"body":"\nThis July, MySQL 9.0 was finally released—a full eight years after its last major version, 8.0 (@2016-09). Read more\n","categories":"","description":"This July, MySQL 9.0 was finally released—a full eight years after its last major version, 8.0 (@2016-09).","excerpt":"This July, MySQL 9.0 was finally released—a full eight years after its …","ref":"/blog/db/mysql-is-dead/","tags":["Database","MySQL","PostgreSQL"],"title":"MySQL is dead, Long live PostgreSQL!"},{"body":"\nThis vulnerability affects EL9, Ubuntu 22.04, Debian 12. Users should promptly update OpenSSH to fix this vulnerability. Read more\n","categories":"","description":"This vulnerability affects EL9, Ubuntu 22.04, Debian 12. Users should promptly update OpenSSH to fix this vulnerability.\n","excerpt":"This vulnerability affects EL9, Ubuntu 22.04, Debian 12. Users should …","ref":"/blog/db/cve-2024-6387/","tags":["Vulnerability"],"title":"CVE-2024-6387 SSH Vulnerability Fix"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/vulnerability/","tags":"","title":"Vulnerability"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/ml/","tags":"","title":"ML"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/postgresml/","tags":"","title":"PostgresML"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/reference/","tags":"","title":"Reference"},{"body":"","categories":"","description":"","excerpt":"","ref":"/module/software/","tags":"","title":"SOFTWARE"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/pigstyapp/","tags":"","title":"PigstyApp"},{"body":"\nDify is an open-source LLM app development platform. This article explains how to self-host Dify using Pigsty. Read more\n","categories":"","description":"Dify is an open-source LLM app development platform. This article explains how to self-host Dify using Pigsty.\n","excerpt":"Dify is an open-source LLM app development platform. This article …","ref":"/blog/pg/dify-setup/","tags":["PostgreSQL","Docker","PigstyApp"],"title":"Self-Hosting Dify with PG, PGVector, and Pigsty"},{"body":" Author: Peter Zaitsev (Percona Founder) | Original: Can Oracle Save MySQL?\nPercona founder Peter Zaitsev publicly expressed disappointment with MySQL and Oracle, criticizing the declining performance with newer versions. Read more\n","categories":"","description":"Percona founder Peter Zaitsev publicly expressed disappointment with MySQL and Oracle, criticizing the declining performance with newer versions.\n","excerpt":"Percona founder Peter Zaitsev publicly expressed disappointment with …","ref":"/blog/db/can-oracle-save-mysql/","tags":["Database","MySQL","Oracle"],"title":"Can Oracle Still Save MySQL?"},{"body":" Author: Peter Zaitsev (Percona Founder) | Original: Is Oracle Finally Killing MySQL?\nPeter Zaitsev, founder of Percona, criticizes how Oracle’s actions and inactions have killed MySQL. About 15 years after acquiring Sun and MySQL. Read more\n","categories":"","description":"Peter Zaitsev, founder of Percona, criticizes how Oracles actions and inactions have killed MySQL. About 15 years after acquiring Sun and MySQL.\n","excerpt":"Peter Zaitsev, founder of Percona, criticizes how Oracles actions and …","ref":"/blog/db/oracle-kill-mysql/","tags":["Database","MySQL","Oracle"],"title":"Oracle Finally Killed MySQL"},{"body":" Author: Marco Tusa (Percona) | Original: Sakila, Where Are You Going?\nHigher MySQL versions mean worse performance? Percona monitoring shows slow migration from 5.7 to 8.x. PostgreSQL is pulling ahead. Read more\n","categories":"","description":"Higher MySQL versions mean worse performance? Percona monitoring shows slow migration from 5.7 to 8.x. PostgreSQL is pulling ahead.\n","excerpt":"Higher MySQL versions mean worse performance? Percona monitoring shows …","ref":"/blog/db/sakila-where-are-you-going/","tags":["Database","MySQL","Performance"],"title":"MySQL Performance Declining: Where is Sakila Going?"},{"body":"\nExperience \u0026 Feeling on the PGCon.Dev 2024 Read more\n","categories":"","description":"Experience \u0026 Feeling on the PGCon.Dev 2024   \n","excerpt":"Experience \u0026 Feeling on the PGCon.Dev 2024   \n","ref":"/blog/pg/pgcondev-2024/","tags":["PostgreSQL","PG-Ecosystem"],"title":"PGCon.Dev 2024, The conf that shutdown PG for a week"},{"body":"\nThe PostgreSQL Global Development Group announces PostgreSQL 17’s first Beta version is now available. This time, PostgreSQL has truly burst the toothpaste tube! Read more\n","categories":"","description":"The PostgreSQL Global Development Group announces PostgreSQL 17's first Beta version is now available. This time, PostgreSQL has truly burst the toothpaste tube!\n","excerpt":"The PostgreSQL Global Development Group announces PostgreSQL 17's …","ref":"/blog/pg/pg-17-beta1/","tags":["PostgreSQL"],"title":"PostgreSQL 17 Beta1 Released!"},{"body":"\nAfter Alibaba-Cloud’s epic global outage on Double 11, setting industry records, how should we evaluate this incident and what lessons can we learn from it? Read more\n","categories":"","description":"After Alibaba-Cloud's epic global outage on Double 11, setting industry records, how should we evaluate this incident and what lessons can we learn from it?\n","excerpt":"After Alibaba-Cloud's epic global outage on Double 11, setting …","ref":"/blog/cloud/ahrefs-saving/","tags":["Cloud-Exit","AWS","EBS"],"title":"How Ahrefs Saved US$400M by NOT Going to the Cloud"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/ebs/","tags":"","title":"EBS"},{"body":" Author: Ajay Kulkarni (TimescaleDB CEO) | Original: Why PostgreSQL Is the Bedrock for the Future of Data\nOne of the biggest trends in software development today is PostgreSQL becoming the de facto database standard. This article explains why. Read more\n","categories":"","description":"One of the biggest trends in software development today is PostgreSQL becoming the de facto database standard. This article explains why.\n","excerpt":"One of the biggest trends in software development today is PostgreSQL …","ref":"/blog/pg/pg-for-everything/","tags":["PostgreSQL","PG-Ecosystem"],"title":"Why PostgreSQL is the Future Standard?"},{"body":"\nDue to an “unprecedented configuration error,” Google Cloud mistakenly deleted trillion-RMB fund giant UniSuper’s entire cloud account, cloud environment and all off-site backups, setting a new record in cloud computing history! Read more\n","categories":"","description":"Due to an \"unprecedented configuration error,\" Google Cloud mistakenly deleted trillion-RMB fund giant **UniSuper**'s entire cloud account, cloud environment and all off-site backups, setting a new record in cloud computing history!\n","excerpt":"Due to an \"unprecedented configuration error,\" Google Cloud mistakenly …","ref":"/blog/cloud/gcp-unisuper/","tags":["Cloud-Exit","Cloud-Outage"],"title":"Database Deletion Supreme - Google Cloud Nuked a Major Fund's Entire Cloud Account"},{"body":"\nThe dark forest law has emerged on public cloud: Anyone who knows your S3 object storage bucket name can explode your cloud bill. Read more\n","categories":"","description":"The dark forest law has emerged on public cloud: **Anyone who knows your S3 object storage bucket name can explode your cloud bill.** \n","excerpt":"The dark forest law has emerged on public cloud: **Anyone who knows …","ref":"/blog/cloud/s3-scam/","tags":["Cloud-Exit","AWS","S3"],"title":"Cloud Dark Forest: Exploding Cloud Bills with Just S3 Bucket Names"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/s3/","tags":"","title":"S3"},{"body":"\nFriends often ask me, can Chinese domestic databases really compete? To be honest, it’s a question that offends people. So let’s try speaking with data - I hope the charts provided in this article can help readers understand the database ecosystem landscape and establish more accurate proportional awareness. Read more\n","categories":"","description":"Friends often ask me, **can Chinese domestic databases really compete?** To be honest, **it's a question that offends people**. So let's try speaking with data - I hope the charts provided in this article can help readers understand the database ecosystem landscape and establish more accurate proportional awareness.\n","excerpt":"Friends often ask me, **can Chinese domestic databases really …","ref":"/blog/db/db-china/","tags":["Database","Domestic-Database"],"title":"Can Chinese Domestic Databases Really Compete?"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/domestic-database/","tags":"","title":"Domestic-Database"},{"body":"\nToday we discuss the fair pricing of commercial databases, open-source databases, cloud databases, and domestic Chinese databases. Read more\n","categories":"","description":"Today we discuss the fair pricing of commercial databases, open-source databases, cloud databases, and domestic Chinese databases.\n","excerpt":"Today we discuss the fair pricing of commercial databases, open-source …","ref":"/blog/db/cheap-polar/","tags":["Database","Domestic-Database"],"title":"The $20 Brother PolarDB: What Should Databases Actually Cost?"},{"body":"\nAs a roundtable guest, I was invited to participate in Cloudflare’s Immerse conference in Shenzhen. During the dinner, I had in-depth discussions with Cloudflare’s APAC CMO, Greater China Technical Director, and front-line engineers about many questions of interest to netizens. Read more\n","categories":"","description":"As a roundtable guest, I was invited to participate in Cloudflare's Immerse conference in Shenzhen. During the dinner, I had in-depth discussions with Cloudflare's APAC CMO, Greater China Technical Director, and front-line engineers about many questions of interest to netizens.\n","excerpt":"As a roundtable guest, I was invited to participate in Cloudflare's …","ref":"/blog/cloud/cf-interview/","tags":["Cloud-Exit","Cloudflare"],"title":"Cloudflare Roundtable Interview and Q\u0026A Record"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/tencent-cloud/","tags":"","title":"Tencent-Cloud"},{"body":"\nTencent Cloud’s epic global outage after Double 11 set industry records. How should we evaluate and view this failure, and what lessons can we learn from it? Read more\n","categories":"","description":"Tencent Cloud's epic global outage after Double 11 set industry records. How should we evaluate and view this failure, and what lessons can we learn from it?\n","excerpt":"Tencent Cloud's epic global outage after Double 11 set industry …","ref":"/blog/cloud/qcloud/","tags":["Cloud-Exit","Tencent-Cloud","Cloud-Outage"],"title":"What Can We Learn from Tencent Cloud's Major Outage?"},{"body":"\nWhile I’ve always advocated for cloud exit, if it’s about adopting a cyber bodhisattva cloud like Cloudflare, I’m all in with both hands raised. Read more\n","categories":"","description":"While I've always advocated for cloud exit, if it's about adopting a cyber bodhisattva cloud like Cloudflare, I'm all in with both hands raised.\n","excerpt":"While I've always advocated for cloud exit, if it's about adopting a …","ref":"/blog/cloud/cloudflare/","tags":["Cloud-Exit","Cloudflare"],"title":"Cloudflare - The Cyber Buddha That Destroys Public Cloud"},{"body":"\nLuo Yonghao’s livestream first spent half an hour selling robot vacuums, then Luo himself belatedly appeared to read scripts selling “cloud computing” for forty minutes — before seamlessly transitioning to selling Colgate enzyme-free toothpaste — leaving viewers bewildered between toothpaste and cloud computing. Read more\n","categories":"","description":"Luo Yonghao's livestream first spent half an hour selling robot vacuums, then Luo himself belatedly appeared to read scripts selling \"cloud computing\" for forty minutes — before seamlessly transitioning to selling Colgate enzyme-free toothpaste — leaving viewers bewildered between toothpaste and cloud computing.\n","excerpt":"Luo Yonghao's livestream first spent half an hour selling robot …","ref":"/blog/cloud/luo-live/","tags":["Cloud-Exit","Alibaba-Cloud"],"title":"Can Luo Yonghao Save Toothpaste Cloud?"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/redis/","tags":"","title":"Redis"},{"body":"\nRedis “going non-open source” is not a disgrace to Redis, but a disgrace to “open source/OSI” and even more so to public cloud. What truly matters has always been software freedom, while open source is just one means to achieve software freedom. Read more\n","categories":"","description":"Redis \"going non-open source\" is not a disgrace to Redis, but a disgrace to \"open source/OSI\" and even more so to public cloud. What truly matters has always been software freedom, while open source is just one means to achieve software freedom.\n","excerpt":"Redis \"going non-open source\" is not a disgrace to Redis, but a …","ref":"/blog/db/redis-oss/","tags":["Database","Open-Source","Redis"],"title":"Redis Going Non-Open-Source is a Disgrace to \"Open-Source\" and Public Cloud"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/license/","tags":"","title":"License"},{"body":" Author: Jonathan Katz (PostgreSQL Core Team) | Original: PostgreSQL Will Not Change Its License\nPostgreSQL will not change its license. This article is a response from PostgreSQL core team members on this question. Read more\n","categories":"","description":"PostgreSQL will not change its license. This article is a response from PostgreSQL core team members on this question.\n","excerpt":"PostgreSQL will not change its license. This article is a response …","ref":"/blog/pg/pg-license/","tags":["PostgreSQL","Open-Source","License"],"title":"Will PostgreSQL Change Its License?"},{"body":"","categories":"","description":"","excerpt":"","ref":"/module/app/","tags":"","title":"APP"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/visualization/","tags":"","title":"Visualization"},{"body":"\nAlibaba-Cloud claimed major price cuts, but a detailed analysis of cloud server costs reveals that cloud computing and storage remain outrageously expensive. Read more\n","categories":"","description":"Alibaba-Cloud claimed major price cuts, but a detailed analysis of cloud server costs reveals that cloud computing and storage remain outrageously expensive. \n","excerpt":"Alibaba-Cloud claimed major price cuts, but a detailed analysis of …","ref":"/blog/cloud/ecs/","tags":["Cloud-Exit","ECS"],"title":"Analyzing Alibaba-Cloud Server Computing Cost"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/ecs/","tags":"","title":"ECS"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/ecosystem/","tags":"","title":"Ecosystem"},{"body":"\nPostgreSQL is not just a simple relational database, but an abstract framework for data management with the power to devour the entire database world. “Just use Postgres for everything” has become a mainstream best practice. Read more\n","categories":"","description":"PostgreSQL is not just a simple relational database, but an abstract framework for data management with the power to devour the entire database world. \"Just use Postgres for everything\" has become a mainstream best practice.\n","excerpt":"PostgreSQL is not just a simple relational database, but an abstract …","ref":"/blog/select/pg-eat-db-world/","tags":["PostgreSQL"],"title":"PostgreSQL is Eating the Database World"},{"body":"PostgreSQL isn’t just a simple relational database; it’s a data management framework with the potential to engulf the entire database realm. The trend of “Using Postgres for Everything” is no longer limited to a few elite teams but is becoming a mainstream best practice.\nOLAP’s New Challenger In a 2016 database meetup, I argued that a significant gap in the PostgreSQL ecosystem was the lack of a sufficiently good columnar storage engine for OLAP workloads. While PostgreSQL itself offers lots of analysis features, its performance in full-scale analysis on larger datasets doesn’t quite measure up to dedicated real-time data warehouses.\nConsider ClickBench, an analytics performance benchmark, where we’ve documented the performance of PostgreSQL, its ecosystem extensions, and derivative databases. The untuned PostgreSQL performs poorly (x1050), but it can reach (x47) with optimization. Additionally, there are three analysis-related extensions: columnar store Hydra (x42), time-series TimescaleDB (x103), and distributed Citus (x262).\nClickBench c6a.4xlarge, 500gb gp2 results in relative time\nThis performance can’t be considered bad, especially compared to pure OLTP databases like MySQL and MariaDB (x3065, x19700); however, its third-tier performance is not “good enough,” lagging behind the first-tier OLAP components like Umbra, ClickHouse, Databend, SelectDB (x3~x4) by an order of magnitude. It’s a tough spot - not satisfying enough to use, but too good to discard.\nHowever, the arrival of ParadeDB and DuckDB changed the game!\nParadeDB’s native PG extension pg_analytics achieves second-tier performance (x10), narrowing the gap to the top tier to just 3–4x. Given the additional benefits, this level of performance discrepancy is often acceptable - ACID, freshness and real-time data without ETL, no additional learning curve, no maintenance of separate services, not to mention its ElasticSearch grade full-text search capabilities.\nDuckDB focuses on pure OLAP, pushing analysis performance to the extreme (x3.2) — excluding the academically focused, closed-source database Umbra, DuckDB is arguably the fastest for practical OLAP performance. It’s not a PG extension, but PostgreSQL can fully leverage DuckDB’s analysis performance boost as an embedded file database through projects like DuckDB FDW and pg_quack.\nThe emergence of ParadeDB and DuckDB propels PostgreSQL’s analysis capabilities to the top tier of OLAP, filling the last crucial gap in its analytic performance.\nThe Pendulum of Database Realm The distinction between OLTP and OLAP didn’t exist at the inception of databases. The separation of OLAP data warehouses from databases emerged in the 1990s due to traditional OLTP databases struggling to support analytics scenarios’ query patterns and performance demands.\nFor a long time, best practice in data processing involved using MySQL/PostgreSQL for OLTP workloads and syncing data to specialized OLAP systems like Greenplum, ClickHouse, Doris, Snowflake, etc., through ETL processes.\nDDIA, Martin Kleppmann, ch3, The republic of OLTP \u0026 Kingdom of OLAP\nLike many “specialized databases,” the strength of dedicated OLAP systems often lies in performance — achieving 1-3 orders of magnitude improvement over native PG or MySQL. The cost, however, is redundant data, excessive data movement, lack of agreement on data values among distributed components, extra labor expense for specialized skills, extra licensing costs, limited query language power, programmability and extensibility, limited tool integration, poor data integrity and availability compared with a complete DMBS.\nHowever, as the saying goes, “What goes around comes around”. With hardware improving over thirty years following Moore’s Law, performance has increased exponentially while costs have plummeted. In 2024, a single x86 machine can have hundreds of cores (512 vCPU EPYC 9754x2), several TBs of RAM, a single NVMe SSD can hold up to 64TB, and a single all-flash rack can reach 2PB; object storage like S3 offers virtually unlimited storage.\nHardware advancements have solved the data volume and performance issue, while database software developments (PostgreSQL, ParadeDB, DuckDB) have addressed access method challenges. This puts the fundamental assumptions of the analytics sector — the so-called “big data” industry — under scrutiny.\nAs DuckDB’s manifesto \"Big Data is Dead\" suggests, the era of big data is over. Most people don’t have that much data, and most data is seldom queried. The frontier of big data recedes as hardware and software evolve, rendering “big data” unnecessary for 99% of scenarios.\nIf 99% of use cases can now be handled on a single machine with standalone DuckDB or PostgreSQL (and its replicas), what’s the point of using dedicated analytics components? If every smartphone can send and receive texts freely, what’s the point of pagers? (With the caveat that North American hospitals still use pagers, indicating that maybe less than 1% of scenarios might genuinely need “big data.”)\nThe shift in fundamental assumptions is steering the database world from a phase of diversification back to convergence, from a big bang to a mass extinction. In this process, a new era of unified, multi-modeled, super-converged databases will emerge, reuniting OLTP and OLAP. But who will lead this monumental task of reconsolidating the database field?\nPostgreSQL: The Database World Eater There are a plethora of niches in the database realm: time-series, geospatial, document, search, graph, vector databases, message queues, and object databases. PostgreSQL makes its presence felt across all these domains.\nA case in point is the PostGIS extension, which sets the de facto standard in geospatial databases; the TimescaleDB extension awkwardly positions “generic” time-series databases; and the vector extension, PGVector, turns the dedicated vector database niche into a punchline.\nThis isn’t the first time; we’re witnessing it again in the oldest and largest subdomain: OLAP analytics. But PostgreSQL’s ambition doesn’t stop at OLAP; it’s eyeing the entire database world!\nWhat makes PostgreSQL so capable? Sure, it’s advanced, but so is Oracle; it’s open-source, as is MySQL. PostgreSQL’s edge comes from being both advanced and open-source, allowing it to compete with Oracle/MySQL. But its true uniqueness lies in its extreme extensibility and thriving extension ecosystem.\nTimescaleDB survey: what is the main reason you choose to use PostgreSQL\nPostgreSQL isn’t just a relational database; it’s a data management framework capable of engulfing the entire database galaxy. Besides being open-source and advanced, its core competitiveness stems from extensibility, i.e., its infra’s reusability and extension’s composability.\nThe Magic of Extreme Extensibility PostgreSQL allows users to develop extensions, leveraging the database’s common infra to deliver features at minimal cost. For instance, the vector database extension pgvector, with just several thousand lines of code, is negligible in complexity compared to PostgreSQL’s millions of lines. Yet, this “insignificant” extension achieves complete vector data types and indexing capabilities, outperforming lots of specialized vector databases.\nWhy? Because pgvector’s creators didn’t need to worry about the database’s general additional complexities: ACID, recovery, backup \u0026 PITR, high availability, access control, monitoring, deployment, 3rd-party ecosystem tools, client drivers, etc., which require millions of lines of code to solve well. They only focused on the essential complexity of their problem.\nFor example, ElasticSearch was developed on the Lucene search library, while the Rust ecosystem has an improved next-gen full-text search library, Tantivy, as a Lucene alternative. ParadeDB only needs to wrap and connect it to PostgreSQL’s interface to offer search services comparable to ElasticSearch. More importantly, it can stand on the shoulders of PostgreSQL, leveraging the entire PG ecosystem’s united strength (e.g., mixed searches with PG Vector) to “unfairly” compete with another dedicated database.\nPigsty has 440 extensions available. And there are 1000+ more in the ecosystem\nThe extensibility brings another huge advantage: the composability of extensions, allowing different extensions to work together, creating a synergistic effect where 1+1 » 2. For instance, TimescaleDB can be combined with PostGIS for spatio-temporal data support; the BM25 extension for full-text search can be combined with the PGVector extension, providing hybrid search capabilities.\nFurthermore, the distributive extension Citus can transparently transform a standalone cluster into a horizontally partitioned distributed database cluster. This capability can be orthogonally combined with other features, making PostGIS a distributed geospatial database, PGVector a distributed vector database, ParadeDB a distributed full-text search database, and so on.\nWhat’s more powerful is that extensions evolve independently, without the cumbersome need for main branch merges and coordination. This allows for scaling — PG’s extensibility lets numerous teams explore database possibilities in parallel, with all extensions being optional, not affecting the core functionality’s reliability. Those features that are mature and robust have the chance to be stably integrated into the main branch.\nPostgreSQL achieves both foundational reliability and agile functionality through the magic of extreme extensibility, making it an outlier in the database world and changing the game rules of the database landscape.\nGame Changer in the DB Arena The emergence of PostgreSQL has shifted the paradigms in the database domain: Teams endeavoring to craft a “new database kernel” now face a formidable trial — how to stand out against the open-source, feature-rich Postgres. What’s their unique value proposition?\nUntil a revolutionary hardware breakthrough occurs, the advent of practical, new, general-purpose database kernels seems unlikely. No singular database can match the overall prowess of PG, bolstered by all its extensions — not even Oracle, given PG’s ace of being open-source and free.\nA niche database product might carve out a space for itself if it can outperform PostgreSQL by an order of magnitude in specific aspects (typically performance). However, it usually doesn’t take long before the PostgreSQL ecosystem spawns open-source extension alternatives. Opting to develop a PG extension rather than a whole new database gives teams a crushing speed advantage in playing catch-up!\nFollowing this logic, the PostgreSQL ecosystem is poised to snowball, accruing advantages and inevitably moving towards a monopoly, mirroring the Linux kernel’s status in server OS within a few years. Developer surveys and database trend reports confirm this trajectory.\nStackOverflow 2023 Survey: PostgreSQL, the Decathlete\nStackOverflow’s Database Trends Over the Past 7 Years\nPostgreSQL has long been the favorite database in HackerNews \u0026 StackOverflow. Many new open-source projects default to PostgreSQL as their primary, if not only, database choice. And many new-gen companies are going All in PostgreSQL.\nAs “Radical Simplicity: Just Use Postgres” says, Simplifying tech stacks, reducing components, accelerating development, lowering risks, and adding more features can be achieved by “Just Use Postgres.” Postgres can replace many backend technologies, including MySQL, Kafka, RabbitMQ, ElasticSearch, Mongo, and Redis, effortlessly serving millions of users. Just Use Postgres is no longer limited to a few elite teams but becoming a mainstream best practice.\nWhat Else Can Be Done? The endgame for the database domain seems predictable. But what can we do, and what should we do?\nPostgreSQL is already a near-perfect database kernel for the vast majority of scenarios, making the idea of a kernel “bottleneck” absurd. Forks of PostgreSQL and MySQL that tout kernel modifications as selling points are essentially going nowhere.\nThis is similar to the situation with the Linux OS kernel today; despite the plethora of Linux distros, everyone opts for the same kernel. Forking the Linux kernel is seen as creating unnecessary difficulties, and the industry frowns upon it.\nAccordingly, the main conflict is no longer the database kernel itself but two directions— database extensions and services! The former pertains to internal extensibility, while the latter relates to external composability. Much like the OS ecosystem, the competitive landscape will concentrate on database distributions. In the database domain, only those distributions centered around extensions and services stand a chance for ultimate success.\nKernel remains lukewarm, with MariaDB, the fork of MySQL’s parent, nearing delisting, while AWS, profiting from offering services and extensions on top of the free kernel, thrives. Investment has flowed into numerous PG ecosystem extensions and service distributions: Citus, TimescaleDB, Hydra, PostgresML, ParadeDB, FerretDB, StackGres, Aiven, Neon, Supabase, Tembo, PostgresAI, and our own PG distro — — Pigsty.\nA dilemma within the PostgreSQL ecosystem is the independent evolution of many extensions and tools, lacking a unifier to synergize them. For instance, Hydra releases its own package and Docker image, and so does PostgresML, each distributing PostgreSQL images with their own extensions and only their own. These images and packages are far from comprehensive database services like AWS RDS.\nEven service providers and ecosystem integrators like AWS fall short in front of numerous extensions, unable to include many due to various reasons (AGPLv3 license, security challenges with multi-tenancy), thus failing to leverage the synergistic amplification potential of PostgreSQL ecosystem extensions.\nExtesion Category Pigsty RDS \u0026 PGDG AWS RDS PG Aliyun RDS PG Add Extension Free to Install Not Allowed Not Allowed Geo Spatial PostGIS 3.4.2 PostGIS 3.4.1 PostGIS 3.3.4 Time Series TimescaleDB 2.14.2 Distributive Citus 12.1 AI / ML PostgresML 2.8.1 Columnar Hydra 1.1.1 Vector PGVector 0.6 PGVector 0.6 pase 0.0.1 Sparse Vector PG Sparse 0.5.6 Full-Text Search pg_bm25 0.5.6\nGraph Apache AGE 1.5.0 GraphQL PG GraphQL 1.5.0 Message Queue pgq 3.5.0 OLAP pg_analytics 0.5.6 DuckDB duckdb_fdw 1.1 CDC wal2json 2.5.3 wal2json 2.5 Bloat Control pg_repack 1.5.0 pg_repack 1.5.0 pg_repack 1.4.8 Point Cloud PG PointCloud 1.2.5 Ganos PointCloud 6.1 Many important extensions are not available on Cloud RDS (PG 16, 2024-02-29)\nExtensions are the soul of PostgreSQL. A Postgres without the freedom to use extensions is like cooking without salt, a giant constrained.\nAddressing this issue is one of our primary goals.\nOur Resolution: Pigsty Despite earlier exposure to MySQL Oracle, and MSSQL, when I first used PostgreSQL in 2015, I was convinced of its future dominance in the database realm. Nearly a decade later, I’ve transitioned from a user and administrator to a contributor and developer, witnessing PG’s march toward that goal.\nInteractions with diverse users revealed that the database field’s shortcoming isn’t the kernel anymore — PostgreSQL is already sufficient. The real issue is leveraging the kernel’s capabilities, which is the reason behind RDS’s booming success.\nHowever, I believe this capability should be as accessible as free software, like the PostgreSQL kernel itself — available to every user, not just renting from cyber feudal lords.\nThus, I created Pigsty, a battery-included, local-first PostgreSQL distribution as an open-source RDS Alternative, which aims to harness the collective power of PostgreSQL ecosystem extensions and democratize access to production-grade database services.\nPigsty stands for PostgreSQL in Great STYle, representing the zenith of PostgreSQL.\nWe’ve defined six core propositions addressing the central issues in PostgreSQL database services:\nExtensible Postgres, Reliable Infras, Observable Graphics, Available Services, Maintainable Toolbox, and Composable Modules.\nThe initials of these value propositions offer another acronym for Pigsty:\nPostgres, Infras, Graphics, Service, Toolbox, Yours.\nYour graphical Postgres infrastructure service toolbox.\nExtensible PostgreSQL is the linchpin of this distribution. In the recently launched Pigsty v2.6, we integrated DuckDB FDW and ParadeDB extensions, massively boosting PostgreSQL’s analytical capabilities and ensuring every user can easily harness this power.\nOur aim is to integrate the strengths within the PostgreSQL ecosystem, creating a synergistic force akin to the Ubuntu of the database world. I believe the kernel debate is settled, and the real competitive frontier lies here.\nPostGIS: Provides geospatial data types and indexes, the de facto standard for GIS (\u0026 pgPointCloud, pgRouting). TimescaleDB: Adds time-series, continuous aggregates, distributed, columnar storage, and automatic compression capabilities. PGVector: Support AI vectors/embeddings and ivfflat, hnsw vector indexes (\u0026 pg_sparse for sparse vectors). Citus: Transforms classic master-slave PG clusters into horizontally partitioned distributed database clusters. Hydra: Adds columnar storage and analytics, rivaling ClickHouse’s analytic capabilities. ParadeDB: Elevates full-text search and mixed retrieval to ElasticSearch levels (\u0026 zhparser for Chinese tokenization). Apache AGE: Graph database extension, adding Neo4J-like OpenCypher query support to PostgreSQL. PG GraphQL: Adds native built-in GraphQL query language support to PostgreSQL. DuckDB FDW: Enables direct access to DuckDB’s powerful embedded analytic database files through PostgreSQL (\u0026 DuckDB CLI). Supabase: An open-source Firebase alternative based on PostgreSQL, providing a complete app development storage solution. FerretDB: An open-source MongoDB alternative based on PostgreSQL, compatible with MongoDB APIs/drivers. PostgresML: Facilitates classic machine learning algorithms, calling, deploying, and training AI models with SQL. Developers, your choices will shape the future of the database world. I hope my work helps you better utilize the world’s most advanced open-source database kernel: PostgreSQL.\nRead in Pigsty’s Blog | GitHub Repo: Pigsty | Official Website\n","categories":"","description":"","excerpt":"PostgreSQL isn’t just a simple relational database; it’s a data …","ref":"/blog/pg/pg-eat-db-world/","tags":["PostgreSQL","Ecosystem"],"title":"Postgres is eating the database world"},{"body":"\nPigsty v2.6 makes PostgreSQL 16.2 the default, introduces ParadeDB and DuckDB support, and brings epic-level OLAP improvements. Read more\n","categories":"","description":"Pigsty v2.6 makes PostgreSQL 16.2 the default, introduces ParadeDB and DuckDB support, and brings epic-level OLAP improvements.\n","excerpt":"Pigsty v2.6 makes PostgreSQL 16.2 the default, introduces ParadeDB and …","ref":"/blog/pigsty/v2.6/","tags":["Pigsty"],"title":"Pigsty v2.6: PostgreSQL Crashes the OLAP Party"},{"body":"\nWhether production databases should be containerized remains a controversial topic. From a DBA’s perspective, I believe that currently, putting production databases in Docker is still a bad idea. Read more\n","categories":"","description":"Whether **production databases** should be containerized remains a controversial topic. From a DBA's perspective, I believe that **currently**, putting production databases in Docker is still a bad idea.\n","excerpt":"Whether **production databases** should be containerized remains a …","ref":"/blog/pg/just-use-pg/","tags":["PostgreSQL","PG-Ecosystem"],"title":"Technical Minimalism: Just Use PostgreSQL for Everything"},{"body":"\nParadeDB aims to be an Elasticsearch alternative: “Modern Elasticsearch Alternative built on Postgres” — PostgreSQL for search and analytics. Read more\n","categories":"","description":"ParadeDB aims to be an Elasticsearch alternative: \"Modern Elasticsearch Alternative built on Postgres\" — PostgreSQL for search and analytics.\n","excerpt":"ParadeDB aims to be an Elasticsearch alternative: \"Modern …","ref":"/blog/pg/paradedb/","tags":["PostgreSQL","PG-Ecosystem","Extension"],"title":"New PostgreSQL Ecosystem Player: ParadeDB"},{"body":"\nTwo days ago, the ninth episode of Open-Source Talks had the theme “Will DBAs Be Eliminated by Cloud?” As the host, I restrained myself from jumping into the debate throughout, so I’m writing this article to discuss this question: Will DBAs be eliminated by cloud? Read more\n","categories":"","description":"Two days ago, the ninth episode of Open-Source Talks had the theme \"Will DBAs Be Eliminated by Cloud?\" As the host, I restrained myself from jumping into the debate throughout, so I'm writing this article to discuss this question: Will DBAs be eliminated by cloud?  \n","excerpt":"Two days ago, the ninth episode of Open-Source Talks had the theme …","ref":"/blog/cloud/dba-vs-rds/","tags":["Cloud-Exit","DBA","RDS"],"title":"Will DBAs Be Eliminated by Cloud?"},{"body":"\nThis article describes how Cloudflare scaled to support 55 million requests per second using 15 PostgreSQL clusters, and PostgreSQL’s scalability performance. Read more\n","categories":"","description":"This article describes how Cloudflare scaled to support 55 million requests per second using 15 PostgreSQL clusters, and PostgreSQL's scalability performance.\n","excerpt":"This article describes how Cloudflare scaled to support 55 million …","ref":"/blog/pg/pg-scalability/","tags":["PostgreSQL","Performance"],"title":"PostgreSQL's Impressive Scalability"},{"body":"\nProgrammers are drawn to complexity like moths to flame. The more complex the system architecture diagram, the greater the intellectual masturbation high. Steadfast resistance to this behavior is a key reason for DHH’s success in cloud-free availability. Read more\n","categories":"","description":"Programmers are drawn to complexity like moths to flame. The more complex the system architecture diagram, the greater the intellectual masturbation high. Steadfast resistance to this behavior is a key reason for DHH's success in cloud-free availability.\n","excerpt":"Programmers are drawn to complexity like moths to flame. The more …","ref":"/blog/cloud/uptime/","tags":["Cloud-Exit"],"title":"Cloud-Exit High Availability Secret: Rejecting Complexity Masturbation"},{"body":" Author: Jonathan Katz (PostgreSQL Core Team) | Original: PostgreSQL 2024\nPostgreSQL core team member Jonathan Katz’s outlook for PostgreSQL in 2024, reviewing the progress made over the past few years. Read more\n","categories":"","description":"PostgreSQL core team member Jonathan Katzs outlook for PostgreSQL in 2024, reviewing the progress made over the past few years.\n","excerpt":"PostgreSQL core team member Jonathan Katzs outlook for PostgreSQL in …","ref":"/blog/pg/pg-in-2024/","tags":["PostgreSQL","PG-Ecosystem"],"title":"PostgreSQL Outlook for 2024"},{"body":"\nDB-Engines officially announced today that PostgreSQL has once again been crowned “Database of the Year.” This is the fifth time PG has received this honor in the past seven years. If not for Snowflake stealing the spotlight for two years, the database world would have almost become a PostgreSQL solo show. Read more\n","categories":"","description":"DB-Engines officially announced today that PostgreSQL has once again been crowned \"Database of the Year.\" This is the fifth time PG has received this honor in the past seven years. If not for Snowflake stealing the spotlight for two years, the database world would have almost become a PostgreSQL solo show.\n","excerpt":"DB-Engines officially announced today that PostgreSQL has once again …","ref":"/blog/pg/pg-dbeng-2024/","tags":["PostgreSQL","PG-Ecosystem"],"title":"PostgreSQL Wins 2024 Database of the Year Award! (Fifth Time)"},{"body":"\nMySQL’s transaction ACID has flaws and doesn’t match documentation promises. This may lead to serious correctness issues - use with caution. Read more\n","categories":"","description":"MySQL's transaction ACID has flaws and doesn't match documentation promises. This may lead to serious correctness issues - use with caution.\n","excerpt":"MySQL's transaction ACID has flaws and doesn't match documentation …","ref":"/blog/db/bad-mysql/","tags":["Database","MySQL"],"title":"How Can MySQL's Correctness Be This Garbage?"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/cloud/","tags":"","title":"Cloud"},{"body":"\nS3 is no longer “cheap” with the evolution of hardware, and other challengers such as cloudflare R2. Read more\n","categories":"","description":"S3 is no longer \"cheap\" with the evolution of hardware, and other challengers such as cloudflare R2.\n","excerpt":"S3 is no longer \"cheap\" with the evolution of hardware, and other …","ref":"/blog/cloud/s3/","tags":["Cloud","Cloud-Exit","S3","MinIO"],"title":"S3: Elite to Mediocre"},{"body":" Author: DHH (David Heinemeier Hansson) | Original: Cloud Exit FAQ\nDHH’s cloud exit journey has reached a new stage, saving nearly a million dollars so far with potential savings of nearly ten million over the next five years. Read more\n","categories":"","description":"DHHs cloud exit journey has reached a new stage, saving nearly a million dollars so far with potential savings of nearly ten million over the next five years.\n","excerpt":"DHHs cloud exit journey has reached a new stage, saving nearly a …","ref":"/blog/cloud/cloud-exit-faq/","tags":["Cloud-Exit","DHH"],"title":"Cloud Exit FAQ: DHH Saves Millions"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/dhh/","tags":"","title":"DHH"},{"body":"Whether databases should be housed in Kubernetes/Docker remains highly controversial. While Kubernetes (k8s) excels in managing stateless applications, it has fundamental drawbacks with stateful services, especially databases like PostgreSQL and MySQL.\nIn the previous article, “Databases in Docker: Good or Bad,” we discussed the pros and cons of containerizing databases. Today, let’s delve into the trade-offs in orchestrating databases in K8S and explore why it’s not a wise decision.\nSummary Kubernetes (k8s) is an exceptional container orchestration tool aimed at helping developers better manage a vast array of complex stateless applications. Despite its offerings like StatefulSet, PV, PVC, and LocalhostPV for supporting stateful services (i.e., databases), these features are still insufficient for running production-level databases that demand higher reliability.\nDatabases are more like “pets” than “cattle” and require careful nurturing. Treating databases as “cattle” in K8S essentially turns external disk/file system/storage services into new “database pets.” Running databases on EBS/network storage presents significant disadvantages in reliability and performance. However, using high-performance local NVMe disks will make the database bound to nodes and non-schedulable, negating the primary purpose of putting them in K8S.\nPlacing databases in K8S results in a “lose-lose” situation - K8S loses its simplicity in statelessness, lacking the flexibility to quickly relocate, schedule, destroy, and rebuild like purely stateless use. On the other hand, databases suffer several crucial attributes: reliability, security, performance, and complexity costs, in exchange for limited “elasticity” and utilization - something virtual machines can also achieve. For users outside public cloud vendors, the disadvantages far outweigh the benefits.\nThe “cloud-native frenzy,” exemplified by K8S, has become a distorted phenomenon: adopting k8s for the sake of k8s. Engineers add extra complexity to increase their irreplaceability, while managers fear being left behind by the industry and getting caught up in deployment races. Using tanks for tasks that could be done with bicycles, to gain experience or prove oneself, without considering if the problem needs such “dragon-slaying” techniques - this kind of architectural juggling will eventually lead to adverse outcomes.\nUntil the reliability and performance of the network storage surpass local storage, placing databases in K8S is an unwise choice. There are other ways to seal the complexity of database management, such as RDS and open-source RDS solutions like Pigsty, which are based on bare Metal or bare OS. Users should make wise decisions based on their situations and needs, carefully weighing the pros and cons.\nThe Status Quo K8S excels in orchestrating stateless application services but was initially limited to stateful services. Despite not being the intended purpose of K8S and Docker, the community’s zeal for expansion has been unstoppable. Evangelists depict K8S as the next-generation cloud operating system, asserting that databases will inevitably become regular applications within Kubernetes. Various abstractions have emerged to support stateful services: StatefulSet, PV, PVC, and LocalhostPV.\nCountless cloud-native enthusiasts have attempted to migrate existing databases into K8S, resulting in a proliferation of CRDs and Operators for databases. Taking PostgreSQL as an example, there are already more than ten different K8S deployment solutions available: PGO, StackGres, CloudNativePG, PostgresOperator, PerconaOperator, CYBERTEC-pg-operator, TemboOperator, Kubegres, KubeDB, KubeBlocks, and so on. The CNCF landscape rapidly expands, turning into a playground of complexity.\nHowever, complexity is a cost. With “cost reduction” becoming mainstream, voices of reflection have begun to emerge. Could-Exit Pioneers like DHH, who deeply utilized K8S in public clouds, abandoned it due to its excessive complexity during the transition to self-hosted open-source solutions, relying only on Docker and a Ruby tool named Kamal as alternatives. Many began to question whether stateful services like databases suit Kubernetes.\nK8S itself, in its effort to support stateful applications, has become increasingly complex, straying from its original intention as a container orchestration platform. Tim Hockin, a co-founder of Kubernetes, also voiced his rare concerns at this year’s KubeCon in “K8s is Cannibalizing Itself!”: “Kubernetes has become too complex; it needs to learn restraint, or it will stop innovating and lose its base.”\nLose-Lose Situation In the cloud-native realm, the analogy of “pets” versus “cattle” is often used for illustrating stateful services. “Pets,” like databases, need careful and individual care, while “cattle” represent disposable, stateless applications (Disposability).\nCloud Native Applications 12 Factors: Disposability\nOne of the leading architectural goals of K8S is to treat what can be treated as cattle as cattle. The attempt to “separate storage from computation” in databases follows this strategy: splitting stateful database services into state storage outside K8S and pure computation inside K8S. The state is stored on the EBS/cloud disk/distributed storage service, allowing the “stateless” database part to be freely created, destroyed, and scheduled in K8S.\nUnfortunately, databases, especially OLTP databases, heavily depend on disk hardware, and network storage’s reliability and performance still lag behind local disks by orders of magnitude. Thus, K8S offers the LocalhostPV option, allowing containers to use data volumes directly lies on the host operating system, utilizing high-performance/high-reliability local NVMe disk storage.\nHowever, this presents a dilemma: should one use subpar cloud disks and tolerate poor database reliability/performance for K8S’s scheduling and orchestration capabilities? Or use high-performance local disks tied to host nodes, virtually losing all flexible scheduling abilities? The former is like stuffing an anchor into K8S’s small boat, slowing overall speed and agility; the latter is like anchoring and pinning the ship to a specific point.\nRunning a stateless K8S cluster is simple and reliable, as is running a stateful database on a physical machine’s bare operating system. Mixing the two, however, results in a lose-lose situation: K8S loses its stateless flexibility and casual scheduling abilities, while the database sacrifices core attributes like reliability, security, efficiency, and simplicity in exchange for elasticity, resource utilization, and Day1 delivery speed that are not fundamentally important to databases.\nA vivid example of the former is the performance optimization of PostgreSQL@K8S, which KubeBlocks contributed. K8S experts employed various advanced methods to solve performance issues that did not exist on bare metal/bare OS at all. A fresh case of the latter is Didi’s K8S architecture juggling disaster; if it weren’t for putting the stateful MySQL in K8S, would rebuilding a stateless K8S cluster and redeploying applications take 12 hours to recover?\nPros and Cons For serious technology decisions, the most crucial aspect is weighing the pros and cons. Here, in the order of “quality, security, performance, cost,” let’s discuss the technical trade-offs of placing databases in K8S versus classic bare metal/VM deployments. I don’t want to write a comprehensive paper that covers everything. Instead, I’ll throw some specific questions for consideration and discussion.\nQuality\nK8S, compared to physical deployments, introduces additional failure points and architectural complexity, increasing the blast radius and significantly prolonging the average recovery time of failures. In “Is it a Good Idea to Put Databases into Docker?”, we provided an argument about reliability, which can also apply to Kubernetes — K8S and Docker introduce additional and unnecessary dependencies and failure points to databases, lacking community failure knowledge accumulation and reliability track record (MTTR/MTBF).\nIn the cloud vendor classification system, K8S belongs to PaaS, while RDS belongs to a more fundamental layer, IaaS. Database services have higher reliability requirements than K8S; for instance, many companies’ cloud management platforms rely on an additional CMDB database. Where should this database be placed? You shouldn’t let K8S manage things it depends on, nor should you add unnecessary extra dependencies. The Alibaba Cloud global epic failure and Didi’s K8S architecture juggling disaster have taught us this lesson. Moreover, maintaining a separate database system inside K8S when there’s already one outside is even more unjustifiable.\nSecurity\nThe database in a multi-tenant environment introduces additional attack surfaces, bringing higher risks and more complex audit compliance challenges. Does K8S make your database more secure? Maybe the complexity of K8S architecture juggling will deter script kiddies unfamiliar with K8S, but for real attackers, more components and dependencies often mean a broader attack surface.\nIn “BrokenSesame Alibaba Cloud PostgreSQL Vulnerability Technical Details”, security personnel escaped to the K8S host node using their own PostgreSQL container and accessed the K8S API and other tenants’ containers and data. This is clearly a K8S-specific issue — the risk is real, such attacks have occurred, and even Alibaba Cloud, a local cloud industry leader, has been compromised.\nSource: The Attacker Perspective - Insights From Hacking Alibaba Cloud\nPerformance\nAs stated in “Is it a Good Idea to Put Databases into Docker?”, whether it’s additional network overhead, Ingress bottlenecks, or underperforming cloud disks, all negatively impact database performance. For example, as revealed in “PostgreSQL@K8s Performance Optimization” — you need a considerable level of technical prowess to make database performance in K8S barely match that on bare metal.\nNote: Latency is measured in ms, not µs — a significant performance difference.\nAnother misconception about efficiency is resource utilization. Unlike offline analytical businesses, critical online OLTP databases should not aim to increase resource utilization but rather deliberately lower it to enhance system reliability and user experience. If there are many fragmented businesses, resource utilization can be improved through PDB/shared database clusters. K8S’s advocated elasticity efficiency is not unique to it — KVM/EC2 can also effectively address this issue.\nIn terms of cost, K8S and various Operators provide a decent abstraction, encapsulating some of the complexity of database management, which is attractive for teams without DBAs. However, the complexity reduced by using it to manage databases pales in comparison to the complexity introduced by using K8S itself. For instance, random IP address drifts and automatic Pod restarts may not be a big issue for stateless applications, but for databases, they are intolerable — many companies have had to attempt to modify kubelet to avoid this behavior, thereby introducing more complexity and maintenance costs.\nAs stated in “From Reducing Costs and Smiles to Reducing Costs and Efficiency” “Reducing Complexity Costs” section: Intellectual power is hard to accumulate spatially: when a database encounters problems, it needs database experts to solve them; when Kubernetes has problems, it needs K8S experts to look into them; however, when you put a database into Kubernetes, complexities combine, the state space explodes, but the intellectual bandwidth of individual database experts and K8S experts is hard to stack — you need a dual expert to solve the problem, and such experts are undoubtedly much rarer and more expensive than pure database experts. Such architectural juggling is enough to cause major setbacks for most teams, including top public clouds/big companies, in the event of a failure.\nThe Cloud-Native Frenzy An interesting question arises: if K8S is unsuitable for stateful databases, why are so many companies, including big players, rushing to do this? The reasons are not technical.\nGoogle open-sourced its K8S battleship, modeled after its internal Borg spaceship, and managers, fearing being left behind, rushed to adopt it, thinking using K8S would put them on par with Google. Ironically, Google doesn’t use K8S; it was more likely to disrupt AWS and mislead the industry. However, most companies don’t have the manpower like Google to operate such a battleship. More importantly, their problems might need a simple vessel. Running MySQL + PHP, PostgreSQL + Go/Python on bare metal has already taken many companies to IPO.\nUnder modern hardware conditions, the complexity of most applications throughout their lifecycle doesn’t justify using K8S. Yet, the “cloud-native” frenzy, epitomized by K8S, has become a distorted phenomenon: adopting k8s just for the sake of k8s. Some engineers are looking for “advanced” and “cool” technologies used by big companies to fulfill their personal goals like job hopping or promotions or to increase their job security by adding complexity, not considering if these “dragon-slaying” techniques are necessary for solving their problems.\nThe cloud-native landscape is filled with fancy projects. Every new development team wants to introduce something new: Helm today, Kubevela tomorrow. They talk big about bright futures and peak efficiency, but in reality, they create a mountain of architectural complexities and a playground for “YAML Boys” - tinkering with the latest tech, inventing concepts, earning experience and reputation at the expense of users who bear the complexity and maintenance costs.\nCNCF Landscape: A sprawling ecosystem of complexity\nThe cloud-native movement’s philosophy is compelling - democratizing the elastic scheduling capabilities of public clouds for every user. K8S indeed excels in stateless applications. However, excessive enthusiasm has led K8S astray from its original intent and direction - simply doing well in orchestrating stateless applications, burdened by the ill-conceived support for stateful applications.\nMaking Wise Decisions Years ago, when I first encountered K8S, I too was fervent —— It was at TanTan. We had over twenty thousand cores and hundreds of database clusters, and I was eager to try putting databases in Kubernetes and testing all the available Operators. However, after two to three years of extensive research and architectural design, I calmed down and abandoned this madness. Instead, I architected our database service based on bare metal/operating systems. For us, the benefits K8S brought to databases were negligible compared to the problems and hassles it introduced.\nShould databases be put into K8S? It depends: for public cloud vendors who thrive on overselling resources, elasticity and utilization are crucial, which are directly linked to revenue and profit, While reliability and performance take a back seat - after all, an availability below three nines means compensating 25% monthly credit. But for most user, including ourselves, these trade-offs hold different: One-time Day1 Setup, elasticity, and resource utilization aren’t their primary concerns; reliability, performance, Day2 Operation costs, these core database attributes are what matter most.\nWe open-sourced our database service architecture — an out-of-the-box PostgreSQL distribution and a local-first RDS alternative: Pigsty. We didn’t choose the so-called “build once, run anywhere” approach of K8S and Docker. Instead, we adapted to different OS distros \u0026 major versions, and used Ansible to achieve a K8S CRD IaC-like API to seal management complexity. This was arduous, but it was the right thing to do - the world does not need another clumsy attempt at putting PostgreSQL into K8S. Still, it does need a production database service architecture that maximizes hardware performance and reliability.\nPigsty vs StackGres: Different approaches to PostgreSQL deployment\nPerhaps one day, when the reliability and performance of distributed network storage surpass local storage and mainstream databases have some native support for storage-computation separation, things might change again — K8S might become suitable for databases. But for now, I believe putting serious production OLTP databases into K8S is immature and inappropriate. I hope readers will make wise choices on this matter.\nReference Database in Docker: Is that a good idea?\n《Kubernetes is Rotten!》\n《Curse of Docker?》\n《What can we learn from DiDi’s Epic k8s Failure》\n《PostgreSQL@K8s Performance Optimization》\n《Running Database on Kubernetes》\n","categories":"","description":"","excerpt":"Whether databases should be housed in Kubernetes/Docker remains highly …","ref":"/blog/db/db-in-k8s/","tags":["Database","Kubernetes"],"title":"Database in K8S: Pros \u0026 Cons"},{"body":"\nWhether databases should be housed in Kubernetes/Docker remains highly controversial. While K8s excels in managing stateless applications, it has fundamental drawbacks with stateful services like databases. Read more\n","categories":"","description":"Whether databases should be housed in Kubernetes/Docker remains highly controversial. While K8s excels in managing stateless applications, it has fundamental drawbacks with stateful services like databases.\n","excerpt":"Whether databases should be housed in Kubernetes/Docker remains highly …","ref":"/blog/select/db-in-k8s/","tags":["Database","Kubernetes"],"title":"Database in K8S: Pros \u0026 Cons"},{"body":"\nPigsty v2.5 adds Ubuntu/Debian support (bullseye, bookworm, jammy, focal), new extensions including pointcloud and imgsmlr, and redesigned monitoring dashboards. Read more\n","categories":"","description":"Pigsty v2.5 adds Ubuntu/Debian support (bullseye, bookworm, jammy, focal), new extensions including pointcloud and imgsmlr, and redesigned monitoring dashboards.\n","excerpt":"Pigsty v2.5 adds Ubuntu/Debian support (bullseye, bookworm, jammy, …","ref":"/blog/pigsty/v2.5/","tags":["Pigsty"],"title":"Pigsty v2.5: Ubuntu \u0026 PG16"},{"body":"\nAlibaba-Cloud and Didi had major outages one after another. This article discusses how to move from cost-reduction jokes to real cost reduction and efficiency — what costs should we really reduce, what efficiency should we improve? Read more\n","categories":"","description":"Alibaba-Cloud and Didi had major outages one after another. This article discusses how to move from cost-reduction jokes to real cost reduction and efficiency — what costs should we really reduce, what efficiency should we improve?\n","excerpt":"Alibaba-Cloud and Didi had major outages one after another. This …","ref":"/blog/cloud/smile/","tags":["Cloud-Exit","Alibaba-Cloud","Cloud-Outage"],"title":"From Cost-Reduction Jokes to Real Cost Reduction and Efficiency"},{"body":"\nVector storage and retrieval is a real need, but specialized vector databases are already dead. Small needs are solved by OpenAI directly, standard needs are captured by existing mature databases with vector extensions. The ecological niche left for specialized vector databases might support one company, but trying to build an industry around AI stories is impossible. Read more\n","categories":"","description":"Vector storage and retrieval is a real need, but specialized vector databases are already dead. Small needs are solved by OpenAI directly, standard needs are captured by existing mature databases with vector extensions. The ecological niche left for specialized vector databases might support one company, but trying to build an industry around AI stories is impossible.\n","excerpt":"Vector storage and retrieval is a real need, but specialized vector …","ref":"/blog/db/svdb-is-dead/","tags":["Database","Vector-Database"],"title":"Are Specialized Vector Databases Dead?"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/vector-database/","tags":"","title":"Vector-Database"},{"body":"\nHardware is interesting again, developments in CPUs and SSDs remain largely unnoticed by the majority of devs. A whole generation of developers is obscured by cloud hype and marketing noise. Read more\n","categories":"","description":"Hardware is interesting again, developments in CPUs and SSDs remain largely unnoticed by the majority of devs. A whole generation of developers is obscured by cloud hype and marketing noise. \n","excerpt":"Hardware is interesting again, developments in CPUs and SSDs remain …","ref":"/blog/cloud/bonus/","tags":["Cloud","Cloud-Exit","Hardware"],"title":"Reclaim Hardware Bonus from the Cloud"},{"body":"\nAlibaba-Cloud’s epic global outage after Double 11 set an industry record. How should we evaluate this incident, and what lessons can we learn from it? Read more\n","categories":"","description":"Alibaba-Cloud's epic global outage after Double 11 set an industry record. How should we evaluate this incident, and what lessons can we learn from it?\n","excerpt":"Alibaba-Cloud's epic global outage after Double 11 set an industry …","ref":"/blog/cloud/aliyun/","tags":["Cloud-Exit","Alibaba-Cloud","Cloud-Outage"],"title":"What Can We Learn from Alibaba-Cloud's Global Outage?"},{"body":"\nAlibaba-Cloud’s Double 11 offered a great deal: 2C2G3M ECS servers for ¥99/year, low price for three years. This article shows how to use this decent ECS to build your own digital homestead. Read more\n","categories":"","description":"Alibaba-Cloud's Double 11 offered a great deal: 2C2G3M ECS servers for ¥99/year, low price for three years. This article shows how to use this decent ECS to build your own digital homestead.\n","excerpt":"Alibaba-Cloud's Double 11 offered a great deal: 2C2G3M ECS servers for …","ref":"/blog/cloud/cheap-ecs/","tags":["Cloud-Exit","Alibaba-Cloud","ECS"],"title":"Harvesting Alibaba-Cloud Wool, Building Your Digital Homestead"},{"body":"\nMany “domestic databases” are just shoddy, inferior products that can’t be helped. Xinchuang domestic OS/databases are essentially IT pre-made meals in schools. Users hold their noses while migrating, developers pretend to work hard, and everyone plays along with leaders who neither understand nor care about technology. The infrastructure software industry isn’t being strangled by anyone - the real chokehold comes from the so-called “insiders.” Read more\n","categories":"","description":"Many \"domestic databases\" are just shoddy, inferior products that can't be helped. Xinchuang domestic OS/databases are essentially **IT pre-made meals in schools**. Users hold their noses while migrating, developers pretend to work hard, and everyone plays along with leaders who neither understand nor care about technology. The infrastructure software industry isn't being strangled by anyone - the real chokehold comes from the so-called \"insiders.\"\n","excerpt":"Many \"domestic databases\" are just shoddy, inferior products that …","ref":"/blog/db/db-choke/","tags":["Database","Domestic-Database"],"title":"Are Databases Really Being Strangled?"},{"body":"\nQuery optimization is one of the core responsibilities of DBAs. This article introduces how to use metrics provided by pg_stat_statements for macro-level PostgreSQL query optimization. Read more\n","categories":"","description":"**Query optimization** is one of the core responsibilities of DBAs. This article introduces how to use metrics provided by `pg_stat_statements` for macro-level PostgreSQL query optimization.\n","excerpt":"**Query optimization** is one of the core responsibilities of DBAs. …","ref":"/blog/pg/pgss/","tags":["PostgreSQL","PG-Admin","Performance"],"title":"PostgreSQL Macro Query Optimization with pg_stat_statements"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/operating-system/","tags":"","title":"Operating-System"},{"body":"\nRHEL-series OS distribution compatibility level: RHEL = Rocky ≈ Anolis \u003e Alma \u003e Oracle » Euler. Recommend using RockyLinux 8.8, or Anolis 8.8 for domestic requirements. Read more\n","categories":"","description":"RHEL-series OS distribution compatibility level: RHEL = Rocky ≈ Anolis \u003e Alma \u003e Oracle \u003e\u003e Euler. Recommend using RockyLinux 8.8, or Anolis 8.8 for domestic requirements.\n","excerpt":"RHEL-series OS distribution compatibility level: RHEL = Rocky ≈ Anolis …","ref":"/blog/db/rhel-compatibility/","tags":["Database","Operating-System"],"title":"Which EL-Series OS Distribution Is Best?"},{"body":"\nFerretDB aims to provide a truly open-source MongoDB alternative based on PostgreSQL. Read more\n","categories":"","description":"FerretDB aims to provide a truly open-source MongoDB alternative based on PostgreSQL.\n","excerpt":"FerretDB aims to provide a truly open-source MongoDB alternative based …","ref":"/blog/pg/ferretdb/","tags":["PostgreSQL","PG-Ecosystem","MongoDB","Extension"],"title":"FerretDB: PostgreSQL Disguised as MongoDB"},{"body":"\nPigsty v2.4 delivers PostgreSQL 16 GA support, RDS/PolarDB monitoring, Redis Sentinel HA, and a wave of new extensions including Apache AGE, zhparser, and pg_embedding. Read more\n","categories":"","description":"Pigsty v2.4 delivers PostgreSQL 16 GA support, RDS/PolarDB monitoring, Redis Sentinel HA, and a wave of new extensions including Apache AGE, zhparser, and pg_embedding.\n","excerpt":"Pigsty v2.4 delivers PostgreSQL 16 GA support, RDS/PolarDB monitoring, …","ref":"/blog/pigsty/v2.4/","tags":["Pigsty"],"title":"Pigsty v2.4: Monitor Cloud RDS"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/data-corruption/","tags":"","title":"Data-Corruption"},{"body":"\nBackups are a DBA’s lifeline — but what if your PostgreSQL database has already exploded and you have no backups? Maybe pg_filedump can help you! Read more\n","categories":"","description":"Backups are a DBA's lifeline — but what if your PostgreSQL database has already exploded and you have no backups? Maybe `pg_filedump` can help you!\n","excerpt":"Backups are a DBA's lifeline — but what if your PostgreSQL database …","ref":"/blog/pg/pg-filedump/","tags":["PostgreSQL","PG-Admin","Data-Corruption","Incident-Report"],"title":"How to Use pg_filedump for Data Recovery?"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/incident-report/","tags":"","title":"Incident-Report"},{"body":"\nWhen we talk about self-reliance and control, what are we really talking about? Operational self-reliance vs. R\u0026D self-reliance - what nations/users truly need is the former, not flashy “self-research”. Read more\n","categories":"","description":"When we talk about self-reliance and control, what are we really talking about? Operational self-reliance vs. R\u0026D self-reliance - what nations/users truly need is the former, not flashy \"self-research\".\n","excerpt":"When we talk about self-reliance and control, what are we really …","ref":"/blog/db/sovereign-dbos/","tags":["Database","Domestic-Database"],"title":"What Kind of Self-Reliance Do Infrastructure Software Need?"},{"body":"\nPigsty v2.3 adds FerretDB MongoDB support, NocoDB integration, L2 VIP for node clusters, PostgreSQL security patches, and Redis 7.2. Read more\n","categories":"","description":"Pigsty v2.3 adds FerretDB MongoDB support, NocoDB integration, L2 VIP for node clusters, PostgreSQL security patches, and Redis 7.2.\n","excerpt":"Pigsty v2.3 adds FerretDB MongoDB support, NocoDB integration, L2 VIP …","ref":"/blog/pigsty/v2.3/","tags":["Pigsty"],"title":"Pigsty v2.3: Richer App Ecosystem"},{"body":" Author: Jonathan Katz (PostgreSQL Core Team) | Original: Vectors are the new JSON in PostgreSQL\nVectors will become a key element in building applications, just like JSON historically. PostgreSQL leads the AI era with vector extensions. Read more\n","categories":"","description":"Vectors will become a key element in building applications, just like JSON historically. PostgreSQL leads the AI era with vector extensions.\n","excerpt":"Vectors will become a key element in building applications, just like …","ref":"/blog/pg/vector-json-pg/","tags":["PostgreSQL","Vector-DB","Extension"],"title":"Vector is the New JSON"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/vector-db/","tags":"","title":"Vector-DB"},{"body":"\nPigsty v2.2 delivers a complete monitoring dashboard overhaul built on Grafana 10, a 42-node production simulation sandbox, Pigsty’s own RPM repos, and UOS compatibility. Read more\n","categories":"","description":"Pigsty v2.2 delivers a complete monitoring dashboard overhaul built on Grafana 10, a 42-node production simulation sandbox, Pigsty's own RPM repos, and UOS compatibility.\n","excerpt":"Pigsty v2.2 delivers a complete monitoring dashboard overhaul built on …","ref":"/blog/pigsty/v2.2/","tags":["Pigsty"],"title":"Pigsty v2.2: Monitoring System Reborn"},{"body":"\nOnce upon a time, “going to cloud” was almost politically correct in tech circles, but few people use hard data to analyze the trade-offs involved. I’m willing to be this skeptic: let me use hard data and personal stories to explain the traps and value of public cloud rental models. Read more\n","categories":"","description":"Once upon a time, \"going to cloud\" was almost politically correct in tech circles, but few people use hard data to analyze the trade-offs involved. I'm willing to be this skeptic: let me use hard data and personal stories to explain the traps and value of public cloud rental models.\n","excerpt":"Once upon a time, \"going to cloud\" was almost politically correct in …","ref":"/blog/cloud/debris/","tags":["Cloud-Exit"],"title":"Cloud Computing Mudslide: Deconstructing Public Cloud with Data"},{"body":" Author: DHH (David Heinemeier Hansson) | Original: DHH’s Hey Blog\nThis article chronicles the complete journey of 37Signals moving off the cloud, led by DHH. Valuable reference for both cloud-bound and cloud-native enterprises. Read more\n","categories":"","description":"This article chronicles the complete journey of 37Signals moving off the cloud, led by DHH. Valuable reference for both cloud-bound and cloud-native enterprises.\n","excerpt":"This article chronicles the complete journey of 37Signals moving off …","ref":"/blog/cloud/odyssey/","tags":["Cloud-Exit","DHH"],"title":"Cloud Exit Odyssey: Time to Leave Cloud?"},{"body":"\nDHH migrated their seven cloud applications from AWS to their own hardware. 2024 is the first year of full savings realization. They’re delighted to find the savings exceed initial estimates. Read more\n","categories":"","description":"DHH migrated their seven cloud applications from AWS to their own hardware. 2024 is the first year of full savings realization. They're delighted to find the savings exceed initial estimates.\n","excerpt":"DHH migrated their seven cloud applications from AWS to their own …","ref":"/blog/cloud/odyssey-done/","tags":["Cloud-Exit","DHH"],"title":"DHH: Cloud-Exit Saves Over Ten Million, More Than Expected!"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/finops/","tags":"","title":"FinOps"},{"body":"\nAt the SACC 2023 FinOps session, I fiercely criticized cloud vendors. This is a transcript of my speech, introducing the ultimate FinOps concept — Cloud-Exit and its implementation path. Read more\n","categories":"","description":"At the SACC 2023 FinOps session, I fiercely criticized cloud vendors. This is a transcript of my speech, introducing the ultimate FinOps concept — Cloud-Exit and its implementation path.\n","excerpt":"At the SACC 2023 FinOps session, I fiercely criticized cloud vendors. …","ref":"/blog/cloud/finops/","tags":["Cloud","Cloud-Exit","FinOps"],"title":"FinOps: Endgame Cloud-Exit"},{"body":"\nStackOverflow 2023 Survey shows PostgreSQL is the most popular, loved, and wanted database, solidifying its status as the ‘Linux of Database’. Read more\n","categories":"","description":"StackOverflow 2023 Survey shows PostgreSQL is the most popular, loved, and wanted database, solidifying its status as the 'Linux of Database'.","excerpt":"StackOverflow 2023 Survey shows PostgreSQL is the most popular, loved, …","ref":"/blog/pg/pg-is-no1/","tags":["PostgreSQL","PG-Ecosystem"],"title":"PostgreSQL, The most successful database"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/business/","tags":"","title":"Business"},{"body":"\nPublic cloud margins worse than sand mining—why are pig-butchering schemes losing money? Resource-selling models heading toward price wars, open source alternatives breaking monopoly dreams! Service competitiveness gradually neutralized—where is the cloud computing industry heading? How did domestic cloud vendors make a business with 30-40% pure profit less profitable than sand mining? Read more\n","categories":"","description":"Public cloud margins worse than sand mining—why are pig-butchering schemes losing money? Resource-selling models heading toward price wars, open source alternatives breaking monopoly dreams! Service competitiveness gradually neutralized—where is the cloud computing industry heading? How did domestic cloud vendors make a business with 30-40% pure profit less profitable than sand mining?\n","excerpt":"Public cloud margins worse than sand mining—why are pig-butchering …","ref":"/blog/cloud/profit/","tags":["Cloud-Exit","Business"],"title":"Why Isn't Cloud Computing More Profitable Than Sand Mining?"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/sla/","tags":"","title":"SLA"},{"body":"\nSLA is a marketing tool rather than insurance. In the worst-case scenario, it’s an unavoidable loss; at best, it provides emotional comfort. Read more\n","categories":"","description":"SLA is a marketing tool rather than insurance. In the worst-case scenario, it's an unavoidable loss; at best, it provides emotional comfort.\n","excerpt":"SLA is a marketing tool rather than insurance. In the worst-case …","ref":"/blog/cloud/sla/","tags":["Cloud","Cloud-Exit","SLA"],"title":"SLA: Placebo or Insurance?"},{"body":"\nPigsty v2.1 provides support for PostgreSQL 12 through 16, with PGVector for AI embeddings. Read more\n","categories":"","description":"Pigsty v2.1 provides support for PostgreSQL 12 through 16, with PGVector for AI embeddings.\n","excerpt":"Pigsty v2.1 provides support for PostgreSQL 12 through 16, with …","ref":"/blog/pigsty/v2.1/","tags":["Pigsty"],"title":"Pigsty v2.1: Vector + Full PG Version Support!"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/architecture/","tags":"","title":"Architecture"},{"body":"\nThe cost-cutting imperative has triggered a reevaluation of all technologies, including databases. This series critiques hot DB technologies and poses fundamental questions about their trade-offs: Are cloud databases, distributed databases, microservices, and containerization real needs or false hype? Read more\n","categories":"","description":"The cost-cutting imperative has triggered a reevaluation of all technologies, including databases. This series critiques hot DB technologies and poses fundamental questions about their trade-offs: Are cloud databases, distributed databases, microservices, and containerization real needs or false hype?\n","excerpt":"The cost-cutting imperative has triggered a reevaluation of all …","ref":"/blog/db/rethink/","tags":["Database","Cloud","Architecture","Tech Commentary"],"title":"Back to Basics: Tech Reflection Chronicles"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/tech-commentary/","tags":"","title":"Tech Commentary"},{"body":"\nThis article focuses on vector databases hyped by AI, introduces the basic principles of AI embeddings and vector storage/retrieval, and demonstrates the functionality, performance, acquisition, and application of the vector database extension PGVECTOR through a concrete knowledge base retrieval case study. Read more\n","categories":"","description":"This article focuses on vector databases hyped by AI, introduces the basic principles of AI embeddings and vector storage/retrieval, and demonstrates the functionality, performance, acquisition, and application of the vector database extension PGVECTOR through a concrete knowledge base retrieval case study.\n","excerpt":"This article focuses on vector databases hyped by AI, introduces the …","ref":"/blog/pg/llm-and-pgvector/","tags":["PostgreSQL","PG-Development","Extension","Vector"],"title":"AI Large Models and Vector Database PGVector"},{"body":"\nSimilar to Maslow’s hierarchy of needs, user demands for databases also have a progressive hierarchy: physiological needs, safety needs, belonging needs, esteem needs, cognitive needs, aesthetic needs, self-actualization needs, and transcendence needs. Read more\n","categories":"","description":"Similar to Maslow's hierarchy of needs, user demands for databases also have a progressive hierarchy: physiological needs, safety needs, belonging needs, esteem needs, cognitive needs, aesthetic needs, self-actualization needs, and transcendence needs.\n","excerpt":"Similar to Maslow's hierarchy of needs, user demands for databases …","ref":"/blog/db/demand-pyramid/","tags":["Database"],"title":"Database Demand Hierarchy Pyramid"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/vector/","tags":"","title":"Vector"},{"body":" Author: DHH (David Heinemeier Hansson) | Original: Even Amazon can’t make sense of serverless or microservices\nEven Amazon’s SOA paradigm team admits microservices and Serverless have problems. Prime Video team switched to monolith, saving 90% operational costs. Read more\n","categories":"","description":"Even Amazons SOA paradigm team admits microservices and Serverless have problems. Prime Video team switched to monolith, saving 90% operational costs.\n","excerpt":"Even Amazons SOA paradigm team admits microservices and Serverless …","ref":"/blog/db/microservice-bad-idea/","tags":["Architecture","Microservices","Serverless"],"title":"Are Microservices a Bad Idea?"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/microservices/","tags":"","title":"Microservices"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/newsql/","tags":"","title":"NewSQL"},{"body":"\nAs hardware technology advances, the capacity and performance of standalone databases have reached unprecedented heights. which makes distributed (TP) databases appear utterly powerless, much like the “data middle platform,” donning the emperor’s new clothes in a state of self-deception. Read more\n","categories":"","description":"As hardware technology advances, the capacity and performance of standalone databases have reached unprecedented heights. which makes distributed (TP) databases appear utterly powerless, much like the \"data middle platform,\" donning the emperor's new clothes in a state of self-deception.  \n","excerpt":"As hardware technology advances, the capacity and performance of …","ref":"/blog/db/distributive-bullshit/","tags":["Database","NewSQL"],"title":"NewSQL: Distributive Nonsens"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/serverless/","tags":"","title":"Serverless"},{"body":"\nThe real business model of cloud: “Cheap” EC2/S3 to attract customers, and fleece with “Expensive” EBS/RDS Read more\n","categories":"","description":"The real business model of cloud: **\"Cheap\" EC2/S3 to attract customers, and fleece with \"Expensive\" EBS/RDS**   \n","excerpt":"The real business model of cloud: **\"Cheap\" EC2/S3 to attract …","ref":"/blog/cloud/ebs/","tags":["Cloud","Cloud-Exit","EBS"],"title":"EBS: Pig Slaughter Scam"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/cdn/","tags":"","title":"CDN"},{"body":"\nI originally believed that at least in IaaS fundamentals — storage, compute, and networking — public cloud vendors could still make significant contributions. However, my personal experience with Tencent Cloud CDN shook that belief: domestic cloud vendors’ products and services are truly unbearable. Read more\n","categories":"","description":"I originally believed that at least in IaaS fundamentals — storage, compute, and networking — public cloud vendors could still make significant contributions. However, my personal experience with Tencent Cloud CDN shook that belief: domestic cloud vendors' products and services are truly unbearable.\n","excerpt":"I originally believed that at least in IaaS fundamentals — storage, …","ref":"/blog/cloud/cdn/","tags":["Cloud","Tencent-Cloud","CDN"],"title":"Garbage QCloud CDN: From Getting Started to Giving Up?"},{"body":"\nGuo Degang has a comedy routine: “Say I tell a rocket scientist, your rocket is no good, the fuel is wrong. I think it should burn wood, better yet coal, and it has to be premium coal, not washed coal. If that scientist takes me seriously, he loses.” Read more\n","categories":"","description":"Guo Degang has a comedy routine: \"Say I tell a rocket scientist, your rocket is no good, the fuel is wrong. I think it should burn wood, better yet coal, and it has to be premium coal, not washed coal. If that scientist takes me seriously, he loses.\"\n","excerpt":"Guo Degang has a comedy routine: \"Say I tell a rocket scientist, your …","ref":"/blog/cloud/no-dba-bullshit/","tags":["Cloud-Exit","RDS","DBA"],"title":"Refuting \"Why You Still Shouldn't Hire a DBA\""},{"body":"\nPigsty v2.0 delivers major improvements in security, compatibility, and feature integration — truly becoming a local open-source RDS alternative. Read more\n","categories":"","description":"Pigsty v2.0 delivers major improvements in security, compatibility, and feature integration — truly becoming a local open-source RDS alternative.\n","excerpt":"Pigsty v2.0 delivers major improvements in security, compatibility, …","ref":"/blog/pigsty/v2.0/","tags":["Pigsty"],"title":"Pigsty v2.0: Open-Source RDS PostgreSQL Alternative"},{"body":"\nCloud databases’ exorbitant markups—sometimes 10x or more—are undoubtedly a scam for users outside the applicable spectrum. But we can dig deeper: why are public clouds, especially cloud databases, like this? And based on their underlying logic, make predictions about the industry’s future. Read more\n","categories":"","description":"Cloud databases' exorbitant markups—sometimes 10x or more—are undoubtedly a scam for users outside the applicable spectrum. But we can dig deeper: why are public clouds, especially cloud databases, like this? And based on their underlying logic, make predictions about the industry's future.\n","excerpt":"Cloud databases' exorbitant markups—sometimes 10x or more—are …","ref":"/blog/cloud/paradigm/","tags":["Cloud-Exit","Open-Source"],"title":"Paradigm Shift: From Cloud to Local-First"},{"body":"\nWinter is coming, tech giants are laying off workers entering cost-reduction mode. Can cloud databases, the number one public cloud cash cow, still tell their story? The money you spend on cloud databases for one year is enough to buy several or even dozens of higher-performing servers. Are you paying an IQ tax by using cloud databases? Read more\n","categories":"","description":"Winter is coming, tech giants are laying off workers entering cost-reduction mode. Can cloud databases, the number one public cloud cash cow, still tell their story? The money you spend on cloud databases for one year is enough to buy several or even dozens of higher-performing servers. **Are you paying an IQ tax by using cloud databases?**\n","excerpt":"Winter is coming, tech giants are laying off workers entering …","ref":"/blog/cloud/rds/","tags":["Cloud-Exit","RDS","AWS","Alibaba-Cloud"],"title":"Are Cloud Databases an IQ Tax?"},{"body":"\nLet performance data speak: Why PostgreSQL is the world’s most advanced open-source relational database, aka the world’s most successful database. MySQL vs PostgreSQL performance showdown and distributed database reality check. Read more\n","categories":"","description":"Let performance data speak: Why PostgreSQL is the world's most advanced open-source relational database, aka the world's most successful database. MySQL vs PostgreSQL performance showdown and distributed database reality check.\n","excerpt":"Let performance data speak: Why PostgreSQL is the world's most …","ref":"/blog/pg/pg-performence/","tags":["PostgreSQL","PG-Ecosystem","Performance"],"title":"How Powerful is PostgreSQL Really?"},{"body":"\nDatabase users are developers, but what about developers’ preferences, likes, and choices? Looking at StackOverflow survey results over the past six years, it’s clear that in 2022, PostgreSQL has won all three categories, becoming literally the “most successful database” Read more\n","categories":"","description":"Database users are developers, but what about developers' preferences, likes, and choices? Looking at StackOverflow survey results over the past six years, it's clear that in 2022, PostgreSQL has won all three categories, becoming literally the \"most successful database\"\n","excerpt":"Database users are developers, but what about developers' preferences, …","ref":"/blog/pg/pg-is-best/","tags":["PostgreSQL","PG-Ecosystem"],"title":"Why PostgreSQL is the Most Successful Database?"},{"body":"\nI recently witnessed a live cloud database drop-and-run incident. This article discusses how to handle accidentally deleted data when using PostgreSQL in production environments. Read more\n","categories":"","description":"I recently witnessed a live cloud database drop-and-run incident. This article discusses how to handle accidentally deleted data when using PostgreSQL in production environments.\n","excerpt":"I recently witnessed a live cloud database drop-and-run incident. This …","ref":"/blog/cloud/drop-rds/","tags":["Cloud-Exit","DBA","RDS"],"title":"Cloud RDS: From Database Drop to Exit"},{"body":"\nAnt Financial had a self-deprecating joke: besides regulation, only DBAs could bring down Alipay. Although DBA sounds like a profession with glorious history and dim prospects, who knows if it might become trendy again after a few terrifying major cloud database incidents? Read more\n","categories":"","description":"Ant Financial had a self-deprecating joke: besides regulation, only DBAs could bring down Alipay. Although DBA sounds like a profession with glorious history and dim prospects, who knows if it might become trendy again after a few terrifying major cloud database incidents?\n","excerpt":"Ant Financial had a self-deprecating joke: besides regulation, only …","ref":"/blog/cloud/is-dba-good-job/","tags":["Cloud-Exit","RDS","DBA"],"title":"Is DBA Still a Good Job?"},{"body":"\nComplete Docker support, infrastructure self-monitoring, ETCD as DCS, better cold backup support, and CMDB improvements. Read more\n","categories":"","description":"Complete Docker support, infrastructure self-monitoring, ETCD as DCS, better cold backup support, and CMDB improvements.\n","excerpt":"Complete Docker support, infrastructure self-monitoring, ETCD as DCS, …","ref":"/blog/pigsty/v1.5/","tags":["Pigsty"],"title":"Pigsty v1.5: Docker Application Support, Infrastructure Self-Monitoring"},{"body":"\nPigsty v1.4 introduces a modular architecture with four independent modules, adds MatrixDB time-series data warehouse support, and delivers global CDN acceleration. Read more\n","categories":"","description":"Pigsty v1.4 introduces a modular architecture with four independent modules, adds MatrixDB time-series data warehouse support, and delivers global CDN acceleration.\n","excerpt":"Pigsty v1.4 introduces a modular architecture with four independent …","ref":"/blog/pigsty/v1.4/","tags":["Pigsty"],"title":"Pigsty v1.4: Modular Architecture, MatrixDB Data Warehouse Support"},{"body":"\nPigsty v1.3 adds Redis support with three deployment modes, rebuilds the PGCAT catalog explorer, and enhances PGSQL monitoring dashboards. Read more\n","categories":"","description":"Pigsty v1.3 adds Redis support with three deployment modes, rebuilds the PGCAT catalog explorer, and enhances PGSQL monitoring dashboards.\n","excerpt":"Pigsty v1.3 adds Redis support with three deployment modes, rebuilds …","ref":"/blog/pigsty/v1.3/","tags":["Pigsty"],"title":"Pigsty v1.3: Redis Support, PGCAT Overhaul, PGSQL Enhancements"},{"body":"\nPigsty v1.2 makes PostgreSQL 14 the default version and adds support for monitoring existing database instances independently. Read more\n","categories":"","description":"Pigsty v1.2 makes PostgreSQL 14 the default version and adds support for monitoring existing database instances independently.\n","excerpt":"Pigsty v1.2 makes PostgreSQL 14 the default version and adds support …","ref":"/blog/pigsty/v1.2/","tags":["Pigsty"],"title":"Pigsty v1.2: PG14 Default, Monitor Existing PG"},{"body":"\nPigsty v1.1.0 ships with a redesigned homepage, plus JupyterLab, PGWeb, Pev2 \u0026 PgBadger integrations. Read more\n","categories":"","description":"Pigsty v1.1.0 ships with a redesigned homepage, plus JupyterLab, PGWeb, Pev2 \u0026 PgBadger integrations.\n","excerpt":"Pigsty v1.1.0 ships with a redesigned homepage, plus JupyterLab, …","ref":"/blog/pigsty/v1.1/","tags":["Pigsty"],"title":"Pigsty v1.1: Homepage, Jupyter, Pev2, PgBadger"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/gpl/","tags":"","title":"GPL"},{"body":" Author: Martin Kleppmann (DDIA Author) | Original: It’s time to say goodbye to the GPL\nDDIA author Martin Kleppmann argues we should move away from GPL licenses. In the 2020s, the enemy of computing freedom is cloud software. Read more\n","categories":"","description":"DDIA author Martin Kleppmann argues we should move away from GPL licenses. In the 2020s, the enemy of computing freedom is cloud software.\n","excerpt":"DDIA author Martin Kleppmann argues we should move away from GPL …","ref":"/blog/db/goodbye-gpl/","tags":["Open-Source","License","GPL"],"title":"Time to Say Goodbye to GPL"},{"body":"\nPigsty v1.0.0 GA is here — a batteries-included, open-source PostgreSQL distribution ready for production. Read more\n","categories":"","description":"Pigsty v1.0.0 GA is here — a batteries-included, open-source PostgreSQL distribution ready for production.\n","excerpt":"Pigsty v1.0.0 GA is here — a batteries-included, open-source …","ref":"/blog/pigsty/v1.0/","tags":["Pigsty"],"title":"Pigsty v1.0: GA Release with Monitoring Overhaul"},{"body":"\nOne-click installs, a beta CLI, and Loki-based logging make Pigsty easier to land. Read more\n","categories":"","description":"One-click installs, a beta CLI, and Loki-based logging make Pigsty easier to land.\n","excerpt":"One-click installs, a beta CLI, and Loki-based logging make Pigsty …","ref":"/blog/pigsty/v0.9/","tags":["Pigsty"],"title":"Pigsty v0.9: CLI + Logs"},{"body":"\nYesterday I gave a live presentation in the PostgreSQL Chinese community, introducing the open-source PostgreSQL full-stack solution — Pigsty Read more\n","categories":"","description":"Yesterday I gave a live presentation in the PostgreSQL Chinese community, introducing the open-source PostgreSQL full-stack solution — Pigsty \n","excerpt":"Yesterday I gave a live presentation in the PostgreSQL Chinese …","ref":"/blog/pg/pigsty-intro/","tags":["PostgreSQL","Pigsty","RDS"],"title":"Ready-to-Use PostgreSQL Distribution: Pigsty"},{"body":"\nServices are now first-class objects, so you can define any routing policy—built-in HAProxy, L4 VIPs, or your own balancer. Read more\n","categories":"","description":"Services are now first-class objects, so you can define any routing policy—built-in HAProxy, L4 VIPs, or your own balancer.\n","excerpt":"Services are now first-class objects, so you can define any routing …","ref":"/blog/pigsty/v0.8/","tags":["Pigsty"],"title":"Pigsty v0.8: Service Provisioning"},{"body":"\nDatabases are the core component of information systems, relational databases are the absolute backbone of databases, and PostgreSQL is the world’s most advanced open source relational database. With such favorable timing and positioning, how can it not achieve great success? Read more\n","categories":"","description":"Databases are the core component of information systems, relational databases are the absolute backbone of databases, and PostgreSQL is the world's most advanced open source relational database. With such favorable timing and positioning, how can it not achieve great success?\n","excerpt":"Databases are the core component of information systems, relational …","ref":"/blog/pg/pg-is-great/","tags":["PostgreSQL","PG-Ecosystem"],"title":"Why Does PostgreSQL Have a Bright Future?"},{"body":"\nMonitor-only deployments unlock hybrid fleets, while DB/user provisioning APIs get a serious cleanup. Read more\n","categories":"","description":"Monitor-only deployments unlock hybrid fleets, while DB/user provisioning APIs get a serious cleanup.\n","excerpt":"Monitor-only deployments unlock hybrid fleets, while DB/user …","ref":"/blog/pigsty/v0.7/","tags":["Pigsty"],"title":"Pigsty v0.7: Monitor-Only Deployments"},{"body":"\nv0.6 reworks the provisioning flow, adds exporter toggles, and makes the monitoring stack portable across environments. Read more\n","categories":"","description":"v0.6 reworks the provisioning flow, adds exporter toggles, and makes the monitoring stack portable across environments.\n","excerpt":"v0.6 reworks the provisioning flow, adds exporter toggles, and makes …","ref":"/blog/pigsty/v0.6/","tags":["Pigsty"],"title":"Pigsty v0.6: Provisioning Upgrades"},{"body":"\nPigsty v0.5 introduces declarative database templates so roles, schemas, extensions, and ACLs can be described entirely in YAML. Read more\n","categories":"","description":"Pigsty v0.5 introduces declarative database templates so roles, schemas, extensions, and ACLs can be described entirely in YAML.\n","excerpt":"Pigsty v0.5 introduces declarative database templates so roles, …","ref":"/blog/pigsty/v0.5/","tags":["Pigsty"],"title":"Pigsty v0.5: Declarative DB Templates"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/full-text-search/","tags":"","title":"Full-Text-Search"},{"body":"\nHow to implement relatively complex fuzzy search logic in PostgreSQL? Read more\n","categories":"","description":"How to implement relatively complex fuzzy search logic in PostgreSQL?\n","excerpt":"How to implement relatively complex fuzzy search logic in PostgreSQL?\n","ref":"/blog/pg/fuzzymatch/","tags":["PostgreSQL","PG-Development","Full-Text-Search"],"title":"Implementing Advanced Fuzzy Search"},{"body":"\nWhat? Don’t know what COLLATION is? Remember one thing: using C COLLATE is always the right choice! Read more\n","categories":"","description":"What? Don't know what COLLATION is? Remember one thing: using C COLLATE is always the right choice!\n","excerpt":"What? Don't know what COLLATION is? Remember one thing: using C …","ref":"/blog/pg/collate/","tags":["PostgreSQL","PG-Admin"],"title":"Localization and Collation Rules in PostgreSQL"},{"body":"\nReplica identity is important - it determines the success or failure of logical replication Read more\n","categories":"","description":"Replica identity is important - it determines the success or failure of logical replication","excerpt":"Replica identity is important - it determines the success or failure …","ref":"/blog/pg/replica-identity/","tags":["PostgreSQL"],"title":"PG Replica Identity Explained"},{"body":"\nThis article introduces the principles and best practices of logical replication in PostgreSQL 13. Read more\n","categories":"","description":"This article introduces the principles and best practices of logical replication in PostgreSQL 13.\n","excerpt":"This article introduces the principles and best practices of logical …","ref":"/blog/pg/logical-replication/","tags":["PostgreSQL","PG-Admin"],"title":"PostgreSQL Logical Replication Deep Dive"},{"body":"\nPigsty v0.4 ships PG13 support, a Grafana 7.3 refresh, and a cleaned-up docs site for the second public beta. Read more\n","categories":"","description":"Pigsty v0.4 ships PG13 support, a Grafana 7.3 refresh, and a cleaned-up docs site for the second public beta.\n","excerpt":"Pigsty v0.4 ships PG13 support, a Grafana 7.3 refresh, and a …","ref":"/blog/pigsty/v0.4/","tags":["Pigsty"],"title":"Pigsty v0.4: PG13 and Better Docs"},{"body":"\nSlow queries are the sworn enemy of OLTP databases. Here’s how to identify, analyze, and fix them using metrics (Pigsty dashboards), pg_stat_statements, and logs. Read more\n","categories":"","description":"Slow queries are the sworn enemy of OLTP databases. Here’s how to identify, analyze, and fix them using metrics (Pigsty dashboards), pg_stat_statements, and logs.\n","excerpt":"Slow queries are the sworn enemy of OLTP databases. Here’s how to …","ref":"/blog/pg/slow-query/","tags":["PostgreSQL","PG-Admin","Performance"],"title":"A Methodology for Diagnosing PostgreSQL Slow Queries"},{"body":"\nMachine restarted due to failure, NTP service corrected PG time after PG startup, causing Patroni to fail to start. Read more\n","categories":"","description":"Machine restarted due to failure, NTP service corrected PG time after PG startup, causing Patroni to fail to start.\n","excerpt":"Machine restarted due to failure, NTP service corrected PG time after …","ref":"/blog/pg/time-travel/","tags":["PostgreSQL","PG-Admin","Incident-Report"],"title":"Incident-Report: Patroni Failure Due to Time Travel"},{"body":"\nHow to change column types online, such as upgrading from INT to BIGINT? Read more\n","categories":"","description":"How to change column types online, such as upgrading from INT to BIGINT?\n","excerpt":"How to change column types online, such as upgrading from INT to …","ref":"/blog/pg/alter-type/","tags":["PostgreSQL","PG-Admin"],"title":"Online Primary Key Column Type Change"},{"body":"\nPigsty v0.3.0, the first public beta, lands with eight battle-tested dashboards and an offline bundle. Read more\n","categories":"","description":"Pigsty v0.3.0, the first public beta, lands with eight battle-tested dashboards and an offline bundle.\n","excerpt":"Pigsty v0.3.0, the first public beta, lands with eight battle-tested …","ref":"/blog/pigsty/v0.3/","tags":["Pigsty"],"title":"Pigsty v0.3: First Public Beta"},{"body":"\nUnderstanding the golden monitoring metrics in PostgreSQL Read more\n","categories":"","description":"Understanding the golden monitoring metrics in PostgreSQL\n","excerpt":"Understanding the golden monitoring metrics in PostgreSQL\n","ref":"/blog/pg/golden-metrics/","tags":["PostgreSQL","PG-Admin","Monitoring","Metrics"],"title":"Golden Monitoring Metrics: Errors, Latency, Throughput, Saturation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/metrics/","tags":"","title":"Metrics"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/monitoring/","tags":"","title":"Monitoring"},{"body":"\nConcepts and their naming are very important. Naming style reflects an engineer’s understanding of system architecture. Poorly defined concepts lead to communication confusion, while carelessly set names create unexpected additional burden. Therefore, they need careful design. Read more\n","categories":"","description":"Concepts and their naming are very important. Naming style reflects an engineer's understanding of system architecture. Poorly defined concepts lead to communication confusion, while carelessly set names create unexpected additional burden. Therefore, they need careful design.\n","excerpt":"Concepts and their naming are very important. Naming style reflects an …","ref":"/blog/pg/entity-and-naming/","tags":["PostgreSQL","PG-Admin","Architecture"],"title":"Database Cluster Management Concepts and Entity Naming Conventions"},{"body":"\nManaging databases is similar to managing people - both need KPIs (Key Performance Indicators). So what are database KPIs? This article introduces a way to measure PostgreSQL load: using a single horizontally comparable metric that is basically independent of workload type and machine type, called PG Load. Read more\n","categories":"","description":"Managing databases is similar to managing people - both need KPIs (Key Performance Indicators). So what are database KPIs? This article introduces a way to measure PostgreSQL load: using a single horizontally comparable metric that is basically independent of workload type and machine type, called **PG Load**.","excerpt":"Managing databases is similar to managing people - both need KPIs (Key …","ref":"/blog/pg/pg-load/","tags":["PostgreSQL","PG-Admin","Monitoring","Metrics"],"title":"PostgreSQL's KPI"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/migration/","tags":"","title":"Migration"},{"body":"\nHow to modify PostgreSQL column types online? A general approach Read more\n","categories":"","description":"How to modify PostgreSQL column types online? A general approach","excerpt":"How to modify PostgreSQL column types online? A general approach","ref":"/blog/pg/migrate-column-type/","tags":["PostgreSQL","PG-Admin","Migration"],"title":"Online PostgreSQL Column Type Migration"},{"body":"\nUnderstanding the TCP protocol used for communication between PostgreSQL server and client, and printing messages using Go Read more\n","categories":"","description":"Understanding the TCP protocol used for communication between PostgreSQL server and client, and printing messages using Go\n","excerpt":"Understanding the TCP protocol used for communication between …","ref":"/blog/pg/wire-protocol/","tags":["PostgreSQL","PG-Development","PG-Kernel"],"title":"Frontend-Backend Communication Wire Protocol"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/pg-kernel/","tags":"","title":"PG-Kernel"},{"body":"\nPostgreSQL actually has only two transaction isolation levels: Read Committed and Serializable Read more\n","categories":"","description":"PostgreSQL actually has only two transaction isolation levels: **Read Committed** and **Serializable**\n","excerpt":"PostgreSQL actually has only two transaction isolation levels: **Read …","ref":"/blog/pg/isolation-level/","tags":["PostgreSQL","PG-Development"],"title":"Transaction Isolation Level Considerations"},{"body":"\nToday encountered an interesting case where a customer reported database connection issues caused by extensions. Read more\n","categories":"","description":"Today encountered an interesting case where a customer reported database connection issues caused by extensions.\n","excerpt":"Today encountered an interesting case where a customer reported …","ref":"/blog/pg/extension/","tags":["PostgreSQL","PG-Admin","Extension","Incident-Report"],"title":"Incident: PostgreSQL Extension Installation Causes Connection Failure"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/cdc/","tags":"","title":"CDC"},{"body":"\nChange Data Capture is an interesting ETL alternative solution. Read more\n","categories":"","description":"Change Data Capture is an interesting ETL alternative solution.\n","excerpt":"Change Data Capture is an interesting ETL alternative solution.\n","ref":"/blog/pg/logical-decoding/","tags":["PostgreSQL","PG-Development","CDC"],"title":"CDC Change Data Capture Mechanisms"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/lock/","tags":"","title":"Lock"},{"body":"\nSnapshot isolation does most of the heavy lifting in PG, but locks still matter. Here’s a practical guide to table locks, row locks, intention locks, and pg_locks. Read more\n","categories":"","description":"Snapshot isolation does most of the heavy lifting in PG, but locks still matter. Here’s a practical guide to table locks, row locks, intention locks, and `pg_locks`.\n","excerpt":"Snapshot isolation does most of the heavy lifting in PG, but locks …","ref":"/blog/pg/pg-lock/","tags":["PostgreSQL","PG-Development","Lock"],"title":"Locks in PostgreSQL"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/gin/","tags":"","title":"GIN"},{"body":"\nWhen GIN indexes are used to search with very long keyword lists, performance degrades significantly. This article explains why GIN index keyword search has O(n^2) time complexity. Read more\n","categories":"","description":"When GIN indexes are used to search with very long keyword lists, performance degrades significantly. This article explains why GIN index keyword search has O(n^2) time complexity.\n","excerpt":"When GIN indexes are used to search with very long keyword lists, …","ref":"/blog/pg/gin/","tags":["PostgreSQL","PG-Development","GIN"],"title":"O(n2) Complexity of GIN Search"},{"body":"\nReplication is one of the core issues in system architecture. Read more\n","categories":"","description":"Replication is one of the core issues in system architecture.\n","excerpt":"Replication is one of the core issues in system architecture.\n","ref":"/blog/pg/replication-plan/","tags":["PostgreSQL","PG-Admin","Architecture"],"title":"PostgreSQL Common Replication Topology Plans"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/backup/","tags":"","title":"Backup"},{"body":"\nThere are various backup strategies. Physical backups can usually be divided into four types. Read more\n","categories":"","description":"There are various backup strategies. Physical backups can usually be divided into four types.\n","excerpt":"There are various backup strategies. Physical backups can usually be …","ref":"/blog/pg/backup-plan/","tags":["PostgreSQL","PG-Admin","Backup"],"title":"Warm Standby: Using pg_receivewal"},{"body":"\nThou shalt not run a prod database inside a container Read more\n","categories":"","description":"Thou shalt not run a prod database inside a container","excerpt":"Thou shalt not run a prod database inside a container","ref":"/blog/db/pg-in-docker/","tags":["Database","Docker"],"title":"Is running postgres in docker a good idea?"},{"body":"\nSometimes, interactions between components manifest in subtle ways. For example, using pg_dump to export data from a connection pool can cause connection pool contamination issues. Read more\n","categories":"","description":"Sometimes, interactions between components manifest in subtle ways. For example, using pg_dump to export data from a connection pool can cause connection pool contamination issues.","excerpt":"Sometimes, interactions between components manifest in subtle ways. …","ref":"/blog/pg/pg-dump-failure/","tags":["PostgreSQL","PG-Admin","Incident-Report"],"title":"Incident-Report: Connection-Pool Contamination Caused by pg_dump"},{"body":"\nA proper understanding of time is very helpful for correctly handling time-related issues in work and life. For example, time representation and processing in computers, as well as time handling in databases and programming languages. Read more\n","categories":"","description":"A proper understanding of time is very helpful for correctly handling time-related issues in work and life. For example, time representation and processing in computers, as well as time handling in databases and programming languages.\n","excerpt":"A proper understanding of time is very helpful for correctly handling …","ref":"/blog/db/reason-about-time/","tags":["Database"],"title":"Understanding Time - Leap Years, Leap Seconds, Time and Time Zones"},{"body":"\nUsing binary editing to repair PostgreSQL data pages, and how to make a primary key query return two records. Read more\n","categories":"","description":"Using binary editing to repair PostgreSQL data pages, and how to make a primary key query return two records.","excerpt":"Using binary editing to repair PostgreSQL data pages, and how to make …","ref":"/blog/pg/page-corruption/","tags":["PostgreSQL","PG-Admin","Data-Corruption","Incident-Report"],"title":"PostgreSQL Data Page Corruption Repair"},{"body":"\nPostgreSQL uses MVCC as its primary concurrency control technology. While it has many benefits, it also brings other effects, such as relation bloat. Read more\n","categories":"","description":"PostgreSQL uses MVCC as its primary concurrency control technology. While it has many benefits, it also brings other effects, such as relation bloat.\n","excerpt":"PostgreSQL uses MVCC as its primary concurrency control technology. …","ref":"/blog/pg/bloat/","tags":["PostgreSQL","PG-Admin"],"title":"Relation Bloat Monitoring and Management"},{"body":"\nPipelineDB is a PostgreSQL extension for streaming analytics. Here’s how to install it and build continuous views over live data. Read more\n","categories":"","description":"PipelineDB is a PostgreSQL extension for streaming analytics. Here’s how to install it and build continuous views over live data.\n","excerpt":"PipelineDB is a PostgreSQL extension for streaming analytics. Here’s …","ref":"/blog/pg/pipeline-intro/","tags":["PostgreSQL","PG-Admin","Extension"],"title":"Getting Started with PipelineDB"},{"body":"\nTimescaleDB is a PostgreSQL extension plugin that provides time-series database functionality. Read more\n","categories":"","description":"TimescaleDB is a PostgreSQL extension plugin that provides time-series database functionality.\n","excerpt":"TimescaleDB is a PostgreSQL extension plugin that provides time-series …","ref":"/blog/pg/timescale-install/","tags":["PostgreSQL","PG-Admin","Extension"],"title":"TimescaleDB Quick Start"},{"body":"\nIf you use Integer sequences on tables, you should consider potential overflow scenarios. Read more\n","categories":"","description":"If you use Integer sequences on tables, you should consider potential overflow scenarios.\n","excerpt":"If you use Integer sequences on tables, you should consider potential …","ref":"/blog/pg/sequence-overflow/","tags":["PostgreSQL","PG-Admin","Incident-Report"],"title":"Incident-Report: Integer Overflow from Rapid Sequence Number Consumption"},{"body":"\nXID WrapAround is perhaps a unique type of failure specific to PostgreSQL Read more\n","categories":"","description":"XID WrapAround is perhaps a unique type of failure specific to PostgreSQL\n","excerpt":"XID WrapAround is perhaps a unique type of failure specific to …","ref":"/blog/pg/xid-wrap-around/","tags":["PostgreSQL","PG-Admin","Incident-Report"],"title":"Incident-Report: PostgreSQL Transaction ID Wraparound"},{"body":"\nA common requirement in application development is GeoIP conversion - converting source IP addresses to geographic coordinates or administrative divisions (country-state-city-county-town-village) Read more\n","categories":"","description":"A common requirement in application development is GeoIP conversion - converting source IP addresses to geographic coordinates or administrative divisions (country-state-city-county-town-village)\n","excerpt":"A common requirement in application development is GeoIP conversion - …","ref":"/blog/pg/geoip/","tags":["PostgreSQL","PG-Development","Extension","GIS"],"title":"GeoIP Geographic Reverse Lookup Optimization"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/gis/","tags":"","title":"GIS"},{"body":"\nDetailed understanding of trigger management and usage in PostgreSQL Read more\n","categories":"","description":"Detailed understanding of trigger management and usage in PostgreSQL\n","excerpt":"Detailed understanding of trigger management and usage in PostgreSQL\n","ref":"/blog/pg/sql-trigger/","tags":["PostgreSQL","PG-Development","Triggers"],"title":"PostgreSQL Trigger Usage Considerations"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/triggers/","tags":"","title":"Triggers"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/encoding/","tags":"","title":"Encoding"},{"body":"\nWithout understanding the basic principles of character encoding, even simple string operations like comparison, sorting, and random access can easily lead you into pitfalls. This article attempts to clarify these issues through a comprehensive explanation. Read more\n","categories":"","description":"Without understanding the basic principles of character encoding, even simple string operations like comparison, sorting, and random access can easily lead you into pitfalls. This article attempts to clarify these issues through a comprehensive explanation.\n","excerpt":"Without understanding the basic principles of character encoding, even …","ref":"/blog/db/character-encoding/","tags":["Database","Encoding"],"title":"Understanding Character Encoding Principles"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/convention/","tags":"","title":"Convention"},{"body":"\nWithout rules, there can be no order. This article compiles a development specification for PostgreSQL database principles and features, which can reduce confusion encountered when using PostgreSQL. Read more\n","categories":"","description":"Without rules, there can be no order. This article compiles a development specification for PostgreSQL database principles and features, which can reduce confusion encountered when using PostgreSQL.","excerpt":"Without rules, there can be no order. This article compiles a …","ref":"/blog/pg/pg-convention-2018/","tags":["PostgreSQL","PG-Development","Convention"],"title":"PostgreSQL Development Convention (2018 Edition)"},{"body":"\nConcurrent programs are hard to write correctly and even harder to write well. Many programmers simply throw these problems at the database… But even the most sophisticated databases won’t help if you don’t understand concurrency anomalies and isolation levels. Read more\n","categories":"","description":"Concurrent programs are hard to write correctly and even harder to write well. Many programmers simply throw these problems at the database... But even the most sophisticated databases won't help if you don't understand concurrency anomalies and isolation levels.\n","excerpt":"Concurrent programs are hard to write correctly and even harder to …","ref":"/blog/db/concurrent-control/","tags":["Database"],"title":"Concurrency Anomalies Explained"},{"body":"\nPostgreSQL’s slogan is “The World’s Most Advanced Open-Source Relational Database,” but I think the most vivid characterization should be: The Full-Stack Database That Does It All - one tool to rule them all. Read more\n","categories":"","description":"PostgreSQL's slogan is \"The World's Most Advanced Open-Source Relational Database,\" but I think the most vivid characterization should be: The Full-Stack Database That Does It All - one tool to rule them all.\n","excerpt":"PostgreSQL's slogan is \"The World's Most Advanced Open-Source …","ref":"/blog/pg/pg-is-good/","tags":["PostgreSQL","PG-Ecosystem"],"title":"What Are PostgreSQL's Advantages?"},{"body":"\nThe technical essence, functionality, and evolution of blockchain is distributed databases. Specifically, it’s a Byzantine Fault Tolerant (resistant to malicious node attacks) distributed (leaderless replication) database. Read more\n","categories":"","description":"The technical essence, functionality, and evolution of blockchain is distributed databases. Specifically, it's a **Byzantine Fault Tolerant (resistant to malicious node attacks) distributed (leaderless replication) database**.\n","excerpt":"The technical essence, functionality, and evolution of blockchain is …","ref":"/blog/db/blockchian/","tags":["Database","Distributed"],"title":"Blockchain and Distributed Databases"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/distributed/","tags":"","title":"Distributed"},{"body":"\nHow to efficiently solve the typical reverse geocoding problem: determining administrative regions based on user coordinates. Read more\n","categories":"","description":"How to efficiently solve the typical reverse geocoding problem: determining administrative regions based on user coordinates.\n","excerpt":"How to efficiently solve the typical reverse geocoding problem: …","ref":"/blog/pg/adcode-geodecode/","tags":["PostgreSQL","PG-Development","GIS"],"title":"Efficient Administrative Region Lookup with PostGIS"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/knn/","tags":"","title":"KNN"},{"body":"\nUltimate optimization of KNN problems, from traditional relational design to PostGIS Read more\n","categories":"","description":"Ultimate optimization of KNN problems, from traditional relational design to PostGIS\n","excerpt":"Ultimate optimization of KNN problems, from traditional relational …","ref":"/blog/pg/knn-optimize/","tags":["PostgreSQL","PG-Development","KNN","GIS"],"title":"KNN Ultimate Optimization: From RDS to PostGIS"},{"body":"\nTables in PostgreSQL correspond to many physical files. This article explains how to calculate the actual size of a table in PostgreSQL. Read more\n","categories":"","description":"Tables in PostgreSQL correspond to many physical files. This article explains how to calculate the actual size of a table in PostgreSQL.","excerpt":"Tables in PostgreSQL correspond to many physical files. This article …","ref":"/blog/pg/mon-table-size/","tags":["PostgreSQL","PG-Admin","Monitoring"],"title":"Monitoring Table Size in PostgreSQL"},{"body":"\nThe term “consistency” is heavily overloaded, representing different concepts in different contexts. For example, the C in ACID and the C in CAP actually refer to different concepts. Read more\n","categories":"","description":"The term \"consistency\" is heavily overloaded, representing different concepts in different contexts. For example, the C in ACID and the C in CAP actually refer to different concepts.\n","excerpt":"The term \"consistency\" is heavily overloaded, representing different …","ref":"/blog/db/consistency/","tags":["Database"],"title":"Consistency: An Overloaded Term"},{"body":"\nThose who only know how to code are just programmers; learn databases well, and you can at least make a living; but for excellent engineers, merely using databases is far from enough. Read more\n","categories":"","description":"Those who only know how to code are just programmers; **learn databases well, and you can at least make a living**; but for **excellent** engineers, merely **using** databases is far from enough.\n","excerpt":"Those who only know how to code are just programmers; **learn …","ref":"/blog/db/why-learn-database/","tags":["Database"],"title":"Why Study Database Principles"},{"body":"\nPgAdmin is a GUI program for managing PostgreSQL, written in Python, but it’s quite dated and requires some additional configuration. Read more\n","categories":"","description":"PgAdmin is a GUI program for managing PostgreSQL, written in Python, but it's quite dated and requires some additional configuration.","excerpt":"PgAdmin is a GUI program for managing PostgreSQL, written in Python, …","ref":"/blog/pg/pgadmin-install/","tags":["PostgreSQL","PG-Admin","Tools"],"title":"PgAdmin Installation and Configuration"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/tools/","tags":"","title":"Tools"},{"body":"\nRecently there was a perplexing incident where a database had half its data volume and load migrated away, but ended up being overwhelmed due to increased load. Read more\n","categories":"","description":"Recently there was a perplexing incident where a database had half its data volume and load migrated away, but ended up being overwhelmed due to increased load.\n","excerpt":"Recently there was a perplexing incident where a database had half its …","ref":"/blog/pg/download-failure/","tags":["PostgreSQL","PG-Admin","Incident-Report"],"title":"Incident-Report: Uneven Load Avalanche"},{"body":"\nSome tips for interacting between PostgreSQL and Bash. Read more\n","categories":"","description":"Some tips for interacting between PostgreSQL and Bash.","excerpt":"Some tips for interacting between PostgreSQL and Bash.","ref":"/blog/pg/psql-and-bash/","tags":["PostgreSQL","PG-Admin","Tools"],"title":"Bash and psql Tips"},{"body":"\nUse Distinct On extension clause to quickly find records with maximum/minimum values within groups Read more\n","categories":"","description":"Use Distinct On extension clause to quickly find records with maximum/minimum values within groups\n","excerpt":"Use Distinct On extension clause to quickly find records with …","ref":"/blog/pg/sql-distinct-on/","tags":["PostgreSQL","PG-Development","SQL"],"title":"Distinct On: Remove Duplicate Data"},{"body":"\nPostgreSQL functions have three volatility levels by default. Proper use can significantly improve performance. Read more\n","categories":"","description":"PostgreSQL functions have three volatility levels by default. Proper use can significantly improve performance.\n","excerpt":"PostgreSQL functions have three volatility levels by default. Proper …","ref":"/blog/pg/sql-func-volatility/","tags":["PostgreSQL","PG-Development","Functions"],"title":"Function Volatility Classification Levels"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/functions/","tags":"","title":"Functions"},{"body":"\nExclude constraint is a PostgreSQL extension that can implement more advanced and sophisticated database constraints. Read more\n","categories":"","description":"Exclude constraint is a PostgreSQL extension that can implement more advanced and sophisticated database constraints.\n","excerpt":"Exclude constraint is a PostgreSQL extension that can implement more …","ref":"/blog/pg/sql-exclude/","tags":["PostgreSQL","PG-Development","SQL"],"title":"Implementing Mutual Exclusion Constraints with Exclude"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/sql/","tags":"","title":"SQL"},{"body":"\nCars need oil changes, databases need maintenance. For PG, three important maintenance tasks: backup, repack, vacuum Read more\n","categories":"","description":"Cars need oil changes, databases need maintenance. For PG, three important maintenance tasks: backup, repack, vacuum\n","excerpt":"Cars need oil changes, databases need maintenance. For PG, three …","ref":"/blog/pg/routine-maintain/","tags":["PostgreSQL","PG-Admin"],"title":"PostgreSQL Routine Maintenance"},{"body":"\nBackup is the foundation of a DBA’s livelihood. With backups, there’s no need to panic. Read more\n","categories":"","description":"Backup is the foundation of a DBA's livelihood. With backups, there's no need to panic.\n","excerpt":"Backup is the foundation of a DBA's livelihood. With backups, there's …","ref":"/blog/pg/backup-overview/","tags":["PostgreSQL","PG-Admin","Backup"],"title":"Backup and Recovery Methods Overview"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/connection-pool/","tags":"","title":"Connection-Pool"},{"body":"\nPgBackRest is a set of PostgreSQL backup tools written in Perl Read more\n","categories":"","description":"PgBackRest is a set of PostgreSQL backup tools written in Perl","excerpt":"PgBackRest is a set of PostgreSQL backup tools written in Perl","ref":"/blog/pg/pgbackrest/","tags":["PostgreSQL","PG-Admin","Backup"],"title":"PgBackRest2 Documentation"},{"body":"\nPgbouncer is a lightweight database connection pool. This guide covers basic Pgbouncer configuration, management, and usage. Read more\n","categories":"","description":"Pgbouncer is a lightweight database connection pool. This guide covers basic Pgbouncer configuration, management, and usage.","excerpt":"Pgbouncer is a lightweight database connection pool. This guide covers …","ref":"/blog/pg/pgbouncer-usage/","tags":["PostgreSQL","PG-Admin","Connection-Pool"],"title":"Pgbouncer Quick Start"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/logging/","tags":"","title":"Logging"},{"body":"\nIt’s recommended to configure PostgreSQL’s log format as CSV for easy analysis, and it can be directly imported into PostgreSQL data tables. Read more\n","categories":"","description":"It's recommended to configure PostgreSQL's log format as CSV for easy analysis, and it can be directly imported into PostgreSQL data tables.\n","excerpt":"It's recommended to configure PostgreSQL's log format as CSV for easy …","ref":"/blog/pg/logging/","tags":["PostgreSQL","PG-Admin","Logging"],"title":"PostgreSQL Server Log Regular Configuration"},{"body":"\nFIO is a convenient tool for testing disk I/O performance Read more\n","categories":"","description":"FIO is a convenient tool for testing disk I/O performance\n","excerpt":"FIO is a convenient tool for testing disk I/O performance\n","ref":"/blog/pg/fio/","tags":["PostgreSQL","PG-Admin","Performance"],"title":"Testing Disk Performance with FIO"},{"body":"\nAlthough PostgreSQL provides pgbench, sometimes you need sysbench to outperform MySQL. Read more\n","categories":"","description":"Although PostgreSQL provides pgbench, sometimes you need sysbench to outperform MySQL.\n","excerpt":"Although PostgreSQL provides pgbench, sometimes you need sysbench to …","ref":"/blog/pg/sysbench/","tags":["PostgreSQL","PG-Admin","Performance"],"title":"Using sysbench to Test PostgreSQL Performance"},{"body":"\nData migration typically involves stopping services for updates. Zero-downtime data migration is a relatively advanced operation. Read more\n","categories":"","description":"Data migration typically involves stopping services for updates. Zero-downtime data migration is a relatively advanced operation.","excerpt":"Data migration typically involves stopping services for updates. …","ref":"/blog/pg/migration-without-downtime/","tags":["PostgreSQL","PG-Admin","Migration"],"title":"Changing Engines Mid-Flight — PostgreSQL Zero-Downtime Data Migration"},{"body":"\nIndexes are useful, but they’re not free. Unused indexes are a waste. Use these methods to identify unused indexes. Read more\n","categories":"","description":"Indexes are useful, but they're not free. Unused indexes are a waste. Use these methods to identify unused indexes.\n","excerpt":"Indexes are useful, but they're not free. Unused indexes are a waste. …","ref":"/blog/pg/find-dummy-index/","tags":["PostgreSQL","PG-Admin"],"title":"Finding Unused Indexes"},{"body":"\nQuick configuration for passwordless login to all machines Read more\n","categories":"","description":"Quick configuration for passwordless login to all machines\n","excerpt":"Quick configuration for passwordless login to all machines\n","ref":"/blog/pg/ssh-add-key/","tags":["PostgreSQL","PG-Admin"],"title":"Batch Configure SSH Passwordless Login"},{"body":"\nWireshark is a very useful tool, especially suitable for analyzing network protocols. Here’s a simple introduction to using Wireshark for packet capture and PostgreSQL protocol analysis. Read more\n","categories":"","description":"Wireshark is a very useful tool, especially suitable for analyzing network protocols. Here's a simple introduction to using Wireshark for packet capture and PostgreSQL protocol analysis.\n","excerpt":"Wireshark is a very useful tool, especially suitable for analyzing …","ref":"/blog/pg/wireshark-capture/","tags":["PostgreSQL","PG-Admin","Tools"],"title":"Wireshark Packet Capture Protocol Analysis"},{"body":"\nWith file_fdw, you can easily view operating system information, fetch network data, and feed various data sources into your database for unified viewing and management. Read more\n","categories":"","description":"With `file_fdw`, you can easily view operating system information, fetch network data, and feed various data sources into your database for unified viewing and management.\n","excerpt":"With `file_fdw`, you can easily view operating system information, …","ref":"/blog/pg/file_fdw/","tags":["PostgreSQL","PG-Admin","Extension"],"title":"The Versatile file_fdw — Reading System Information from Your Database"},{"body":"\ntop, free, vmstat, iostat: Quick reference for four commonly used CLI tools Read more\n","categories":"","description":"top, free, vmstat, iostat: Quick reference for four commonly used CLI tools\n","excerpt":"top, free, vmstat, iostat: Quick reference for four commonly used CLI …","ref":"/blog/pg/unix-tool/","tags":["PostgreSQL","PG-Admin","Tools"],"title":"Common Linux Statistics CLI Tools"},{"body":"\nPostGIS is PostgreSQL’s killer extension, but compiling and installing it isn’t easy. Read more\n","categories":"","description":"PostGIS is PostgreSQL's killer extension, but compiling and installing it isn't easy.","excerpt":"PostGIS is PostgreSQL's killer extension, but compiling and installing …","ref":"/blog/pg/postgis-install/","tags":["PostgreSQL","PG-Admin","Extension"],"title":"Installing PostGIS from Source"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/go/","tags":"","title":"Go"},{"body":"\nSimilar to JDBC, Go also has a standard database access interface. This article details how to use database/sql in Go and important considerations. Read more\n","categories":"","description":"Similar to JDBC, Go also has a standard database access interface. This article details how to use database/sql in Go and important considerations.","excerpt":"Similar to JDBC, Go also has a standard database access interface. …","ref":"/blog/pg/pg-go-driver/","tags":["PostgreSQL","Go"],"title":"Go Database Tutorial: database/sql"},{"body":"\nCleverly utilizing PostgreSQL’s Notify feature, you can conveniently notify applications of metadata changes and implement trigger-based logical replication. Read more\n","categories":"","description":"Cleverly utilizing PostgreSQL's Notify feature, you can conveniently notify applications of metadata changes and implement trigger-based logical replication.","excerpt":"Cleverly utilizing PostgreSQL's Notify feature, you can conveniently …","ref":"/blog/pg/notify-trigger-based-repl/","tags":["PostgreSQL","PG-Development","Triggers"],"title":"Implementing Cache Synchronization with Go and PostgreSQL"},{"body":"\nSometimes we want to record important metadata changes for audit purposes. PostgreSQL triggers can conveniently solve this need automatically. Read more\n","categories":"","description":"Sometimes we want to record important metadata changes for audit purposes. PostgreSQL triggers can conveniently solve this need automatically.\n","excerpt":"Sometimes we want to record important metadata changes for audit …","ref":"/blog/pg/audit-change/","tags":["PostgreSQL","PG-Development","Triggers"],"title":"Auditing Data Changes with Triggers"},{"body":"\nFive minutes, PostgreSQL, and the MovieLens dataset—that’s all you need to implement a classic item-based collaborative filtering recommender. Read more\n","categories":"","description":"Five minutes, PostgreSQL, and the MovieLens dataset—that’s all you need to implement a classic item-based collaborative filtering recommender.\n","excerpt":"Five minutes, PostgreSQL, and the MovieLens dataset—that’s all you …","ref":"/blog/pg/pg-recsys/","tags":["PostgreSQL","PG-Development","Recommendation System"],"title":"Building an ItemCF Recommender in Pure SQL"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/recommendation-system/","tags":"","title":"Recommendation System"},{"body":"\nUUID properties, principles and applications, and how to manipulate UUIDs using PostgreSQL stored procedures. Read more\n","categories":"","description":"UUID properties, principles and applications, and how to manipulate UUIDs using PostgreSQL stored procedures.\n","excerpt":"UUID properties, principles and applications, and how to manipulate …","ref":"/blog/pg/uuid/","tags":["PostgreSQL","PG-Development","Architecture"],"title":"UUID Properties, Principles and Applications"},{"body":"\nRecently had business requirements to access MongoDB through PostgreSQL FDW, but compiling MongoDB FDW is really a nightmare. Read more\n","categories":"","description":"Recently had business requirements to access MongoDB through PostgreSQL FDW, but compiling MongoDB FDW is really a nightmare.","excerpt":"Recently had business requirements to access MongoDB through …","ref":"/blog/pg/mongo_fdw-install/","tags":["PostgreSQL","PG-Admin","Extension"],"title":"PostgreSQL MongoFDW Installation and Deployment"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/admin/","tags":"","title":"Admin"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/application/","tags":"","title":"Application"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/auth/","tags":"","title":"Auth"},{"body":"","categories":"","description":"","excerpt":"","ref":"/module/blog/","tags":"","title":"BLOG"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/citus/","tags":"","title":"Citus"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/cloudberry/","tags":"","title":"Cloudberry"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/cmdb/","tags":"","title":"CMDB"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/configuration/","tags":"","title":"Configuration"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/configuration-management/","tags":"","title":"Configuration Management"},{"body":"","categories":"","description":"","excerpt":"","ref":"/module/consul/","tags":"","title":"CONSUL"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/cost/","tags":"","title":"Cost"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/database/","tags":"","title":"Database"},{"body":"","categories":"","description":"","excerpt":"","ref":"/module/docker/","tags":"","title":"DOCKER"},{"body":"","categories":"","description":"","excerpt":"","ref":"/module/duckdb/","tags":"","title":"DUCKDB"},{"body":"","categories":"","description":"","excerpt":"","ref":"/module/etcd/","tags":"","title":"ETCD"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/faq/","tags":"","title":"FAQ"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/greenplum/","tags":"","title":"Greenplum"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/hba/","tags":"","title":"HBA"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/iac/","tags":"","title":"IaC"},{"body":"","categories":"","description":"","excerpt":"","ref":"/module/infra/","tags":"","title":"INFRA"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/installation/","tags":"","title":"Installation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/introduction/","tags":"","title":"Introduction"},{"body":"","categories":"","description":"","excerpt":"","ref":"/module/jupyter/","tags":"","title":"JUPYTER"},{"body":"","categories":"","description":"","excerpt":"","ref":"/module/kafka/","tags":"","title":"KAFKA"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kernel/","tags":"","title":"Kernel"},{"body":"","categories":"","description":"","excerpt":"","ref":"/module/kube/","tags":"","title":"KUBE"},{"body":"","categories":"","description":"","excerpt":"","ref":"/module/minio/","tags":"","title":"MINIO"},{"body":"","categories":"","description":"","excerpt":"","ref":"/module/mysql/","tags":"","title":"MYSQL"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/neon/","tags":"","title":"Neon"},{"body":"","categories":"","description":"","excerpt":"","ref":"/module/pig/","tags":"","title":"PIG"},{"body":"","categories":"","description":"","excerpt":"","ref":"/module/pigsty/","tags":"","title":"PIGSTY"},{"body":"","categories":"","description":"Pigsty, the FOSS PostgreSQL RDS, self-host PG like a pro, with Monitoring, HA, PITR, IAC, 9 kernel and 440 PG extensions","excerpt":"Pigsty, the FOSS PostgreSQL RDS, self-host PG like a pro, with …","ref":"/","tags":"","title":"PIGSTY - The FLOSS PG RDS"},{"body":"","categories":"","description":"","excerpt":"","ref":"/module/pilot/","tags":"","title":"PILOT"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/pitr/","tags":"","title":"PITR"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/polardb/","tags":"","title":"PolarDB"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/positioning/","tags":"","title":"Positioning"},{"body":"","categories":"","description":"Pigsty offers flexible service plans, from free open source to enterprise support, tailored to your needs","excerpt":"Pigsty offers flexible service plans, from free open source to …","ref":"/price/","tags":"","title":"Pricing"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/recovery/","tags":"","title":"Recovery"},{"body":"","categories":"","description":"","excerpt":"","ref":"/module/redis/","tags":"","title":"REDIS"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/security/","tags":"","title":"Security"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/service/","tags":"","title":"Service"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/sop/","tags":"","title":"SOP"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/sponsor/","tags":"","title":"Sponsor"},{"body":"","categories":"","description":"","excerpt":"","ref":"/module/tigerbeetle/","tags":"","title":"TIGERBEETLE"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/user/","tags":"","title":"User"},{"body":"","categories":"","description":"","excerpt":"","ref":"/module/victoria/","tags":"","title":"VICTORIA"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/wizard/","tags":"","title":"Wizard"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"搜索"}]